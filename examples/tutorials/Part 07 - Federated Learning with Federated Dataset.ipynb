{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 - Federated Learning with FederatedDataset\n",
    "\n",
    "Here we introduce a new tool for using federated datasets. We have created a `FederatedDataset` class which is intended to be used like the PyTorch Dataset class, and is given to a federated data loader `FederatedDataLoader` which will iterate on it in a federated fashion.\n",
    "\n",
    "\n",
    "Authors:\n",
    "- Andrew Trask - Twitter: [@iamtrask](https://twitter.com/iamtrask)\n",
    "- Th√©o Ryffel - GitHub: [@LaRiffle](https://github.com/LaRiffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the sandbox that we discovered last lesson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import syft as sy\n",
    "sy.create_sandbox(globals(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then search for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data = grid.search(\"#boston\", \"#data\", verbose=False, return_counter=False)\n",
    "boston_target = grid.search(\"#boston\", \"#target\", verbose=False, return_counter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load a model and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = boston_data['alice'][0].shape[1]\n",
    "n_targets = 1\n",
    "\n",
    "model = th.nn.Linear(n_features, n_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we cast the data fetched in a `FederatedDataset`. See the workers which hold part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bob', 'theo', 'jason', 'alice', 'andy', 'jon']\n"
     ]
    }
   ],
   "source": [
    "# Cast the result in BaseDatasets\n",
    "datasets = []\n",
    "for worker in boston_data.keys():\n",
    "    dataset = sy.BaseDataset(boston_data[worker][0], boston_target[worker][0])\n",
    "    datasets.append(dataset)\n",
    "\n",
    "# Build the FederatedDataset object\n",
    "dataset = sy.FederatedDataset(datasets)\n",
    "print(dataset.workers)\n",
    "optimizers = {}\n",
    "for worker in dataset.workers:\n",
    "    optimizers[worker] = th.optim.Adam(params=model.parameters(),lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We put it in a `FederatedDataLoader` and specify options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = sy.FederatedDataLoader(dataset, batch_size=32, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we iterate over epochs. You can see how similar this is compared to pure and local PyTorch training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/16 (0%)]\tBatch loss: 3156.205322\n",
      "Train Epoch: 1 [8/16 (50%)]\tBatch loss: 59.617584\n",
      "Train Epoch: 1 [16/16 (100%)]\tBatch loss: 879.110596\n",
      "Total loss 25813.378814697266\n",
      "Train Epoch: 2 [0/16 (0%)]\tBatch loss: 1049.295288\n",
      "Train Epoch: 2 [8/16 (50%)]\tBatch loss: 142.419449\n",
      "Train Epoch: 2 [16/16 (100%)]\tBatch loss: 385.578247\n",
      "Total loss 12938.65673828125\n",
      "Train Epoch: 3 [0/16 (0%)]\tBatch loss: 1212.881592\n",
      "Train Epoch: 3 [8/16 (50%)]\tBatch loss: 61.300438\n",
      "Train Epoch: 3 [16/16 (100%)]\tBatch loss: 174.486832\n",
      "Total loss 9758.002326965332\n",
      "Train Epoch: 4 [0/16 (0%)]\tBatch loss: 1113.664429\n",
      "Train Epoch: 4 [8/16 (50%)]\tBatch loss: 72.734505\n",
      "Train Epoch: 4 [16/16 (100%)]\tBatch loss: 156.222260\n",
      "Total loss 8611.730628967285\n",
      "Train Epoch: 5 [0/16 (0%)]\tBatch loss: 704.376953\n",
      "Train Epoch: 5 [8/16 (50%)]\tBatch loss: 108.787155\n",
      "Train Epoch: 5 [16/16 (100%)]\tBatch loss: 59.056713\n",
      "Total loss 5583.505939483643\n",
      "Train Epoch: 6 [0/16 (0%)]\tBatch loss: 588.390381\n",
      "Train Epoch: 6 [8/16 (50%)]\tBatch loss: 88.086395\n",
      "Train Epoch: 6 [16/16 (100%)]\tBatch loss: 50.938488\n",
      "Total loss 3429.2552757263184\n",
      "Train Epoch: 7 [0/16 (0%)]\tBatch loss: 493.895111\n",
      "Train Epoch: 7 [8/16 (50%)]\tBatch loss: 85.962540\n",
      "Train Epoch: 7 [16/16 (100%)]\tBatch loss: 62.055855\n",
      "Total loss 2460.794744491577\n",
      "Train Epoch: 8 [0/16 (0%)]\tBatch loss: 275.886078\n",
      "Train Epoch: 8 [8/16 (50%)]\tBatch loss: 96.523705\n",
      "Train Epoch: 8 [16/16 (100%)]\tBatch loss: 71.164154\n",
      "Total loss 1674.9585437774658\n",
      "Train Epoch: 9 [0/16 (0%)]\tBatch loss: 124.773972\n",
      "Train Epoch: 9 [8/16 (50%)]\tBatch loss: 82.355904\n",
      "Train Epoch: 9 [16/16 (100%)]\tBatch loss: 106.273682\n",
      "Total loss 1359.2087535858154\n",
      "Train Epoch: 10 [0/16 (0%)]\tBatch loss: 53.476452\n",
      "Train Epoch: 10 [8/16 (50%)]\tBatch loss: 65.665939\n",
      "Train Epoch: 10 [16/16 (100%)]\tBatch loss: 105.537766\n",
      "Total loss 1321.780421257019\n",
      "Train Epoch: 11 [0/16 (0%)]\tBatch loss: 23.041601\n",
      "Train Epoch: 11 [8/16 (50%)]\tBatch loss: 58.526306\n",
      "Train Epoch: 11 [16/16 (100%)]\tBatch loss: 82.434570\n",
      "Total loss 1344.7198657989502\n",
      "Train Epoch: 12 [0/16 (0%)]\tBatch loss: 33.982090\n",
      "Train Epoch: 12 [8/16 (50%)]\tBatch loss: 51.923664\n",
      "Train Epoch: 12 [16/16 (100%)]\tBatch loss: 71.666611\n",
      "Total loss 1420.0852355957031\n",
      "Train Epoch: 13 [0/16 (0%)]\tBatch loss: 53.072071\n",
      "Train Epoch: 13 [8/16 (50%)]\tBatch loss: 46.088745\n",
      "Train Epoch: 13 [16/16 (100%)]\tBatch loss: 62.153725\n",
      "Total loss 1414.037546157837\n",
      "Train Epoch: 14 [0/16 (0%)]\tBatch loss: 61.432991\n",
      "Train Epoch: 14 [8/16 (50%)]\tBatch loss: 44.375298\n",
      "Train Epoch: 14 [16/16 (100%)]\tBatch loss: 51.755058\n",
      "Total loss 1325.2543201446533\n",
      "Train Epoch: 15 [0/16 (0%)]\tBatch loss: 57.004494\n",
      "Train Epoch: 15 [8/16 (50%)]\tBatch loss: 45.408813\n",
      "Train Epoch: 15 [16/16 (100%)]\tBatch loss: 42.930199\n",
      "Total loss 1207.832085609436\n",
      "Train Epoch: 16 [0/16 (0%)]\tBatch loss: 41.909943\n",
      "Train Epoch: 16 [8/16 (50%)]\tBatch loss: 47.311516\n",
      "Train Epoch: 16 [16/16 (100%)]\tBatch loss: 34.088875\n",
      "Total loss 1097.0017757415771\n",
      "Train Epoch: 17 [0/16 (0%)]\tBatch loss: 28.952778\n",
      "Train Epoch: 17 [8/16 (50%)]\tBatch loss: 50.566902\n",
      "Train Epoch: 17 [16/16 (100%)]\tBatch loss: 26.027718\n",
      "Total loss 1030.4123096466064\n",
      "Train Epoch: 18 [0/16 (0%)]\tBatch loss: 25.858299\n",
      "Train Epoch: 18 [8/16 (50%)]\tBatch loss: 53.949982\n",
      "Train Epoch: 18 [16/16 (100%)]\tBatch loss: 19.568899\n",
      "Total loss 1007.2871789932251\n",
      "Train Epoch: 19 [0/16 (0%)]\tBatch loss: 32.356831\n",
      "Train Epoch: 19 [8/16 (50%)]\tBatch loss: 55.524818\n",
      "Train Epoch: 19 [16/16 (100%)]\tBatch loss: 15.204272\n",
      "Total loss 1010.2519159317017\n",
      "Train Epoch: 20 [0/16 (0%)]\tBatch loss: 44.068043\n",
      "Train Epoch: 20 [8/16 (50%)]\tBatch loss: 55.615528\n",
      "Train Epoch: 20 [16/16 (100%)]\tBatch loss: 12.976721\n",
      "Total loss 1019.6085443496704\n",
      "Train Epoch: 21 [0/16 (0%)]\tBatch loss: 54.451321\n",
      "Train Epoch: 21 [8/16 (50%)]\tBatch loss: 54.670746\n",
      "Train Epoch: 21 [16/16 (100%)]\tBatch loss: 12.432034\n",
      "Total loss 1015.7627086639404\n",
      "Train Epoch: 22 [0/16 (0%)]\tBatch loss: 59.559566\n",
      "Train Epoch: 22 [8/16 (50%)]\tBatch loss: 52.910492\n",
      "Train Epoch: 22 [16/16 (100%)]\tBatch loss: 13.123858\n",
      "Total loss 995.332706451416\n",
      "Train Epoch: 23 [0/16 (0%)]\tBatch loss: 58.577301\n",
      "Train Epoch: 23 [8/16 (50%)]\tBatch loss: 51.006618\n",
      "Train Epoch: 23 [16/16 (100%)]\tBatch loss: 14.529527\n",
      "Total loss 965.210844039917\n",
      "Train Epoch: 24 [0/16 (0%)]\tBatch loss: 52.942257\n",
      "Train Epoch: 24 [8/16 (50%)]\tBatch loss: 49.282093\n",
      "Train Epoch: 24 [16/16 (100%)]\tBatch loss: 16.257212\n",
      "Total loss 933.2088041305542\n",
      "Train Epoch: 25 [0/16 (0%)]\tBatch loss: 45.461979\n",
      "Train Epoch: 25 [8/16 (50%)]\tBatch loss: 47.733994\n",
      "Train Epoch: 25 [16/16 (100%)]\tBatch loss: 17.965858\n",
      "Total loss 906.1298294067383\n",
      "Train Epoch: 26 [0/16 (0%)]\tBatch loss: 38.472519\n",
      "Train Epoch: 26 [8/16 (50%)]\tBatch loss: 46.439308\n",
      "Train Epoch: 26 [16/16 (100%)]\tBatch loss: 19.301504\n",
      "Total loss 886.8441505432129\n",
      "Train Epoch: 27 [0/16 (0%)]\tBatch loss: 33.182323\n",
      "Train Epoch: 27 [8/16 (50%)]\tBatch loss: 45.453724\n",
      "Train Epoch: 27 [16/16 (100%)]\tBatch loss: 20.096941\n",
      "Total loss 874.4864692687988\n",
      "Train Epoch: 28 [0/16 (0%)]\tBatch loss: 29.860424\n",
      "Train Epoch: 28 [8/16 (50%)]\tBatch loss: 44.784172\n",
      "Train Epoch: 28 [16/16 (100%)]\tBatch loss: 20.336018\n",
      "Total loss 866.9493885040283\n",
      "Train Epoch: 29 [0/16 (0%)]\tBatch loss: 28.156925\n",
      "Train Epoch: 29 [8/16 (50%)]\tBatch loss: 44.447277\n",
      "Train Epoch: 29 [16/16 (100%)]\tBatch loss: 20.098642\n",
      "Total loss 862.1956396102905\n",
      "Train Epoch: 30 [0/16 (0%)]\tBatch loss: 27.574831\n",
      "Train Epoch: 30 [8/16 (50%)]\tBatch loss: 44.402184\n",
      "Train Epoch: 30 [16/16 (100%)]\tBatch loss: 19.542740\n",
      "Total loss 858.7245779037476\n",
      "Train Epoch: 31 [0/16 (0%)]\tBatch loss: 27.749989\n",
      "Train Epoch: 31 [8/16 (50%)]\tBatch loss: 44.542336\n",
      "Train Epoch: 31 [16/16 (100%)]\tBatch loss: 18.831099\n",
      "Total loss 855.648473739624\n",
      "Train Epoch: 32 [0/16 (0%)]\tBatch loss: 28.452106\n",
      "Train Epoch: 32 [8/16 (50%)]\tBatch loss: 44.748314\n",
      "Train Epoch: 32 [16/16 (100%)]\tBatch loss: 18.099174\n",
      "Total loss 852.4422521591187\n",
      "Train Epoch: 33 [0/16 (0%)]\tBatch loss: 29.483768\n",
      "Train Epoch: 33 [8/16 (50%)]\tBatch loss: 44.906120\n",
      "Train Epoch: 33 [16/16 (100%)]\tBatch loss: 17.449026\n",
      "Total loss 848.7872800827026\n",
      "Train Epoch: 34 [0/16 (0%)]\tBatch loss: 30.614393\n",
      "Train Epoch: 34 [8/16 (50%)]\tBatch loss: 44.936432\n",
      "Train Epoch: 34 [16/16 (100%)]\tBatch loss: 16.937519\n",
      "Total loss 844.6193780899048\n",
      "Train Epoch: 35 [0/16 (0%)]\tBatch loss: 31.597649\n",
      "Train Epoch: 35 [8/16 (50%)]\tBatch loss: 44.816074\n",
      "Train Epoch: 35 [16/16 (100%)]\tBatch loss: 16.579081\n",
      "Total loss 840.1143188476562\n",
      "Train Epoch: 36 [0/16 (0%)]\tBatch loss: 32.248840\n",
      "Train Epoch: 36 [8/16 (50%)]\tBatch loss: 44.563957\n",
      "Train Epoch: 36 [16/16 (100%)]\tBatch loss: 16.359095\n",
      "Total loss 835.5920009613037\n",
      "Train Epoch: 37 [0/16 (0%)]\tBatch loss: 32.505077\n",
      "Train Epoch: 37 [8/16 (50%)]\tBatch loss: 44.221420\n",
      "Train Epoch: 37 [16/16 (100%)]\tBatch loss: 16.246435\n",
      "Total loss 831.391242980957\n",
      "Train Epoch: 38 [0/16 (0%)]\tBatch loss: 32.420982\n",
      "Train Epoch: 38 [8/16 (50%)]\tBatch loss: 43.835026\n",
      "Train Epoch: 38 [16/16 (100%)]\tBatch loss: 16.205231\n",
      "Total loss 827.7498416900635\n",
      "Train Epoch: 39 [0/16 (0%)]\tBatch loss: 32.119923\n",
      "Train Epoch: 39 [8/16 (50%)]\tBatch loss: 43.442734\n",
      "Train Epoch: 39 [16/16 (100%)]\tBatch loss: 16.202524\n",
      "Total loss 824.7718410491943\n",
      "Train Epoch: 40 [0/16 (0%)]\tBatch loss: 31.735329\n",
      "Train Epoch: 40 [8/16 (50%)]\tBatch loss: 43.071121\n",
      "Train Epoch: 40 [16/16 (100%)]\tBatch loss: 16.211815\n",
      "Total loss 822.4543037414551\n",
      "Train Epoch: 41 [0/16 (0%)]\tBatch loss: 31.369652\n",
      "Train Epoch: 41 [8/16 (50%)]\tBatch loss: 42.737549\n",
      "Train Epoch: 41 [16/16 (100%)]\tBatch loss: 16.214901\n",
      "Total loss 820.7176504135132\n",
      "Train Epoch: 42 [0/16 (0%)]\tBatch loss: 31.079966\n",
      "Train Epoch: 42 [8/16 (50%)]\tBatch loss: 42.451820\n",
      "Train Epoch: 42 [16/16 (100%)]\tBatch loss: 16.201923\n",
      "Total loss 819.4225702285767\n",
      "Train Epoch: 43 [0/16 (0%)]\tBatch loss: 30.882053\n",
      "Train Epoch: 43 [8/16 (50%)]\tBatch loss: 42.218243\n",
      "Train Epoch: 43 [16/16 (100%)]\tBatch loss: 16.169653\n",
      "Total loss 818.3853406906128\n",
      "Train Epoch: 44 [0/16 (0%)]\tBatch loss: 30.762981\n",
      "Train Epoch: 44 [8/16 (50%)]\tBatch loss: 42.036167\n",
      "Train Epoch: 44 [16/16 (100%)]\tBatch loss: 16.119072\n",
      "Total loss 817.4064502716064\n",
      "Train Epoch: 45 [0/16 (0%)]\tBatch loss: 30.695518\n",
      "Train Epoch: 45 [8/16 (50%)]\tBatch loss: 41.900177\n",
      "Train Epoch: 45 [16/16 (100%)]\tBatch loss: 16.052885\n",
      "Total loss 816.3154420852661\n",
      "Train Epoch: 46 [0/16 (0%)]\tBatch loss: 30.650360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [8/16 (50%)]\tBatch loss: 41.801105\n",
      "Train Epoch: 46 [16/16 (100%)]\tBatch loss: 15.974016\n",
      "Total loss 815.01096534729\n",
      "Train Epoch: 47 [0/16 (0%)]\tBatch loss: 30.604919\n",
      "Train Epoch: 47 [8/16 (50%)]\tBatch loss: 41.727428\n",
      "Train Epoch: 47 [16/16 (100%)]\tBatch loss: 15.885193\n",
      "Total loss 813.4740800857544\n",
      "Train Epoch: 48 [0/16 (0%)]\tBatch loss: 30.547512\n",
      "Train Epoch: 48 [8/16 (50%)]\tBatch loss: 41.667236\n",
      "Train Epoch: 48 [16/16 (100%)]\tBatch loss: 15.789288\n",
      "Total loss 811.7557349205017\n",
      "Train Epoch: 49 [0/16 (0%)]\tBatch loss: 30.476841\n",
      "Train Epoch: 49 [8/16 (50%)]\tBatch loss: 41.609627\n",
      "Train Epoch: 49 [16/16 (100%)]\tBatch loss: 15.689669\n",
      "Total loss 809.9440565109253\n",
      "Train Epoch: 50 [0/16 (0%)]\tBatch loss: 30.398361\n",
      "Train Epoch: 50 [8/16 (50%)]\tBatch loss: 41.546104\n",
      "Train Epoch: 50 [16/16 (100%)]\tBatch loss: 15.590181\n",
      "Total loss 808.1320648193359\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loss_accum = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        model.send(data.location)\n",
    "        \n",
    "        optimizer = optimizers[data.location.id]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = ((pred.view(-1) - target)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.get()\n",
    "        loss = loss.get()\n",
    "        \n",
    "        loss_accum += float(loss)\n",
    "        \n",
    "        if batch_idx % 8 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBatch loss: {:.6f}'.format(\n",
    "                epoch, batch_idx, len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))            \n",
    "            \n",
    "    print('Total loss', loss_accum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the Repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
