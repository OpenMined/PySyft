{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an Image-Classifier using Federated Learning with Differential Privacy\n",
    "## A complete End-to-End Real World Example - PySyft, PyGrid\n",
    "* This tutorial aims at demonstrating a high level of privacy using **Secured Federated Learning** and **Differential Privacy based on the Laplace-Mechanism** as part of the **PATE framework**. To tackle a problem similair to a real-world-problem, yet still using a well explored example (the privacy tools are of main interest in this tutorial) it was chosen to train an Image-Classifier on the **[Cifar-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html)**. In the [original paper on the PATE framework](https://arxiv.org/pdf/1610.05755.pdf), simpler problems such as training a classifier on MNIST and SVHN were tackled.\n",
    "* Containing 60000 32x32 pixels coloured images of 10 different classes (airplanes, birds, etc.) this should showcase a reasonably similair task to another real-world example such as the training of a classifier for skin-cancer-classification, which heavily relies on sensitive private data. See [Stanford's Skin Cancer Classification with Deep Learning](https://cs.stanford.edu/people/esteva/nature/) for more information on this specific example.\n",
    "* To be able to give a complete end-to-end example after having performed the example using `PySyft.VirtualWorkers` we'll be using a Public/Private Grid and actual workers using the PyGrid work.  \n",
    "\n",
    "### Specific Situation - Real-World-Scenario: \n",
    "To make the example as real as possible and to best give a quick overview over the PATE framework for DP along with FL, we define  three different parties taking part in this setup. \n",
    "  1. **Data Provider:** Owns personal data, in this case some photographers who all made pictures of some objects in real life and sorted them (into the ten classes for this examle). His/her goal is to either *create/train a classifier* which automates the sorting of the images for him/her or to *participate in creating a general image classifier which can recognize objects on images in general*. Possibly he also wants somebody to *host the classifier to speed up inference time*. \n",
    "  2. **Model Provider:** Creates the model, in this case the classifier-model. His/her goal is to use his model-architecture and train it on the photographers (Data Provider) personal data and to then make this trained model available to a broad range of user. \n",
    "  3. **Hosting Provider:** Provides computing ressources along possible hosting capabilities. His/her goal is to provide the Data and Model Provider with the neccessary computing power for training and hosting capabilities for deployment. \n",
    "  * **How is this example representative for the need/usage of privacy preserving ML?**\n",
    "    This example portrays some of the key characteristics of privacy-preserving ML. \n",
    "      * **Combining Knowledge:** A single photographer wouldn't provide enough data to train a good image classifier and wouldn't have the knowledge to build a classifier. Together with different other (possibly even competing photographers) and a model provider he can train a good model together with the other parties. \n",
    "      * **Federated Learning:** He doesn't want his sorted photographs to be accessable by either the other competing photographers or the model creator. (He wants to sell his photographs)\n",
    "      * **Differential Privacy:** Classifiers as ML-Models in general can memorize specific parts of the training data, making it possible to retrieve information about the training data (or even parts of some datapoints). In this case the photographer doesn't want users of the classifier (e.g.: the competing photographers) to be able to retrieve information about his unique way of photograhping (e.g.: key motives, etc.)  \n",
    "      * All the above points are also exactly the critical points when trainng a classifier e.g. on a skin-cancer dataset.\n",
    "\n",
    "### The PP-techniques that will be used here:\n",
    "* **FL** - Federated Learning: We will be using **SMPC-Encrypted Federated Learning** *(For a quick overview see below)*\n",
    "* **DP** - Differential Privacy: We will be using the **Laplace-Mechanism** as part of the secure aggregation in the PATE procedure *(More information below)*\n",
    "\n",
    "\n",
    "### The PP-tools that will be used here:\n",
    "* **PySyft**\n",
    "  * Plans \n",
    "  * Protocolls\n",
    "  * VirtualWorkers\n",
    "* **PyGrid** \n",
    "  * Gateway\n",
    "  * GridNode\n",
    "  \n",
    "**TODO: CHECK SPECIFIC TYPES AND IN WHICH LIBRARIES THEY ARE! (Apparently currently changing)**\n",
    "\n",
    "Author:\n",
    "- Nicolas Remerscheid - GitHub: [@NiWaRe](https://github.com/NiWaRe)\n",
    "\n",
    "References: \n",
    "- References of different authors \n",
    "  *This example/usecase is heavily based in previous tutorials on PySyft and PyGrid from ......*\n",
    " \n",
    "***TODO: LINK SPECIFIC LEARNING MATERIALS AS PREREQUESITS AND SPECIFIC AUTHORS.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The PATE Framework\n",
    "## The assumptions \n",
    "Introduced in [this paper](https://arxiv.org/pdf/1610.05755.pdf) the *Private Aggregation of Teacher\n",
    "Ensembles* (PATE) framework consists of the following basic setup. \n",
    "It is important to note that the PATE framework assumes a certain situation: \n",
    "* **Discrete Model Output:** The PATE framework (acoording to the original paper) assumes that a model should be trained that has a discrete output i.e. it categorizes or classifies some input into categories. In this case this is an image-classifier. \n",
    "* **Data structure:** The PATE framework assumes that the following data exists and is stored in the following way:\n",
    "  * **Private Data:** The private data is stored on multiple workers (photographers here) The private data is *labeled* and each worker has *different datapoints* (different photographs). Furthermore the private data isn't sufficient to train a good classifier alone.  \n",
    "  * **Public Data:** In addition to the private data PATE assumes that there exists a second larger dataset of *unlabeled, public and unsensitive* images. In this case for example this could be very large database of pictures from various newspapers. They aren't labeled and everybody can access them since the newspapers decided to share them, as they don't sell them directly. \n",
    "* **Semi-Supervised Learning:** Given the particular datasets that are given PATE assumes that we want to leverage smaller labeled datasets (normally used for supervised learning) together with an unlabeled dataset (normally used for unsupervised learning) to train a final model, meaning we want to train it using semi-supervised learning. \n",
    "\n",
    "\n",
    "## The procedure\n",
    "(For more info see in the learning material section above, notably Andrew Trask's tutorial) <br>\n",
    "This image shows an overview over the procedure: \n",
    "<img src=\"./material/PATE_framework_overview_from_paper.png\" width=\"600\">\n",
    "\n",
    "1. STEP: **Multiple Teacher-Models are trained on each of the private datasets**\n",
    "  * This is also where we want to use **Federated Learning** to not *directly* access the data at all. To make sure that the photographers don't see the model of the model provider (he/she sells that) we'll be encrypting both the model and the data via Additive Secret Sharing using Secure Multi-Party Computing (SMPC). *Overview on that later.*\n",
    "2. STEP: **For each unlabeled datapoint in the public dataset teacher-models jointly predict a label**\n",
    "  * This is where noise is added for **Differential Privacy**. \n",
    "3. STEP: **One Student-Model is trained on the newly labeled public dataset**\n",
    "  * This is where **the final model** is created. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Up and Testing without Grid\n",
    "First we want to implement the procedure in raw form and as an automated protocoll and test it with a simple infrastructure using `PySyft.VirtualWorkers()` to then later deploy it to an actual grid-infrastructure whith actual workers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start: Setting up the real-case scenario\n",
    "We will beginn with VirtualWorkers and no grid to test the different other components and protocolls (FL, PATE). <br>\n",
    "First we will create all the workers with correct data stored on them. This step isn't necessary in a normal scenario, as a group of workers with data already exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Beginning with standard setup , Setup workers, etc. \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import numpy\n",
    "import syft as sy \n",
    "\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Note: \n",
    "# - Federated Dataset could be used \n",
    "# - data attribute with VirtualWorkes can't be used to not use efficient data loading of DataLoader\n",
    "#   everything would be stored in one place....think further about that (relook at forum)\n",
    "# - DOC. better\n",
    "\n",
    "# Create the VirtualWorkers \n",
    "# TODO: Potentially extend the function to support creation of actual workers with additional argument \n",
    "def createVWorkers(w_number, w_name):\n",
    "    worker = []\n",
    "    for i in range(w_number):\n",
    "        worker.append(sy.VirtualWorker(hook, id=w_name+str(i)))\n",
    "    return worker\n",
    "\n",
    "# Takes in worker array and a dataset which should be stored equally on workers \n",
    "def PATE_DATA_VSETUP(teachers, student, dataset, batch_size, ratio=0.4):\n",
    "    # No differentiation between training and testing is made, \n",
    "    # because in reality the data providers won't have the data seperated for ML \n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    numb_workers = len(teachers)+1\n",
    "    increment = len(dataset) / numb_workers\n",
    "\n",
    "    dist_dataset_teachers = {\"data\":[], \"target\":[]}\n",
    "    dist_dataset_student = {\"data\":[]}\n",
    "    \n",
    "    for i, (data, target) in enumerate(dataloader):\n",
    "        ### PySyft ###\n",
    "        # Teachers: The first 40% (normally less than on public) of the data\n",
    "        if i < int(ratio * (len(dataset)/batch_size)):\n",
    "            data = data.tag(\"cifar\").send(teachers[i%len(teachers)])\n",
    "            target = target.tag(\"cifar\").send(teachers[i%len(teachers)])\n",
    "            # Store the pointers to the send data.\n",
    "            dist_dataset_teachers[\"data\"].append(data)\n",
    "            dist_dataset_teachers[\"target\"].append(target)\n",
    "        # Student: The larger part is unlabeled \n",
    "        else: \n",
    "            data = data.tag(\"cifar\").send(student)\n",
    "            # Store the pointers to the send data.\n",
    "            dist_dataset_student[\"data\"].append(data)\n",
    "        \n",
    "    return dist_dataset_teachers, dist_dataset_student \n",
    "\n",
    "# Normalize data and convert to torch.FloatTensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "# Get Cifar10 Dataset from torchvision.datasets\n",
    "cifar10_train_data = datasets.CIFAR10('data', train=True,\n",
    "                              download=True, transform=transform)\n",
    "\n",
    "# Don't use to store on workers because we want to test performance of decentralized training at the end\n",
    "cifar10_test_data = datasets.CIFAR10('data', train=False,\n",
    "                              download=True, transform=transform)\n",
    "\n",
    "# Args\n",
    "batch_size = 20 \n",
    "ratio = 0.4\n",
    "\n",
    "# Private Data Provider\n",
    "teachers = createVWorkers(3, \"photographer\")\n",
    "\n",
    "# Public Data Provider\n",
    "student = createVWorkers(1, \"public_cloud\")\n",
    "\n",
    "# Distribute Data \n",
    "dist_dataset_teachers, dist_dataset_student = PATE_DATA_VSETUP(teachers, student,\n",
    "                                                                 cifar10_train_data, batch_size)\n",
    "\n",
    "# Model Provider\n",
    "model_provider = createVWorkers(1, \"model_provider\")\n",
    "\n",
    "# Hosting Provider \n",
    "hosting_provider = createVWorkers(3, \"hosting_provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>[PointerTensor | me:42183060114 -> photographer0:85629921556]\n",
      "(Wrapper)>[PointerTensor | me:66639053926 -> public_cloud0:88462510572]\n"
     ]
    }
   ],
   "source": [
    "# As an example: Get the target of the first example in the first batch -> Note: Not one-hot-encoded\n",
    "print(dist_dataset_teachers[\"target\"][0][0])\n",
    "print(dist_dataset_student[\"data\"][0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model + Rest Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Think about Validation (from test or training data)\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders (combine dataset and sampler)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "    sampler=train_sampler, num_workers=num_workers)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
    "    sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, \n",
    "    num_workers=num_workers)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATE Step 1: Traning Teacher Models \n",
    "Here FL learning will be done. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement SMPC-FL to train teacher models \n",
    "#       (Possibly first test in raw form, then implemented as Plan/Part of Protocoll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: for protocolls check again how the pate-framework is already implemented in PySyft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATE Step 2: Label Public Dataset with aggregated teacher-predictions\n",
    "Here DP aggregation will be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement DP-secure aggregation of teacher predictions on public dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PATE Step 3: Train Student Model\n",
    "Here Student Model will be trained "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement training of student model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing of PATE Procedure \n",
    "Here using the testing infrastructure the PATE procedure will be tested. <br>\n",
    "The goal is to implement the PATE procedure as a `PySyft.protocoll` using `PySyft.plans` which also should be tested as a general automated procedure before deploying everything on actual workers and a grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do testing of complete procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The final project on the grid\n",
    "Here the final real end-to-end example of photographers training a image-classfier together with a model provider should be showcased using an actual grid, whith a gateway and workers which could actually run on remote machines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Setting up a grid with workers + documentation + further links to grid tutorials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Evaluation \n",
    "* **Performance Evaluation:** *Depending on time can be extended or not*\n",
    "  Analyse Performance of Decentralized Training with normal Training, Possible metrics: \n",
    "    * Training Time: Convergence Speed, Compuation Time (incl. communication, etc.)\n",
    "    * Model-Performance: Relevant Metrics (Accuracy, etc.)\n",
    "    * Privacy-Leakage-Anlysis in different security situations (active and passive security)\n",
    "    * Model-Robustness: Impact of decentralized \n",
    "    * Model-Fairness: Impact of decentralized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('py-grid': conda)",
   "language": "python",
   "name": "python38264bitpygridcondaf38530e3ed324efaac7c8fb1b149269d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
