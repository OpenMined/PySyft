{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8 - Federated Learning on MNIST using a CNN\n",
    "\n",
    "We show here how  simple it is to switch to Pytorch models to syft models so as to perform Federated Learning, by using directly the canonical example of training a CNN on MNIST which provided by PyTorch [here](https://github.com/pytorch/examples/blob/master/mnist/main.py). We only changed the code to run in a notebook and we mention all the changed made which enable Federated Learning.\n",
    "\n",
    "**Roadmap of future improvements:**\n",
    "\n",
    " _See the #TODO for details about the following issues:_\n",
    " - support **momentum args in optim SGD** #1896\n",
    " - silence the Warning in log_softmax -> needs a hook_kwargs\n",
    "\n",
    "\n",
    "Authors:\n",
    "- Th√©o Ryffel - GitHub: [@LaRiffle](https://github.com/LaRiffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and model specifications\n",
    "\n",
    "First we make the official imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And than those specific to PySyft. In particular we define remote workers `alice` and `bob`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")  # <-- NEW: define remote worker bob\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")  # <-- NEW: and alice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the setting of the learning task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 1000\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 10\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading and sending to workers\n",
    "We first load the data and transform and training Dataset into a Federated Dataset split across the workers using the `.federate` method. This federated dataset is now given to a Federated DataLoader. The test dataset remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning and sending data to bob, alice...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ]))\n",
    "    .federate((bob, alice)), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "    batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN specification\n",
    "Here we use exactly the same CNN as in the official example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1) #TODO: shouldn't raise warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the train and test functions\n",
    "For the train function, because the data batches are distributed across `alice` and `bob`, you need for each batch to send the model to the right location, then you perform all the operations remotely with the same syntax than if you were doing local PyTorch. When you're done, you get back the model updated and the loss to look for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader): # <-- now it is a distributed dataset\n",
    "        model.send(data.location) # <-- NEW: send the model to the right location\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size, #batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test function does not change!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch the training !\n",
    "The traning is done as usual!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ryffel/Documents/Code/PySyft/syft/frameworks/torch/tensors/interpreters/native.py:216: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  response = eval(cmd)(*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60032 (0%)]\tLoss: 2.305134\n",
      "Train Epoch: 1 [640/60032 (1%)]\tLoss: 2.273475\n",
      "Train Epoch: 1 [1280/60032 (2%)]\tLoss: 2.216173\n",
      "Train Epoch: 1 [1920/60032 (3%)]\tLoss: 2.156802\n",
      "Train Epoch: 1 [2560/60032 (4%)]\tLoss: 2.139428\n",
      "Train Epoch: 1 [3200/60032 (5%)]\tLoss: 2.053060\n",
      "Train Epoch: 1 [3840/60032 (6%)]\tLoss: 1.896600\n",
      "Train Epoch: 1 [4480/60032 (7%)]\tLoss: 1.917216\n",
      "Train Epoch: 1 [5120/60032 (9%)]\tLoss: 1.655011\n",
      "Train Epoch: 1 [5760/60032 (10%)]\tLoss: 1.440342\n",
      "Train Epoch: 1 [6400/60032 (11%)]\tLoss: 1.231290\n",
      "Train Epoch: 1 [7040/60032 (12%)]\tLoss: 0.983513\n",
      "Train Epoch: 1 [7680/60032 (13%)]\tLoss: 0.867222\n",
      "Train Epoch: 1 [8320/60032 (14%)]\tLoss: 0.890978\n",
      "Train Epoch: 1 [8960/60032 (15%)]\tLoss: 0.862243\n",
      "Train Epoch: 1 [9600/60032 (16%)]\tLoss: 0.654238\n",
      "Train Epoch: 1 [10240/60032 (17%)]\tLoss: 0.587152\n",
      "Train Epoch: 1 [10880/60032 (18%)]\tLoss: 0.693086\n",
      "Train Epoch: 1 [11520/60032 (19%)]\tLoss: 0.593367\n",
      "Train Epoch: 1 [12160/60032 (20%)]\tLoss: 0.531858\n",
      "Train Epoch: 1 [12800/60032 (21%)]\tLoss: 0.400073\n",
      "Train Epoch: 1 [13440/60032 (22%)]\tLoss: 0.455612\n",
      "Train Epoch: 1 [14080/60032 (23%)]\tLoss: 0.438797\n",
      "Train Epoch: 1 [14720/60032 (25%)]\tLoss: 0.398438\n",
      "Train Epoch: 1 [15360/60032 (26%)]\tLoss: 0.370568\n",
      "Train Epoch: 1 [16000/60032 (27%)]\tLoss: 0.289150\n",
      "Train Epoch: 1 [16640/60032 (28%)]\tLoss: 0.416752\n",
      "Train Epoch: 1 [17280/60032 (29%)]\tLoss: 0.304047\n",
      "Train Epoch: 1 [17920/60032 (30%)]\tLoss: 0.367522\n",
      "Train Epoch: 1 [18560/60032 (31%)]\tLoss: 0.387216\n",
      "Train Epoch: 1 [19200/60032 (32%)]\tLoss: 0.313699\n",
      "Train Epoch: 1 [19840/60032 (33%)]\tLoss: 0.238436\n",
      "Train Epoch: 1 [20480/60032 (34%)]\tLoss: 0.535388\n",
      "Train Epoch: 1 [21120/60032 (35%)]\tLoss: 0.369703\n",
      "Train Epoch: 1 [21760/60032 (36%)]\tLoss: 0.464722\n",
      "Train Epoch: 1 [22400/60032 (37%)]\tLoss: 0.279503\n",
      "Train Epoch: 1 [23040/60032 (38%)]\tLoss: 0.237703\n",
      "Train Epoch: 1 [23680/60032 (39%)]\tLoss: 0.183209\n",
      "Train Epoch: 1 [24320/60032 (41%)]\tLoss: 0.322500\n",
      "Train Epoch: 1 [24960/60032 (42%)]\tLoss: 0.187515\n",
      "Train Epoch: 1 [25600/60032 (43%)]\tLoss: 0.286565\n",
      "Train Epoch: 1 [26240/60032 (44%)]\tLoss: 0.366353\n",
      "Train Epoch: 1 [26880/60032 (45%)]\tLoss: 0.523438\n",
      "Train Epoch: 1 [27520/60032 (46%)]\tLoss: 0.147916\n",
      "Train Epoch: 1 [28160/60032 (47%)]\tLoss: 0.131089\n",
      "Train Epoch: 1 [28800/60032 (48%)]\tLoss: 0.224736\n",
      "Train Epoch: 1 [29440/60032 (49%)]\tLoss: 0.273598\n",
      "Train Epoch: 1 [30080/60032 (50%)]\tLoss: 0.224271\n",
      "Train Epoch: 1 [30720/60032 (51%)]\tLoss: 0.174007\n",
      "Train Epoch: 1 [31360/60032 (52%)]\tLoss: 0.233640\n",
      "Train Epoch: 1 [32000/60032 (53%)]\tLoss: 0.281473\n",
      "Train Epoch: 1 [32640/60032 (54%)]\tLoss: 0.200960\n",
      "Train Epoch: 1 [33280/60032 (55%)]\tLoss: 0.204821\n",
      "Train Epoch: 1 [33920/60032 (57%)]\tLoss: 0.133486\n",
      "Train Epoch: 1 [34560/60032 (58%)]\tLoss: 0.208491\n",
      "Train Epoch: 1 [35200/60032 (59%)]\tLoss: 0.200890\n",
      "Train Epoch: 1 [35840/60032 (60%)]\tLoss: 0.625615\n",
      "Train Epoch: 1 [36480/60032 (61%)]\tLoss: 0.312820\n",
      "Train Epoch: 1 [37120/60032 (62%)]\tLoss: 0.243666\n",
      "Train Epoch: 1 [37760/60032 (63%)]\tLoss: 0.286155\n",
      "Train Epoch: 1 [38400/60032 (64%)]\tLoss: 0.138777\n",
      "Train Epoch: 1 [39040/60032 (65%)]\tLoss: 0.535752\n",
      "Train Epoch: 1 [39680/60032 (66%)]\tLoss: 0.169599\n",
      "Train Epoch: 1 [40320/60032 (67%)]\tLoss: 0.333904\n",
      "Train Epoch: 1 [40960/60032 (68%)]\tLoss: 0.196411\n",
      "Train Epoch: 1 [41600/60032 (69%)]\tLoss: 0.226253\n",
      "Train Epoch: 1 [42240/60032 (70%)]\tLoss: 0.186156\n",
      "Train Epoch: 1 [42880/60032 (71%)]\tLoss: 0.049237\n",
      "Train Epoch: 1 [43520/60032 (72%)]\tLoss: 0.214919\n",
      "Train Epoch: 1 [44160/60032 (74%)]\tLoss: 0.198379\n",
      "Train Epoch: 1 [44800/60032 (75%)]\tLoss: 0.242010\n",
      "Train Epoch: 1 [45440/60032 (76%)]\tLoss: 0.345877\n",
      "Train Epoch: 1 [46080/60032 (77%)]\tLoss: 0.251421\n",
      "Train Epoch: 1 [46720/60032 (78%)]\tLoss: 0.211861\n",
      "Train Epoch: 1 [47360/60032 (79%)]\tLoss: 0.163704\n",
      "Train Epoch: 1 [48000/60032 (80%)]\tLoss: 0.420659\n",
      "Train Epoch: 1 [48640/60032 (81%)]\tLoss: 0.188004\n",
      "Train Epoch: 1 [49280/60032 (82%)]\tLoss: 0.221743\n",
      "Train Epoch: 1 [49920/60032 (83%)]\tLoss: 0.274392\n",
      "Train Epoch: 1 [50560/60032 (84%)]\tLoss: 0.223508\n",
      "Train Epoch: 1 [51200/60032 (85%)]\tLoss: 0.132136\n",
      "Train Epoch: 1 [51840/60032 (86%)]\tLoss: 0.055294\n",
      "Train Epoch: 1 [52480/60032 (87%)]\tLoss: 0.348542\n",
      "Train Epoch: 1 [53120/60032 (88%)]\tLoss: 0.233862\n",
      "Train Epoch: 1 [53760/60032 (90%)]\tLoss: 0.164028\n",
      "Train Epoch: 1 [54400/60032 (91%)]\tLoss: 0.158319\n",
      "Train Epoch: 1 [55040/60032 (92%)]\tLoss: 0.129608\n",
      "Train Epoch: 1 [55680/60032 (93%)]\tLoss: 0.280557\n",
      "Train Epoch: 1 [56320/60032 (94%)]\tLoss: 0.277981\n",
      "Train Epoch: 1 [56960/60032 (95%)]\tLoss: 0.115785\n",
      "Train Epoch: 1 [57600/60032 (96%)]\tLoss: 0.069509\n",
      "Train Epoch: 1 [58240/60032 (97%)]\tLoss: 0.141677\n",
      "Train Epoch: 1 [58880/60032 (98%)]\tLoss: 0.214913\n",
      "Train Epoch: 1 [59520/60032 (99%)]\tLoss: 0.107504\n",
      "\n",
      "Test set: Average loss: 0.0002, Accuracy: 9534/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60032 (0%)]\tLoss: 0.113778\n",
      "Train Epoch: 2 [640/60032 (1%)]\tLoss: 0.265380\n",
      "Train Epoch: 2 [1280/60032 (2%)]\tLoss: 0.490340\n",
      "Train Epoch: 2 [1920/60032 (3%)]\tLoss: 0.105865\n",
      "Train Epoch: 2 [2560/60032 (4%)]\tLoss: 0.237229\n",
      "Train Epoch: 2 [3200/60032 (5%)]\tLoss: 0.210286\n",
      "Train Epoch: 2 [3840/60032 (6%)]\tLoss: 0.179636\n",
      "Train Epoch: 2 [4480/60032 (7%)]\tLoss: 0.195306\n",
      "Train Epoch: 2 [5120/60032 (9%)]\tLoss: 0.123206\n",
      "Train Epoch: 2 [5760/60032 (10%)]\tLoss: 0.090122\n",
      "Train Epoch: 2 [6400/60032 (11%)]\tLoss: 0.185041\n",
      "Train Epoch: 2 [7040/60032 (12%)]\tLoss: 0.061494\n",
      "Train Epoch: 2 [7680/60032 (13%)]\tLoss: 0.126376\n",
      "Train Epoch: 2 [8320/60032 (14%)]\tLoss: 0.165156\n",
      "Train Epoch: 2 [8960/60032 (15%)]\tLoss: 0.191307\n",
      "Train Epoch: 2 [9600/60032 (16%)]\tLoss: 0.138116\n",
      "Train Epoch: 2 [10240/60032 (17%)]\tLoss: 0.227656\n",
      "Train Epoch: 2 [10880/60032 (18%)]\tLoss: 0.101588\n",
      "Train Epoch: 2 [11520/60032 (19%)]\tLoss: 0.087671\n",
      "Train Epoch: 2 [12160/60032 (20%)]\tLoss: 0.116377\n",
      "Train Epoch: 2 [12800/60032 (21%)]\tLoss: 0.141603\n",
      "Train Epoch: 2 [13440/60032 (22%)]\tLoss: 0.074873\n",
      "Train Epoch: 2 [14080/60032 (23%)]\tLoss: 0.112723\n",
      "Train Epoch: 2 [14720/60032 (25%)]\tLoss: 0.218503\n",
      "Train Epoch: 2 [15360/60032 (26%)]\tLoss: 0.076412\n",
      "Train Epoch: 2 [16000/60032 (27%)]\tLoss: 0.199001\n",
      "Train Epoch: 2 [16640/60032 (28%)]\tLoss: 0.039382\n",
      "Train Epoch: 2 [17280/60032 (29%)]\tLoss: 0.168493\n",
      "Train Epoch: 2 [17920/60032 (30%)]\tLoss: 0.116843\n",
      "Train Epoch: 2 [18560/60032 (31%)]\tLoss: 0.106893\n",
      "Train Epoch: 2 [19200/60032 (32%)]\tLoss: 0.105159\n",
      "Train Epoch: 2 [19840/60032 (33%)]\tLoss: 0.078884\n",
      "Train Epoch: 2 [20480/60032 (34%)]\tLoss: 0.224894\n",
      "Train Epoch: 2 [21120/60032 (35%)]\tLoss: 0.154605\n",
      "Train Epoch: 2 [21760/60032 (36%)]\tLoss: 0.127195\n",
      "Train Epoch: 2 [22400/60032 (37%)]\tLoss: 0.177285\n",
      "Train Epoch: 2 [23040/60032 (38%)]\tLoss: 0.174740\n",
      "Train Epoch: 2 [23680/60032 (39%)]\tLoss: 0.036621\n",
      "Train Epoch: 2 [24320/60032 (41%)]\tLoss: 0.157190\n",
      "Train Epoch: 2 [24960/60032 (42%)]\tLoss: 0.245118\n",
      "Train Epoch: 2 [25600/60032 (43%)]\tLoss: 0.077399\n",
      "Train Epoch: 2 [26240/60032 (44%)]\tLoss: 0.091219\n",
      "Train Epoch: 2 [26880/60032 (45%)]\tLoss: 0.259576\n",
      "Train Epoch: 2 [27520/60032 (46%)]\tLoss: 0.109497\n",
      "Train Epoch: 2 [28160/60032 (47%)]\tLoss: 0.149797\n",
      "Train Epoch: 2 [28800/60032 (48%)]\tLoss: 0.102605\n",
      "Train Epoch: 2 [29440/60032 (49%)]\tLoss: 0.030372\n",
      "Train Epoch: 2 [30080/60032 (50%)]\tLoss: 0.048369\n",
      "Train Epoch: 2 [30720/60032 (51%)]\tLoss: 0.047150\n",
      "Train Epoch: 2 [31360/60032 (52%)]\tLoss: 0.102446\n",
      "Train Epoch: 2 [32000/60032 (53%)]\tLoss: 0.057166\n",
      "Train Epoch: 2 [32640/60032 (54%)]\tLoss: 0.166243\n",
      "Train Epoch: 2 [33280/60032 (55%)]\tLoss: 0.179077\n",
      "Train Epoch: 2 [33920/60032 (57%)]\tLoss: 0.125094\n",
      "Train Epoch: 2 [34560/60032 (58%)]\tLoss: 0.131601\n",
      "Train Epoch: 2 [35200/60032 (59%)]\tLoss: 0.081786\n",
      "Train Epoch: 2 [35840/60032 (60%)]\tLoss: 0.164020\n",
      "Train Epoch: 2 [36480/60032 (61%)]\tLoss: 0.080464\n",
      "Train Epoch: 2 [37120/60032 (62%)]\tLoss: 0.125747\n",
      "Train Epoch: 2 [37760/60032 (63%)]\tLoss: 0.079909\n",
      "Train Epoch: 2 [38400/60032 (64%)]\tLoss: 0.229873\n",
      "Train Epoch: 2 [39040/60032 (65%)]\tLoss: 0.181451\n",
      "Train Epoch: 2 [39680/60032 (66%)]\tLoss: 0.071050\n",
      "Train Epoch: 2 [40320/60032 (67%)]\tLoss: 0.097496\n",
      "Train Epoch: 2 [40960/60032 (68%)]\tLoss: 0.151769\n",
      "Train Epoch: 2 [41600/60032 (69%)]\tLoss: 0.141909\n",
      "Train Epoch: 2 [42240/60032 (70%)]\tLoss: 0.143685\n",
      "Train Epoch: 2 [42880/60032 (71%)]\tLoss: 0.175538\n",
      "Train Epoch: 2 [43520/60032 (72%)]\tLoss: 0.117112\n",
      "Train Epoch: 2 [44160/60032 (74%)]\tLoss: 0.048485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44800/60032 (75%)]\tLoss: 0.100407\n",
      "Train Epoch: 2 [45440/60032 (76%)]\tLoss: 0.106600\n",
      "Train Epoch: 2 [46080/60032 (77%)]\tLoss: 0.070548\n",
      "Train Epoch: 2 [46720/60032 (78%)]\tLoss: 0.059123\n",
      "Train Epoch: 2 [47360/60032 (79%)]\tLoss: 0.185734\n",
      "Train Epoch: 2 [48000/60032 (80%)]\tLoss: 0.067709\n",
      "Train Epoch: 2 [48640/60032 (81%)]\tLoss: 0.090186\n",
      "Train Epoch: 2 [49280/60032 (82%)]\tLoss: 0.040856\n",
      "Train Epoch: 2 [49920/60032 (83%)]\tLoss: 0.133938\n",
      "Train Epoch: 2 [50560/60032 (84%)]\tLoss: 0.067237\n",
      "Train Epoch: 2 [51200/60032 (85%)]\tLoss: 0.046040\n",
      "Train Epoch: 2 [51840/60032 (86%)]\tLoss: 0.037075\n",
      "Train Epoch: 2 [52480/60032 (87%)]\tLoss: 0.226975\n",
      "Train Epoch: 2 [53120/60032 (88%)]\tLoss: 0.054416\n",
      "Train Epoch: 2 [53760/60032 (90%)]\tLoss: 0.064413\n",
      "Train Epoch: 2 [54400/60032 (91%)]\tLoss: 0.050753\n",
      "Train Epoch: 2 [55040/60032 (92%)]\tLoss: 0.117795\n",
      "Train Epoch: 2 [55680/60032 (93%)]\tLoss: 0.148967\n",
      "Train Epoch: 2 [56320/60032 (94%)]\tLoss: 0.109889\n",
      "Train Epoch: 2 [56960/60032 (95%)]\tLoss: 0.041369\n",
      "Train Epoch: 2 [57600/60032 (96%)]\tLoss: 0.139175\n",
      "Train Epoch: 2 [58240/60032 (97%)]\tLoss: 0.030761\n",
      "Train Epoch: 2 [58880/60032 (98%)]\tLoss: 0.071408\n",
      "Train Epoch: 2 [59520/60032 (99%)]\tLoss: 0.068403\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9728/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60032 (0%)]\tLoss: 0.094504\n",
      "Train Epoch: 3 [640/60032 (1%)]\tLoss: 0.175515\n",
      "Train Epoch: 3 [1280/60032 (2%)]\tLoss: 0.215308\n",
      "Train Epoch: 3 [1920/60032 (3%)]\tLoss: 0.043300\n",
      "Train Epoch: 3 [2560/60032 (4%)]\tLoss: 0.077664\n",
      "Train Epoch: 3 [3200/60032 (5%)]\tLoss: 0.117181\n",
      "Train Epoch: 3 [3840/60032 (6%)]\tLoss: 0.197888\n",
      "Train Epoch: 3 [4480/60032 (7%)]\tLoss: 0.403120\n",
      "Train Epoch: 3 [5120/60032 (9%)]\tLoss: 0.125018\n",
      "Train Epoch: 3 [5760/60032 (10%)]\tLoss: 0.067487\n",
      "Train Epoch: 3 [6400/60032 (11%)]\tLoss: 0.099375\n",
      "Train Epoch: 3 [7040/60032 (12%)]\tLoss: 0.108976\n",
      "Train Epoch: 3 [7680/60032 (13%)]\tLoss: 0.131343\n",
      "Train Epoch: 3 [8320/60032 (14%)]\tLoss: 0.051671\n",
      "Train Epoch: 3 [8960/60032 (15%)]\tLoss: 0.088475\n",
      "Train Epoch: 3 [9600/60032 (16%)]\tLoss: 0.255222\n",
      "Train Epoch: 3 [10240/60032 (17%)]\tLoss: 0.150786\n",
      "Train Epoch: 3 [10880/60032 (18%)]\tLoss: 0.063349\n",
      "Train Epoch: 3 [11520/60032 (19%)]\tLoss: 0.256849\n",
      "Train Epoch: 3 [12160/60032 (20%)]\tLoss: 0.184736\n",
      "Train Epoch: 3 [12800/60032 (21%)]\tLoss: 0.011737\n",
      "Train Epoch: 3 [13440/60032 (22%)]\tLoss: 0.064377\n",
      "Train Epoch: 3 [14080/60032 (23%)]\tLoss: 0.126612\n",
      "Train Epoch: 3 [14720/60032 (25%)]\tLoss: 0.205086\n",
      "Train Epoch: 3 [15360/60032 (26%)]\tLoss: 0.233569\n",
      "Train Epoch: 3 [16000/60032 (27%)]\tLoss: 0.113368\n",
      "Train Epoch: 3 [16640/60032 (28%)]\tLoss: 0.064194\n",
      "Train Epoch: 3 [17280/60032 (29%)]\tLoss: 0.186044\n",
      "Train Epoch: 3 [17920/60032 (30%)]\tLoss: 0.086486\n",
      "Train Epoch: 3 [18560/60032 (31%)]\tLoss: 0.019166\n",
      "Train Epoch: 3 [19200/60032 (32%)]\tLoss: 0.039846\n",
      "Train Epoch: 3 [19840/60032 (33%)]\tLoss: 0.051053\n",
      "Train Epoch: 3 [20480/60032 (34%)]\tLoss: 0.012270\n",
      "Train Epoch: 3 [21120/60032 (35%)]\tLoss: 0.051601\n",
      "Train Epoch: 3 [21760/60032 (36%)]\tLoss: 0.161225\n",
      "Train Epoch: 3 [22400/60032 (37%)]\tLoss: 0.176382\n",
      "Train Epoch: 3 [23040/60032 (38%)]\tLoss: 0.078459\n",
      "Train Epoch: 3 [23680/60032 (39%)]\tLoss: 0.132966\n",
      "Train Epoch: 3 [24320/60032 (41%)]\tLoss: 0.061684\n",
      "Train Epoch: 3 [24960/60032 (42%)]\tLoss: 0.020250\n",
      "Train Epoch: 3 [25600/60032 (43%)]\tLoss: 0.114438\n",
      "Train Epoch: 3 [26240/60032 (44%)]\tLoss: 0.017788\n",
      "Train Epoch: 3 [26880/60032 (45%)]\tLoss: 0.406198\n",
      "Train Epoch: 3 [27520/60032 (46%)]\tLoss: 0.077796\n",
      "Train Epoch: 3 [28160/60032 (47%)]\tLoss: 0.090879\n",
      "Train Epoch: 3 [28800/60032 (48%)]\tLoss: 0.097319\n",
      "Train Epoch: 3 [29440/60032 (49%)]\tLoss: 0.148500\n",
      "Train Epoch: 3 [30080/60032 (50%)]\tLoss: 0.067710\n",
      "Train Epoch: 3 [30720/60032 (51%)]\tLoss: 0.072762\n",
      "Train Epoch: 3 [31360/60032 (52%)]\tLoss: 0.018036\n",
      "Train Epoch: 3 [32000/60032 (53%)]\tLoss: 0.037193\n",
      "Train Epoch: 3 [32640/60032 (54%)]\tLoss: 0.025647\n",
      "Train Epoch: 3 [33280/60032 (55%)]\tLoss: 0.040560\n",
      "Train Epoch: 3 [33920/60032 (57%)]\tLoss: 0.080107\n",
      "Train Epoch: 3 [34560/60032 (58%)]\tLoss: 0.101703\n",
      "Train Epoch: 3 [35200/60032 (59%)]\tLoss: 0.119948\n",
      "Train Epoch: 3 [35840/60032 (60%)]\tLoss: 0.067655\n",
      "Train Epoch: 3 [36480/60032 (61%)]\tLoss: 0.036288\n",
      "Train Epoch: 3 [37120/60032 (62%)]\tLoss: 0.239366\n",
      "Train Epoch: 3 [37760/60032 (63%)]\tLoss: 0.014185\n",
      "Train Epoch: 3 [38400/60032 (64%)]\tLoss: 0.016541\n",
      "Train Epoch: 3 [39040/60032 (65%)]\tLoss: 0.110971\n",
      "Train Epoch: 3 [39680/60032 (66%)]\tLoss: 0.017821\n",
      "Train Epoch: 3 [40320/60032 (67%)]\tLoss: 0.092991\n",
      "Train Epoch: 3 [40960/60032 (68%)]\tLoss: 0.020135\n",
      "Train Epoch: 3 [41600/60032 (69%)]\tLoss: 0.079164\n",
      "Train Epoch: 3 [42240/60032 (70%)]\tLoss: 0.124947\n",
      "Train Epoch: 3 [42880/60032 (71%)]\tLoss: 0.085779\n",
      "Train Epoch: 3 [43520/60032 (72%)]\tLoss: 0.049976\n",
      "Train Epoch: 3 [44160/60032 (74%)]\tLoss: 0.069461\n",
      "Train Epoch: 3 [44800/60032 (75%)]\tLoss: 0.028483\n",
      "Train Epoch: 3 [45440/60032 (76%)]\tLoss: 0.048948\n",
      "Train Epoch: 3 [46080/60032 (77%)]\tLoss: 0.090524\n",
      "Train Epoch: 3 [46720/60032 (78%)]\tLoss: 0.083893\n",
      "Train Epoch: 3 [47360/60032 (79%)]\tLoss: 0.125592\n",
      "Train Epoch: 3 [48000/60032 (80%)]\tLoss: 0.019467\n",
      "Train Epoch: 3 [48640/60032 (81%)]\tLoss: 0.182297\n",
      "Train Epoch: 3 [49280/60032 (82%)]\tLoss: 0.252414\n",
      "Train Epoch: 3 [49920/60032 (83%)]\tLoss: 0.056368\n",
      "Train Epoch: 3 [50560/60032 (84%)]\tLoss: 0.098034\n",
      "Train Epoch: 3 [51200/60032 (85%)]\tLoss: 0.157216\n",
      "Train Epoch: 3 [51840/60032 (86%)]\tLoss: 0.037600\n",
      "Train Epoch: 3 [52480/60032 (87%)]\tLoss: 0.160155\n",
      "Train Epoch: 3 [53120/60032 (88%)]\tLoss: 0.053553\n",
      "Train Epoch: 3 [53760/60032 (90%)]\tLoss: 0.065677\n",
      "Train Epoch: 3 [54400/60032 (91%)]\tLoss: 0.090585\n",
      "Train Epoch: 3 [55040/60032 (92%)]\tLoss: 0.040871\n",
      "Train Epoch: 3 [55680/60032 (93%)]\tLoss: 0.062513\n",
      "Train Epoch: 3 [56320/60032 (94%)]\tLoss: 0.022405\n",
      "Train Epoch: 3 [56960/60032 (95%)]\tLoss: 0.092917\n",
      "Train Epoch: 3 [57600/60032 (96%)]\tLoss: 0.041981\n",
      "Train Epoch: 3 [58240/60032 (97%)]\tLoss: 0.016229\n",
      "Train Epoch: 3 [58880/60032 (98%)]\tLoss: 0.018168\n",
      "Train Epoch: 3 [59520/60032 (99%)]\tLoss: 0.592580\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9751/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60032 (0%)]\tLoss: 0.189194\n",
      "Train Epoch: 4 [640/60032 (1%)]\tLoss: 0.032256\n",
      "Train Epoch: 4 [1280/60032 (2%)]\tLoss: 0.034044\n",
      "Train Epoch: 4 [1920/60032 (3%)]\tLoss: 0.070243\n",
      "Train Epoch: 4 [2560/60032 (4%)]\tLoss: 0.107529\n",
      "Train Epoch: 4 [3200/60032 (5%)]\tLoss: 0.104869\n",
      "Train Epoch: 4 [3840/60032 (6%)]\tLoss: 0.039525\n",
      "Train Epoch: 4 [4480/60032 (7%)]\tLoss: 0.063465\n",
      "Train Epoch: 4 [5120/60032 (9%)]\tLoss: 0.084965\n",
      "Train Epoch: 4 [5760/60032 (10%)]\tLoss: 0.115541\n",
      "Train Epoch: 4 [6400/60032 (11%)]\tLoss: 0.021847\n",
      "Train Epoch: 4 [7040/60032 (12%)]\tLoss: 0.085357\n",
      "Train Epoch: 4 [7680/60032 (13%)]\tLoss: 0.048931\n",
      "Train Epoch: 4 [8320/60032 (14%)]\tLoss: 0.010885\n",
      "Train Epoch: 4 [8960/60032 (15%)]\tLoss: 0.035983\n",
      "Train Epoch: 4 [9600/60032 (16%)]\tLoss: 0.019732\n",
      "Train Epoch: 4 [10240/60032 (17%)]\tLoss: 0.014308\n",
      "Train Epoch: 4 [10880/60032 (18%)]\tLoss: 0.104648\n",
      "Train Epoch: 4 [11520/60032 (19%)]\tLoss: 0.044326\n",
      "Train Epoch: 4 [12160/60032 (20%)]\tLoss: 0.132829\n",
      "Train Epoch: 4 [12800/60032 (21%)]\tLoss: 0.046491\n",
      "Train Epoch: 4 [13440/60032 (22%)]\tLoss: 0.154175\n",
      "Train Epoch: 4 [14080/60032 (23%)]\tLoss: 0.296980\n",
      "Train Epoch: 4 [14720/60032 (25%)]\tLoss: 0.019776\n",
      "Train Epoch: 4 [15360/60032 (26%)]\tLoss: 0.048978\n",
      "Train Epoch: 4 [16000/60032 (27%)]\tLoss: 0.069609\n",
      "Train Epoch: 4 [16640/60032 (28%)]\tLoss: 0.112281\n",
      "Train Epoch: 4 [17280/60032 (29%)]\tLoss: 0.039404\n",
      "Train Epoch: 4 [17920/60032 (30%)]\tLoss: 0.066465\n",
      "Train Epoch: 4 [18560/60032 (31%)]\tLoss: 0.041616\n",
      "Train Epoch: 4 [19200/60032 (32%)]\tLoss: 0.113768\n",
      "Train Epoch: 4 [19840/60032 (33%)]\tLoss: 0.051469\n",
      "Train Epoch: 4 [20480/60032 (34%)]\tLoss: 0.040714\n",
      "Train Epoch: 4 [21120/60032 (35%)]\tLoss: 0.014769\n",
      "Train Epoch: 4 [21760/60032 (36%)]\tLoss: 0.025533\n",
      "Train Epoch: 4 [22400/60032 (37%)]\tLoss: 0.022887\n",
      "Train Epoch: 4 [23040/60032 (38%)]\tLoss: 0.039690\n",
      "Train Epoch: 4 [23680/60032 (39%)]\tLoss: 0.108294\n",
      "Train Epoch: 4 [24320/60032 (41%)]\tLoss: 0.068795\n",
      "Train Epoch: 4 [24960/60032 (42%)]\tLoss: 0.192994\n",
      "Train Epoch: 4 [25600/60032 (43%)]\tLoss: 0.097070\n",
      "Train Epoch: 4 [26240/60032 (44%)]\tLoss: 0.230767\n",
      "Train Epoch: 4 [26880/60032 (45%)]\tLoss: 0.237870\n",
      "Train Epoch: 4 [27520/60032 (46%)]\tLoss: 0.062007\n",
      "Train Epoch: 4 [28160/60032 (47%)]\tLoss: 0.092524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [28800/60032 (48%)]\tLoss: 0.083896\n",
      "Train Epoch: 4 [29440/60032 (49%)]\tLoss: 0.109251\n",
      "Train Epoch: 4 [30080/60032 (50%)]\tLoss: 0.019829\n",
      "Train Epoch: 4 [30720/60032 (51%)]\tLoss: 0.015455\n",
      "Train Epoch: 4 [31360/60032 (52%)]\tLoss: 0.223642\n",
      "Train Epoch: 4 [32000/60032 (53%)]\tLoss: 0.036851\n",
      "Train Epoch: 4 [32640/60032 (54%)]\tLoss: 0.211329\n",
      "Train Epoch: 4 [33280/60032 (55%)]\tLoss: 0.099179\n",
      "Train Epoch: 4 [33920/60032 (57%)]\tLoss: 0.030599\n",
      "Train Epoch: 4 [34560/60032 (58%)]\tLoss: 0.031227\n",
      "Train Epoch: 4 [35200/60032 (59%)]\tLoss: 0.057524\n",
      "Train Epoch: 4 [35840/60032 (60%)]\tLoss: 0.108883\n",
      "Train Epoch: 4 [36480/60032 (61%)]\tLoss: 0.018255\n",
      "Train Epoch: 4 [37120/60032 (62%)]\tLoss: 0.070820\n",
      "Train Epoch: 4 [37760/60032 (63%)]\tLoss: 0.105461\n",
      "Train Epoch: 4 [38400/60032 (64%)]\tLoss: 0.051815\n",
      "Train Epoch: 4 [39040/60032 (65%)]\tLoss: 0.015466\n",
      "Train Epoch: 4 [39680/60032 (66%)]\tLoss: 0.068018\n",
      "Train Epoch: 4 [40320/60032 (67%)]\tLoss: 0.065431\n",
      "Train Epoch: 4 [40960/60032 (68%)]\tLoss: 0.015747\n",
      "Train Epoch: 4 [41600/60032 (69%)]\tLoss: 0.146246\n",
      "Train Epoch: 4 [42240/60032 (70%)]\tLoss: 0.136086\n",
      "Train Epoch: 4 [42880/60032 (71%)]\tLoss: 0.036798\n",
      "Train Epoch: 4 [43520/60032 (72%)]\tLoss: 0.039341\n",
      "Train Epoch: 4 [44160/60032 (74%)]\tLoss: 0.076322\n",
      "Train Epoch: 4 [44800/60032 (75%)]\tLoss: 0.029520\n",
      "Train Epoch: 4 [45440/60032 (76%)]\tLoss: 0.082919\n",
      "Train Epoch: 4 [46080/60032 (77%)]\tLoss: 0.016102\n",
      "Train Epoch: 4 [46720/60032 (78%)]\tLoss: 0.035956\n",
      "Train Epoch: 4 [47360/60032 (79%)]\tLoss: 0.010544\n",
      "Train Epoch: 4 [48000/60032 (80%)]\tLoss: 0.020601\n",
      "Train Epoch: 4 [48640/60032 (81%)]\tLoss: 0.014240\n",
      "Train Epoch: 4 [49280/60032 (82%)]\tLoss: 0.186839\n",
      "Train Epoch: 4 [49920/60032 (83%)]\tLoss: 0.199970\n",
      "Train Epoch: 4 [50560/60032 (84%)]\tLoss: 0.028384\n",
      "Train Epoch: 4 [51200/60032 (85%)]\tLoss: 0.111806\n",
      "Train Epoch: 4 [51840/60032 (86%)]\tLoss: 0.101863\n",
      "Train Epoch: 4 [52480/60032 (87%)]\tLoss: 0.027288\n",
      "Train Epoch: 4 [53120/60032 (88%)]\tLoss: 0.046056\n",
      "Train Epoch: 4 [53760/60032 (90%)]\tLoss: 0.065299\n",
      "Train Epoch: 4 [54400/60032 (91%)]\tLoss: 0.057996\n",
      "Train Epoch: 4 [55040/60032 (92%)]\tLoss: 0.117684\n",
      "Train Epoch: 4 [55680/60032 (93%)]\tLoss: 0.079980\n",
      "Train Epoch: 4 [56320/60032 (94%)]\tLoss: 0.062615\n",
      "Train Epoch: 4 [56960/60032 (95%)]\tLoss: 0.063873\n",
      "Train Epoch: 4 [57600/60032 (96%)]\tLoss: 0.093196\n",
      "Train Epoch: 4 [58240/60032 (97%)]\tLoss: 0.016062\n",
      "Train Epoch: 4 [58880/60032 (98%)]\tLoss: 0.020257\n",
      "Train Epoch: 4 [59520/60032 (99%)]\tLoss: 0.018852\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9810/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60032 (0%)]\tLoss: 0.069880\n",
      "Train Epoch: 5 [640/60032 (1%)]\tLoss: 0.263289\n",
      "Train Epoch: 5 [1280/60032 (2%)]\tLoss: 0.033130\n",
      "Train Epoch: 5 [1920/60032 (3%)]\tLoss: 0.047764\n",
      "Train Epoch: 5 [2560/60032 (4%)]\tLoss: 0.023587\n",
      "Train Epoch: 5 [3200/60032 (5%)]\tLoss: 0.015622\n",
      "Train Epoch: 5 [3840/60032 (6%)]\tLoss: 0.009252\n",
      "Train Epoch: 5 [4480/60032 (7%)]\tLoss: 0.195281\n",
      "Train Epoch: 5 [5120/60032 (9%)]\tLoss: 0.098414\n",
      "Train Epoch: 5 [5760/60032 (10%)]\tLoss: 0.047592\n",
      "Train Epoch: 5 [6400/60032 (11%)]\tLoss: 0.149808\n",
      "Train Epoch: 5 [7040/60032 (12%)]\tLoss: 0.066401\n",
      "Train Epoch: 5 [7680/60032 (13%)]\tLoss: 0.102542\n",
      "Train Epoch: 5 [8320/60032 (14%)]\tLoss: 0.068882\n",
      "Train Epoch: 5 [8960/60032 (15%)]\tLoss: 0.097273\n",
      "Train Epoch: 5 [9600/60032 (16%)]\tLoss: 0.027167\n",
      "Train Epoch: 5 [10240/60032 (17%)]\tLoss: 0.027088\n",
      "Train Epoch: 5 [10880/60032 (18%)]\tLoss: 0.021064\n",
      "Train Epoch: 5 [11520/60032 (19%)]\tLoss: 0.197135\n",
      "Train Epoch: 5 [12160/60032 (20%)]\tLoss: 0.154357\n",
      "Train Epoch: 5 [12800/60032 (21%)]\tLoss: 0.199024\n",
      "Train Epoch: 5 [13440/60032 (22%)]\tLoss: 0.011630\n",
      "Train Epoch: 5 [14080/60032 (23%)]\tLoss: 0.030642\n",
      "Train Epoch: 5 [14720/60032 (25%)]\tLoss: 0.068244\n",
      "Train Epoch: 5 [15360/60032 (26%)]\tLoss: 0.158483\n",
      "Train Epoch: 5 [16000/60032 (27%)]\tLoss: 0.045986\n",
      "Train Epoch: 5 [16640/60032 (28%)]\tLoss: 0.010116\n",
      "Train Epoch: 5 [17280/60032 (29%)]\tLoss: 0.069042\n",
      "Train Epoch: 5 [17920/60032 (30%)]\tLoss: 0.051785\n",
      "Train Epoch: 5 [18560/60032 (31%)]\tLoss: 0.089404\n",
      "Train Epoch: 5 [19200/60032 (32%)]\tLoss: 0.037696\n",
      "Train Epoch: 5 [19840/60032 (33%)]\tLoss: 0.054298\n",
      "Train Epoch: 5 [20480/60032 (34%)]\tLoss: 0.020412\n",
      "Train Epoch: 5 [21120/60032 (35%)]\tLoss: 0.057935\n",
      "Train Epoch: 5 [21760/60032 (36%)]\tLoss: 0.076881\n",
      "Train Epoch: 5 [22400/60032 (37%)]\tLoss: 0.031817\n",
      "Train Epoch: 5 [23040/60032 (38%)]\tLoss: 0.022903\n",
      "Train Epoch: 5 [23680/60032 (39%)]\tLoss: 0.011616\n",
      "Train Epoch: 5 [24320/60032 (41%)]\tLoss: 0.061614\n",
      "Train Epoch: 5 [24960/60032 (42%)]\tLoss: 0.015055\n",
      "Train Epoch: 5 [25600/60032 (43%)]\tLoss: 0.126458\n",
      "Train Epoch: 5 [26240/60032 (44%)]\tLoss: 0.204381\n",
      "Train Epoch: 5 [26880/60032 (45%)]\tLoss: 0.020382\n",
      "Train Epoch: 5 [27520/60032 (46%)]\tLoss: 0.142050\n",
      "Train Epoch: 5 [28160/60032 (47%)]\tLoss: 0.034298\n",
      "Train Epoch: 5 [28800/60032 (48%)]\tLoss: 0.030266\n",
      "Train Epoch: 5 [29440/60032 (49%)]\tLoss: 0.132318\n",
      "Train Epoch: 5 [30080/60032 (50%)]\tLoss: 0.052639\n",
      "Train Epoch: 5 [30720/60032 (51%)]\tLoss: 0.050288\n",
      "Train Epoch: 5 [31360/60032 (52%)]\tLoss: 0.091955\n",
      "Train Epoch: 5 [32000/60032 (53%)]\tLoss: 0.014679\n",
      "Train Epoch: 5 [32640/60032 (54%)]\tLoss: 0.071397\n",
      "Train Epoch: 5 [33280/60032 (55%)]\tLoss: 0.045389\n",
      "Train Epoch: 5 [33920/60032 (57%)]\tLoss: 0.066269\n",
      "Train Epoch: 5 [34560/60032 (58%)]\tLoss: 0.138635\n",
      "Train Epoch: 5 [35200/60032 (59%)]\tLoss: 0.160678\n",
      "Train Epoch: 5 [35840/60032 (60%)]\tLoss: 0.108584\n",
      "Train Epoch: 5 [36480/60032 (61%)]\tLoss: 0.051976\n",
      "Train Epoch: 5 [37120/60032 (62%)]\tLoss: 0.031799\n",
      "Train Epoch: 5 [37760/60032 (63%)]\tLoss: 0.029021\n",
      "Train Epoch: 5 [38400/60032 (64%)]\tLoss: 0.016748\n",
      "Train Epoch: 5 [39040/60032 (65%)]\tLoss: 0.070038\n",
      "Train Epoch: 5 [39680/60032 (66%)]\tLoss: 0.027272\n",
      "Train Epoch: 5 [40320/60032 (67%)]\tLoss: 0.012061\n",
      "Train Epoch: 5 [40960/60032 (68%)]\tLoss: 0.027657\n",
      "Train Epoch: 5 [41600/60032 (69%)]\tLoss: 0.011491\n",
      "Train Epoch: 5 [42240/60032 (70%)]\tLoss: 0.019635\n",
      "Train Epoch: 5 [42880/60032 (71%)]\tLoss: 0.035603\n",
      "Train Epoch: 5 [43520/60032 (72%)]\tLoss: 0.018627\n",
      "Train Epoch: 5 [44160/60032 (74%)]\tLoss: 0.020587\n",
      "Train Epoch: 5 [44800/60032 (75%)]\tLoss: 0.042868\n",
      "Train Epoch: 5 [45440/60032 (76%)]\tLoss: 0.009467\n",
      "Train Epoch: 5 [46080/60032 (77%)]\tLoss: 0.022541\n",
      "Train Epoch: 5 [46720/60032 (78%)]\tLoss: 0.022671\n",
      "Train Epoch: 5 [47360/60032 (79%)]\tLoss: 0.042235\n",
      "Train Epoch: 5 [48000/60032 (80%)]\tLoss: 0.016348\n",
      "Train Epoch: 5 [48640/60032 (81%)]\tLoss: 0.007422\n",
      "Train Epoch: 5 [49280/60032 (82%)]\tLoss: 0.012596\n",
      "Train Epoch: 5 [49920/60032 (83%)]\tLoss: 0.082685\n",
      "Train Epoch: 5 [50560/60032 (84%)]\tLoss: 0.017650\n",
      "Train Epoch: 5 [51200/60032 (85%)]\tLoss: 0.044282\n",
      "Train Epoch: 5 [51840/60032 (86%)]\tLoss: 0.035240\n",
      "Train Epoch: 5 [52480/60032 (87%)]\tLoss: 0.020934\n",
      "Train Epoch: 5 [53120/60032 (88%)]\tLoss: 0.040733\n",
      "Train Epoch: 5 [53760/60032 (90%)]\tLoss: 0.008878\n",
      "Train Epoch: 5 [54400/60032 (91%)]\tLoss: 0.025896\n",
      "Train Epoch: 5 [55040/60032 (92%)]\tLoss: 0.080595\n",
      "Train Epoch: 5 [55680/60032 (93%)]\tLoss: 0.076350\n",
      "Train Epoch: 5 [56320/60032 (94%)]\tLoss: 0.018844\n",
      "Train Epoch: 5 [56960/60032 (95%)]\tLoss: 0.018708\n",
      "Train Epoch: 5 [57600/60032 (96%)]\tLoss: 0.039405\n",
      "Train Epoch: 5 [58240/60032 (97%)]\tLoss: 0.012979\n",
      "Train Epoch: 5 [58880/60032 (98%)]\tLoss: 0.025720\n",
      "Train Epoch: 5 [59520/60032 (99%)]\tLoss: 0.023650\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9848/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60032 (0%)]\tLoss: 0.049073\n",
      "Train Epoch: 6 [640/60032 (1%)]\tLoss: 0.055349\n",
      "Train Epoch: 6 [1280/60032 (2%)]\tLoss: 0.153501\n",
      "Train Epoch: 6 [1920/60032 (3%)]\tLoss: 0.101429\n",
      "Train Epoch: 6 [2560/60032 (4%)]\tLoss: 0.074923\n",
      "Train Epoch: 6 [3200/60032 (5%)]\tLoss: 0.024894\n",
      "Train Epoch: 6 [3840/60032 (6%)]\tLoss: 0.005418\n",
      "Train Epoch: 6 [4480/60032 (7%)]\tLoss: 0.037078\n",
      "Train Epoch: 6 [5120/60032 (9%)]\tLoss: 0.082673\n",
      "Train Epoch: 6 [5760/60032 (10%)]\tLoss: 0.112611\n",
      "Train Epoch: 6 [6400/60032 (11%)]\tLoss: 0.015806\n",
      "Train Epoch: 6 [7040/60032 (12%)]\tLoss: 0.052644\n",
      "Train Epoch: 6 [7680/60032 (13%)]\tLoss: 0.014022\n",
      "Train Epoch: 6 [8320/60032 (14%)]\tLoss: 0.049864\n",
      "Train Epoch: 6 [8960/60032 (15%)]\tLoss: 0.042224\n",
      "Train Epoch: 6 [9600/60032 (16%)]\tLoss: 0.017757\n",
      "Train Epoch: 6 [10240/60032 (17%)]\tLoss: 0.103851\n",
      "Train Epoch: 6 [10880/60032 (18%)]\tLoss: 0.013060\n",
      "Train Epoch: 6 [11520/60032 (19%)]\tLoss: 0.025830\n",
      "Train Epoch: 6 [12160/60032 (20%)]\tLoss: 0.048161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [12800/60032 (21%)]\tLoss: 0.041316\n",
      "Train Epoch: 6 [13440/60032 (22%)]\tLoss: 0.065919\n",
      "Train Epoch: 6 [14080/60032 (23%)]\tLoss: 0.009189\n",
      "Train Epoch: 6 [14720/60032 (25%)]\tLoss: 0.037372\n",
      "Train Epoch: 6 [15360/60032 (26%)]\tLoss: 0.099166\n",
      "Train Epoch: 6 [16000/60032 (27%)]\tLoss: 0.069649\n",
      "Train Epoch: 6 [16640/60032 (28%)]\tLoss: 0.053255\n",
      "Train Epoch: 6 [17280/60032 (29%)]\tLoss: 0.084463\n",
      "Train Epoch: 6 [17920/60032 (30%)]\tLoss: 0.005429\n",
      "Train Epoch: 6 [18560/60032 (31%)]\tLoss: 0.104761\n",
      "Train Epoch: 6 [19200/60032 (32%)]\tLoss: 0.068382\n",
      "Train Epoch: 6 [19840/60032 (33%)]\tLoss: 0.052001\n",
      "Train Epoch: 6 [20480/60032 (34%)]\tLoss: 0.029537\n",
      "Train Epoch: 6 [21120/60032 (35%)]\tLoss: 0.082311\n",
      "Train Epoch: 6 [21760/60032 (36%)]\tLoss: 0.069474\n",
      "Train Epoch: 6 [22400/60032 (37%)]\tLoss: 0.101588\n",
      "Train Epoch: 6 [23040/60032 (38%)]\tLoss: 0.043048\n",
      "Train Epoch: 6 [23680/60032 (39%)]\tLoss: 0.080579\n",
      "Train Epoch: 6 [24320/60032 (41%)]\tLoss: 0.003213\n",
      "Train Epoch: 6 [24960/60032 (42%)]\tLoss: 0.007041\n",
      "Train Epoch: 6 [25600/60032 (43%)]\tLoss: 0.016668\n",
      "Train Epoch: 6 [26240/60032 (44%)]\tLoss: 0.010233\n",
      "Train Epoch: 6 [26880/60032 (45%)]\tLoss: 0.038012\n",
      "Train Epoch: 6 [27520/60032 (46%)]\tLoss: 0.021155\n",
      "Train Epoch: 6 [28160/60032 (47%)]\tLoss: 0.075637\n",
      "Train Epoch: 6 [28800/60032 (48%)]\tLoss: 0.015172\n",
      "Train Epoch: 6 [29440/60032 (49%)]\tLoss: 0.032065\n",
      "Train Epoch: 6 [30080/60032 (50%)]\tLoss: 0.034428\n",
      "Train Epoch: 6 [30720/60032 (51%)]\tLoss: 0.014073\n",
      "Train Epoch: 6 [31360/60032 (52%)]\tLoss: 0.042666\n",
      "Train Epoch: 6 [32000/60032 (53%)]\tLoss: 0.026599\n",
      "Train Epoch: 6 [32640/60032 (54%)]\tLoss: 0.010666\n",
      "Train Epoch: 6 [33280/60032 (55%)]\tLoss: 0.038190\n",
      "Train Epoch: 6 [33920/60032 (57%)]\tLoss: 0.032556\n",
      "Train Epoch: 6 [34560/60032 (58%)]\tLoss: 0.019670\n",
      "Train Epoch: 6 [35200/60032 (59%)]\tLoss: 0.067198\n",
      "Train Epoch: 6 [35840/60032 (60%)]\tLoss: 0.046497\n",
      "Train Epoch: 6 [36480/60032 (61%)]\tLoss: 0.056750\n",
      "Train Epoch: 6 [37120/60032 (62%)]\tLoss: 0.120729\n",
      "Train Epoch: 6 [37760/60032 (63%)]\tLoss: 0.035556\n",
      "Train Epoch: 6 [38400/60032 (64%)]\tLoss: 0.008282\n",
      "Train Epoch: 6 [39040/60032 (65%)]\tLoss: 0.018525\n",
      "Train Epoch: 6 [39680/60032 (66%)]\tLoss: 0.025123\n",
      "Train Epoch: 6 [40320/60032 (67%)]\tLoss: 0.097320\n",
      "Train Epoch: 6 [40960/60032 (68%)]\tLoss: 0.007579\n",
      "Train Epoch: 6 [41600/60032 (69%)]\tLoss: 0.035299\n",
      "Train Epoch: 6 [42240/60032 (70%)]\tLoss: 0.047911\n",
      "Train Epoch: 6 [42880/60032 (71%)]\tLoss: 0.092744\n",
      "Train Epoch: 6 [43520/60032 (72%)]\tLoss: 0.093077\n",
      "Train Epoch: 6 [44160/60032 (74%)]\tLoss: 0.013814\n",
      "Train Epoch: 6 [44800/60032 (75%)]\tLoss: 0.114638\n",
      "Train Epoch: 6 [45440/60032 (76%)]\tLoss: 0.006275\n",
      "Train Epoch: 6 [46080/60032 (77%)]\tLoss: 0.045389\n",
      "Train Epoch: 6 [46720/60032 (78%)]\tLoss: 0.013915\n",
      "Train Epoch: 6 [47360/60032 (79%)]\tLoss: 0.247417\n",
      "Train Epoch: 6 [48000/60032 (80%)]\tLoss: 0.082022\n",
      "Train Epoch: 6 [48640/60032 (81%)]\tLoss: 0.146815\n",
      "Train Epoch: 6 [49280/60032 (82%)]\tLoss: 0.007068\n",
      "Train Epoch: 6 [49920/60032 (83%)]\tLoss: 0.025560\n",
      "Train Epoch: 6 [50560/60032 (84%)]\tLoss: 0.005810\n",
      "Train Epoch: 6 [51200/60032 (85%)]\tLoss: 0.021867\n",
      "Train Epoch: 6 [51840/60032 (86%)]\tLoss: 0.019897\n",
      "Train Epoch: 6 [52480/60032 (87%)]\tLoss: 0.225858\n",
      "Train Epoch: 6 [53120/60032 (88%)]\tLoss: 0.028713\n",
      "Train Epoch: 6 [53760/60032 (90%)]\tLoss: 0.055500\n",
      "Train Epoch: 6 [54400/60032 (91%)]\tLoss: 0.086934\n",
      "Train Epoch: 6 [55040/60032 (92%)]\tLoss: 0.018369\n",
      "Train Epoch: 6 [55680/60032 (93%)]\tLoss: 0.015428\n",
      "Train Epoch: 6 [56320/60032 (94%)]\tLoss: 0.003191\n",
      "Train Epoch: 6 [56960/60032 (95%)]\tLoss: 0.125372\n",
      "Train Epoch: 6 [57600/60032 (96%)]\tLoss: 0.150187\n",
      "Train Epoch: 6 [58240/60032 (97%)]\tLoss: 0.089356\n",
      "Train Epoch: 6 [58880/60032 (98%)]\tLoss: 0.098658\n",
      "Train Epoch: 6 [59520/60032 (99%)]\tLoss: 0.008328\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9840/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60032 (0%)]\tLoss: 0.083935\n",
      "Train Epoch: 7 [640/60032 (1%)]\tLoss: 0.054384\n",
      "Train Epoch: 7 [1280/60032 (2%)]\tLoss: 0.168117\n",
      "Train Epoch: 7 [1920/60032 (3%)]\tLoss: 0.039736\n",
      "Train Epoch: 7 [2560/60032 (4%)]\tLoss: 0.078862\n",
      "Train Epoch: 7 [3200/60032 (5%)]\tLoss: 0.008488\n",
      "Train Epoch: 7 [3840/60032 (6%)]\tLoss: 0.023925\n",
      "Train Epoch: 7 [4480/60032 (7%)]\tLoss: 0.008682\n",
      "Train Epoch: 7 [5120/60032 (9%)]\tLoss: 0.003254\n",
      "Train Epoch: 7 [5760/60032 (10%)]\tLoss: 0.050213\n",
      "Train Epoch: 7 [6400/60032 (11%)]\tLoss: 0.075580\n",
      "Train Epoch: 7 [7040/60032 (12%)]\tLoss: 0.046236\n",
      "Train Epoch: 7 [7680/60032 (13%)]\tLoss: 0.057312\n",
      "Train Epoch: 7 [8320/60032 (14%)]\tLoss: 0.031910\n",
      "Train Epoch: 7 [8960/60032 (15%)]\tLoss: 0.011901\n",
      "Train Epoch: 7 [9600/60032 (16%)]\tLoss: 0.044327\n",
      "Train Epoch: 7 [10240/60032 (17%)]\tLoss: 0.021562\n",
      "Train Epoch: 7 [10880/60032 (18%)]\tLoss: 0.062330\n",
      "Train Epoch: 7 [11520/60032 (19%)]\tLoss: 0.095059\n",
      "Train Epoch: 7 [12160/60032 (20%)]\tLoss: 0.017340\n",
      "Train Epoch: 7 [12800/60032 (21%)]\tLoss: 0.090702\n",
      "Train Epoch: 7 [13440/60032 (22%)]\tLoss: 0.033078\n",
      "Train Epoch: 7 [14080/60032 (23%)]\tLoss: 0.006663\n",
      "Train Epoch: 7 [14720/60032 (25%)]\tLoss: 0.034910\n",
      "Train Epoch: 7 [15360/60032 (26%)]\tLoss: 0.025350\n",
      "Train Epoch: 7 [16000/60032 (27%)]\tLoss: 0.034458\n",
      "Train Epoch: 7 [16640/60032 (28%)]\tLoss: 0.095943\n",
      "Train Epoch: 7 [17280/60032 (29%)]\tLoss: 0.099088\n",
      "Train Epoch: 7 [17920/60032 (30%)]\tLoss: 0.020519\n",
      "Train Epoch: 7 [18560/60032 (31%)]\tLoss: 0.030778\n",
      "Train Epoch: 7 [19200/60032 (32%)]\tLoss: 0.131887\n",
      "Train Epoch: 7 [19840/60032 (33%)]\tLoss: 0.061203\n",
      "Train Epoch: 7 [20480/60032 (34%)]\tLoss: 0.026432\n",
      "Train Epoch: 7 [21120/60032 (35%)]\tLoss: 0.074431\n",
      "Train Epoch: 7 [21760/60032 (36%)]\tLoss: 0.303541\n",
      "Train Epoch: 7 [22400/60032 (37%)]\tLoss: 0.016764\n",
      "Train Epoch: 7 [23040/60032 (38%)]\tLoss: 0.004770\n",
      "Train Epoch: 7 [23680/60032 (39%)]\tLoss: 0.021162\n",
      "Train Epoch: 7 [24320/60032 (41%)]\tLoss: 0.019410\n",
      "Train Epoch: 7 [24960/60032 (42%)]\tLoss: 0.049806\n",
      "Train Epoch: 7 [25600/60032 (43%)]\tLoss: 0.064185\n",
      "Train Epoch: 7 [26240/60032 (44%)]\tLoss: 0.229843\n",
      "Train Epoch: 7 [26880/60032 (45%)]\tLoss: 0.036166\n",
      "Train Epoch: 7 [27520/60032 (46%)]\tLoss: 0.035306\n",
      "Train Epoch: 7 [28160/60032 (47%)]\tLoss: 0.007606\n",
      "Train Epoch: 7 [28800/60032 (48%)]\tLoss: 0.004079\n",
      "Train Epoch: 7 [29440/60032 (49%)]\tLoss: 0.017094\n",
      "Train Epoch: 7 [30080/60032 (50%)]\tLoss: 0.089674\n",
      "Train Epoch: 7 [30720/60032 (51%)]\tLoss: 0.007820\n",
      "Train Epoch: 7 [31360/60032 (52%)]\tLoss: 0.017584\n",
      "Train Epoch: 7 [32000/60032 (53%)]\tLoss: 0.013011\n",
      "Train Epoch: 7 [32640/60032 (54%)]\tLoss: 0.076581\n",
      "Train Epoch: 7 [33280/60032 (55%)]\tLoss: 0.008936\n",
      "Train Epoch: 7 [33920/60032 (57%)]\tLoss: 0.021391\n",
      "Train Epoch: 7 [34560/60032 (58%)]\tLoss: 0.038660\n",
      "Train Epoch: 7 [35200/60032 (59%)]\tLoss: 0.014687\n",
      "Train Epoch: 7 [35840/60032 (60%)]\tLoss: 0.190013\n",
      "Train Epoch: 7 [36480/60032 (61%)]\tLoss: 0.003479\n",
      "Train Epoch: 7 [37120/60032 (62%)]\tLoss: 0.008273\n",
      "Train Epoch: 7 [37760/60032 (63%)]\tLoss: 0.037853\n",
      "Train Epoch: 7 [38400/60032 (64%)]\tLoss: 0.045425\n",
      "Train Epoch: 7 [39040/60032 (65%)]\tLoss: 0.042775\n",
      "Train Epoch: 7 [39680/60032 (66%)]\tLoss: 0.131094\n",
      "Train Epoch: 7 [40320/60032 (67%)]\tLoss: 0.037090\n",
      "Train Epoch: 7 [40960/60032 (68%)]\tLoss: 0.021988\n",
      "Train Epoch: 7 [41600/60032 (69%)]\tLoss: 0.021360\n",
      "Train Epoch: 7 [42240/60032 (70%)]\tLoss: 0.011166\n",
      "Train Epoch: 7 [42880/60032 (71%)]\tLoss: 0.004860\n",
      "Train Epoch: 7 [43520/60032 (72%)]\tLoss: 0.022198\n",
      "Train Epoch: 7 [44160/60032 (74%)]\tLoss: 0.010070\n",
      "Train Epoch: 7 [44800/60032 (75%)]\tLoss: 0.053608\n",
      "Train Epoch: 7 [45440/60032 (76%)]\tLoss: 0.001795\n",
      "Train Epoch: 7 [46080/60032 (77%)]\tLoss: 0.035745\n",
      "Train Epoch: 7 [46720/60032 (78%)]\tLoss: 0.032268\n",
      "Train Epoch: 7 [47360/60032 (79%)]\tLoss: 0.057521\n",
      "Train Epoch: 7 [48000/60032 (80%)]\tLoss: 0.052390\n",
      "Train Epoch: 7 [48640/60032 (81%)]\tLoss: 0.050640\n",
      "Train Epoch: 7 [49280/60032 (82%)]\tLoss: 0.033948\n",
      "Train Epoch: 7 [49920/60032 (83%)]\tLoss: 0.089637\n",
      "Train Epoch: 7 [50560/60032 (84%)]\tLoss: 0.013595\n",
      "Train Epoch: 7 [51200/60032 (85%)]\tLoss: 0.170098\n",
      "Train Epoch: 7 [51840/60032 (86%)]\tLoss: 0.023411\n",
      "Train Epoch: 7 [52480/60032 (87%)]\tLoss: 0.015354\n",
      "Train Epoch: 7 [53120/60032 (88%)]\tLoss: 0.091161\n",
      "Train Epoch: 7 [53760/60032 (90%)]\tLoss: 0.050284\n",
      "Train Epoch: 7 [54400/60032 (91%)]\tLoss: 0.018390\n",
      "Train Epoch: 7 [55040/60032 (92%)]\tLoss: 0.030622\n",
      "Train Epoch: 7 [55680/60032 (93%)]\tLoss: 0.006012\n",
      "Train Epoch: 7 [56320/60032 (94%)]\tLoss: 0.114261\n",
      "Train Epoch: 7 [56960/60032 (95%)]\tLoss: 0.112863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [57600/60032 (96%)]\tLoss: 0.221846\n",
      "Train Epoch: 7 [58240/60032 (97%)]\tLoss: 0.014029\n",
      "Train Epoch: 7 [58880/60032 (98%)]\tLoss: 0.100463\n",
      "Train Epoch: 7 [59520/60032 (99%)]\tLoss: 0.047687\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9851/10000 (99%)\n",
      "\n",
      "Train Epoch: 8 [0/60032 (0%)]\tLoss: 0.053752\n",
      "Train Epoch: 8 [640/60032 (1%)]\tLoss: 0.004248\n",
      "Train Epoch: 8 [1280/60032 (2%)]\tLoss: 0.007790\n",
      "Train Epoch: 8 [1920/60032 (3%)]\tLoss: 0.044958\n",
      "Train Epoch: 8 [2560/60032 (4%)]\tLoss: 0.001204\n",
      "Train Epoch: 8 [3200/60032 (5%)]\tLoss: 0.050965\n",
      "Train Epoch: 8 [3840/60032 (6%)]\tLoss: 0.075709\n",
      "Train Epoch: 8 [4480/60032 (7%)]\tLoss: 0.048470\n",
      "Train Epoch: 8 [5120/60032 (9%)]\tLoss: 0.082573\n",
      "Train Epoch: 8 [5760/60032 (10%)]\tLoss: 0.115793\n",
      "Train Epoch: 8 [6400/60032 (11%)]\tLoss: 0.012298\n",
      "Train Epoch: 8 [7040/60032 (12%)]\tLoss: 0.014983\n",
      "Train Epoch: 8 [7680/60032 (13%)]\tLoss: 0.016053\n",
      "Train Epoch: 8 [8320/60032 (14%)]\tLoss: 0.134600\n",
      "Train Epoch: 8 [8960/60032 (15%)]\tLoss: 0.007649\n",
      "Train Epoch: 8 [9600/60032 (16%)]\tLoss: 0.015175\n",
      "Train Epoch: 8 [10240/60032 (17%)]\tLoss: 0.014006\n",
      "Train Epoch: 8 [10880/60032 (18%)]\tLoss: 0.029118\n",
      "Train Epoch: 8 [11520/60032 (19%)]\tLoss: 0.004045\n",
      "Train Epoch: 8 [12160/60032 (20%)]\tLoss: 0.034717\n",
      "Train Epoch: 8 [12800/60032 (21%)]\tLoss: 0.007049\n",
      "Train Epoch: 8 [13440/60032 (22%)]\tLoss: 0.062399\n",
      "Train Epoch: 8 [14080/60032 (23%)]\tLoss: 0.081743\n",
      "Train Epoch: 8 [14720/60032 (25%)]\tLoss: 0.009105\n",
      "Train Epoch: 8 [15360/60032 (26%)]\tLoss: 0.012445\n",
      "Train Epoch: 8 [16000/60032 (27%)]\tLoss: 0.007545\n",
      "Train Epoch: 8 [16640/60032 (28%)]\tLoss: 0.001913\n",
      "Train Epoch: 8 [17280/60032 (29%)]\tLoss: 0.024648\n",
      "Train Epoch: 8 [17920/60032 (30%)]\tLoss: 0.009592\n",
      "Train Epoch: 8 [18560/60032 (31%)]\tLoss: 0.031866\n",
      "Train Epoch: 8 [19200/60032 (32%)]\tLoss: 0.026506\n",
      "Train Epoch: 8 [19840/60032 (33%)]\tLoss: 0.129172\n",
      "Train Epoch: 8 [20480/60032 (34%)]\tLoss: 0.026511\n",
      "Train Epoch: 8 [21120/60032 (35%)]\tLoss: 0.003323\n",
      "Train Epoch: 8 [21760/60032 (36%)]\tLoss: 0.035243\n",
      "Train Epoch: 8 [22400/60032 (37%)]\tLoss: 0.039328\n",
      "Train Epoch: 8 [23040/60032 (38%)]\tLoss: 0.118567\n",
      "Train Epoch: 8 [23680/60032 (39%)]\tLoss: 0.007567\n",
      "Train Epoch: 8 [24320/60032 (41%)]\tLoss: 0.008309\n",
      "Train Epoch: 8 [24960/60032 (42%)]\tLoss: 0.092179\n",
      "Train Epoch: 8 [25600/60032 (43%)]\tLoss: 0.229340\n",
      "Train Epoch: 8 [26240/60032 (44%)]\tLoss: 0.027525\n",
      "Train Epoch: 8 [26880/60032 (45%)]\tLoss: 0.003655\n",
      "Train Epoch: 8 [27520/60032 (46%)]\tLoss: 0.004638\n",
      "Train Epoch: 8 [28160/60032 (47%)]\tLoss: 0.033448\n",
      "Train Epoch: 8 [28800/60032 (48%)]\tLoss: 0.003785\n",
      "Train Epoch: 8 [29440/60032 (49%)]\tLoss: 0.025289\n",
      "Train Epoch: 8 [30080/60032 (50%)]\tLoss: 0.053276\n",
      "Train Epoch: 8 [30720/60032 (51%)]\tLoss: 0.046213\n",
      "Train Epoch: 8 [31360/60032 (52%)]\tLoss: 0.023914\n",
      "Train Epoch: 8 [32000/60032 (53%)]\tLoss: 0.135218\n",
      "Train Epoch: 8 [32640/60032 (54%)]\tLoss: 0.008315\n",
      "Train Epoch: 8 [33280/60032 (55%)]\tLoss: 0.028188\n",
      "Train Epoch: 8 [33920/60032 (57%)]\tLoss: 0.058153\n",
      "Train Epoch: 8 [34560/60032 (58%)]\tLoss: 0.023487\n",
      "Train Epoch: 8 [35200/60032 (59%)]\tLoss: 0.050560\n",
      "Train Epoch: 8 [35840/60032 (60%)]\tLoss: 0.031390\n",
      "Train Epoch: 8 [36480/60032 (61%)]\tLoss: 0.040343\n",
      "Train Epoch: 8 [37120/60032 (62%)]\tLoss: 0.033898\n",
      "Train Epoch: 8 [37760/60032 (63%)]\tLoss: 0.052640\n",
      "Train Epoch: 8 [38400/60032 (64%)]\tLoss: 0.014625\n",
      "Train Epoch: 8 [39040/60032 (65%)]\tLoss: 0.012863\n",
      "Train Epoch: 8 [39680/60032 (66%)]\tLoss: 0.009710\n",
      "Train Epoch: 8 [40320/60032 (67%)]\tLoss: 0.046061\n",
      "Train Epoch: 8 [40960/60032 (68%)]\tLoss: 0.148319\n",
      "Train Epoch: 8 [41600/60032 (69%)]\tLoss: 0.048791\n",
      "Train Epoch: 8 [42240/60032 (70%)]\tLoss: 0.044280\n",
      "Train Epoch: 8 [42880/60032 (71%)]\tLoss: 0.002947\n",
      "Train Epoch: 8 [43520/60032 (72%)]\tLoss: 0.076791\n",
      "Train Epoch: 8 [44160/60032 (74%)]\tLoss: 0.017610\n",
      "Train Epoch: 8 [44800/60032 (75%)]\tLoss: 0.183218\n",
      "Train Epoch: 8 [45440/60032 (76%)]\tLoss: 0.030100\n",
      "Train Epoch: 8 [46080/60032 (77%)]\tLoss: 0.062100\n",
      "Train Epoch: 8 [46720/60032 (78%)]\tLoss: 0.123763\n",
      "Train Epoch: 8 [47360/60032 (79%)]\tLoss: 0.011085\n",
      "Train Epoch: 8 [48000/60032 (80%)]\tLoss: 0.032007\n",
      "Train Epoch: 8 [48640/60032 (81%)]\tLoss: 0.156049\n",
      "Train Epoch: 8 [49280/60032 (82%)]\tLoss: 0.004451\n",
      "Train Epoch: 8 [49920/60032 (83%)]\tLoss: 0.015565\n",
      "Train Epoch: 8 [50560/60032 (84%)]\tLoss: 0.060283\n",
      "Train Epoch: 8 [51200/60032 (85%)]\tLoss: 0.038335\n",
      "Train Epoch: 8 [51840/60032 (86%)]\tLoss: 0.039826\n",
      "Train Epoch: 8 [52480/60032 (87%)]\tLoss: 0.092250\n",
      "Train Epoch: 8 [53120/60032 (88%)]\tLoss: 0.004145\n",
      "Train Epoch: 8 [53760/60032 (90%)]\tLoss: 0.051536\n",
      "Train Epoch: 8 [54400/60032 (91%)]\tLoss: 0.017799\n",
      "Train Epoch: 8 [55040/60032 (92%)]\tLoss: 0.040607\n",
      "Train Epoch: 8 [55680/60032 (93%)]\tLoss: 0.012474\n",
      "Train Epoch: 8 [56320/60032 (94%)]\tLoss: 0.044304\n",
      "Train Epoch: 8 [56960/60032 (95%)]\tLoss: 0.007339\n",
      "Train Epoch: 8 [57600/60032 (96%)]\tLoss: 0.121681\n",
      "Train Epoch: 8 [58240/60032 (97%)]\tLoss: 0.003468\n",
      "Train Epoch: 8 [58880/60032 (98%)]\tLoss: 0.006786\n",
      "Train Epoch: 8 [59520/60032 (99%)]\tLoss: 0.184568\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9887/10000 (99%)\n",
      "\n",
      "Train Epoch: 9 [0/60032 (0%)]\tLoss: 0.005472\n",
      "Train Epoch: 9 [640/60032 (1%)]\tLoss: 0.136337\n",
      "Train Epoch: 9 [1280/60032 (2%)]\tLoss: 0.010678\n",
      "Train Epoch: 9 [1920/60032 (3%)]\tLoss: 0.013154\n",
      "Train Epoch: 9 [2560/60032 (4%)]\tLoss: 0.098899\n",
      "Train Epoch: 9 [3200/60032 (5%)]\tLoss: 0.028810\n",
      "Train Epoch: 9 [3840/60032 (6%)]\tLoss: 0.015352\n",
      "Train Epoch: 9 [4480/60032 (7%)]\tLoss: 0.010735\n",
      "Train Epoch: 9 [5120/60032 (9%)]\tLoss: 0.006416\n",
      "Train Epoch: 9 [5760/60032 (10%)]\tLoss: 0.023884\n",
      "Train Epoch: 9 [6400/60032 (11%)]\tLoss: 0.008634\n",
      "Train Epoch: 9 [7040/60032 (12%)]\tLoss: 0.047315\n",
      "Train Epoch: 9 [7680/60032 (13%)]\tLoss: 0.074640\n",
      "Train Epoch: 9 [8320/60032 (14%)]\tLoss: 0.017081\n",
      "Train Epoch: 9 [8960/60032 (15%)]\tLoss: 0.012029\n",
      "Train Epoch: 9 [9600/60032 (16%)]\tLoss: 0.013261\n",
      "Train Epoch: 9 [10240/60032 (17%)]\tLoss: 0.025641\n",
      "Train Epoch: 9 [10880/60032 (18%)]\tLoss: 0.026196\n",
      "Train Epoch: 9 [11520/60032 (19%)]\tLoss: 0.083312\n",
      "Train Epoch: 9 [12160/60032 (20%)]\tLoss: 0.196616\n",
      "Train Epoch: 9 [12800/60032 (21%)]\tLoss: 0.024646\n",
      "Train Epoch: 9 [13440/60032 (22%)]\tLoss: 0.005109\n",
      "Train Epoch: 9 [14080/60032 (23%)]\tLoss: 0.025529\n",
      "Train Epoch: 9 [14720/60032 (25%)]\tLoss: 0.013453\n",
      "Train Epoch: 9 [15360/60032 (26%)]\tLoss: 0.034294\n",
      "Train Epoch: 9 [16000/60032 (27%)]\tLoss: 0.000800\n",
      "Train Epoch: 9 [16640/60032 (28%)]\tLoss: 0.017486\n",
      "Train Epoch: 9 [17280/60032 (29%)]\tLoss: 0.052314\n",
      "Train Epoch: 9 [17920/60032 (30%)]\tLoss: 0.033177\n",
      "Train Epoch: 9 [18560/60032 (31%)]\tLoss: 0.036974\n",
      "Train Epoch: 9 [19200/60032 (32%)]\tLoss: 0.027973\n",
      "Train Epoch: 9 [19840/60032 (33%)]\tLoss: 0.002680\n",
      "Train Epoch: 9 [20480/60032 (34%)]\tLoss: 0.012917\n",
      "Train Epoch: 9 [21120/60032 (35%)]\tLoss: 0.011769\n",
      "Train Epoch: 9 [21760/60032 (36%)]\tLoss: 0.013558\n",
      "Train Epoch: 9 [22400/60032 (37%)]\tLoss: 0.066672\n",
      "Train Epoch: 9 [23040/60032 (38%)]\tLoss: 0.008521\n",
      "Train Epoch: 9 [23680/60032 (39%)]\tLoss: 0.015918\n",
      "Train Epoch: 9 [24320/60032 (41%)]\tLoss: 0.012285\n",
      "Train Epoch: 9 [24960/60032 (42%)]\tLoss: 0.056162\n",
      "Train Epoch: 9 [25600/60032 (43%)]\tLoss: 0.046286\n",
      "Train Epoch: 9 [26240/60032 (44%)]\tLoss: 0.017365\n",
      "Train Epoch: 9 [26880/60032 (45%)]\tLoss: 0.048815\n",
      "Train Epoch: 9 [27520/60032 (46%)]\tLoss: 0.060587\n",
      "Train Epoch: 9 [28160/60032 (47%)]\tLoss: 0.030276\n",
      "Train Epoch: 9 [28800/60032 (48%)]\tLoss: 0.178376\n",
      "Train Epoch: 9 [29440/60032 (49%)]\tLoss: 0.011708\n",
      "Train Epoch: 9 [30080/60032 (50%)]\tLoss: 0.005529\n",
      "Train Epoch: 9 [30720/60032 (51%)]\tLoss: 0.007438\n",
      "Train Epoch: 9 [31360/60032 (52%)]\tLoss: 0.010696\n",
      "Train Epoch: 9 [32000/60032 (53%)]\tLoss: 0.003730\n",
      "Train Epoch: 9 [32640/60032 (54%)]\tLoss: 0.032761\n",
      "Train Epoch: 9 [33280/60032 (55%)]\tLoss: 0.077019\n",
      "Train Epoch: 9 [33920/60032 (57%)]\tLoss: 0.013414\n",
      "Train Epoch: 9 [34560/60032 (58%)]\tLoss: 0.034860\n",
      "Train Epoch: 9 [35200/60032 (59%)]\tLoss: 0.008503\n",
      "Train Epoch: 9 [35840/60032 (60%)]\tLoss: 0.038426\n",
      "Train Epoch: 9 [36480/60032 (61%)]\tLoss: 0.006379\n",
      "Train Epoch: 9 [37120/60032 (62%)]\tLoss: 0.082594\n",
      "Train Epoch: 9 [37760/60032 (63%)]\tLoss: 0.009510\n",
      "Train Epoch: 9 [38400/60032 (64%)]\tLoss: 0.014253\n",
      "Train Epoch: 9 [39040/60032 (65%)]\tLoss: 0.081987\n",
      "Train Epoch: 9 [39680/60032 (66%)]\tLoss: 0.005063\n",
      "Train Epoch: 9 [40320/60032 (67%)]\tLoss: 0.002966\n",
      "Train Epoch: 9 [40960/60032 (68%)]\tLoss: 0.023548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [41600/60032 (69%)]\tLoss: 0.009618\n",
      "Train Epoch: 9 [42240/60032 (70%)]\tLoss: 0.054172\n",
      "Train Epoch: 9 [42880/60032 (71%)]\tLoss: 0.017948\n",
      "Train Epoch: 9 [43520/60032 (72%)]\tLoss: 0.043156\n",
      "Train Epoch: 9 [44160/60032 (74%)]\tLoss: 0.006336\n",
      "Train Epoch: 9 [44800/60032 (75%)]\tLoss: 0.006354\n",
      "Train Epoch: 9 [45440/60032 (76%)]\tLoss: 0.063234\n",
      "Train Epoch: 9 [46080/60032 (77%)]\tLoss: 0.020687\n",
      "Train Epoch: 9 [46720/60032 (78%)]\tLoss: 0.011056\n",
      "Train Epoch: 9 [47360/60032 (79%)]\tLoss: 0.116945\n",
      "Train Epoch: 9 [48000/60032 (80%)]\tLoss: 0.012899\n",
      "Train Epoch: 9 [48640/60032 (81%)]\tLoss: 0.046951\n",
      "Train Epoch: 9 [49280/60032 (82%)]\tLoss: 0.043778\n",
      "Train Epoch: 9 [49920/60032 (83%)]\tLoss: 0.009360\n",
      "Train Epoch: 9 [50560/60032 (84%)]\tLoss: 0.010767\n",
      "Train Epoch: 9 [51200/60032 (85%)]\tLoss: 0.019597\n",
      "Train Epoch: 9 [51840/60032 (86%)]\tLoss: 0.076839\n",
      "Train Epoch: 9 [52480/60032 (87%)]\tLoss: 0.009633\n",
      "Train Epoch: 9 [53120/60032 (88%)]\tLoss: 0.025454\n",
      "Train Epoch: 9 [53760/60032 (90%)]\tLoss: 0.002705\n",
      "Train Epoch: 9 [54400/60032 (91%)]\tLoss: 0.030396\n",
      "Train Epoch: 9 [55040/60032 (92%)]\tLoss: 0.015226\n",
      "Train Epoch: 9 [55680/60032 (93%)]\tLoss: 0.012619\n",
      "Train Epoch: 9 [56320/60032 (94%)]\tLoss: 0.060326\n",
      "Train Epoch: 9 [56960/60032 (95%)]\tLoss: 0.129614\n",
      "Train Epoch: 9 [57600/60032 (96%)]\tLoss: 0.008801\n",
      "Train Epoch: 9 [58240/60032 (97%)]\tLoss: 0.006621\n",
      "Train Epoch: 9 [58880/60032 (98%)]\tLoss: 0.063886\n",
      "Train Epoch: 9 [59520/60032 (99%)]\tLoss: 0.001234\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9882/10000 (99%)\n",
      "\n",
      "Train Epoch: 10 [0/60032 (0%)]\tLoss: 0.004721\n",
      "Train Epoch: 10 [640/60032 (1%)]\tLoss: 0.013141\n",
      "Train Epoch: 10 [1280/60032 (2%)]\tLoss: 0.011433\n",
      "Train Epoch: 10 [1920/60032 (3%)]\tLoss: 0.012325\n",
      "Train Epoch: 10 [2560/60032 (4%)]\tLoss: 0.007077\n",
      "Train Epoch: 10 [3200/60032 (5%)]\tLoss: 0.009795\n",
      "Train Epoch: 10 [3840/60032 (6%)]\tLoss: 0.038200\n",
      "Train Epoch: 10 [4480/60032 (7%)]\tLoss: 0.007969\n",
      "Train Epoch: 10 [5120/60032 (9%)]\tLoss: 0.023451\n",
      "Train Epoch: 10 [5760/60032 (10%)]\tLoss: 0.003120\n",
      "Train Epoch: 10 [6400/60032 (11%)]\tLoss: 0.058214\n",
      "Train Epoch: 10 [7040/60032 (12%)]\tLoss: 0.013669\n",
      "Train Epoch: 10 [7680/60032 (13%)]\tLoss: 0.122030\n",
      "Train Epoch: 10 [8320/60032 (14%)]\tLoss: 0.039214\n",
      "Train Epoch: 10 [8960/60032 (15%)]\tLoss: 0.028158\n",
      "Train Epoch: 10 [9600/60032 (16%)]\tLoss: 0.003801\n",
      "Train Epoch: 10 [10240/60032 (17%)]\tLoss: 0.009307\n",
      "Train Epoch: 10 [10880/60032 (18%)]\tLoss: 0.000631\n",
      "Train Epoch: 10 [11520/60032 (19%)]\tLoss: 0.038873\n",
      "Train Epoch: 10 [12160/60032 (20%)]\tLoss: 0.007543\n",
      "Train Epoch: 10 [12800/60032 (21%)]\tLoss: 0.003098\n",
      "Train Epoch: 10 [13440/60032 (22%)]\tLoss: 0.006524\n",
      "Train Epoch: 10 [14080/60032 (23%)]\tLoss: 0.014532\n",
      "Train Epoch: 10 [14720/60032 (25%)]\tLoss: 0.040785\n",
      "Train Epoch: 10 [15360/60032 (26%)]\tLoss: 0.003055\n",
      "Train Epoch: 10 [16000/60032 (27%)]\tLoss: 0.075643\n",
      "Train Epoch: 10 [16640/60032 (28%)]\tLoss: 0.154053\n",
      "Train Epoch: 10 [17280/60032 (29%)]\tLoss: 0.004782\n",
      "Train Epoch: 10 [17920/60032 (30%)]\tLoss: 0.017314\n",
      "Train Epoch: 10 [18560/60032 (31%)]\tLoss: 0.016672\n",
      "Train Epoch: 10 [19200/60032 (32%)]\tLoss: 0.067439\n",
      "Train Epoch: 10 [19840/60032 (33%)]\tLoss: 0.009421\n",
      "Train Epoch: 10 [20480/60032 (34%)]\tLoss: 0.008168\n",
      "Train Epoch: 10 [21120/60032 (35%)]\tLoss: 0.049734\n",
      "Train Epoch: 10 [21760/60032 (36%)]\tLoss: 0.026181\n",
      "Train Epoch: 10 [22400/60032 (37%)]\tLoss: 0.003982\n",
      "Train Epoch: 10 [23040/60032 (38%)]\tLoss: 0.268162\n",
      "Train Epoch: 10 [23680/60032 (39%)]\tLoss: 0.001006\n",
      "Train Epoch: 10 [24320/60032 (41%)]\tLoss: 0.024128\n",
      "Train Epoch: 10 [24960/60032 (42%)]\tLoss: 0.006776\n",
      "Train Epoch: 10 [25600/60032 (43%)]\tLoss: 0.040564\n",
      "Train Epoch: 10 [26240/60032 (44%)]\tLoss: 0.019175\n",
      "Train Epoch: 10 [26880/60032 (45%)]\tLoss: 0.005357\n",
      "Train Epoch: 10 [27520/60032 (46%)]\tLoss: 0.007280\n",
      "Train Epoch: 10 [28160/60032 (47%)]\tLoss: 0.056448\n",
      "Train Epoch: 10 [28800/60032 (48%)]\tLoss: 0.006641\n",
      "Train Epoch: 10 [29440/60032 (49%)]\tLoss: 0.009715\n",
      "Train Epoch: 10 [30080/60032 (50%)]\tLoss: 0.054553\n",
      "Train Epoch: 10 [30720/60032 (51%)]\tLoss: 0.004666\n",
      "Train Epoch: 10 [31360/60032 (52%)]\tLoss: 0.019343\n",
      "Train Epoch: 10 [32000/60032 (53%)]\tLoss: 0.040490\n",
      "Train Epoch: 10 [32640/60032 (54%)]\tLoss: 0.004274\n",
      "Train Epoch: 10 [33280/60032 (55%)]\tLoss: 0.126359\n",
      "Train Epoch: 10 [33920/60032 (57%)]\tLoss: 0.015985\n",
      "Train Epoch: 10 [34560/60032 (58%)]\tLoss: 0.044084\n",
      "Train Epoch: 10 [35200/60032 (59%)]\tLoss: 0.058329\n",
      "Train Epoch: 10 [35840/60032 (60%)]\tLoss: 0.006802\n",
      "Train Epoch: 10 [36480/60032 (61%)]\tLoss: 0.026162\n",
      "Train Epoch: 10 [37120/60032 (62%)]\tLoss: 0.003641\n",
      "Train Epoch: 10 [37760/60032 (63%)]\tLoss: 0.021088\n",
      "Train Epoch: 10 [38400/60032 (64%)]\tLoss: 0.021132\n",
      "Train Epoch: 10 [39040/60032 (65%)]\tLoss: 0.028337\n",
      "Train Epoch: 10 [39680/60032 (66%)]\tLoss: 0.036063\n",
      "Train Epoch: 10 [40320/60032 (67%)]\tLoss: 0.098198\n",
      "Train Epoch: 10 [40960/60032 (68%)]\tLoss: 0.081533\n",
      "Train Epoch: 10 [41600/60032 (69%)]\tLoss: 0.077212\n",
      "Train Epoch: 10 [42240/60032 (70%)]\tLoss: 0.016039\n",
      "Train Epoch: 10 [42880/60032 (71%)]\tLoss: 0.057290\n",
      "Train Epoch: 10 [43520/60032 (72%)]\tLoss: 0.017379\n",
      "Train Epoch: 10 [44160/60032 (74%)]\tLoss: 0.006091\n",
      "Train Epoch: 10 [44800/60032 (75%)]\tLoss: 0.004210\n",
      "Train Epoch: 10 [45440/60032 (76%)]\tLoss: 0.023904\n",
      "Train Epoch: 10 [46080/60032 (77%)]\tLoss: 0.004678\n",
      "Train Epoch: 10 [46720/60032 (78%)]\tLoss: 0.075535\n",
      "Train Epoch: 10 [47360/60032 (79%)]\tLoss: 0.020151\n",
      "Train Epoch: 10 [48000/60032 (80%)]\tLoss: 0.103490\n",
      "Train Epoch: 10 [48640/60032 (81%)]\tLoss: 0.012155\n",
      "Train Epoch: 10 [49280/60032 (82%)]\tLoss: 0.136908\n",
      "Train Epoch: 10 [49920/60032 (83%)]\tLoss: 0.043596\n",
      "Train Epoch: 10 [50560/60032 (84%)]\tLoss: 0.265230\n",
      "Train Epoch: 10 [51200/60032 (85%)]\tLoss: 0.010226\n",
      "Train Epoch: 10 [51840/60032 (86%)]\tLoss: 0.003450\n",
      "Train Epoch: 10 [52480/60032 (87%)]\tLoss: 0.022121\n",
      "Train Epoch: 10 [53120/60032 (88%)]\tLoss: 0.029348\n",
      "Train Epoch: 10 [53760/60032 (90%)]\tLoss: 0.127011\n",
      "Train Epoch: 10 [54400/60032 (91%)]\tLoss: 0.089324\n",
      "Train Epoch: 10 [55040/60032 (92%)]\tLoss: 0.064638\n",
      "Train Epoch: 10 [55680/60032 (93%)]\tLoss: 0.003559\n",
      "Train Epoch: 10 [56320/60032 (94%)]\tLoss: 0.015595\n",
      "Train Epoch: 10 [56960/60032 (95%)]\tLoss: 0.006612\n",
      "Train Epoch: 10 [57600/60032 (96%)]\tLoss: 0.010964\n",
      "Train Epoch: 10 [58240/60032 (97%)]\tLoss: 0.036587\n",
      "Train Epoch: 10 [58880/60032 (98%)]\tLoss: 0.134881\n",
      "Train Epoch: 10 [59520/60032 (99%)]\tLoss: 0.011405\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 9894/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, federated_train_loader, optimizer, epoch)\n",
    "    test(args, model, device, test_loader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Done!\n",
    "\n",
    "And voil√†! We now are training a real world Learning model using Federated Learning! As you observed, we modified 10 lines of code to upgrade the official Pytorch example on MNIST to a real Federated Learning setting!\n",
    "\n",
    "## Shortcomings of this Example\n",
    "\n",
    "Of course, there are dozen of improvements we could think of. We would like the computation to operate in parallel on the workers, to update the central model every `n` batches only, to reduce the number of messages we use to communicate between workers, etc.\n",
    "\n",
    "On the security side it still has some major shortcomings. Most notably, when we call `model.get()` and receive the updated model from Bob or Alice, we can actually learn a lot about Bob and Alice's training data by looking at their gradients. We could **average the gradient across multiple individuals before uploading it to the central server**, like we did in Part 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the repositories! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Pick our tutorials on GitHub!\n",
    "\n",
    "We made really nice tutorials to get a better understanding of what Federated and Privacy-Preserving Learning should look like and how we are building the bricks for this to happen.\n",
    "\n",
    "- [Checkout the PySyft tutorials](https://github.com/OpenMined/PySyft/tree/master/examples/tutorials)\n",
    "\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! \n",
    "\n",
    "- [Join slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! If you want to start \"one off\" mini-projects, you can go to PySyft GitHub Issues page and search for issues marked `Good First Issue`.\n",
    "\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "- [Donate through OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
