{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encrypted Inference using ResNet-18\n",
    "\n",
    "_Encrypted Machine Learning as a Service_ allows owners of sensitive data to use external AI services to get insights over their data. Let's consider a practical scenario where a data owner holds private images and would like to use a service to have those images labeled, without disclosing the images nor the label predictions, and without having to get access the model, which is often considered to be a business asset by such services and is therefore not accessible.\n",
    "\n",
    "To get a realistic example, we will consider [the task of distinguishing between bees and ants](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html), which uses a ResNet-18 model to achieve around 95% accuracy. We will not consider the training of such a model, as we assume the AI service provider has already trained it using some data. Instead, we will showcase how we can use PySyft to encrypt both the model and some images and to label those images in a fully private way.\n",
    "\n",
    "Author:\n",
    "- Théo Ryffel - Twitter: [@theoryffel](https://twitter.com/theoryffel) · GitHub: [@LaRiffle](https://github.com/LaRiffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Did you just say _encrypted_?\n",
    "\n",
    "First, let's try to understand what mechanisms we use to make the data and the model private. If you want to jump straight to the code, you can skip this section! \n",
    "$\\newcommand{\\shared}[1]{[\\![ #1 ]\\!]}$\n",
    "$\\newcommand{\\key}{\\mathsf{k}}$\n",
    "\n",
    "### Secret Sharing\n",
    "\n",
    "The cryptography protocol that we use to encrypt data is called Function Secret Sharing (FSS). It belongs to the family of [Secure Multi-Party Computation](https://blog.openmined.org/what-is-secure-multi-party-computation/) (SMPC) protocols, which involves several parties that share a secret to ensure privacy. A party alone holds a share of the private value and can't reconstruct the value, and a quorum of parties (sometimes all parties) need to collaborate to reconstruct the private data. Therefore, saying that we _encrypt_ the data is an abuse of language and we should say that we _secret share_ it.\n",
    "\n",
    "Other families of protocols exist like those based on Homomorphic Encryption, where data is truely encrypted and a party only needs a key to decrypt it. I recommend reading this [OpenMined blog](https://blog.openmined.org/what-is-homomorphic-encryption/) to learn more about Homomorphic Encryption.\n",
    "\n",
    "### Function Secret Sharing\n",
    "\n",
    "Unlike classical data secret sharing schemes like [SecureNN](https://eprint.iacr.org/2018/442.pdf) (which is also supported by PySyft), where a shared input $\\shared{x}$ is applied on a public $f$, function secret sharing applies a public input $x$ on a private shared function $\\shared{f}$. Shares or _keys_ $(\\shared{f}_0, \\shared{f}_1)$ of a function $f$ satisfy $f(x) = \\shared{f}_0(x) + \\shared{f}_1(x)$. Both approaches output a secret shared result.\n",
    "\n",
    "Let us take an example: say Alice and Bob respectively have shares $\\shared{y}_0$ and $\\shared{y}_1$ of a private input $y$, and they want to compute $\\shared{y \\ge 0}$.\n",
    "They receive some crypto material, namely each get a share of a random value (or _mask_) $\\shared{\\alpha}$ and a share of the shared function $\\shared{f_\\alpha}$ of $f_{\\alpha} : x \\rightarrow (x \\ge \\alpha)$.\n",
    "\n",
    "They first mask their shares of $\\shared{y}$ using $\\shared{\\alpha}$, by computing $\\shared{y}_0 + \\shared{\\alpha}_0$ and $\\shared{y}_1 + \\shared{\\alpha}_1$ and then revealing these values to reconstruct $x = y + \\alpha$. Next, they apply this public $x$ on their function shares $\\shared{f_\\alpha}_{j=0,1}$, to obtain a shared output $(\\shared{f_{\\alpha}}_0(x), \\shared{f_{\\alpha}}_1(x)) = \\shared{f_{\\alpha}(y + \\alpha)} = \\shared{(y + \\alpha) \\ge \\alpha} = \\shared{y \\ge 0}$. [Previous works on FSS](https://eprint.iacr.org/2018/707) have shown the existence of such function shares for comparison which perfectly hide $y$ and the result.\n",
    "\n",
    "For more details about how FSS can be implemented, [this article](https://arxiv.org/abs/2006.04593) details the FSS algorithms that we currently use in PySyft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Show me the code\n",
    "\n",
    "Enough explications, let's open the code!\n",
    "We will first load the data and the model and store them on the `data_owner` and the `model_owner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_num_threads(1) # We ask torch to use a single thread as we run async code which conflicts with multithreading\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "We download the data and load it on a `dataLoader` with small batches of size 2, to reduce the inference time and the memory pressure on the RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, download the dataset\n",
    "# You can comment out this cell if you have already downloaded the dataset\n",
    "!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "!unzip hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "data_dir = 'hymenoptera_data'\n",
    "image_dataset = datasets.ImageFolder('hymenoptera_data/val', data_transform)\n",
    "dataloader = torch.utils.data.DataLoader(image_dataset, batch_size=2, shuffle=True, num_workers=4)\n",
    "\n",
    "dataset_size = len(image_dataset)\n",
    "class_names = image_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to have a look at your data? Check the samples on [this tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "Now let's download the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can comment out this cell if you have already downloaded the model\n",
    "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-1_M81rMYoB1A8_nKXr0BBOwSIKXPp2v' -O resnet18_ants_bees.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_You can also download the file_ [from here](https://drive.google.com/file/d/1-1_M81rMYoB1A8_nKXr0BBOwSIKXPp2v/view?usp=sharing) _if the command above is not working._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "# Here the size of each output sample is set to 2.\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)\n",
    "state = torch.load(\"./resnet18_ants_bees.pt\", map_location='cpu')\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "# This is a small trick because these two consecutive operations can be switched without\n",
    "# changing the result but it reduces the number of comparisons we have to compute\n",
    "model.maxpool, model.relu = model.relu, model.maxpool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now we're ready to start!\n",
    "\n",
    "### Virtual Setup\n",
    "\n",
    "First let's create a virtual setup with 2 workers names `data_owner` and `model_owner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "\n",
    "hook = sy.TorchHook(torch) \n",
    "data_owner = sy.VirtualWorker(hook, id=\"data_owner\")\n",
    "model_owner = sy.VirtualWorker(hook, id=\"model_owner\")\n",
    "crypto_provider = sy.VirtualWorker(hook, id=\"crypto_provider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove compression to have faster communication, because compression time \n",
    "# is non-negligible: we send to workers crypto material which is very heavy\n",
    "# and pseudo-random, so compressing it takes a long time and isn't useful:\n",
    "# randomness can't be compressed, otherwise it wouldn't be random!\n",
    "from syft.serde.compression import NO_COMPRESSION\n",
    "sy.serde.compression.default_compress_scheme = NO_COMPRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put some data on the `data_owner` and the model on the `model_owner`. _In a real setting, the data and the model would already be located respectively on the two workers and we would just ask for pointers to these objects._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, true_labels = next(iter(dataloader))\n",
    "data_ptr = data.send(data_owner)\n",
    "\n",
    "# We store the true output of the model for comparison purpose\n",
    "true_prediction = model(data)\n",
    "model_ptr = model.send(model_owner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, when calling `.send()`, we only have access to pointers to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>[PointerTensor | me:85268602991 -> data_owner:95928743858]\n"
     ]
    }
   ],
   "source": [
    "print(data_ptr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encryption time!\n",
    "\n",
    "We will now encrypt both the model and the data. To do this, we encrypt them remotely using the pointers and get back the encrypted objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encryption_kwargs = dict(\n",
    "    workers=(data_owner, model_owner), # the workers holding shares of the secret-shared encrypted data\n",
    "    crypto_provider=crypto_provider, # a third party providing some cryptography primitives\n",
    "    protocol=\"fss\", # the name of the crypto protocol, fss stands for \"Function Secret Sharing\"\n",
    "    precision_fractional=4, # the encoding fixed precision (i.e. floats are truncated to the 4th decimal)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_data = data_ptr.encrypt(**encryption_kwargs).get()\n",
    "encrypted_model = model_ptr.encrypt(**encryption_kwargs).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secure inference\n",
    "We are now able to run our secure inference, so let's do it and let's compare it to the `true_labels` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/PySyft/syft/frameworks/torch/tensors/interpreters/additive_shared.py:122: UserWarning: Use dtype instead of field\n",
      "  warnings.warn(\"Use dtype instead of field\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313.13965487480164 seconds\n",
      "Predicted labels: tensor([0., 1.])\n",
      "     True labels: tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "encrypted_prediction = encrypted_model(encrypted_data)\n",
    "encrypted_labels = encrypted_prediction.argmax(dim=1)\n",
    "\n",
    "print(time.time() - start_time, \"seconds\")\n",
    "\n",
    "labels = encrypted_labels.decrypt()\n",
    "\n",
    "print(\"Predicted labels:\", labels)\n",
    "print(\"     True labels:\", true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hooray! This works!! Well at least with a probability of 95% which is the accuracy of the model.\n",
    "\n",
    "But is the computation _exactly_ the same than the plaintext model? Well not exactly, because we sometime use approximations, but let's open the model output logits to verify how close we are from plaintext execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0316, -0.3674],\n",
      "        [-1.3748,  2.0235]])\n",
      "tensor([[ 1.0112, -0.3442],\n",
      "        [-1.3962,  2.0563]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(encrypted_prediction.decrypt())\n",
    "print(true_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can observe, this is quite close and in practice the accuracy of the model is preserved, as you can observe by running inference over more images. The approximations mentioned are due to approximated layers such as BatchNorm and the fixed precision encoding.\n",
    "\n",
    "Regarding **runtime**, we manage to predict a batch of 2 images in ~400 seconds, which isn't super fast but is already reasonable for our usecase!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extension\n",
    "\n",
    "> Ok that's good, but in real life I won't use virtual workers!\n",
    "\n",
    "That's right, actually you can run exactly the same experiment using PyGrid and workers which live in a PrivateGridNetwork. Those workers are independent processes which can live on your machine or on remote machines.\n",
    "\n",
    "To do so, first clone [PyGrid](https://github.com/OpenMined/PyGrid) and then start new nodes in your terminal (one per tab) as such:\n",
    "```\n",
    "cd PyGrid/apps/node\n",
    "./run.sh --id data_owner      --port 7600 --host localhost --start_local_db\n",
    "./run.sh --id model_owner     --port 7601 --host localhost --start_local_db\n",
    "./run.sh --id crypto_provider --port 7602 --host localhost --start_local_db\n",
    "```\n",
    "\n",
    "And you replace the `syft` imports in this notebook as such:\n",
    "```\n",
    "import syft as sy\n",
    "from syft.grid.clients.data_centric_fl_client import DataCentricFLClient\n",
    "\n",
    "hook = sy.TorchHook(th)\n",
    "data_owner = DataCentricFLClient(hook, \"ws://localhost:7600\")\n",
    "model_owner = DataCentricFLClient(hook, \"ws://localhost:7601\")\n",
    "crypto_provider = DataCentricFLClient(hook, \"ws://localhost:7602\")\n",
    "\n",
    "my_grid = sy.PrivateGridNetwork(data_owner, model_owner, crypto_provider)\n",
    "\n",
    "sy.local_worker.object_store.garbage_delay = 1 # at time of writing, the garbage collection processus of remote values when using websockets should be batched to avoid sending too many messages to the remote workers. This is done by requesting the GC messages to be sent by batches every 1 second.\n",
    "\n",
    "```\n",
    "\n",
    "The computation will be exactly the same, and the runtime will roughtly double. You can run the experiment to verify this, and it's a nice intro to PyGrid! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. What's  next?\n",
    "\n",
    "Next is improving this first proof of concept! How can this be done?\n",
    "\n",
    "- First, we can optimize our implementation, for example by switching from Python to Rust.\n",
    "- Second, we can try to adapt the model structure or model layers to have a faster execution given our constraints without compromising accuracy. Think of the swap we made between maxpool and relu in the ResNet-18 architecture at thhe beginning.\n",
    "- Last, we can investigate new Function Secret Sharing crypto protocols, this is a new and promising field, we expect new breakthroughs to help us improving the inference time!\n",
    "\n",
    "### Join us!\n",
    "\n",
    "If you want to help, come and [apply to join one of our cryptography teams](https://forms.gle/BWmYQJrCwqe1m3ex5)!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "You can also help our community by starring the repositories! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! \n",
    "\n",
    "- [Join slack.openmined.org](http://slack.openmined.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Pysyft)",
   "language": "python",
   "name": "pysyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
