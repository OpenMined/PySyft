{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Intro to Federated Learning\n",
    "\n",
    "In the last section, we learned about PointerTensors, which create the underlying infrastructure we need for privacy preserving Deep Learning. In this section, we're going to see how to use these basic tools to implement our first privacy preserving deep learning algorithm, Federated Learning.\n",
    "\n",
    "### What is Federated Learning?\n",
    "\n",
    "It's a simple, powerful way to train Deep Learning models. If you think about training data, it's always the result of some sort of collection process. People (via devices) generate data by recording events in the real world. Normally, this data is aggregated to a single, central location so that you can train a machine learning model. Federated Learning turns this on its head!\n",
    "\n",
    "Instead of bringing training data to the model (a central server), you bring the model to the training data (wherever it may live).\n",
    "\n",
    "The idea is that this allows whoever is creating the data to own the only permanent copy, and thus maintain control over who ever has access to it. Pretty cool, eh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.1 - A Toy Federated Learning Example\n",
    "\n",
    "Let's start by training a toy model the centralized way. This is about a simple as models get. We first need:\n",
    "\n",
    "- a toy dataset\n",
    "- a model\n",
    "- some basic training logic for training a model to fit the data.\n",
    "\n",
    "Note: If this API is un-familiar to you - head on over to [fast.ai](http://fast.ai) and take their course before continuing in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook()\n",
    "from torch import nn, optim\n",
    "\n",
    "# A Toy Dataset\n",
    "data = sy.Var(sy.FloatTensor([[0,0],[0,1],[1,0],[1,1]]))\n",
    "target = sy.Var(sy.FloatTensor([[0],[0],[1],[1]]))\n",
    "\n",
    "# A Toy Model\n",
    "model = nn.Linear(2,1)\n",
    "\n",
    "def train():\n",
    "    # Training Logic\n",
    "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
    "    for iter in range(20):\n",
    "\n",
    "        # 1) erase previous gradients (if they exist)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 2) make a prediction\n",
    "        pred = model(data)\n",
    "\n",
    "        # 3) calculate how much the missed\n",
    "        loss = ((pred - target)**2).sum()\n",
    "\n",
    "        # 4) figure out which weights caused us to miss\n",
    "        loss.backward()\n",
    "\n",
    "        # 5) change those weights\n",
    "        opt.step()\n",
    "\n",
    "        # 6) print our progress\n",
    "        print(loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7935236096382141\n",
      "0.21217259764671326\n",
      "0.12388776242733002\n",
      "0.08632121235132217\n",
      "0.06183892861008644\n",
      "0.044710226356983185\n",
      "0.0325467512011528\n",
      "0.02383730188012123\n",
      "0.017555570229887962\n",
      "0.012994087301194668\n",
      "0.009660916402935982\n",
      "0.007211238145828247\n",
      "0.005401470698416233\n",
      "0.004058181773871183\n",
      "0.003056995337828994\n",
      "0.002308045281097293\n",
      "0.0017459806986153126\n",
      "0.0013229941250756383\n",
      "0.0010039110202342272\n",
      "0.0007627044105902314\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it! We've trained a basic model in the conventional manner. All our data is aggregated into our local machine and we can use it to make updates to our model. Federated Learning, however, doesn't work this way. So, let's modify this example to do it the Federated Learning way! \n",
    "\n",
    "So, what do we need:\n",
    "\n",
    "- create a couple workers\n",
    "- get pointers to training data on each worker\n",
    "- updated training logic to do federated learning\n",
    "\n",
    "    New Training Steps:\n",
    "    - send model to correct worker\n",
    "    - train on the data located there\n",
    "    - get the model back and repeat with next worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Worker bob already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker alice already exists. Replacing old worker which could cause unexpected behavior\n"
     ]
    }
   ],
   "source": [
    "# create a couple workers\n",
    "\n",
    "bob = sy.VirtualWorker(id=\"bob\")\n",
    "alice = sy.VirtualWorker(id=\"alice\")\n",
    "\n",
    "# get pointers to training data on each worker by\n",
    "# sending some training data to bob and alice\n",
    "data_bob = data[0:2].send(bob)\n",
    "target_bob = target[0:2].send(bob)\n",
    "\n",
    "data_alice = data[2:].send(alice)\n",
    "target_alice = target[2:].send(alice)\n",
    "\n",
    "# organize pointers into a list\n",
    "datasets = [(data_bob,target_bob),(data_alice,target_alice)]\n",
    "\n",
    "# Iniitalize A Toy Model\n",
    "model = nn.Linear(2,1)\n",
    "\n",
    "def train():\n",
    "    # Training Logic\n",
    "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
    "    for iter in range(20):\n",
    "        \n",
    "        # NEW) iterate through each worker's dataset\n",
    "        for data,target in datasets:\n",
    "            \n",
    "            # NEW) send model to correct worker\n",
    "            model.send(data.location)\n",
    "\n",
    "            # 1) erase previous gradients (if they exist)\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # 2) make a prediction\n",
    "            pred = model(data)\n",
    "\n",
    "            # 3) calculate how much the missed\n",
    "            loss = ((pred - target)**2).sum()\n",
    "\n",
    "            # 4) figure out which weights caused us to miss\n",
    "            loss.backward()\n",
    "\n",
    "            # NEW) get model (with gradients)\n",
    "            model.get()\n",
    "\n",
    "            # 5) change those weights\n",
    "            opt.step()\n",
    "\n",
    "            # 6) print our progress\n",
    "            print(loss.get().data[0]) # NEW) slight edit... need to call .get() on loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49059900641441345\n",
      "0.48288848996162415\n",
      "0.5257140398025513\n",
      "0.20319302380084991\n",
      "0.3727419972419739\n",
      "0.14014576375484467\n",
      "0.2604595422744751\n",
      "0.10102146863937378\n",
      "0.18343526124954224\n",
      "0.07364650815725327\n",
      "0.13027068972587585\n",
      "0.05409587174654007\n",
      "0.0932079330086708\n",
      "0.039966512471437454\n",
      "0.06712788343429565\n",
      "0.029659217223525047\n",
      "0.04862105846405029\n",
      "0.022084159776568413\n",
      "0.03538985550403595\n",
      "0.016484789550304413\n",
      "0.02586808241903782\n",
      "0.012327495031058788\n",
      "0.018976470455527306\n",
      "0.00923063326627016\n",
      "0.013963752426207066\n",
      "0.006918045692145824\n",
      "0.010302114300429821\n",
      "0.005188031122088432\n",
      "0.007617602590471506\n",
      "0.0038922084495425224\n",
      "0.005643305368721485\n",
      "0.002920745871961117\n",
      "0.004187457729130983\n",
      "0.0021920367144048214\n",
      "0.0031114660669118166\n",
      "0.0016452015843242407\n",
      "0.0023146746680140495\n",
      "0.0012347802985459566\n",
      "0.001723655266687274\n",
      "0.0009266938432119787\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Well Done!\n",
    "\n",
    "And voila! We now are training a very simple Deep Learning model using Federated Learning! We send the model to each worker, generate a new gradient, and then bring the gradient back to our local server where we update our global model. Never in this process do we ever see or request access to the underlying training data! We preserve the privacy of Bob and Alice!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.2 - "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
