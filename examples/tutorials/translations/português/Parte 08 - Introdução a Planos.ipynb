{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8 - Introdução a Planos\n",
    "\n",
    "\n",
    "### Contexto \n",
    "\n",
    "Aqui nós introduzimos um objeto que é crucial para escalar a indústria do Aprendizado Federado: Planos. Isso reduz dramaticamente o uso de largura de banda, permite esquecas assíncronos e da mais autonomia para dispositivos remotos. O conceiro original do plano pode ser encontrado no artigo [Towards Federated Learning at Scale: System Design](https://arxiv.org/pdf/1902.01046.pdf), porém com algumas adaptações para as nossas necessidades no PySyft.\n",
    "\n",
    "Um plano tem a intenção de salvar uma sequência de operações do torch, como uma função, mas permitindo enviar essa sequência de operações para um worker remoto e manter a referência para isso. Dessa forma, para computar remotamente essa sequência de $n$ operações em alguma entrada referenciada através desses ponteiros, ao invés de mandar $n$ mensagens, você precisa agora enviar apenas uma única mensagem com as referências para o plano e para os ponteiros. Você pode também fornecer tensores com sua funcão (que chamamos _state tensors_) para ter as funcionalidades extendidas. Planos podem ser vistos tanto como uma função que você pode enviar, como uma classe que também pode ser enviada e executada remotamente. Portanto, para usuários de nível alto, a noção de plano desaparece e é substituída por um recurso mágico que permite enviar para workers remotos funcões arbitrárias que contém uma sequência de funções torch.\n",
    "\n",
    "Uma coisa a se notar é que a classe de funções que você quer transformar em planos é atualmente limitada para sequências de operações _hook_ (gancho) do torch exclusivamente. Em particular, isso exclúi estruturas lógicas como `if`, `for` e `while`, mesmo que estejamos trabalhando para ter soluções em breve. _Para ser completamente preciso, você pode usá-los mas o caminho lógico que você toma (primeiro `if` é falso e 5 laços `for`, por exemplo) na primeira computação do seu plano será levado por todas as próximas computações, cujo queremos evitar nos casos majoritárias._\n",
    "\n",
    "Autores:\n",
    "- Théo Ryffel - Twitter [@theoryffel](https://twitter.com/theoryffel) - GitHub: [@LaRiffle](https://github.com/LaRiffle)\n",
    "- Bobby Wagner - Twitter [@bobbyawagner](https://twitter.com/bobbyawagner) - GitHub: [@robert-wagner](https://github.com/robert-wagner)\n",
    "- Marianne Monteiro - Twitter [@hereismari](https://twitter.com/hereismari) - GitHub: [@mari-linhares](https://github.com/mari-linhares)\t\n",
    "\n",
    "Tradutor:\n",
    "- João Lucas - GitHub: [@joaolcaas](https://github.com/joaolcaas) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importações e especificações do modelo\n",
    "\n",
    "Primeiro faremos as importações oficiais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E então aqueles específicos do PySyft, com uma importante observação: **o worker local não deve ser um worker cliente**. Workers não clientes podem salvar objetos e precisamos dessa habilidade para usar um plano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy  # importe a biblioteca PySyft\n",
    "hook = sy.TorchHook(torch)  # hook PyTorch ie adiciona funcionalidades extras \n",
    "\n",
    "# IMPORTANTE: worker local não deve ser um worker cliente\n",
    "hook.local_worker.is_client_worker = False\n",
    "\n",
    "\n",
    "server = hook.local_worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ser consistente com o conceito fornecido no artigo referenciado, definimos _workers_ remotos ou _devices_, provendo eles com algum dado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x11 = torch.tensor([-1, 2.]).tag('input_data')\n",
    "x12 = torch.tensor([1, -2.]).tag('input_data2')\n",
    "x21 = torch.tensor([-1, 2.]).tag('input_data')\n",
    "x22 = torch.tensor([1, -2.]).tag('input_data2')\n",
    "\n",
    "device_1 = sy.VirtualWorker(hook, id=\"device_1\", data=(x11, x12)) \n",
    "device_2 = sy.VirtualWorker(hook, id=\"device_2\", data=(x21, x22))\n",
    "devices = device_1, device_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo Básico\n",
    "\n",
    "Vamos definir uma função que queremos transformar em um plano. Para isso, apenas adiciona-se um _decorator_ acima da definição da função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.func2plan()\n",
    "def plan_double_abs(x):\n",
    "    x = x + x\n",
    "    x = torch.abs(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar.\n",
    "Sim, agora nós temos um plano!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Plan plan_double_abs id:89796211423 owner:me>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan_double_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para usar um plano, precisamos de duas coisas: contruir um plano _(i.e. registrar uma sequência de operações presentes na função)_ e mandar isso para um worker / device. Felizmente você pode fazer isso de uma maneira muito fácil.\n",
    "\n",
    "#### Construindo um plano\n",
    "\n",
    "Para construir um plano, você só precisa chamar isso em algum dado.\n",
    "\n",
    "Vamos primeiro pegar a referência para algum dado remoto: uma requisição é enviada para a rede e o ponteiro de referência é retornado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:33127708416 -> device_1:47766305516]\n",
       "\tTags: input_data \n",
       "\tShape: torch.Size([2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointer_to_data = device_1.search('input_data')[0]\n",
    "pointer_to_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se dissermos para um plano que ele deve ser executado remotamente no dispositivo `location:device_1`, isso retornará um erro pois o plano não foi construído ainda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan_double_abs.is_built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A plan needs to be built before being sent to a worker.\n"
     ]
    }
   ],
   "source": [
    "# Falha acontece se mandar um plano não construído\n",
    "try:\n",
    "    plan_double_abs.send(device_1)\n",
    "except RuntimeError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para construir um plano só precisamos chamar `build` no plano e passar os argumentos necessários para a execução de um plano (a.k.a algum dado). Quando um plano é construído, todos os comandos são executados sequencialmente por um worker local, capturados pelo plano e então salvos no atributo `readable_plan`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_double_abs.build(torch.tensor([1., -2.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan_double_abs.is_built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, se tentarmos enviar um plano, irá funcionar!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PointerPlan | me:85123354948 -> device_1:89796211423]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Essa célula é executada com sucesso\n",
    "pointer_plan = plan_double_abs.send(device_1)\n",
    "pointer_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como nos tensores, podemos ver um ponteiro para o objeto que foi enviado, sendo chamado simplesmente de `PointerPlan`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um lembrete importante é que, quando um plano é construído, nós de antemão pré-definimos os id(s) dos resultados das computações que devem ser salvos. Isso irá permitir enviar os comandos de forma assíncrona, já ter uma referência para de um resultado virtual e continuar as computações locais sem ter que esperar os resultados remotos serem computados. Uma aplicação importante é quando você exige a computação de um grupo no device_1 e não quer esperar pelo fim dessa computação para mandar outro grupo de computação para o device_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executando um Plano Remotamente\n",
    "\n",
    "Nós agora podemos executar um plano remotamente chamando um ponteiro para um plano com um ponteiro para algum dado. Isso emite um comando para executar esse plano remotamente, sendo assim, a localização pré-definida da saída do plano agora contém o resultado (lembrese que  pré-definimos a localização do resultado antes da computação). Isso também requer uma única rodada de comunicação.\n",
    "\n",
    "O resultado é simplesmente um ponteiro, assim como quando você chamada uma função do Torch enganchada (hooked)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer_to_result = pointer_plan(pointer_to_data)\n",
    "print(pointer_to_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E você pode simplesmentes pegar o valor de volta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer_to_result.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Towards a concrete example\n",
    "\n",
    "Mas nós queremos que Planos sejam aplicados para aprendizago profundi e federado, certo? Então vamos dar uma olhada em um exemplo ligeiramente mais complicado, usando rede neural que certamente você deve estar ansioso a usá-lo.\n",
    "Note que estamos agora transformando uma classe em um Plano. Para isso, nós herdamos a nossa classe de sy.Plan (ao invés de herdar de nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(sy.Plan):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3)\n",
    "        self.fc2 = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Net Net id:58498301328 owner:me>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos construir um plano usando dados de exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.build(torch.tensor([1., 2.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora enviamos o Plano para um worker remoto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PointerPlan | me:66386943511 -> device_1:58498301328]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointer_to_net = net.send(device_1)\n",
    "pointer_to_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos recuperar alguns dados remotos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer_to_data = device_1.search('input_data')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sintaxe é como a execução sequencial remota normal, isso é, como uma execução local. Mas comparada a uma execução remota clássica, tem apenas uma rodada de comunicação para cada execução."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:87899785368 -> device_1:93434720381]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointer_to_result = pointer_to_net(pointer_to_data)\n",
    "pointer_to_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E recebemos o resultado como sempre!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer_to_result.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! Nós vimos como dramaticamente reduzir a comunicação entre worker local (ou servidor) e os dispositivos remotos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troca entre Workers\n",
    "\n",
    "Uma característica importante que queremos ter é usar o mesmo plano para vários workers, que podemos mudar dependendo da parte remota do dado que estamos considerando.\n",
    "Em especial, não queremos reconstruir o plano cada vez que temos que mudar de worker. Vamos ver como fazer isso usando o exemplo visto ateriormente com a nossa pequena rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(sy.Plan):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3)\n",
    "        self.fc2 = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "\n",
    "# Build plan\n",
    "net.build(torch.tensor([1., 2.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui são executados os passos principais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer_to_net_1 = net.send(device_1)\n",
    "pointer_to_data = device_1.search('input_data')[0]\n",
    "pointer_to_result = pointer_to_net_1(pointer_to_data)\n",
    "pointer_to_result.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E você pode construir outro PointerPlan a partir do mesmo plano, sendo a sintaxe a mesma para executar o plano remotamente em outro dispositivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer_to_net_2 = net.send(device_2)\n",
    "pointer_to_data = device_2.search('input_data')[0]\n",
    "pointer_to_result = pointer_to_net_2(pointer_to_data)\n",
    "pointer_to_result.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Atualmente, com a classe Plano, você pode apenas usar um único método e tem que nomear como \"forward\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo planos automaticamente que são funcções\n",
    "\n",
    "Para funcões (`@` `sy.func2plan`) nós podemos automaticamente construir um plano sem e necessidada de explicitar a chamada do `build`, uma vez que no momento da criação o plano já é criado.\n",
    "\n",
    "Para usar essa funcionalidade a única coisa que você deve mudar na criação do plano é definir um argumento para o decorator chamado `args_shape` que deve ser uma lista que contém os shapes de cada argumento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.func2plan(args_shape=[(-1, 1)])\n",
    "def plan_double_abs(x):\n",
    "    x = x + x\n",
    "    x = torch.abs(x)\n",
    "    return x\n",
    "\n",
    "plan_double_abs.is_built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O parâmetro `args_shape` é usado internamente para criar tensores de exemplo com o formato em qual foi passado para construir o plano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.func2plan(args_shape=[(1, 2), (-1, 2)])\n",
    "def plan_sum_abs(x, y):\n",
    "    s = x + y\n",
    "    return torch.abs(s)\n",
    "\n",
    "plan_sum_abs.is_built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também é possível prover\n",
    "You can also provide state elements to functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.func2plan(args_shape=[(1,)], state=(torch.tensor([1]), ))\n",
    "def plan_abs(x, state):\n",
    "    bias, = state.read()\n",
    "    x = x.abs()\n",
    "    return x + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointer_plan = plan_abs.send(device_1)\n",
    "x_ptr = torch.tensor([-1, 0]).send(device_1)\n",
    "p = pointer_plan(x_ptr)\n",
    "p.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para aprender mais sobre isso, você pode descobrir como usar Planos com Protocolos no tutorial Part 8 bis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dê-nos uma estrela em nosso repo do PySyft no GitHub\n",
    "\n",
    "A maneira mais fácil de ajudar nossa comunidade é adicionando uma estrela nos nossos repositórios! Isso ajuda a aumentar a conscientização sobre essas ferramentas legais que estamos construindo.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Veja nossos tutoriais no GitHub!\n",
    "\n",
    "Fizemos tutoriais muito bons para entender melhor como deve ser a Aprendizagem Federada e a proteção de Privacidade, e como estamos construindo as coisas básicas que precisamos para fazer com que isso aconteça.\n",
    "\n",
    "- [Tutoriais do PySyft](https://github.com/OpenMined/PySyft/tree/master/examples/tutorials)\n",
    "\n",
    "### Junte-se ao Slack!\n",
    "\n",
    "A melhor maneira de manter-se atualizado sobre os últimos avanços é se juntar à nossa comunidade! \n",
    "\n",
    "- [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Contribua com o projeto!\n",
    "\n",
    "A melhor maneira de contribuir para a nossa comunidade é se tornando um contribuidor do código! A qualquer momento, você pode acessar a página de *Issues* (problemas) do PySyft no GitHub e filtrar por \"Projetos\". Isso mostrará todas as etiquetas (tags) na parte superior, com uma visão geral de quais projetos você pode participar! Se você não deseja ingressar em um projeto, mas gostaria de codificar um pouco, também pode procurar mais mini-projetos \"independentes\" pesquisando problemas no GitHub marcados como \"good first issue\".\n",
    "\n",
    "- [Etiquetados como Good First Issue](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Doar\n",
    "\n",
    "Se você não tem tempo para contribuir com nossa base de códigos, mas ainda deseja nos apoiar, também pode se tornar um Apoiador em nosso Open Collective. Todas as doações vão para hospedagem na web e outras despesas da comunidade, como hackathons e meetups!\n",
    "\n",
    "[Página do Open Collective do OpenMined](https://opencollective.com/openmined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
