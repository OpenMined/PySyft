{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 7 - Aprendizado federado com Base de Dados Federado\n",
    "\n",
    "Aqui introduzimos uma nova ferramenta para o uso de base de dados (datasets) federados. Nós criamos a classe `FederatedDataset`cuja intenção é ser usada como a classe Dataset do Pytorch, que é repassado para um carregador de dados federado `FederatedDataLoader` no qual irá iterar sobre isso em um estilo federado\n",
    "\n",
    "Autores:\n",
    "- Andrew Trask - Twitter: [@iamtrask](https://twitter.com/iamtrask)\n",
    "- Théo Ryffel - GitHub: [@LaRiffle](https://github.com/LaRiffle)\n",
    "\n",
    "Tradutor:\n",
    "- João Lucas - GitHub: [@joaolcaas](https://github.com/joaolcaas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos o sandbox que foi discutido no último tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import syft as sy\n",
    "sy.create_sandbox(globals(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procure pelo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_data = grid.search(\"#boston\", \"#data\")\n",
    "boston_target = grid.search(\"#boston\", \"#target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carrega-se um modelo e um otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = boston_data['alice'][0].shape[1]\n",
    "n_targets = 1\n",
    "\n",
    "model = th.nn.Linear(n_features, n_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui carregaremos os dados buscados em um `FederatedDataset`. Olhe os workers contém parte dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bob', 'theo', 'jason', 'alice', 'andy', 'jon']\n"
     ]
    }
   ],
   "source": [
    "# Cast the result in BaseDatasets\n",
    "datasets = []\n",
    "for worker in boston_data.keys():\n",
    "    dataset = sy.BaseDataset(boston_data[worker][0], boston_target[worker][0])\n",
    "    datasets.append(dataset)\n",
    "\n",
    "# Build the FederatedDataset object\n",
    "dataset = sy.FederatedDataset(datasets)\n",
    "print(dataset.workers)\n",
    "optimizers = {}\n",
    "for worker in dataset.workers:\n",
    "    optimizers[worker] = th.optim.Adam(params=model.parameters(),lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colocamos os dados em um `FederatedDataLoader` juntamente com informações específicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = sy.FederatedDataLoader(dataset, batch_size=32, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E finalmente nós iteramos sobre as épocas. É visível a similaridae quando comparado com o trainamento do PyTorch puro e local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/16 (0%)]\tBatch loss: 748.940247\n",
      "Train Epoch: 1 [8/16 (50%)]\tBatch loss: 51.151550\n",
      "Train Epoch: 1 [16/16 (100%)]\tBatch loss: 1034.458130\n",
      "Total loss 16811.58366394043\n",
      "Train Epoch: 2 [0/16 (0%)]\tBatch loss: 2326.067871\n",
      "Train Epoch: 2 [8/16 (50%)]\tBatch loss: 51.231117\n",
      "Train Epoch: 2 [16/16 (100%)]\tBatch loss: 1064.509888\n",
      "Total loss 18240.483978271484\n",
      "Train Epoch: 3 [0/16 (0%)]\tBatch loss: 1244.071533\n",
      "Train Epoch: 3 [8/16 (50%)]\tBatch loss: 67.221535\n",
      "Train Epoch: 3 [16/16 (100%)]\tBatch loss: 523.781494\n",
      "Total loss 9836.87712097168\n",
      "Train Epoch: 4 [0/16 (0%)]\tBatch loss: 1256.591309\n",
      "Train Epoch: 4 [8/16 (50%)]\tBatch loss: 56.861378\n",
      "Train Epoch: 4 [16/16 (100%)]\tBatch loss: 349.512665\n",
      "Total loss 7150.3307456970215\n",
      "Train Epoch: 5 [0/16 (0%)]\tBatch loss: 769.756470\n",
      "Train Epoch: 5 [8/16 (50%)]\tBatch loss: 43.537251\n",
      "Train Epoch: 5 [16/16 (100%)]\tBatch loss: 227.863876\n",
      "Total loss 4574.713926315308\n",
      "Train Epoch: 6 [0/16 (0%)]\tBatch loss: 385.063324\n",
      "Train Epoch: 6 [8/16 (50%)]\tBatch loss: 48.292015\n",
      "Train Epoch: 6 [16/16 (100%)]\tBatch loss: 181.630203\n",
      "Total loss 2743.4373445510864\n",
      "Train Epoch: 7 [0/16 (0%)]\tBatch loss: 224.823502\n",
      "Train Epoch: 7 [8/16 (50%)]\tBatch loss: 64.853928\n",
      "Train Epoch: 7 [16/16 (100%)]\tBatch loss: 131.345764\n",
      "Total loss 1835.2554845809937\n",
      "Train Epoch: 8 [0/16 (0%)]\tBatch loss: 77.025970\n",
      "Train Epoch: 8 [8/16 (50%)]\tBatch loss: 59.944653\n",
      "Train Epoch: 8 [16/16 (100%)]\tBatch loss: 85.479057\n",
      "Total loss 1410.7403211593628\n",
      "Train Epoch: 9 [0/16 (0%)]\tBatch loss: 32.187149\n",
      "Train Epoch: 9 [8/16 (50%)]\tBatch loss: 66.540588\n",
      "Train Epoch: 9 [16/16 (100%)]\tBatch loss: 67.327797\n",
      "Total loss 1403.7436208724976\n",
      "Train Epoch: 10 [0/16 (0%)]\tBatch loss: 49.208336\n",
      "Train Epoch: 10 [8/16 (50%)]\tBatch loss: 71.933121\n",
      "Train Epoch: 10 [16/16 (100%)]\tBatch loss: 40.383926\n",
      "Total loss 1414.2987146377563\n",
      "Train Epoch: 11 [0/16 (0%)]\tBatch loss: 96.806885\n",
      "Train Epoch: 11 [8/16 (50%)]\tBatch loss: 66.887230\n",
      "Train Epoch: 11 [16/16 (100%)]\tBatch loss: 23.606499\n",
      "Total loss 1517.43394947052\n",
      "Train Epoch: 12 [0/16 (0%)]\tBatch loss: 141.650146\n",
      "Train Epoch: 12 [8/16 (50%)]\tBatch loss: 64.342293\n",
      "Train Epoch: 12 [16/16 (100%)]\tBatch loss: 16.087069\n",
      "Total loss 1607.7717590332031\n",
      "Train Epoch: 13 [0/16 (0%)]\tBatch loss: 162.539032\n",
      "Train Epoch: 13 [8/16 (50%)]\tBatch loss: 60.681587\n",
      "Train Epoch: 13 [16/16 (100%)]\tBatch loss: 10.701948\n",
      "Total loss 1589.0451583862305\n",
      "Train Epoch: 14 [0/16 (0%)]\tBatch loss: 167.777802\n",
      "Train Epoch: 14 [8/16 (50%)]\tBatch loss: 56.364094\n",
      "Train Epoch: 14 [16/16 (100%)]\tBatch loss: 9.705211\n",
      "Total loss 1523.6869611740112\n",
      "Train Epoch: 15 [0/16 (0%)]\tBatch loss: 154.180344\n",
      "Train Epoch: 15 [8/16 (50%)]\tBatch loss: 53.738117\n",
      "Train Epoch: 15 [16/16 (100%)]\tBatch loss: 12.191919\n",
      "Total loss 1409.8249435424805\n",
      "Train Epoch: 16 [0/16 (0%)]\tBatch loss: 128.859604\n",
      "Train Epoch: 16 [8/16 (50%)]\tBatch loss: 51.555077\n",
      "Train Epoch: 16 [16/16 (100%)]\tBatch loss: 17.718491\n",
      "Total loss 1268.7462015151978\n",
      "Train Epoch: 17 [0/16 (0%)]\tBatch loss: 100.839447\n",
      "Train Epoch: 17 [8/16 (50%)]\tBatch loss: 49.826363\n",
      "Train Epoch: 17 [16/16 (100%)]\tBatch loss: 23.923746\n",
      "Total loss 1143.4673614501953\n",
      "Train Epoch: 18 [0/16 (0%)]\tBatch loss: 73.683601\n",
      "Train Epoch: 18 [8/16 (50%)]\tBatch loss: 49.014385\n",
      "Train Epoch: 18 [16/16 (100%)]\tBatch loss: 28.588547\n",
      "Total loss 1047.6732873916626\n",
      "Train Epoch: 19 [0/16 (0%)]\tBatch loss: 52.247581\n",
      "Train Epoch: 19 [8/16 (50%)]\tBatch loss: 48.720665\n",
      "Train Epoch: 19 [16/16 (100%)]\tBatch loss: 30.892593\n",
      "Total loss 985.6572570800781\n",
      "Train Epoch: 20 [0/16 (0%)]\tBatch loss: 38.464764\n",
      "Train Epoch: 20 [8/16 (50%)]\tBatch loss: 48.995556\n",
      "Train Epoch: 20 [16/16 (100%)]\tBatch loss: 30.813383\n",
      "Total loss 951.0123882293701\n",
      "Train Epoch: 21 [0/16 (0%)]\tBatch loss: 31.169998\n",
      "Train Epoch: 21 [8/16 (50%)]\tBatch loss: 49.894032\n",
      "Train Epoch: 21 [16/16 (100%)]\tBatch loss: 29.537027\n",
      "Total loss 930.7954607009888\n",
      "Train Epoch: 22 [0/16 (0%)]\tBatch loss: 28.120176\n",
      "Train Epoch: 22 [8/16 (50%)]\tBatch loss: 51.114582\n",
      "Train Epoch: 22 [16/16 (100%)]\tBatch loss: 27.934805\n",
      "Total loss 913.8161697387695\n",
      "Train Epoch: 23 [0/16 (0%)]\tBatch loss: 27.056038\n",
      "Train Epoch: 23 [8/16 (50%)]\tBatch loss: 52.476173\n",
      "Train Epoch: 23 [16/16 (100%)]\tBatch loss: 26.340906\n",
      "Total loss 893.6044769287109\n",
      "Train Epoch: 24 [0/16 (0%)]\tBatch loss: 26.791210\n",
      "Train Epoch: 24 [8/16 (50%)]\tBatch loss: 53.599285\n",
      "Train Epoch: 24 [16/16 (100%)]\tBatch loss: 24.862741\n",
      "Total loss 869.2549285888672\n",
      "Train Epoch: 25 [0/16 (0%)]\tBatch loss: 27.164413\n",
      "Train Epoch: 25 [8/16 (50%)]\tBatch loss: 54.057549\n",
      "Train Epoch: 25 [16/16 (100%)]\tBatch loss: 23.472359\n",
      "Total loss 843.9724135398865\n",
      "Train Epoch: 26 [0/16 (0%)]\tBatch loss: 28.418194\n",
      "Train Epoch: 26 [8/16 (50%)]\tBatch loss: 53.697834\n",
      "Train Epoch: 26 [16/16 (100%)]\tBatch loss: 22.140163\n",
      "Total loss 822.2668995857239\n",
      "Train Epoch: 27 [0/16 (0%)]\tBatch loss: 30.702930\n",
      "Train Epoch: 27 [8/16 (50%)]\tBatch loss: 52.577049\n",
      "Train Epoch: 27 [16/16 (100%)]\tBatch loss: 20.839071\n",
      "Total loss 807.633846282959\n",
      "Train Epoch: 28 [0/16 (0%)]\tBatch loss: 33.891994\n",
      "Train Epoch: 28 [8/16 (50%)]\tBatch loss: 50.945702\n",
      "Train Epoch: 28 [16/16 (100%)]\tBatch loss: 19.562260\n",
      "Total loss 801.3516693115234\n",
      "Train Epoch: 29 [0/16 (0%)]\tBatch loss: 37.581245\n",
      "Train Epoch: 29 [8/16 (50%)]\tBatch loss: 49.120327\n",
      "Train Epoch: 29 [16/16 (100%)]\tBatch loss: 18.349764\n",
      "Total loss 802.220639705658\n",
      "Train Epoch: 30 [0/16 (0%)]\tBatch loss: 41.211853\n",
      "Train Epoch: 30 [8/16 (50%)]\tBatch loss: 47.340645\n",
      "Train Epoch: 30 [16/16 (100%)]\tBatch loss: 17.288702\n",
      "Total loss 807.4572710990906\n",
      "Train Epoch: 31 [0/16 (0%)]\tBatch loss: 44.216698\n",
      "Train Epoch: 31 [8/16 (50%)]\tBatch loss: 45.764854\n",
      "Train Epoch: 31 [16/16 (100%)]\tBatch loss: 16.472401\n",
      "Total loss 813.9849834442139\n",
      "Train Epoch: 32 [0/16 (0%)]\tBatch loss: 46.167980\n",
      "Train Epoch: 32 [8/16 (50%)]\tBatch loss: 44.490604\n",
      "Train Epoch: 32 [16/16 (100%)]\tBatch loss: 15.944544\n",
      "Total loss 819.4114699363708\n",
      "Train Epoch: 33 [0/16 (0%)]\tBatch loss: 46.898613\n",
      "Train Epoch: 33 [8/16 (50%)]\tBatch loss: 43.579147\n",
      "Train Epoch: 33 [16/16 (100%)]\tBatch loss: 15.679055\n",
      "Total loss 822.4776434898376\n",
      "Train Epoch: 34 [0/16 (0%)]\tBatch loss: 46.512180\n",
      "Train Epoch: 34 [8/16 (50%)]\tBatch loss: 43.068195\n",
      "Train Epoch: 34 [16/16 (100%)]\tBatch loss: 15.596180\n",
      "Total loss 822.9101481437683\n",
      "Train Epoch: 35 [0/16 (0%)]\tBatch loss: 45.301762\n",
      "Train Epoch: 35 [8/16 (50%)]\tBatch loss: 42.958138\n",
      "Train Epoch: 35 [16/16 (100%)]\tBatch loss: 15.596425\n",
      "Total loss 821.0182456970215\n",
      "Train Epoch: 36 [0/16 (0%)]\tBatch loss: 43.633644\n",
      "Train Epoch: 36 [8/16 (50%)]\tBatch loss: 43.201069\n",
      "Train Epoch: 36 [16/16 (100%)]\tBatch loss: 15.593637\n",
      "Total loss 817.3270864486694\n",
      "Train Epoch: 37 [0/16 (0%)]\tBatch loss: 41.849007\n",
      "Train Epoch: 37 [8/16 (50%)]\tBatch loss: 43.704304\n",
      "Train Epoch: 37 [16/16 (100%)]\tBatch loss: 15.537870\n",
      "Total loss 812.3508620262146\n",
      "Train Epoch: 38 [0/16 (0%)]\tBatch loss: 40.211769\n",
      "Train Epoch: 38 [8/16 (50%)]\tBatch loss: 44.346947\n",
      "Train Epoch: 38 [16/16 (100%)]\tBatch loss: 15.421297\n",
      "Total loss 806.5174489021301\n",
      "Train Epoch: 39 [0/16 (0%)]\tBatch loss: 38.893044\n",
      "Train Epoch: 39 [8/16 (50%)]\tBatch loss: 45.003853\n",
      "Train Epoch: 39 [16/16 (100%)]\tBatch loss: 15.265565\n",
      "Total loss 800.1863136291504\n",
      "Train Epoch: 40 [0/16 (0%)]\tBatch loss: 37.976578\n",
      "Train Epoch: 40 [8/16 (50%)]\tBatch loss: 45.565704\n",
      "Train Epoch: 40 [16/16 (100%)]\tBatch loss: 15.099861\n",
      "Total loss 793.6912755966187\n",
      "Train Epoch: 41 [0/16 (0%)]\tBatch loss: 37.475521\n",
      "Train Epoch: 41 [8/16 (50%)]\tBatch loss: 45.951454\n",
      "Train Epoch: 41 [16/16 (100%)]\tBatch loss: 14.943696\n",
      "Total loss 787.3672804832458\n",
      "Train Epoch: 42 [0/16 (0%)]\tBatch loss: 37.351746\n",
      "Train Epoch: 42 [8/16 (50%)]\tBatch loss: 46.115128\n",
      "Train Epoch: 42 [16/16 (100%)]\tBatch loss: 14.801724\n",
      "Total loss 781.5397930145264\n",
      "Train Epoch: 43 [0/16 (0%)]\tBatch loss: 37.532974\n",
      "Train Epoch: 43 [8/16 (50%)]\tBatch loss: 46.049461\n",
      "Train Epoch: 43 [16/16 (100%)]\tBatch loss: 14.668260\n",
      "Total loss 776.4824547767639\n",
      "Train Epoch: 44 [0/16 (0%)]\tBatch loss: 37.926670\n",
      "Train Epoch: 44 [8/16 (50%)]\tBatch loss: 45.784264\n",
      "Train Epoch: 44 [16/16 (100%)]\tBatch loss: 14.534847\n",
      "Total loss 772.3628282546997\n",
      "Train Epoch: 45 [0/16 (0%)]\tBatch loss: 38.431923\n",
      "Train Epoch: 45 [8/16 (50%)]\tBatch loss: 45.377098\n",
      "Train Epoch: 45 [16/16 (100%)]\tBatch loss: 14.396005\n",
      "Total loss 769.2093877792358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 46 [0/16 (0%)]\tBatch loss: 38.950951\n",
      "Train Epoch: 46 [8/16 (50%)]\tBatch loss: 44.898899\n",
      "Train Epoch: 46 [16/16 (100%)]\tBatch loss: 14.251371\n",
      "Total loss 766.9138081073761\n",
      "Train Epoch: 47 [0/16 (0%)]\tBatch loss: 39.400421\n",
      "Train Epoch: 47 [8/16 (50%)]\tBatch loss: 44.418545\n",
      "Train Epoch: 47 [16/16 (100%)]\tBatch loss: 14.104561\n",
      "Total loss 765.271416425705\n",
      "Train Epoch: 48 [0/16 (0%)]\tBatch loss: 39.721554\n",
      "Train Epoch: 48 [8/16 (50%)]\tBatch loss: 43.991951\n",
      "Train Epoch: 48 [16/16 (100%)]\tBatch loss: 13.960462\n",
      "Total loss 764.0386564731598\n",
      "Train Epoch: 49 [0/16 (0%)]\tBatch loss: 39.886292\n",
      "Train Epoch: 49 [8/16 (50%)]\tBatch loss: 43.656456\n",
      "Train Epoch: 49 [16/16 (100%)]\tBatch loss: 13.822918\n",
      "Total loss 762.9845123291016\n",
      "Train Epoch: 50 [0/16 (0%)]\tBatch loss: 39.897415\n",
      "Train Epoch: 50 [8/16 (50%)]\tBatch loss: 43.429821\n",
      "Train Epoch: 50 [16/16 (100%)]\tBatch loss: 13.693880\n",
      "Total loss 761.9211814403534\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    loss_accum = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        model.send(data.location)\n",
    "        \n",
    "        optimizer = optimizers[data.location.id]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(data)\n",
    "        loss = ((pred.view(-1) - target)**2).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.get()\n",
    "        loss = loss.get()\n",
    "        \n",
    "        loss_accum += float(loss)\n",
    "        \n",
    "        if batch_idx % 8 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tBatch loss: {:.6f}'.format(\n",
    "                epoch, batch_idx, len(train_loader),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))            \n",
    "            \n",
    "    print('Total loss', loss_accum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parabéns!!! - Hora de se juntar a comunidade!\n",
    "\n",
    "Parabéns por concluir esta etapa do tutorial! Se você gostou e gostaria de se juntar ao movimento em direção à proteção de privacidade, propriedade descentralizada e geração, demanda em cadeia, de dados em IA, você pode fazê-lo das seguintes maneiras!\n",
    "\n",
    "### Dê-nos uma estrela em nosso repo do PySyft no GitHub\n",
    "\n",
    "A maneira mais fácil de ajudar nossa comunidade é adicionando uma estrela nos nossos repositórios! Isso ajuda a aumentar a conscientização sobre essas ferramentas legais que estamos construindo.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Junte-se ao Slack!\n",
    "\n",
    "A melhor maneira de manter-se atualizado sobre os últimos avanços é se juntar à nossa comunidade! Você pode fazer isso preenchendo o formulário em [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Contribua com o projeto!\n",
    "\n",
    "A melhor maneira de contribuir para a nossa comunidade é se tornando um contribuidor do código! A qualquer momento, você pode acessar a página de *Issues* (problemas) do PySyft no GitHub e filtrar por \"Projetos\". Isso mostrará todas as etiquetas (tags) na parte superior, com uma visão geral de quais projetos você pode participar! Se você não deseja ingressar em um projeto, mas gostaria de codificar um pouco, também pode procurar mais mini-projetos \"independentes\" pesquisando problemas no GitHub marcados como \"good first issue\".\n",
    "\n",
    "- [Projetos do PySyft](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Etiquetados como Good First Issue](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Doar\n",
    "\n",
    "Se você não tem tempo para contribuir com nossa base de códigos, mas ainda deseja nos apoiar, também pode se tornar um Apoiador em nosso Open Collective. Todas as doações vão para hospedagem na web e outras despesas da comunidade, como hackathons e meetups!\n",
    "\n",
    "[Página do Open Collective do OpenMined](https://opencollective.com/openmined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envS",
   "language": "python",
   "name": "envs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
