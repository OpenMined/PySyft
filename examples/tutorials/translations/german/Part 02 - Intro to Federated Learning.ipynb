{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teil 2: Einführung in Federated Learning\n",
    "\n",
    "Im letzten Abschnitt haben wir uns mit PointerTensors vertraut gemacht, die die zugrunde liegende Infrastruktur erstellen, die wir zum Schutz der Privatsphäre von Deep Learning benötigen. In diesem Abschnitt erfahren Sie, wie Sie mit diesen grundlegenden Tools unseren ersten Deep-Learning-Algorithmus zum Schutz der Privatsphäre, Federated Learning, implementieren.\n",
    "\n",
    "Autoren:\n",
    "- Andrew Trask - Twitter: [@iamtrask](https://twitter.com/iamtrask)\n",
    "\n",
    "Übersetzer:\n",
    "- Vineet Jain - Github: [@vineetjai](https://github.com/vineetjai)\n",
    "\n",
    "### Was ist Federated Learning?\n",
    "\n",
    "Es ist eine einfache und leistungsstarke Methode, um Deep Learning-Modelle zu trainieren. Wenn Sie an Trainingsdaten denken, ist dies immer das Ergebnis eines Erfassungsprozesses. Menschen (über Geräte) erzeugen Daten, indem sie Ereignisse in der realen Welt aufzeichnen. Normalerweise werden diese Daten an einem einzigen zentralen Ort zusammengefasst, sodass Sie ein Modell für maschinelles Lernen trainieren können. Federated Learning stellt dies auf den Kopf!\n",
    "\n",
    "Anstatt Trainingsdaten in das Modell (einen zentralen Server) zu bringen, bringen Sie das Modell in die Trainingsdaten (wo immer es sich befindet).\n",
    "\n",
    "Die Idee ist, dass dies jedem, der die Daten erstellt, ermöglicht, die einzige permanente Kopie zu besitzen, und somit die Kontrolle darüber behält, wer jemals Zugriff darauf hat. Ziemlich cool, oder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abschnitt 2.1 - Ein Beispiel für spielerisches Lernen\n",
    "\n",
    "Beginnen wir damit, ein Spielzeugmodell zentral zu trainieren. Hier geht es um eine einfache wie Modelle bekommen. Wir brauchen zuerst:\n",
    "\n",
    "- ein Spielzeugdatensatz\n",
    "- ein Model\n",
    "- eine grundlegende Trainingslogik zum Trainieren eines Modells, um die Daten anzupassen.\n",
    "\n",
    "Hinweis: Wenn Ihnen diese API nicht bekannt ist, gehen Sie zu [fast.ai](http://fast.ai) und nehmen Sie an ihrem Kurs teil, bevor Sie mit diesem Tutorial fortfahren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A Toy Dataset\n",
    "data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True)\n",
    "target = torch.tensor([[0],[0],[1],[1.]], requires_grad=True)\n",
    "\n",
    "# A Toy Model\n",
    "model = nn.Linear(2,1)\n",
    "\n",
    "def train():\n",
    "    # Training Logic\n",
    "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
    "    for iter in range(20):\n",
    "\n",
    "        # 1) erase previous gradients (if they exist)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 2) make a prediction\n",
    "        pred = model(data)\n",
    "\n",
    "        # 3) calculate how much we missed\n",
    "        loss = ((pred - target)**2).sum()\n",
    "\n",
    "        # 4) figure out which weights caused us to miss\n",
    "        loss.backward()\n",
    "\n",
    "        # 5) change those weights\n",
    "        opt.step()\n",
    "\n",
    "        # 6) print our progress\n",
    "        print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6514)\n",
      "tensor(0.4835)\n",
      "tensor(0.3618)\n",
      "tensor(0.2716)\n",
      "tensor(0.2045)\n",
      "tensor(0.1543)\n",
      "tensor(0.1167)\n",
      "tensor(0.0884)\n",
      "tensor(0.0671)\n",
      "tensor(0.0509)\n",
      "tensor(0.0387)\n",
      "tensor(0.0295)\n",
      "tensor(0.0225)\n",
      "tensor(0.0171)\n",
      "tensor(0.0130)\n",
      "tensor(0.0100)\n",
      "tensor(0.0076)\n",
      "tensor(0.0058)\n",
      "tensor(0.0044)\n",
      "tensor(0.0034)\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und da hast du es! Wir haben ein Grundmodell auf herkömmliche Weise trainiert. Alle unsere Daten werden auf unserem lokalen Computer zusammengefasst und wir können sie verwenden, um Aktualisierungen an unserem Modell vorzunehmen. Federated Learning funktioniert jedoch nicht so. Lassen Sie uns dieses Beispiel so ändern, dass es auf die Art und Weise des Federierten Lernens durchgeführt wird!\n",
    "\n",
    "Also, was brauchen wir:\n",
    "\n",
    "- ein paar Arbeiter schaffen\n",
    "- Hinweise auf Trainingsdaten für jeden Mitarbeiter erhalten\n",
    "- Aktualisierte Trainingslogik für Verbundlernen\n",
    "\n",
    "     Neue Trainingsschritte:\n",
    "     - Modell an korrekten Arbeiter senden\n",
    "     - auf die dort befindlichen Daten trainieren\n",
    "     - Holen Sie das Modell zurück und wiederholen Sie es mit dem nächsten Arbeiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a couple workers\n",
    "\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Toy Dataset\n",
    "data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True)\n",
    "target = torch.tensor([[0],[0],[1],[1.]], requires_grad=True)\n",
    "\n",
    "# get pointers to training data on each worker by\n",
    "# sending some training data to bob and alice\n",
    "data_bob = data[0:2]\n",
    "target_bob = target[0:2]\n",
    "\n",
    "data_alice = data[2:]\n",
    "target_alice = target[2:]\n",
    "\n",
    "# Iniitalize A Toy Model\n",
    "model = nn.Linear(2,1)\n",
    "\n",
    "data_bob = data_bob.send(bob)\n",
    "data_alice = data_alice.send(alice)\n",
    "target_bob = target_bob.send(bob)\n",
    "target_alice = target_alice.send(alice)\n",
    "\n",
    "# organize pointers into a list\n",
    "datasets = [(data_bob,target_bob),(data_alice,target_alice)]\n",
    "\n",
    "opt = optim.SGD(params=model.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Training Logic\n",
    "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
    "    for iter in range(10):\n",
    "        \n",
    "        # NEW) iterate through each worker's dataset\n",
    "        for data,target in datasets:\n",
    "            \n",
    "            # NEW) send model to correct worker\n",
    "            model.send(data.location)\n",
    "\n",
    "            # 1) erase previous gradients (if they exist)\n",
    "            opt.zero_grad()\n",
    "\n",
    "            # 2) make a prediction\n",
    "            pred = model(data)\n",
    "\n",
    "            # 3) calculate how much we missed\n",
    "            loss = ((pred - target)**2).sum()\n",
    "\n",
    "            # 4) figure out which weights caused us to miss\n",
    "            loss.backward()\n",
    "\n",
    "            # 5) change those weights\n",
    "            opt.step()\n",
    "            \n",
    "            # NEW) get model (with gradients)\n",
    "            model.get()\n",
    "\n",
    "            # 6) print our progress\n",
    "            print(loss.get()) # NEW) slight edit... need to call .get() on loss\\\n",
    "    \n",
    "# federated averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2889, requires_grad=True)\n",
      "tensor(4.6312, requires_grad=True)\n",
      "tensor(1.6807, requires_grad=True)\n",
      "tensor(0.8786, requires_grad=True)\n",
      "tensor(1.0805, requires_grad=True)\n",
      "tensor(0.4585, requires_grad=True)\n",
      "tensor(0.6363, requires_grad=True)\n",
      "tensor(0.2632, requires_grad=True)\n",
      "tensor(0.3731, requires_grad=True)\n",
      "tensor(0.1522, requires_grad=True)\n",
      "tensor(0.2191, requires_grad=True)\n",
      "tensor(0.0881, requires_grad=True)\n",
      "tensor(0.1290, requires_grad=True)\n",
      "tensor(0.0511, requires_grad=True)\n",
      "tensor(0.0761, requires_grad=True)\n",
      "tensor(0.0296, requires_grad=True)\n",
      "tensor(0.0451, requires_grad=True)\n",
      "tensor(0.0172, requires_grad=True)\n",
      "tensor(0.0268, requires_grad=True)\n",
      "tensor(0.0100, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gut gemacht!\n",
    "\n",
    "Und voilà! Wir trainieren jetzt ein sehr einfaches Deep Learning-Modell mit Federated Learning! Wir senden das Modell an jeden Mitarbeiter, generieren einen neuen Verlauf und bringen den Verlauf dann zurück zu unserem lokalen Server, wo wir unser globales Modell aktualisieren. Niemals in diesem Prozess sehen oder fordern wir Zugriff auf die zugrunde liegenden Trainingsdaten! Wir bewahren die Privatsphäre von Bob und Alice !!!\n",
    "\n",
    "## Mängel dieses Beispiels\n",
    "\n",
    "Obwohl dieses Beispiel eine schöne Einführung in Federated Learning ist, weist es dennoch einige große Mängel auf. Insbesondere wenn wir \"model.get ()\" aufrufen und das aktualisierte Modell von Bob oder Alice erhalten, können wir tatsächlich viel über die Trainingsdaten von Bob und Alice lernen, indem wir ihre Steigungen betrachten. In einigen Fällen können wir ihre Trainingsdaten perfekt wiederherstellen!\n",
    "\n",
    "Also, was gibt es zu tun? Nun, die erste Strategie, die Menschen anwenden, besteht darin, den Gradienten über mehrere Personen zu mitteln, bevor er auf den zentralen Server hochgeladen wird. Diese Strategie erfordert jedoch eine differenziertere Verwendung von PointerTensor-Objekten. Im nächsten Abschnitt nehmen wir uns also etwas Zeit, um mehr über erweiterte Zeigerfunktionen zu erfahren, und aktualisieren dann dieses Beispiel für Federated Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Herzliche Glückwünsche!!! - Zeit, der Community beizutreten!\n",
    "\n",
    "Herzlichen Glückwunsch zum Abschluss dieses Notizbuch-Tutorials! Wenn Ihnen dies gefallen hat und Sie sich der Bewegung zur Wahrung der Privatsphäre, zum dezentralen Besitz von KI und der KI-Lieferkette (Daten) anschließen möchten, können Sie dies auf folgende Weise tun!\n",
    "\n",
    "### Star PySyft auf GitHub\n",
    "\n",
    "Der einfachste Weg, unserer Community zu helfen, besteht darin, die Repos in der Hauptrolle zu spielen! Dies hilft, das Bewusstsein für die coolen Tools zu schärfen, die wir bauen.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Mach mit bei unserem Slack!\n",
    "\n",
    "Der beste Weg, um über die neuesten Entwicklungen auf dem Laufenden zu bleiben, ist, sich unserer Community anzuschließen! Sie können dies tun, indem Sie das Formular unter [http://slack.openmined.org](http://slack.openmined.org) ausfüllen.\n",
    "\n",
    "### Treten Sie einem Code-Projekt bei!\n",
    "\n",
    "Der beste Weg, um zu unserer Community beizutragen, besteht darin, Code-Mitwirkender zu werden! Sie können jederzeit zur Seite PySyft GitHub Issues gehen und nach \"Projekten\" filtern. Dies zeigt Ihnen alle Top-Level-Tickets und gibt einen Überblick darüber, an welchen Projekten Sie teilnehmen können! Wenn Sie nicht an einem Projekt teilnehmen möchten, aber ein wenig programmieren möchten, können Sie auch nach weiteren \"einmaligen\" Miniprojekten suchen, indem Sie nach GitHub-Problemen suchen, die als \"gute erste Ausgabe\" gekennzeichnet sind.\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "\n",
    "\n",
    "### Spenden\n",
    "\n",
    "Wenn Sie keine Zeit haben, zu unserer Codebasis beizutragen, aber dennoch Unterstützung leisten möchten, können Sie auch Unterstützer unseres Open Collective werden. Alle Spenden fließen in unser Webhosting und andere Community-Ausgaben wie Hackathons und Meetups!\n",
    "\n",
    "[Open Collective Open Collective Page] (https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
