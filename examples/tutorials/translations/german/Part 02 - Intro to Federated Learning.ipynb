{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Einführung in das Federated Learning\n",
    "\n",
    "In dem letzten Kapitel wurden *PointerTensors* eingeführt, welche das nötige Grundgerüst für *Privacy-Preserving (privatsphäre-erhaltend) Deep Learning* darstellen. In diesem Kapitel lernen wir wie diese grundlegenden Werkzeuge uns dabei helfen unseren ersten privatsphäre-erhaltenden Deep Learning Algorithmus zu implementieren, *Federated Learning*. \n",
    "\n",
    "Authoren:\n",
    "- Andrew Trask - Twitter: [@iamtrask](https://twitter.com/iamtrask)\n",
    "\n",
    "### Was ist Federated Learning?\n",
    "\n",
    "Federated Learning ist eine einfache, mächtige Art um Deep Learning Modelle zu trainieren. Wenn du über Trainingsdaten nachdenkst, sind diese immer das Resultat eines speziellen Datensammlungsprozesses. Personen generieren Daten indem sie über ihre Geräte reale Ereignisse in der Welt aufnehmen. Normalerweise werden alle diese Daten an einen einzelnen, zentralen Ort gebracht um dort ein Machine Learning Model zu trainieren. \n",
    "Federated Learning stellt dieses Konzept komplett auf dem Kopf!\n",
    "\n",
    "Anstelle die Trainingsdaten zu dem zu trainierenden Model zu bringen (auf einem zentralen Server), wird das Model nun zu den Trainingsdaten gebracht (Wo auch immer sich diese befinden)\n",
    "\n",
    "Die Grundidee dabei ist, dass der Datenbesitzer die einzige Person bleibt, die eine permanente Kopie der Daten hat (seiner Daten hat) und so die Kontrolle darüber behält wer Zugriff auf diese Daten hat bzw. wer diese zu sehen bekommt. \n",
    "Nicht schlecht oder? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kapitel 2.1 - Ein einfaches Federated Learning Beispiel \n",
    "\n",
    "Fangen wir an ein einfaches Beispiel Model zentralisiert zu trainieren. Der Einfachkeitshalber verwenden wir ein so einfach wie mögliches Model. Dazu brauchen wir: \n",
    "\n",
    "- Ein Beispiel Datensatz \n",
    "- Ein Model \n",
    "- Grundlegende Trainingslogik um das Model auf den Daten zu trainieren\n",
    "\n",
    "Hinweis: Falls dir diese API unbekannt vorkommt schau einfach mal bei [fast.ai](http://fast.ai) vorbei und schau dir deren Kurse an bevor du dieses Tutorial weiter verfolgst. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ein Beispiel Datensatz\n",
    "data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]])\n",
    "target = torch.tensor([[0],[0],[1],[1.]])\n",
    "\n",
    "# Ein einfaches Beispiel-Model \n",
    "model = nn.Linear(2,1)\n",
    "\n",
    "def train():\n",
    "    # Trainingslogik\n",
    "    opt = optim.SGD(params=model.parameters(),lr=0.1)\n",
    "    for iter in range(20):\n",
    "\n",
    "        # 1) Lösche vorherige Gradienten (falls vorhanden)\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # 2) Mache eine Prognose mittels des Models\n",
    "        pred = model(data)\n",
    "\n",
    "        # 3) Berechne den Fehler (Wie weit wir daneben lagen)\n",
    "        loss = ((pred - target)**2).sum()\n",
    "\n",
    "        # 4) Finde heraus welche Gewichte in unserem Model dazu beigetragen haben, dass es ein Fehler gab\n",
    "        loss.backward()\n",
    "\n",
    "        # 5) Ändere diese Gewichte \n",
    "        opt.step()\n",
    "\n",
    "        # 6) Gebe den Fortschritt aus \n",
    "        print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und fertig sind wir! Wir haben ein einfaches Model mittels des Standard-Trainingsprozesses trainiert. Der komplette Datensatz ist auf unserem lokalen Rechner gespeichert und kann benutzt werden um unser Model zu verbessern. \n",
    "Federated Learning funktioniert jedoch anders. \n",
    "Also, lass uns dieses Beispiel nun mittels Federated Learning durchführen. \n",
    "\n",
    "Also, was brauchen wir: \n",
    "\n",
    "- Erstelle ein paar *worker* (Arbeiter)\n",
    "- Auf jedem worker speichere einen *Pointer* (Zeiger) auf den jeweiligen Trainingsdatensatz \n",
    "- Verändere die Trainingslogik um Federated Learning zu machen\n",
    "\n",
    "    Neue Trainingsschritte: \n",
    "    - Sende Model zu dem richtigen worker \n",
    "    - Trainiere auf den dort vorhanden Daten \n",
    "    - Gebe des Model zurück und fahre mit nächsten worker fort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle ein paar worker\n",
    "\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ein Beispiel Datensatz \n",
    "data = torch.tensor([[0,0],[0,1],[1,0],[1,1.]], requires_grad=True)\n",
    "target = torch.tensor([[0],[0],[1],[1.]], requires_grad=True)\n",
    "\n",
    "# Speichere einen pointer auf den jeweiligen Datensatz auf jedem Arbeiter \n",
    "# indem du Trainingsdaten zu bob und alice sendest \n",
    "data_bob = data[0:2]\n",
    "target_bob = target[0:2]\n",
    "\n",
    "data_alice = data[2:]\n",
    "target_alice = target[2:]\n",
    "\n",
    "# Initialisiere das Beispiel Model \n",
    "model = nn.Linear(2,1)\n",
    "\n",
    "data_bob = data_bob.send(bob)\n",
    "data_alice = data_alice.send(alice)\n",
    "target_bob = target_bob.send(bob)\n",
    "target_alice = target_alice.send(alice)\n",
    "\n",
    "# Organisiere die Pointer in einer Liste \n",
    "datasets = [(data_bob,target_bob),(data_alice,target_alice)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.federated.floptimizer import Optims\n",
    "workers = ['bob', 'alice']\n",
    "optims = Optims(workers, optim=optim.Adam(params=model.parameters(),lr=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Trainingslogik\n",
    "    for iter in range(10):\n",
    "        \n",
    "        # NEU) iteriere durch den Datensatz von jedem worker \n",
    "        for data,target in datasets:\n",
    "            \n",
    "            # NEU) Sende Model zu korrektem worker\n",
    "            model.send(data.location)\n",
    "            \n",
    "            # Rufe durch get_optim den optimizer für den worker auf \n",
    "            opt = optims.get_optim(data.location.id)\n",
    "            #print(data.location.id)\n",
    "\n",
    "            # 1) Lösche vorherige Gradienten (falls vorhanden) \n",
    "            opt.zero_grad()\n",
    "\n",
    "            # 2) Mache eine Prognose \n",
    "            pred = model(data)\n",
    "\n",
    "            # 3) Berechen den Fehler \n",
    "            loss = ((pred - target)**2).sum()\n",
    "\n",
    "            # 4) Finde heraus welche Gewichte im Model zu dem Fehler beigtragen haben \n",
    "            loss.backward()\n",
    "\n",
    "            # 5) Ändere diese Gewichte\n",
    "            opt.step()\n",
    "            \n",
    "            # NEU) Erhalte Model zurück (mit Gradienten)\n",
    "            model.get()\n",
    "\n",
    "            # 6) Gebe den Fortschritt aus \n",
    "            print(loss.get()) # NEU) kleine Änderung... müssen .get() von loss aufrufen \n",
    "    \n",
    "# federated averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasse!\n",
    "\n",
    "Und voilà! Wir sind nun dabei ein sehr einfaches Deep Learning Model mittels Federated Learning zu trainieren! Wir senden das Model zu jedem worker, berechnen die Gradienten, und schicken die Gradienten dann zu unserem lokalen Rechner zurück, wo wir die Modelparameter entsprechend ändern. *Niemals* während des gesamten Prozesses sehen wir oder fordern wir Zugriff auf die verschiedenen Datensätze auf deren Basis wir die Modelparameter ändern. \n",
    "Wir erhalten so die Privatsphäre von Bob und Alice!! \n",
    "\n",
    "## Grenzen dieses Beispiels \n",
    "\n",
    "Obowhl dieses Beispiel eine gute Einführung das Federated Learning bietet, hat es jedoch ein paar wichtige Grenzen. Vorallem, wenn wir `model.get()` aufrufen und das veränderte Model von Bob oder Alice bekommen, können wir tatsächlich viel über deren Trainingsdatensatz in Erfahrung bringen indem wir uns einfach die berechneten Gradienten anschauen. \n",
    "Teilweise kann so sogar der komplette Trainingsdatensatz wiederhergestellt werden!\n",
    "\n",
    "Was können wir also dagegen machen? Die erste Strategie gegen dieses Problem ist **über die verschiedenen Gradienten mehrerer Individuen den Durschnitt zu bilden und diesen auf den zentralen Server hochzuladen.** Diese Strategie erfordert jedoch eine etwas fortgeschrittene Benutzung von PointerTensor Objekten. \n",
    "Deswegen werden wir uns im nächsten Kapitel etwas fortgeschrittene Funktionalitäten von Pointern anschauen und damit dieses Federated Learning Beispiel verbessern.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glückwunsch!!! - Zeit um der Gemeinschaft beizutreten! \n",
    "\n",
    "Glückwunsch für die Vollendung dieses Tutorials! Falls dir das Tutorial gefallen hat und du der Gemeinschaft für privatsphäre-erhaltendem, dezentralisiertem Besitz von KI und der KI \"Lieferkette\" (Daten) beitreten möchtest, kannst du das gerne durch folgende Wege machen! \n",
    "\n",
    "### Star PySyft auf GitHub\n",
    "\n",
    "Der einfachste Weg um unserer Gemeinschaft zu helfen ist eines der Repos auf Github zu \"star[n]\"! Das hilft uns Aufmerksamkeit für die coolen Tools, die wir bauen, zu sammeln.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "\n",
    "### Tritt unserem Slack bei! \n",
    "\n",
    "Der beste Weg um über die neuesten Errungenschaften unserer Gemeinschaft zu erfahren ist unserer Gemeinschaft beizutreten! Das kannst du indem du folgendes Formular ausfüllst:  [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "\n",
    "### Werde Mitglied eines Programmierprojektes!\n",
    "\n",
    "Der beste Weg um dich an unserer Gemeinschaft zu beteiligen ist Programmier-Mitwirkender zu werden (Contributer)! Zu jeder Zeit kannst du zu der Issue Seite von PySyft auf GitHub gehen und nach \"Projects\" filtern. Das wird dir alle top level tickets anzeigen und somit kannst du einen guten Überblick bekommen über die Projekte, denen du beitreten kannst! Falls du keinem Projekt beitreten möchtest, aber trotzdem ein bisschen Programmieren möchtest, kannst du auch nach kleineren Mini-Projekten Ausschau halten, die mit \"good first issue\" gekennzeichnet sind.  \n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Spenden\n",
    "\n",
    "Wenn du keine Zeit hast dich an unserer Programmierarbeit zu beteiligen, du uns aber trotzdem unterstützen möchtest, kannst du ein Spender unserer Open Collective werden. Alle Spenden werden für Web Hosting und anderen Gemeinschaftsausgaben, wie z.B Hackathons und Meetups benutzt! \n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
