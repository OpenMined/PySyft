{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extended_Vanilla_SplitNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0NdEpm45cSa",
        "colab_type": "text"
      },
      "source": [
        "# Tutorial - Extended Vanilla Split Learning\n",
        "\n",
        "Extended Vanilla Split Learning is basically a combination of multi-layer splitnn and vertically partitioned data. Here all our partitioned input data goes to one location and labels to another location. But instead of just training our model in two locations we can use few trusted locations having more complex model through which we can train our model.\n",
        "\n",
        "![alt text](https://media.arxiv-vanity.com/render-output/2178060/Supplem1.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtlaO14r6HhH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"import necessary modules\"\"\"\n",
        "\n",
        "import syft, torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjQF7PYTEcdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Create hook and virtual workers.\"\"\"\n",
        "hook = syft.TorchHook(torch)\n",
        "\n",
        "\"\"\"Bob will hold the first cut layer and the input data.\"\"\"\n",
        "bob = syft.VirtualWorker(hook, id='bob')\n",
        "\n",
        "\"\"\"Alice will hold the last cut layer and the labels.\"\"\"\n",
        "alice = syft.VirtualWorker(hook, id='alice')\n",
        "\n",
        "\"\"\"Secure Worker will hold the middle cut layer whom we can trust and train the \n",
        "   model through it.\"\"\"\n",
        "secure_worker = syft.VirtualWorker(hook, id='secure_worker')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnMh0MDJEmcq",
        "colab_type": "text"
      },
      "source": [
        "### Defining a function create_models which returns a list of models for training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93LQg8b8E43q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_models(partition, input_size, hidden_sizes, output_size):\n",
        "    models = list()\n",
        "\n",
        "    \"\"\"Create models for each partition in bob's location\"\"\"\n",
        "    for i in range(partition-1):\n",
        "        models.append(nn.Sequential(nn.Linear(int(input_size/partition), hidden_sizes[0]),\n",
        "                                    nn.ReLU()))\n",
        "\n",
        "    \"\"\"Compute the last remaining features as features may or may not be divided equally.\"\"\"\n",
        "    rem = input_size - int(input_size/partition * (partition-1))\n",
        "    models.append(nn.Sequential(nn.Linear(rem, hidden_sizes[0]),\n",
        "                                nn.ReLU()))\n",
        "\n",
        "    \"\"\"Create a model for secure_worker\"\"\"\n",
        "    \"\"\"Since all partition models will send information having equal second dimension which is hidden size. So we need\n",
        "       to multiply input size with partition to match the concatenated layer dimensions.\"\"\"\n",
        "    models.append(nn.Sequential(nn.Linear(hidden_sizes[0]*partition, hidden_sizes[1]),\n",
        "                                nn.ReLU()))\n",
        "\n",
        "    \"\"\"Create a model for alice\"\"\"\n",
        "    models.append(nn.Sequential(nn.Linear(hidden_sizes[1], output_size),\n",
        "                                nn.LogSoftmax(dim=1)))\n",
        "\n",
        "    return models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlk33b7FFGvO",
        "colab_type": "text"
      },
      "source": [
        "### Creating SplitNN class for adding split learning functionality to our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ErOyWeZFYnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SplitNN:\n",
        "    def __init__(self, models, optimizers, partition, hidden_sizes):\n",
        "        super().__init__()\n",
        "        self.models = models\n",
        "        self.optimizers = optimizers\n",
        "        self.partition = partition\n",
        "        self.hidden = hidden_sizes\n",
        "        self.outputs = [None] * partition\n",
        "\n",
        "    def zero_grads(self):\n",
        "        for opt in self.optimizers:\n",
        "            opt.zero_grad()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.partition):\n",
        "            self.outputs[i] = self.models[i](x[i])\n",
        "\n",
        "        \"\"\"Concatenate outputs of each partitioned model\"\"\"\n",
        "        concat_out = torch.cat(tuple(self.outputs[i] for i in range(self.partition)), dim=1)\n",
        "\n",
        "        \"\"\"Transfer this concatenated layer to secure_worker's location\"\"\"\n",
        "        if concat_out.location == self.models[-2].location:\n",
        "            secure_inp = concat_out.detach().requires_grad_()\n",
        "        else:\n",
        "            secure_inp = concat_out.move(self.models[-2].location, requires_grad=True)\n",
        "\n",
        "        \"\"\"Get the output from secure worker's model\"\"\"\n",
        "        secure_out = self.models[-2](secure_inp)\n",
        "\n",
        "        \"\"\"Transfer this output to alice's location\"\"\"\n",
        "        if secure_out.location == self.models[-1].location:\n",
        "            alice_inp = secure_out.detach().requires_grad_()\n",
        "        else:\n",
        "            alice_inp = secure_out.move(self.models[-1].location, requires_grad=True)\n",
        "\n",
        "        \"\"\"Get the output from alice's model and return it\"\"\"\n",
        "        alice_out = self.models[-1](alice_inp)\n",
        "\n",
        "        self.concat_out = concat_out\n",
        "        self.secure_inp = secure_inp\n",
        "        self.secure_out = secure_out\n",
        "        self.alice_inp = alice_inp\n",
        "        self.alice_out = alice_out\n",
        "        return alice_out\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"Get the gradients from alice's location and pass it to secure_worker's location\"\"\"\n",
        "        if self.secure_out.location == self.alice_inp.location:\n",
        "            grad1 = self.alice_inp.grad.copy()\n",
        "        else:\n",
        "            grad1 = self.alice_inp.grad.copy().move(self.secure_out.location)\n",
        "\n",
        "        \"\"\"Backpropagate and find the gradients of secure_worker's model\"\"\"\n",
        "        self.secure_out.backward(grad1)\n",
        "\n",
        "        \"\"\"Get the gradients from secure_worker's location, and divide and pass it to bob's partitioned models\"\"\"\n",
        "        if self.concat_out.location == self.secure_inp.location:\n",
        "            grad2 = self.secure_inp.grad.copy()\n",
        "        else:\n",
        "            grad2 = self.secure_inp.grad.copy().move(self.models[0].location)\n",
        "\n",
        "        i = 0\n",
        "        while i < self.partition - 1:\n",
        "            self.outputs[i].backward(grad2[:, self.hidden[0] * i: self.hidden[0] * (i + 1)], retain_graph=True)\n",
        "            i += 1\n",
        "        self.outputs[i].backward(grad2[:, self.hidden[0] * i:], retain_graph=True)\n",
        "\n",
        "\n",
        "    def step(self):\n",
        "        for opt in self.optimizers:\n",
        "            opt.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlvAeCPtFinH",
        "colab_type": "text"
      },
      "source": [
        "### Defining a function to train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FznG7XzsFruh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(models, splitNN, data, target, epochs):\n",
        "    def train(data, target, splitnn):\n",
        "        splitnn.zero_grads()\n",
        "        pred = splitnn.forward(data)\n",
        "        criterion = nn.NLLLoss()\n",
        "        loss = criterion(pred, target)\n",
        "        loss.backward()\n",
        "        splitnn.backward()\n",
        "        splitnn.step()\n",
        "        return loss\n",
        "\n",
        "    avg_loss = 0.\n",
        "    for e in range(epochs):\n",
        "        total_loss = 0.\n",
        "        data1, data2, data3 = data\n",
        "        for x1, x2, x3, y in zip(data1, data2, data3, target):\n",
        "            x1, x2, x3 = x1.send(models[0].location), x2.send(models[0].location), x3.send(models[0].location)\n",
        "            y = y.send(models[-1].location)\n",
        "            loss = train([x1, x2, x3], y, splitNN)\n",
        "            total_loss += loss.get()\n",
        "\n",
        "        avg_loss += total_loss/len(data[0])\n",
        "        print(f\"Epoch: {e+1}... Training Loss: {total_loss/len(data[0])}\")\n",
        "\n",
        "    print(f\"Average Loss: {avg_loss/epochs}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZSSA6DDFtNo",
        "colab_type": "text"
      },
      "source": [
        "Now lets define a main function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmSJX3uwFxfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    \"\"\"Assign the number of partitions in vertically partitioned data.\"\"\"\n",
        "    partition = 3\n",
        "\n",
        "    \"\"\"Define a transform\"\"\"\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "    \"\"\"Import dataset and load it.\"\"\"\n",
        "    trainset = datasets.MNIST('mnist', download=False, train=True, transform=transform)\n",
        "    trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "    \"\"\"Lets define sizes.\"\"\"\n",
        "    input_size = 784\n",
        "    hidden_sizes = [128, 256, 512]\n",
        "    output_size = 10\n",
        "\n",
        "    \"\"\"Lets create 3 lists that would act as our vertically paritioned datasets and one list for labels\"\"\"\n",
        "    image_set1, image_set2, image_set3 = list(), list(), list()\n",
        "    labels = list()\n",
        "\n",
        "    \"\"\"Assign how many data should contain in a single dataset. Since mnist dataset have a shape of [28 x 28 x 1] and we\n",
        "       sliced the dataset in 64 batches, after reshaping it would be [64 x 784]. Now we have to divide these 784 \n",
        "       features into three datasets\"\"\"\n",
        "    distr = int(input_size/partition)\n",
        "\n",
        "    for image, label in trainloader:\n",
        "        image = image.view(image.shape[0], -1)\n",
        "        image_set1.append(image[:, 0:distr])\n",
        "        image_set2.append(image[:, distr:distr*2])\n",
        "        image_set3.append(image[:, distr*2:])\n",
        "        labels.append(label)\n",
        "\n",
        "    models = create_models(partition, input_size, hidden_sizes, output_size)\n",
        "\n",
        "    optimizers = [optim.SGD(model.parameters(), lr=0.01) for model in models]\n",
        "\n",
        "    \"\"\"Build a list locations where each model has to be sent.\"\"\"\n",
        "    model_locations = list()\n",
        "    for _ in range(partition):\n",
        "        model_locations.append(bob)\n",
        "    model_locations.append(secure_worker)\n",
        "    for _ in range(partition):\n",
        "        model_locations.append(alice)\n",
        "\n",
        "    \"\"\"Send each model to its specific location.\"\"\"\n",
        "    for model, location in zip(models, model_locations):\n",
        "        model.send(location)\n",
        "\n",
        "    \"\"\"Create an object of SplitNN class\"\"\"\n",
        "    splitNN = SplitNN(models, optimizers, partition, hidden_sizes)\n",
        "\n",
        "    \"\"\"Now to train our model, call training function.\"\"\"\n",
        "    epochs = 20\n",
        "    training(models, splitNN, [image_set1, image_set2, image_set3], labels, epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4g8zCo7F1DJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}