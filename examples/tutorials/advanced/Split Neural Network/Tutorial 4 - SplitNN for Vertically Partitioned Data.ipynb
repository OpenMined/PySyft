{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "epochs = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4 - SplitNN for Vertically Partitioned Data\n",
    "\n",
    "<b>Recap: </b> The previous tutorial looked at building a basic SplitNN, where an NN was split into two or multiple segments on two or multiple seperate hosts. However, what if clients have multi-modal multi-institutional collaboration?\n",
    "\n",
    "<b>What is a SplitNN? </b>The training of a neural network (NN) is 'split' accross one or more hosts. Each model segment is a self contained NN that feeds into the segment in front. In the case, both parties can train the model without knowing each others data or full details of the model.\n",
    "\n",
    "<b>Why use a SplitNN? </b>The SplitNN has been shown to provide a dramatic reduction to the computational burden of training while maintaining higher accuracies when training over large number of clients [[2](https://arxiv.org/abs/1812.00564)].\n",
    "\n",
    "<b>Advantages </b>\n",
    "- The accuracy should be identical to a non-split version of the same model, trained locally. \n",
    "- the model is distributed, meaning all segment holders must consent in order to aggregate the model at the end of training.\n",
    "- The scalability of this approach, in terms of both network and computational resources, could make this an a valid alternative to FL and LBSGD, particularly on low power devices.\n",
    "- This could be an effective mechanism for both horizontal and vertical data distributions.\n",
    "- As computational cost is already quite low, the cost of applying homomorphic encryption is also minimised.\n",
    "- Only activation signal gradients are sent/ recieved, meaning that malicious actors cannot use gradients of model parameters to reverse engineer the original values.\n",
    "\n",
    "<b>Constraints </b>\n",
    "- A new technique with little surroundung literature, a large amount of comparison and evaluation is still to be performed.\n",
    "- This approach requires all hosts to remain online during the entire learning process (less fesible for hand-held devices).\n",
    "- Not as established in privacy-preserving toolkits as FL and LBSGD.\n",
    "- Activation signals and their corresponding gradients still have the capacity to leak information, however this is yet to be fully addressed in the literature.\n",
    "\n",
    "<b>Description: </b>This configuration allows for multiple clients holding different modalities of data to learn distributed models without data sharing. As a concrete example we walkthrough the case where radiology centers collaborate with pathology test centers and a server for disease diagnosis. Radiology centers holding imaging data modalities train a partial model upto the cut layer. In the same way the pathology test center having patient test results trains a partial model upto its own cut layer. The outputs at the cut layer from both these centers are then concatenated and sent to the disease diagnosis server that trains the rest of the model. This process is continued back and forth to complete the forward and backward propagations in order to train the distributed deep learning model without sharing each others raw data. In this tutorial, we split a single flatten image into two segments to mimic different modalities of data, you can also split it into arbitrary number.\n",
    "\n",
    "\n",
    "<img src=\"images/SplitNN_Veritically.png\" width=\"20%\">\n",
    "\n",
    "\n",
    "In this tutorial, we demonstrate the SplitNN class with a 3 segment distribution [[1](https://arxiv.org/abs/1812.00564)]. This time;\n",
    "\n",
    "- <b>$Alice_{0}$</b>\n",
    "    - Has Model Segment 1\n",
    "    - Has the handwritten images Segment 1\n",
    "- <b>$Alice_{1}$</b>\n",
    "    - Has model Segment 2\n",
    "    - Has the handwritten images Segment 2\n",
    "- <b>$Bob$</b> \n",
    "    - Has Model Segment 3\n",
    "    - Has the image labels\n",
    "    \n",
    "\n",
    "Author:\n",
    "- Haofan Wang - githubï¼š[@haofanwang](https://github.com/haofanwang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "trainset = datasets.MNIST('mnist', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "# define the input size for each client, each client may have different input size\n",
    "# you can also try arbitrary number of vertical splits, [size_1, size_2, ... size_n] where n is the total number of segments\n",
    "input_sizes = [392, 392]\n",
    "\n",
    "# define the hidden layer size, each client can have different hidden layer size, all clients share the same size for convinence\n",
    "hidden_sizes = [128, 320]\n",
    "\n",
    "# define the input size for the server\n",
    "concatenated_size = hidden_sizes[-1]*len(input_sizes)\n",
    "\n",
    "# define the output size, it should be the number of classes\n",
    "output_size = 10\n",
    "\n",
    "# define clients models, you can define any model here, but remember to make input sizes consistent with the size of splitted segments\n",
    "client_models = []\n",
    "for i in range(len(input_sizes)):\n",
    "  client_models.append(\n",
    "      nn.Sequential(\n",
    "                nn.Linear(input_sizes[i], hidden_sizes[0]),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                nn.ReLU(),\n",
    "    )\n",
    "  )\n",
    "\n",
    "# define server model\n",
    "server_model = [\n",
    "    nn.Sequential(\n",
    "                nn.Linear(concatenated_size, output_size),\n",
    "                nn.LogSoftmax(dim=1),\n",
    "    )               \n",
    "]\n",
    "\n",
    "# complete model array\n",
    "models = client_models + server_model\n",
    "\n",
    "# create optimisers for each segment and link to their segment\n",
    "optimizers = [\n",
    "    optim.SGD(model.parameters(), lr=0.03,)\n",
    "    for model in models\n",
    "]\n",
    "\n",
    "# create client workers\n",
    "client_locations = []\n",
    "for i in range(len(input_sizes)):\n",
    "  alice = sy.VirtualWorker(hook, id=\"alice_\"+str(i))\n",
    "  client_locations.append(alice)\n",
    "\n",
    "# create server worker\n",
    "server_location = []\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "server_location.append(bob)\n",
    "\n",
    "# Send Model Segments to starting locations\n",
    "model_locations = client_locations + server_location\n",
    "\n",
    "for model, location in zip(models, model_locations):\n",
    "    model.send(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "def train(images, target, models, optimizers):\n",
    "      # Training Logic\n",
    "\n",
    "      #1) erase previous gradients (if they exist)\n",
    "      for opt in optimizers:\n",
    "        opt.zero_grad()\n",
    "\n",
    "      #2) define a empty tensor to save concatenated output\n",
    "      remote_a = torch.zeros(images[0].shape[0],concatenated_size)\n",
    "\n",
    "      #3) make a prediction on each client model\n",
    "      output_a = []\n",
    "      for i in range(len(input_sizes)):\n",
    "        a = models[i](images[i])\n",
    "        output_a.append(a)\n",
    "        remote_a[:,hidden_sizes[1]*i:hidden_sizes[1]*(i+1)] = a.copy().get()\n",
    "      remote_a = remote_a.detach().send(models[-1].location).requires_grad_()\n",
    "\n",
    "      #4) make prediction on server model using recieved signal\n",
    "      pred = models[-1](remote_a)\n",
    "\n",
    "      #5) calculate the loss\n",
    "      criterion = nn.NLLLoss()\n",
    "      loss = criterion(pred, labels)\n",
    "\n",
    "      #6) backpropagate on bottom models given this gradient\n",
    "      loss.backward()\n",
    "\n",
    "      #7) send gradient of the recieved activation signal to the model behind\n",
    "      for i in range(len(input_sizes)):\n",
    "        grad_a = remote_a.grad[:,hidden_sizes[1]*i:hidden_sizes[1]*(i+1)].copy().move(models[i].location)\n",
    "        output_a[i].backward(grad_a)\n",
    "\n",
    "      #8) update params\n",
    "      for opt in optimizers:\n",
    "          opt.step()\n",
    "      \n",
    "      #9) print our progress\n",
    "      return loss.detach().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # flatten images\n",
    "        images = images.view(images.shape[0],-1)\n",
    "\n",
    "        # splitted image segments\n",
    "        split_images = []\n",
    "        for i in range(len(input_sizes)):\n",
    "          if i == 0:\n",
    "            split_image = images[:,0:input_sizes[i]].send(model_locations[i])\n",
    "          else:\n",
    "            split_image = images[:,input_sizes[i-1]:input_sizes[i-1]+input_sizes[i]].send(model_locations[i])\n",
    "          split_image = split_image.view(images.shape[0], -1)\n",
    "          split_images.append(split_image)\n",
    "\n",
    "        # image labels\n",
    "        labels = labels.send(bob)\n",
    "\n",
    "        loss = train(split_images, labels, models, optimizers)\n",
    "        running_loss += loss\n",
    "\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(i, running_loss/len(trainloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(models, dataloader, dataset_name):\n",
    "    for model in models:\n",
    "      model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, target in testloader:\n",
    "          # flatten images\n",
    "          images = images.view(images.shape[0],-1)\n",
    "          # splitted image segments\n",
    "          split_images = []\n",
    "          for i in range(len(input_sizes)):\n",
    "            if i == 0:\n",
    "              split_image = images[:,0:input_sizes[i]].send(model_locations[i])\n",
    "            else:\n",
    "              split_image = images[:,input_sizes[i-1]:input_sizes[i-1]+input_sizes[i]].send(model_locations[i])\n",
    "            split_image = split_image.view(images.shape[0], -1)\n",
    "            split_images.append(split_image)\n",
    "          # define a empty tensor to save concatenated output\n",
    "          remote_a = torch.zeros(split_images[0].shape[0],concatenated_size)\n",
    "          # make a prediction on each client model\n",
    "          output_a = []\n",
    "          for i in range(len(input_sizes)):\n",
    "            a = models[i](split_images[i])\n",
    "            output_a.append(a)\n",
    "            remote_a[:,hidden_sizes[1]*i:hidden_sizes[1]*(i+1)] = a.copy().get()\n",
    "          remote_a = remote_a.detach().send(models[-1].location).requires_grad_()\n",
    "          # make prediction on server model using recieved signal\n",
    "          output = models[-1](remote_a).get()\n",
    "          # calculate the the number of correct predictions\n",
    "          pred = output.data.max(1, keepdim=True)[1]\n",
    "          correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    \n",
    "    print(\"{}: Accuracy {}/{} ({:.0f}%)\".format(dataset_name, \n",
    "                                                correct,\n",
    "                                                len(dataloader.dataset), \n",
    "                                                100. * correct / len(dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = datasets.MNIST('mnist', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "test(models, testloader, \"Test set\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}