{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tutorial 4 - SplitNN for Vertically Partitioned Data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOzGf+AUPUZ2sTywlof48MS"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"TizOk-q4Ay4a"},"outputs":[],"source":["epochs = 15"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ra0xOwIOcsoc"},"source":["# Tutorial 4 - SplitNN for Vertically Partitioned Data\n","\n","<b>Recap:</b> The previous tutorial looked at building a basic SplitNN, where an NN was split into two segments on two seperate hosts. However, what if clients have multi-modal multi-institutional collaboration?\n","\n","<b>Description: </b>Here we simply use two same images to represent the multi-modal data. We demonstrate the SplitNN class with a 3 segment distribution. This time,\n","\n","\n","\n","<img src=\"images/SplitNN_Veritically.png\" width=\"20%\">\n","\n","\n","In this tutorial, we demonstrate the SplitNN class with a 3 segment distribution [[1](https://arxiv.org/abs/1812.00564)]. This time;\n","\n","- <b>$Alice_{0}$</b>\n","    - Has Model Segment 1\n","    - Has the handwritten images\n","- <b>$Alice_{1}$</b>\n","    - Has model Segment 2\n","    - Has the handwritten images\n","- <b>$Bob$</b> \n","    - Has Model Segment 3\n","    - Has the image labels\n","    \n","We use the exact same model as we used in the previous tutorial, only this time we have two clients and one host.\n","\n","\n","Author:\n","- Haofan Wang - githubï¼š[@haofanwang](https://github.com/haofanwang)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"Y8WAF7Q3AzHQ"},"outputs":[],"source":["import torch\n","from torchvision import datasets, transforms\n","from torch import nn, optim\n","import syft as sy\n","hook = sy.TorchHook(torch)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"AXVaccsPAzR0"},"outputs":[],"source":["# Data preprocessing\n","transform = transforms.Compose([transforms.ToTensor(),\n","                              transforms.Normalize((0.5,), (0.5,)),\n","                              ])\n","trainset = datasets.MNIST('mnist', download=True, train=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"pnVTdoQ6AzWk"},"outputs":[],"source":["torch.manual_seed(0)\n","\n","# Define our model segments\n","\n","input_size = 784\n","hidden_sizes = [128, 320, 640]\n","output_size = 10\n","\n","models = [\n","    nn.Sequential(\n","                nn.Linear(input_size, hidden_sizes[0]),\n","                nn.ReLU(),\n","                nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n","                nn.ReLU(),\n","    ),\n","    nn.Sequential(\n","                nn.Linear(input_size, hidden_sizes[0]),\n","                nn.ReLU(),\n","                nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n","                nn.ReLU(),\n","    ),\n","    nn.Sequential(\n","                nn.Linear(hidden_sizes[2], output_size),\n","                nn.LogSoftmax(dim=1)\n","    )\n","]\n","\n","# Create optimisers for each segment and link to their segment\n","optimizers = [\n","    optim.SGD(model.parameters(), lr=0.03,)\n","    for model in models\n","]\n","\n","# create some workers\n","alice_0 = sy.VirtualWorker(hook, id=\"alice_0\")\n","alice_1 = sy.VirtualWorker(hook, id=\"alice_1\")\n","bob = sy.VirtualWorker(hook, id=\"bob\")\n","workers = alice_0, alice_1, bob\n","\n","# Send Model Segments to starting locations\n","model_locations = [alice_0, alice_1, bob]\n","\n","for model, location in zip(models, model_locations):\n","    model.send(location)"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"LOJTUbXXXIaN"},"outputs":[],"source":["def train(images_0, images_1, target, models, optimizers):\n","      # Training Logic\n","\n","      #1) erase previous gradients (if they exist)\n","      for opt in optimizers:\n","        opt.zero_grad()\n","\n","      #2) make a prediction\n","      a_0 = models[0](images_0)\n","      a_1 = models[1](images_1)\n","\n","      #3) break the computation graph link, and send the activation signal to the next model\n","      remote_a_0 = a_0.detach().move(models[2].location).requires_grad_()\n","      remote_a_1 = a_1.detach().move(models[2].location).requires_grad_()\n","      remote_a = torch.zeros(images.shape[0],hidden_sizes[2])\n","      remote_a[:,:hidden_sizes[1]] = remote_a_0.copy().get()\n","      remote_a[:,hidden_sizes[1]:] = remote_a_1.copy().get()\n","      remote_a = remote_a.detach().send(models[2].location).requires_grad_()\n","\n","      #4) make prediction on next model using recieved signal\n","      pred = models[2](remote_a)\n","\n","      #5) calculate how much we missed\n","      criterion = nn.NLLLoss()\n","      loss = criterion(pred, labels)\n","\n","      #6) figure out which weights caused us to miss\n","      loss.backward()\n","\n","      #7) send gradient of the recieved activation signal to the model behind\n","      grad_a_0 = remote_a.grad[:,:hidden_sizes[1]].copy().move(models[0].location)\n","      grad_a_1 = remote_a.grad[:,hidden_sizes[1]:hidden_sizes[2]].copy().move(models[1].location)\n","\n","      #8) backpropagate on bottom model given this gradient\n","      a_0.backward(grad_a_0)\n","      a_1.backward(grad_a_1)\n","\n","      #9) change the weights\n","      for opt in optimizers:\n","          opt.step()\n","      \n","      #10) print our progress\n","      return loss.detach().get()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":263},"colab_type":"code","executionInfo":{"elapsed":667691,"status":"ok","timestamp":1583742550238,"user":{"displayName":"Haofan Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgFo8dCYld3jeNYIu5hMFiC8FomohzLgyL16eI=s64","userId":"06950049024825524104"},"user_tz":420},"id":"sqmdK244XWxI","outputId":"43b83b7e-45d0-4660-abaf-bd32ef6e1ec1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 - Training loss: 0.5656647682189941\n","Epoch 1 - Training loss: 0.2735804319381714\n","Epoch 2 - Training loss: 0.21353811025619507\n","Epoch 3 - Training loss: 0.17315588891506195\n","Epoch 4 - Training loss: 0.1451188325881958\n","Epoch 5 - Training loss: 0.12480196356773376\n","Epoch 6 - Training loss: 0.10766370594501495\n","Epoch 7 - Training loss: 0.09647190570831299\n","Epoch 8 - Training loss: 0.0861542671918869\n","Epoch 9 - Training loss: 0.077510304749012\n","Epoch 10 - Training loss: 0.0705472007393837\n","Epoch 11 - Training loss: 0.06403899192810059\n","Epoch 12 - Training loss: 0.05939096584916115\n","Epoch 13 - Training loss: 0.05365265905857086\n","Epoch 14 - Training loss: 0.04996408149600029\n"]}],"source":["for i in range(epochs):\n","    running_loss = 0\n","    for images, labels in trainloader:\n","        images_0 = images.send(alice_0)\n","        images_0 = images_0.view(images_0.shape[0], -1)\n","        images_1 = images.send(alice_1)\n","        images_1 = images_1.view(images_1.shape[0], -1)\n","        labels = labels.send(bob)\n","\n","        loss = train(images_0, images_1, labels, models, optimizers)\n","        running_loss += loss\n","\n","    else:\n","        print(\"Epoch {} - Training loss: {}\".format(i, running_loss/len(trainloader)))"]},{"cell_type":"code","execution_count":0,"metadata":{"colab":{},"colab_type":"code","id":"R_kM9dL2Z1ey"},"outputs":[],"source":["def test(models, dataloader, dataset_name):\n","    for model in models:\n","      model.eval()\n","    correct = 0\n","    with torch.no_grad():\n","        for images, target in testloader:\n","          images_0 = images.send(alice_0)\n","          images_0 = images_0.view(images_0.shape[0], -1)\n","          images_1 = images.send(alice_1)\n","          images_1 = images_1.view(images_1.shape[0], -1)\n","          a_0 = models[0](images_0)\n","          a_1 = models[1](images_1)\n","          remote_a_0 = a_0.detach().move(models[2].location).requires_grad_()\n","          remote_a_1 = a_1.detach().move(models[2].location).requires_grad_()\n","          remote_a = torch.zeros(images.shape[0],hidden_sizes[2])\n","          remote_a[:,:hidden_sizes[1]] = remote_a_0.copy().get()\n","          remote_a[:,hidden_sizes[1]:] = remote_a_1.copy().get()\n","          remote_a = remote_a.detach().send(models[2].location).requires_grad_()\n","          output = models[2](remote_a).get()\n","          pred = output.data.max(1, keepdim=True)[1]\n","          correct += pred.eq(target.data.view_as(pred)).sum()\n","    \n","    print(\"{}: Accuracy {}/{} ({:.0f}%)\".format(dataset_name, \n","                                                correct,\n","                                                len(dataloader.dataset), \n","                                                100. * correct / len(dataloader.dataset)))"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":32},"colab_type":"code","executionInfo":{"elapsed":4943,"status":"ok","timestamp":1583742812692,"user":{"displayName":"Haofan Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhgFo8dCYld3jeNYIu5hMFiC8FomohzLgyL16eI=s64","userId":"06950049024825524104"},"user_tz":420},"id":"LX97fhHhZ4G0","outputId":"94133880-380c-4ca5-c819-aba26331415b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test set: Accuracy 9738/10000 (97%)\n"]}],"source":["test(models, testloader, \"Test set\")"]}]}