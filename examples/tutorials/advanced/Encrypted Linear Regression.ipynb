{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encrypted Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial you are going to see how you can run a linear regression model on **data distributed in a pool of workers** with **encrypted computations leveraged by Secured Multi-Party Computation**. For this demonstration we are going to use the classical Housing Prices dataset that is already available in the VirtualGrid set up by the Syft Sandbox.\n",
    "\n",
    "The idea for the implementation of the Encrypted Linear Regression algorithm in PySyft is based on the section 2 of [this paper](https://arxiv.org/abs/1901.09531) written by Jonathan Bloom of the Broad Institute of MIT and Harvard.\n",
    "\n",
    "**Author**: André Macedo Farias. Github: [@andrelmfarias](https://github.com/andrelmfarias) | Twitter: [@andrelmfarias](https://twitter.com/andrelmfarias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import PySyft and PyTorch and set up the Syft sandbox, which will create all the objects and tools we will need to run our simulation (Virtual Workers, VirtualGrid with datasets, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T13:50:00.733943Z",
     "start_time": "2019-09-26T13:50:00.729762Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T13:50:04.648835Z",
     "start_time": "2019-09-26T13:50:01.286501Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/Users/andre.farias/venvs/pysyft/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andre.farias/venvs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/andre.farias/venvs/pysyft/lib/python3.7/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import syft as sy\n",
    "sy.create_sandbox(globals(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have several workers already set up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T15:07:01.351012Z",
     "start_time": "2019-09-26T15:07:01.343296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<VirtualWorker id:bob #objects:7584>,\n",
       " <VirtualWorker id:theo #objects:16>,\n",
       " <VirtualWorker id:jason #objects:7584>,\n",
       " <VirtualWorker id:alice #objects:16>,\n",
       " <VirtualWorker id:andy #objects:16>,\n",
       " <VirtualWorker id:jon #objects:16>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And each one has a chunk of the Housing Prices dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T15:08:13.015891Z",
     "start_time": "2019-09-26T15:08:12.925936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Wrapper)>[PointerTensor | me:15704123455 -> bob:7656584092]\n",
      "\tTags: #housing #boston_housing #data #boston _boston_dataset: .. \n",
      "\tShape: torch.Size([84, 13])\n",
      "\tDescription: .. _boston_dataset:...]\n",
      "[(Wrapper)>[PointerTensor | me:6967402526 -> theo:97080357589]\n",
      "\tTags: #housing #boston_housing #data #boston _boston_dataset: .. \n",
      "\tShape: torch.Size([84, 13])\n",
      "\tDescription: .. _boston_dataset:...]\n",
      "[(Wrapper)>[PointerTensor | me:25742840514 -> jason:39214753169]\n",
      "\tTags: #housing #boston_housing #data #boston _boston_dataset: .. \n",
      "\tShape: torch.Size([84, 13])\n",
      "\tDescription: .. _boston_dataset:...]\n",
      "[(Wrapper)>[PointerTensor | me:59710269881 -> alice:48332360695]\n",
      "\tTags: #housing #boston_housing #data #boston _boston_dataset: .. \n",
      "\tShape: torch.Size([84, 13])\n",
      "\tDescription: .. _boston_dataset:...]\n",
      "[(Wrapper)>[PointerTensor | me:35310100676 -> andy:31863090160]\n",
      "\tTags: #housing #boston_housing #data #boston _boston_dataset: .. \n",
      "\tShape: torch.Size([84, 13])\n",
      "\tDescription: .. _boston_dataset:...]\n",
      "[(Wrapper)>[PointerTensor | me:46964999822 -> jon:15524471034]\n",
      "\tTags: #housing #boston_housing #data #boston _boston_dataset: .. \n",
      "\tShape: torch.Size([86, 13])\n",
      "\tDescription: .. _boston_dataset:...]\n"
     ]
    }
   ],
   "source": [
    "for worker in workers:\n",
    "    print(worker.search(\"#housing\", \"#data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encrypted Linear Regression with PySyft "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Loading Housing Prices data from Virtual Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our Syft environment set, let's load the data.\n",
    "\n",
    "Please note that in order to avoid overflow with the SMPC computations performed by the linear model, and to maintain its stability, **we need to scale the data in a such way that the magnitude of each coordinate average lies in the interval [0.1, 10]**.\n",
    "\n",
    "Usually that can be done without revealing the data or the averages, you only need to have an idea of the order of magnitude. For example, if one of the coordinate is the surface of the house and it is represented in m², you should scale it by dividing by 100, as we know the surfaces of houses have an order of magnitude close to 100 in average.\n",
    "\n",
    "After running the model and obtaining the main statistics, we can rescale them back if needed. The same can be done with predictions.\n",
    "\n",
    "In this tutorial I will be loading the data and scale them following this idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T14:47:13.320716Z",
     "start_time": "2019-09-26T14:47:13.021320Z"
    }
   },
   "outputs": [],
   "source": [
    "scale_data = torch.Tensor([10., 10.,  10., 1., 1., 10., 100., 10., 10., 1000., 10., 1000., 10.])\n",
    "scale_target = 100.0\n",
    "\n",
    "housing_data = []\n",
    "housing_targets = []\n",
    "for worker in workers:\n",
    "    housing_data.append(worker.search(\"#housing\", \"#data\")[0] / scale_data.send(worker))\n",
    "    housing_targets.append(worker.search(\"#housing\", \"#target\")[0] / scale_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Setting up 2 more Virtual workers: the crypto provider and the \"honest but curious\" worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the linear regression, we will need **two more workers**, a *crypto provider* and a *honest but curious* worker. Both are necessary to assure the security of the SMPC computations when we run the model in a pool with more than 3 workers.\n",
    "\n",
    "> *Note: the **honest but curious** worker is a legitimate participant in a communication protocol who will not deviate from the defined protocol but will attempt to learn all possible information from legitimately received messages.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T13:54:18.021526Z",
     "start_time": "2019-09-26T13:54:18.016496Z"
    }
   },
   "outputs": [],
   "source": [
    "crypto_prov = sy.VirtualWorker(hook, id=\"crypto_prov\")\n",
    "hbc_worker = sy.VirtualWorker(hook, id=\"hbc_worker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Running Encrypted Linear Regression with SMPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import the BloomRegressor from the linalg module of pysyft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.frameworks.torch.linalg import BloomRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T13:55:57.469423Z",
     "start_time": "2019-09-26T13:54:18.416337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<syft.frameworks.torch.linalg.lr.BloomRegressor at 0x1463a0c90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crypto_lr = BloomRegressor(crypto_provider=crypto_prov, hbc_worker=hbc_worker)\n",
    "crypto_lr.fit(housing_data, housing_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the results with the method `.summarize()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T13:55:57.590073Z",
     "start_time": "2019-09-26T13:55:57.569935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "           SMPC Linear Regression Results\n",
      "====================================================\n",
      "                 value         stderr        p-value\n",
      "----------------------------------------------------\n",
      "coef1          -0.0108         0.0033         0.0011\n",
      "coef2           0.0046         0.0014         0.0008\n",
      "coef3           0.0020         0.0062         0.7408\n",
      "coef4           0.0269         0.0086         0.0019\n",
      "coef5          -0.1777         0.0382         0.0000\n",
      "coef6           0.3800         0.0418         0.0000\n",
      "coef7           0.0009         0.0132         0.9442\n",
      "coef8          -0.1473         0.0200         0.0000\n",
      "coef9           0.0306         0.0066         0.0000\n",
      "coef10         -0.1232         0.0376         0.0011\n",
      "coef11         -0.0953         0.0131         0.0000\n",
      "coef12          0.0932         0.0269         0.0006\n",
      "coef13         -0.0526         0.0051         0.0000\n",
      "intercept       0.3650         0.0511         0.0000\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "crypto_lr.summarize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the BloomRegressor does not only give the coefficients and intercept values, but also their standard errors and the p-values!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparing results with other linear regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to show the effectiveness of the BloomRegressor, let's compare it with the Linear Regression from other known libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Sending data to local server for comparison purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's send the data to the local worker and transform the `torch.Tensor`s in `numpy.array`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T13:58:06.094469Z",
     "start_time": "2019-09-26T13:58:06.073201Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_tensors = [x.copy().get() for x in housing_data] \n",
    "target_tensors = [y.copy().get() for y in housing_targets]\n",
    "\n",
    "data_np = torch.cat(data_tensors, dim=0).numpy()\n",
    "target_np = torch.cat(target_tensors, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's compare the results with the sklearn's Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T14:03:38.164358Z",
     "start_time": "2019-09-26T14:03:38.157311Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(data_np, target_np.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T14:08:33.158806Z",
     "start_time": "2019-09-26T14:08:33.142957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "Sklearn Linear Regression\n",
      "=========================\n",
      " coeff1          -0.0108\n",
      " coeff2           0.0046\n",
      " coeff3           0.0021\n",
      " coeff4           0.0269\n",
      " coeff5          -0.1777\n",
      " coeff6           0.3810\n",
      " coeff7           0.0007\n",
      " coeff8          -0.1476\n",
      " coeff9           0.0306\n",
      " coeff10         -0.1233\n",
      " coeff11         -0.0953\n",
      " coeff12          0.0931\n",
      " coeff13         -0.0525\n",
      " intercept:       0.3646\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 25)\n",
    "print(\"Sklearn Linear Regression\")\n",
    "print(\"=\" * 25)\n",
    "for i, coef in enumerate(lr.coef_, 1):\n",
    "    print(\" coeff{:<3d}\".format(i), \"{:>14.4f}\".format(coef))\n",
    "print(\" intercept:\", \"{:>12.4f}\".format(lr.intercept_))\n",
    "print(\"=\" * 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can notice that the are results are pretty much the same!! The are some small differences, but they are never higher than 0.2% of the value computed by the sklearn model!!**\n",
    "\n",
    "**For an ecrypted model that can compute linear regression coefficients without ever revealing the data, this is a huge achievement!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Statsmodel API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same using the Linear Regression from Statsmodel API, which also gives us the **standard errors** and **p-values** of the coefficients. We can then compare it with the results given by the BloomRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-26T13:58:29.092336Z",
     "start_time": "2019-09-26T13:58:27.263581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.741\n",
      "Model:                            OLS   Adj. R-squared:                  0.734\n",
      "Method:                 Least Squares   F-statistic:                     108.1\n",
      "Date:                Thu, 26 Sep 2019   Prob (F-statistic):          6.72e-135\n",
      "Time:                        15:58:29   Log-Likelihood:                 831.41\n",
      "No. Observations:                 506   AIC:                            -1635.\n",
      "Df Residuals:                     492   BIC:                            -1576.\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.3646      0.051      7.144      0.000       0.264       0.465\n",
      "x1            -0.0108      0.003     -3.287      0.001      -0.017      -0.004\n",
      "x2             0.0046      0.001      3.382      0.001       0.002       0.007\n",
      "x3             0.0021      0.006      0.334      0.738      -0.010       0.014\n",
      "x4             0.0269      0.009      3.118      0.002       0.010       0.044\n",
      "x5            -0.1777      0.038     -4.651      0.000      -0.253      -0.103\n",
      "x6             0.3810      0.042      9.116      0.000       0.299       0.463\n",
      "x7             0.0007      0.013      0.052      0.958      -0.025       0.027\n",
      "x8            -0.1476      0.020     -7.398      0.000      -0.187      -0.108\n",
      "x9             0.0306      0.007      4.613      0.000       0.018       0.044\n",
      "x10           -0.1233      0.038     -3.280      0.001      -0.197      -0.049\n",
      "x11           -0.0953      0.013     -7.283      0.000      -0.121      -0.070\n",
      "x12            0.0931      0.027      3.467      0.001       0.040       0.146\n",
      "x13           -0.0525      0.005    -10.347      0.000      -0.062      -0.043\n",
      "==============================================================================\n",
      "Omnibus:                      178.041   Durbin-Watson:                   1.078\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              783.126\n",
      "Skew:                           1.521   Prob(JB):                    8.84e-171\n",
      "Kurtosis:                       8.281   Cond. No.                         107.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "mod = sm.OLS(target_np.squeeze(), sm.add_constant(data_np), hasconst=True)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Once again, we can see that all results are pretty much the same!!**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Well Done!\n",
    "\n",
    "And voilà! We were able to train an OLS Regression model on distributed data and without ever seeing it. We were even able to compute standard errors and p-values for each coefficient.\n",
    "\n",
    "Also, after comparing our results with results given by other known libraries, we were able to validate this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the repositories! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Pick our tutorials on GitHub!\n",
    "\n",
    "We made really nice tutorials to get a better understanding of what Federated and Privacy-Preserving Learning should look like and how we are building the bricks for this to happen.\n",
    "\n",
    "- [Checkout the PySyft tutorials](https://github.com/OpenMined/PySyft/tree/master/examples/tutorials)\n",
    "\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! \n",
    "\n",
    "- [Join slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! If you want to start \"one off\" mini-projects, you can go to PySyft GitHub Issues page and search for issues marked `Good First Issue`.\n",
    "\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "- [Donate through OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
