{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building your own tensor type\n",
    "\n",
    "We present here a core concept of the PySyft library. It is the ability to add new custom tensor types that can provide specific behavior such as encryption or traceability. This feature makes our library universal and completely open to new innovations in the field of privacy-preserving machine learning.\n",
    "\n",
    "We will go through a very simple example which could be the base for a traceability feature that would keep track of the operations performed on the data in a verifiable way. This new tensor type will log all operations executed on tensors of its type. Let's call this type the CustomLoggingTensor.\n",
    "\n",
    "Authors:\n",
    "- Théo Ryffel - GitHub: [@LaRiffle](https://github.com/LaRiffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preliminaries\n",
    "\n",
    "We use the sandbox that we have already discovered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atrask/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/atrask/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/atrask/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/atrask/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import syft as sy\n",
    "sy.create_sandbox(globals(), verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first recall the notions of torch and syft tensors. All the object the end user manipulates are torch tensors. This is of course the case when it's a pure torch tensor (ex: `x = th.tensor([1., 2])`), but also when you deal with syft objects, such as the pointer tensor which is a particular case of syft tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>[PointerTensor | me:50467611289 -> bob:59436307353]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptr = th.tensor([1., 2]).send(bob)\n",
    "ptr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapper object you see is actually an empty torch tensor with a child argument which is a PointerTensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(ptr, th.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syft.frameworks.torch.tensors.interpreters.pointer.PointerTensor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ptr.child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also true for more complex objects, where you also see this Wrapper at the beginning. You can then have multiple Syft or Torch tensors chained through the `.child` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>FixedPrecisionTensor>(Wrapper)>[AdditiveSharingTensor]\n",
       "\t-> (Wrapper)>[PointerTensor | me:2632435195 -> alice:9137429922]\n",
       "\t-> (Wrapper)>[PointerTensor | me:72480781544 -> bob:38466088886]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = th.tensor([1., 2]).fix_prec().share(alice, bob)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FixedPrecisionTensor>(Wrapper)>[AdditiveSharingTensor]\n",
       "\t-> (Wrapper)>[PointerTensor | me:2632435195 -> alice:9137429922]\n",
       "\t-> (Wrapper)>[PointerTensor | me:72480781544 -> bob:38466088886]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AdditiveSharingTensor]\n",
       "\t-> (Wrapper)>[PointerTensor | me:2632435195 -> alice:9137429922]\n",
       "\t-> (Wrapper)>[PointerTensor | me:72480781544 -> bob:38466088886]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.child.child.child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the general behaviour is the following: each time a command in called on the top object, it goes down the chain where it can be modified, it is then executed at the bottom and the result is wrapped back to have exactly the some chain structure, to keep the same properties (such as traceability for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're going to do here is to create our own syft Tensor type that we will be able to put in this chain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The MVP of the CustomLoggingTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, there isn't much things to do. First, we need to create the tensor class.\n",
    "\n",
    "This is done in `syft/frameworks/torch/tensors/`, choose the folder:\n",
    "- `interpreters` if the functionality you want to build will modify the results or functions, or\n",
    "- `decorators` if the functionality is just ... decorative.\n",
    "\n",
    "Here we'll put it in the decorator folder. Choose a simple name, for now `decorators/custom_logging.py` will be sufficient.\n",
    "\n",
    "Write there the minimal class definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.frameworks.torch.tensors.interpreters.abstract import AbstractTensor\n",
    "\n",
    "class CustomLoggingTensor(AbstractTensor):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was quite fast, wasn't it?\n",
    "\n",
    "You now need to declare this type in the imports so that you can use it in real. Add it in the files:\n",
    "```\n",
    "- syft/frameworks/torch/tensors/decorator/__init__.py\n",
    "- syft/__init__.py\n",
    "```\n",
    "You should now be able to import the tensor type: `from syft import CustomLoggingTensor`\n",
    "\n",
    "Et voilà! You can already do many things with your new tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomLoggingTensor>None"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = CustomLoggingTensor()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok this is not super useful, but it comes with a `.on` method which works as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Wrapper)>CustomLoggingTensor>tensor([1., 2.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = th.tensor([1., 2])\n",
    "x = CustomLoggingTensor().on(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.on` simply inserts the tensor node into a tensor chain. As we always need to have a torch tensor at the top of the chain, a wrapper was automatically added.\n",
    "\n",
    " > As this point, if you want to have the behaviour desired, you should make the code changes in the repository: integrating the code in the repository allows you to benefit from the hooking functionalities. In particular, now your `CustomLoggingTensor` should have the methods a pure torch tensor has.\n",
    " \n",
    "You can already do computation of this chain such as `x * 2`, and the call `__mul__` made will be forwarded all through the chain down to the last node which is a pure torch tensor, whose value is doubled.\n",
    "\n",
    "```\n",
    "> x * 2\n",
    "\n",
    "Output:\n",
    "(Wrapper)>CustomLoggingTensor>tensor([2., 4.])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding special functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have defined your own tensor type, you should specify it's behaviour, as by default it won't do anything thing special and will just act passively.\n",
    "\n",
    "In this part, we will see how to specify custom functionalities. We'll use for the execution parts the already existing `LoggingTensor` instead of the `CustomLoggingTensor` and highlight which part of code produces which functionalities, so that you can run code in this notebook without reloading the kernel. If you want to practice more, you can report the code changes in the `CustomLoggingTensor` class definition and you'll observe the same behaviours (just reload the notebook each time to perform a modification in the library code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft import LoggingTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Default behaviour for functions\n",
    "\n",
    "You can add a special functionality each time a (hooked) torch function is called on `LoggingTensor`: here we just log the call.\n",
    "\n",
    "Note that this is for functions exclusively and not for methods, and applies for all hooked torch functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingTensor(AbstractTensor):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def on_function_call(cls, command):\n",
    "        \"\"\"\n",
    "        Override this to perform a specific action for each call of a torch\n",
    "        function with arguments containing syft tensors of the class doing\n",
    "        the overloading\n",
    "        \"\"\"\n",
    "        cmd, _, args, kwargs = command\n",
    "        print(\"Default log\", cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default log torch.div\n",
      "Default log torch.nn.functional.celu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Wrapper)>LoggingTensor>tensor([1., 2.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = th.tensor([1., 2])\n",
    "x = LoggingTensor().on(x)\n",
    "\n",
    "th.div(x, x)\n",
    "th.nn.functional.celu(x) # celu is a variant of the activation function relu(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Overloading torch methods\n",
    "\n",
    "We introduce here an important decorator object which is @overloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.frameworks.torch.overload_torch import overloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly overwrite torch methods like this, where we overload the `.add` method so that we first print that it was called and then forward the call to the .child attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingTensor(AbstractTensor):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    @overloaded.method\n",
    "    def add(self, _self, *args, **kwargs):\n",
    "        print(\"Log method add\")\n",
    "        response = _self.add(*args, **kwargs)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how to use the `@` `overloaded.method` decorator. To see\n",
    "what this decorator do, just look at the next method manual_add: it does\n",
    "exactly the same but without the decorator.\n",
    "\n",
    "Note the subtlety between `self` and `_self`: you should use `_self` and **NOT** `self`. We kept `self` because it can hold useful attributes that you might want to access (for example, for the fixed precision tensor it stores the field size)\n",
    "\n",
    "Here is the version of the add method without the decorator: as you can see\n",
    "it is much more complicated. However you might need sometimes to use it to specify\n",
    "some particular behaviour: so here what to start from!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingTensor(AbstractTensor):\n",
    "    \n",
    "    # [...]\n",
    "    \n",
    "    def manual_add(self, *args, **kwargs):\n",
    "        # Replace all syft tensor with their child attribute\n",
    "        new_self, new_args, new_kwargs = syft.frameworks.torch.hook_args.hook_method_args(\n",
    "            \"add\", self, args, kwargs\n",
    "        )\n",
    "\n",
    "        print(\"Log method manual_add\")\n",
    "        # Send it to the appropriate class and get the response\n",
    "        response = getattr(new_self, \"add\")(*new_args, **new_kwargs)\n",
    "\n",
    "        # Put back SyftTensor on the tensors found in the response\n",
    "        response = syft.frameworks.torch.hook_args.hook_response(\n",
    "            \"add\", response, wrap_type=type(self)\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They behave exactly the same and print a line when called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>LoggingTensor>tensor([1., 2.])\n",
      "Log method add\n"
     ]
    }
   ],
   "source": [
    "x = LoggingTensor().on(th.tensor([1., 2]))\n",
    "print(x)\n",
    "\n",
    "r = x.add(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_You might want to try to run_ `r = x.manual_add(x)` _but this will fail: if the LoggingTensor which is x.child had a_ `.manual_add` _method, this is not the case for the wrapper as torch tensor don't have_ `.manual_add` _by default._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Overloading torch functions\n",
    "\n",
    "We will still use the @overloaded decorator but now with:\n",
    "\n",
    "```\n",
    "- @overloaded.module\n",
    "- @overloaded.function\n",
    "```\n",
    "\n",
    "What we want to do is to overload \n",
    "\n",
    "```\n",
    "- torch.add\n",
    "- torch.nn.functional.relu\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoggingTensor(AbstractTensor):\n",
    "    \n",
    "    # [...] \n",
    "    \n",
    "    @staticmethod\n",
    "    @overloaded.module\n",
    "    def torch(module):\n",
    "        \"\"\"\n",
    "        We use the @overloaded.module to specify we're writing here\n",
    "        a function which should overload the function with the same\n",
    "        name in the <torch> module\n",
    "        :param module: object which stores the overloading functions\n",
    "\n",
    "        Note that we used the @staticmethod decorator as we're in a\n",
    "        class\n",
    "        \"\"\"\n",
    "\n",
    "        def add(x, y):\n",
    "            \"\"\"\n",
    "            You can write the function to overload in the most natural\n",
    "            way, so this will be called whenever you call torch.add on\n",
    "            Logging Tensors, and the x and y you get are also Logging\n",
    "            Tensors, so compared to the @overloaded.method, you see\n",
    "            that the @overloaded.module does not hook the arguments.\n",
    "            \"\"\"\n",
    "            print(\"Log function torch.add\")\n",
    "            return x + y\n",
    "\n",
    "        # Just register it using the module variable\n",
    "        module.add = add\n",
    "\n",
    "        @overloaded.function\n",
    "        def mul(x, y):\n",
    "            \"\"\"\n",
    "            You can also add the @overloaded.function decorator to also\n",
    "            hook arguments, ie all the LoggingTensor are replaced with\n",
    "            their child attribute\n",
    "            \"\"\"\n",
    "            print(\"Log function torch.mul\")\n",
    "            return x * y\n",
    "\n",
    "        # Just register it using the module variable\n",
    "        module.mul = mul\n",
    "\n",
    "        # You can also overload functions in submodules!\n",
    "        @overloaded.module\n",
    "        def nn(module):\n",
    "            \"\"\"\n",
    "            The syntax is the same, so @overloaded.module handles recursion\n",
    "            Note that we don't need to add the @staticmethod decorator\n",
    "            \"\"\"\n",
    "\n",
    "            @overloaded.module\n",
    "            def functional(module):\n",
    "                def relu(x):\n",
    "                    print(\"Log function torch.nn.functional.relu\")\n",
    "                    return x * (x.child > 0)\n",
    "\n",
    "                module.relu = relu\n",
    "\n",
    "            module.functional = functional\n",
    "\n",
    "        # Modules should be registered just like functions\n",
    "        module.nn = nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look how it changes compared to 2.1: the behaviour is not much different but now the functions modified are very precisely targetted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default log torch.div\n",
      "Log function torch.add\n"
     ]
    }
   ],
   "source": [
    "x = th.tensor([1., 2])\n",
    "x = LoggingTensor().on(x)\n",
    "\n",
    "# Default overloading made in 2.1\n",
    "r = th.div(x, x)\n",
    "\n",
    "# Targetted overloading made in 2.3\n",
    "r = th.add(x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here you are, you should now understand all the tools we've builded so that you can easily build new tensor types and focus on their behaviour rather than on their integration in the PySyft library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the Repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
