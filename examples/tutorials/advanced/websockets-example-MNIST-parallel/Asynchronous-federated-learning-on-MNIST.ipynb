{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Asynchronous federated learning on MNIST\n",
    "\n",
    "This notebook will go through the steps to run a federated learning via websocket workers in an asynchronous way using [TrainConfig](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/advanced/Federated%20Learning%20with%20TrainConfig/Introduction%20to%20TrainConfig.ipynb). We will use federated averaging to join the remotely trained models.\n",
    "\n",
    "Authors:\n",
    "- Silvia - GitHub [@midokura-silvia](https://github.com/midokura-silvia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning setup\n",
    "\n",
    "For a Federated Learning setup with TrainConfig we need different participants:\n",
    "\n",
    "* _Workers_: own datasets.\n",
    "\n",
    "* _Coordinator_: an entity that knows the workers and the dataset name that lives in each worker. \n",
    "\n",
    "* _Evaluator_: holds the testing data and tracks model performance \n",
    "\n",
    "Each worker is represented by two parts, a proxy local to the scheduler (websocket client worker) and the remote instance that holds the data and performs the computations. The remote part is called a websocket server worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Start the websocket workers\n",
    "So first, we need to create the remote workers. For this, you need to run in a terminal (not possible from the notebook):\n",
    "\n",
    "```bash\n",
    "python start_websocket_servers.py\n",
    "```\n",
    "\n",
    "#### What's going on?\n",
    "\n",
    "The script will instantiate three workers, Alice, Bob and Charlie and prepare their local data. \n",
    "Each worker is set up to have a subset of the MNIST training dataset. \n",
    "Alice holds all images corresponding to the digits 0-3, \n",
    "Bob holds all images corresponding to the digits 4-6 and \n",
    "Charlie holds all images corresponding to the digits 7-9. \n",
    "\n",
    "| Worker      | Digits in local dataset | Number of samples |\n",
    "| ----------- | ----------------------- | ----------------- |\n",
    "| Alice       | 0-3                     | 24754             |\n",
    "| Bob         | 4-6                     | 17181             |\n",
    "| Charlie     | 7-9                     | 18065             |\n",
    "\n",
    "\n",
    "The evaluator will be called Testing and holds the entire MNIST testing dataset.\n",
    "\n",
    "| Evaluator   | Digits in local dataset | Number of samples |\n",
    "| ----------- | ----------------------- | ----------------- |\n",
    "| Testing     | 0-9                     | 10000             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following to see the code of the function that starts a worker\n",
    "# import run_websocket_server\n",
    "\n",
    "# print(inspect.getsource(run_websocket_server.start_websocket_server_worker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing let's first need to import dependencies, setup needed arguments and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import sys\n",
    "import asyncio\n",
    "\n",
    "import syft as sy\n",
    "from syft.workers.websocket_client import WebsocketClientWorker\n",
    "from syft.frameworks.torch.federated import utils\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "\n",
    "import run_websocket_client as rwc\n",
    "if torch.__version__>= \"1.0.2\":\n",
    "    raise ValueError(f\"This tutorial currently does not support torch versions >= 1.0.2, you have version {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook torch\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=32, cuda=False, federate_after_n_batches=10, lr=0.1, save_model=False, seed=1, test_batch_size=128, training_rounds=40, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "args = rwc.define_and_get_arguments(args=[])\n",
    "use_cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"run_websocket_client\")\n",
    "\n",
    "if not len(logger.handlers):\n",
    "    FORMAT = \"%(asctime)s - %(message)s\"\n",
    "    DATE_FMT = \"%H:%M:%S\"\n",
    "    formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "LOG_LEVEL = logging.DEBUG\n",
    "logger.setLevel(LOG_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's instantiate the websocket client workers, our local proxies to the remote workers.\n",
    "Note that **this step will fail, if the websocket server workers are not running**.\n",
    "\n",
    "The workers Alice, Bob and Charlie will perform the training, wheras the testing worker hosts the test data and performs the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_websocket = {\"host\": \"0.0.0.0\", \"hook\": hook, \"verbose\": args.verbose}\n",
    "alice = WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket)\n",
    "bob = WebsocketClientWorker(id=\"bob\", port=8778, **kwargs_websocket)\n",
    "charlie = WebsocketClientWorker(id=\"charlie\", port=8779, **kwargs_websocket)\n",
    "testing = WebsocketClientWorker(id=\"testing\", port=8780, **kwargs_websocket)\n",
    "\n",
    "worker_instances = [alice, bob, charlie]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Let's instantiate the machine learning model. It is a small neural network with 2 convolutional and two fully connected layers. \n",
    "It uses ReLU activations and max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
      "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
      "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
      "        self.fc2 = nn.Linear(500, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = F.relu(self.conv1(x))\n",
      "        x = F.max_pool2d(x, 2, 2)\n",
      "        x = F.relu(self.conv2(x))\n",
      "        x = F.max_pool2d(x, 2, 2)\n",
      "        x = x.view(-1, 4 * 4 * 50)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return F.log_softmax(x, dim=1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(rwc.Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = rwc.Net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the model serializable\n",
    "\n",
    "In order to send the model to the workers we need the model to be serializable, for this we use [`jit`](https://pytorch.org/docs/stable/jit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, torch.zeros([1, 1, 28, 28], dtype=torch.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start the training\n",
    "\n",
    "Now we are ready to start the federated training. We will perform training over a given number of batches separately on each worker and then calculate the federated average of the resulting model.\n",
    "\n",
    "Every 10th training round we will evaluate the performance of the models returned by the workers and of the model obtained by federated averaging. \n",
    "\n",
    "The performance will be given both as the accuracy (ratio of correct predictions) and as the histograms of predicted digits. This is of interest, as each worker only owns a subset of the digits. Therefore, in the beginning each worker will only predict their numbers and only know about the other numbers via the federated averaging process.\n",
    "\n",
    "The training is done in an asynchronous manner. This means that the scheduler just tell the workers to train and does not block to wait for the result of the training before talking to the next worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the training are given in the arguments. \n",
    "Each worker will train on a given number of batches, given by the value of federate_after_n_batches.\n",
    "The training batch size and learning rate are also configured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federate_after_n_batches: 10\n",
      "Batch size: 32\n",
      "Initial learning rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Federate_after_n_batches: \" + str(args.federate_after_n_batches))\n",
    "print(\"Batch size: \" + str(args.batch_size))\n",
    "print(\"Initial learning rate: \" + str(args.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:47:36 - Training round 1/40\n",
      "12:47:40 - Evaluating models\n",
      "12:47:42 - Model update alice: Percentage numbers 0-3: 100%, 4-6: 0%, 7-9: 0%\n",
      "12:47:42 - Model update alice: Average loss: 0.0194, Accuracy: 2109/10000 (21.09%)\n",
      "12:47:44 - Model update bob: Percentage numbers 0-3: 0%, 4-6: 100%, 7-9: 0%\n",
      "12:47:44 - Model update bob: Average loss: 0.0409, Accuracy: 958/10000 (9.58%)\n",
      "12:47:46 - Model update charlie: Percentage numbers 0-3: 0%, 4-6: 0%, 7-9: 100%\n",
      "12:47:46 - Model update charlie: Average loss: 0.0182, Accuracy: 1009/10000 (10.09%)\n",
      "12:47:48 - Federated model: Percentage numbers 0-3: 0%, 4-6: 99%, 7-9: 0%\n",
      "12:47:48 - Federated model: Average loss: 0.0179, Accuracy: 958/10000 (9.58%)\n",
      "12:47:48 - Training round 2/40\n",
      "12:47:52 - Training round 3/40\n",
      "12:47:57 - Training round 4/40\n",
      "12:48:02 - Training round 5/40\n",
      "12:48:07 - Training round 6/40\n",
      "12:48:12 - Training round 7/40\n",
      "12:48:17 - Training round 8/40\n",
      "12:48:21 - Training round 9/40\n",
      "12:48:26 - Training round 10/40\n",
      "12:48:31 - Training round 11/40\n",
      "12:48:36 - Evaluating models\n",
      "12:48:38 - Model update alice: Percentage numbers 0-3: 82%, 4-6: 13%, 7-9: 3%\n",
      "12:48:38 - Model update alice: Average loss: 0.0114, Accuracy: 5407/10000 (54.07%)\n",
      "12:48:39 - Model update bob: Percentage numbers 0-3: 18%, 4-6: 73%, 7-9: 8%\n",
      "12:48:39 - Model update bob: Average loss: 0.0125, Accuracy: 5364/10000 (53.64%)\n",
      "12:48:41 - Model update charlie: Percentage numbers 0-3: 7%, 4-6: 0%, 7-9: 92%\n",
      "12:48:41 - Model update charlie: Average loss: 0.0215, Accuracy: 3659/10000 (36.59%)\n",
      "12:48:43 - Federated model: Percentage numbers 0-3: 40%, 4-6: 18%, 7-9: 41%\n",
      "12:48:43 - Federated model: Average loss: 0.0036, Accuracy: 8283/10000 (82.83%)\n",
      "12:48:43 - Training round 12/40\n",
      "12:48:48 - Training round 13/40\n",
      "12:48:53 - Training round 14/40\n",
      "12:48:58 - Training round 15/40\n",
      "12:49:03 - Training round 16/40\n",
      "12:49:08 - Training round 17/40\n",
      "12:49:13 - Training round 18/40\n",
      "12:49:18 - Training round 19/40\n",
      "12:49:23 - Training round 20/40\n",
      "12:49:28 - Training round 21/40\n",
      "12:49:33 - Evaluating models\n",
      "12:49:35 - Model update alice: Percentage numbers 0-3: 64%, 4-6: 21%, 7-9: 14%\n",
      "12:49:35 - Model update alice: Average loss: 0.0062, Accuracy: 7351/10000 (73.51%)\n",
      "12:49:37 - Model update bob: Percentage numbers 0-3: 27%, 4-6: 59%, 7-9: 13%\n",
      "12:49:37 - Model update bob: Average loss: 0.0086, Accuracy: 6647/10000 (66.47%)\n",
      "12:49:39 - Model update charlie: Percentage numbers 0-3: 15%, 4-6: 3%, 7-9: 80%\n",
      "12:49:39 - Model update charlie: Average loss: 0.0167, Accuracy: 4817/10000 (48.17%)\n",
      "12:49:40 - Federated model: Percentage numbers 0-3: 41%, 4-6: 25%, 7-9: 33%\n",
      "12:49:40 - Federated model: Average loss: 0.0020, Accuracy: 9118/10000 (91.18%)\n",
      "12:49:40 - Training round 22/40\n",
      "12:49:45 - Training round 23/40\n",
      "12:49:50 - Training round 24/40\n",
      "12:49:55 - Training round 25/40\n",
      "12:50:01 - Training round 26/40\n",
      "12:50:06 - Training round 27/40\n",
      "12:50:11 - Training round 28/40\n",
      "12:50:16 - Training round 29/40\n",
      "12:50:22 - Training round 30/40\n",
      "12:50:27 - Training round 31/40\n",
      "12:50:32 - Evaluating models\n",
      "12:50:34 - Model update alice: Percentage numbers 0-3: 59%, 4-6: 17%, 7-9: 23%\n",
      "12:50:34 - Model update alice: Average loss: 0.0064, Accuracy: 8033/10000 (80.33%)\n",
      "12:50:36 - Model update bob: Percentage numbers 0-3: 34%, 4-6: 48%, 7-9: 16%\n",
      "12:50:36 - Model update bob: Average loss: 0.0058, Accuracy: 7768/10000 (77.68%)\n",
      "12:50:38 - Model update charlie: Percentage numbers 0-3: 23%, 4-6: 11%, 7-9: 65%\n",
      "12:50:38 - Model update charlie: Average loss: 0.0083, Accuracy: 6364/10000 (63.64%)\n",
      "12:50:40 - Federated model: Percentage numbers 0-3: 42%, 4-6: 26%, 7-9: 31%\n",
      "12:50:40 - Federated model: Average loss: 0.0014, Accuracy: 9427/10000 (94.27%)\n",
      "12:50:40 - Training round 32/40\n",
      "12:50:46 - Training round 33/40\n",
      "12:50:51 - Training round 34/40\n",
      "12:50:56 - Training round 35/40\n",
      "12:51:02 - Training round 36/40\n",
      "12:51:07 - Training round 37/40\n",
      "12:51:13 - Training round 38/40\n",
      "12:51:17 - Training round 39/40\n",
      "12:51:22 - Training round 40/40\n",
      "12:51:28 - Evaluating models\n",
      "12:51:30 - Model update alice: Percentage numbers 0-3: 53%, 4-6: 21%, 7-9: 24%\n",
      "12:51:30 - Model update alice: Average loss: 0.0031, Accuracy: 8638/10000 (86.38%)\n",
      "12:51:32 - Model update bob: Percentage numbers 0-3: 31%, 4-6: 50%, 7-9: 17%\n",
      "12:51:32 - Model update bob: Average loss: 0.0061, Accuracy: 7598/10000 (75.98%)\n",
      "12:51:34 - Model update charlie: Percentage numbers 0-3: 32%, 4-6: 17%, 7-9: 50%\n",
      "12:51:34 - Model update charlie: Average loss: 0.0048, Accuracy: 7838/10000 (78.38%)\n",
      "12:51:36 - Federated model: Percentage numbers 0-3: 41%, 4-6: 28%, 7-9: 29%\n",
      "12:51:36 - Federated model: Average loss: 0.0011, Accuracy: 9563/10000 (95.63%)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = args.lr\n",
    "traced_model = torch.jit.trace(model, torch.zeros([1, 1, 28, 28], dtype=torch.float))\n",
    "for curr_round in range(1, args.training_rounds + 1):\n",
    "    logger.info(\"Training round %s/%s\", curr_round, args.training_rounds)\n",
    "\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            rwc.fit_model_on_worker(\n",
    "                worker=worker,\n",
    "                traced_model=traced_model,\n",
    "                batch_size=args.batch_size,\n",
    "                curr_round=curr_round,\n",
    "                max_nr_batches=args.federate_after_n_batches,\n",
    "                lr=learning_rate,\n",
    "            )\n",
    "            for worker in worker_instances\n",
    "        ]\n",
    "    )\n",
    "    models = {}\n",
    "    loss_values = {}\n",
    "\n",
    "    test_models = curr_round % 10 == 1 or curr_round == args.training_rounds\n",
    "    if test_models:\n",
    "        logger.info(\"Evaluating models\")\n",
    "        np.set_printoptions(formatter={\"float\": \"{: .0f}\".format})\n",
    "        for worker_id, worker_model, _ in results:\n",
    "            rwc.evaluate_model_on_worker(\n",
    "                model_identifier=\"Model update \" + worker_id,\n",
    "                worker=testing,\n",
    "                dataset_key=\"mnist_testing\",\n",
    "                model=worker_model,\n",
    "                nr_bins=10,\n",
    "                batch_size=128,\n",
    "                print_target_hist=False,\n",
    "            )\n",
    "\n",
    "    # Federate models (note that this will also change the model in models[0]\n",
    "    for worker_id, worker_model, worker_loss in results:\n",
    "        if worker_model is not None:\n",
    "            models[worker_id] = worker_model\n",
    "            loss_values[worker_id] = worker_loss\n",
    "\n",
    "    traced_model = utils.federated_avg(models)\n",
    "\n",
    "    if test_models:\n",
    "        rwc.evaluate_model_on_worker(\n",
    "            model_identifier=\"Federated model\",\n",
    "            worker=testing,\n",
    "            dataset_key=\"mnist_testing\",\n",
    "            model=traced_model,\n",
    "            nr_bins=10,\n",
    "            batch_size=128,\n",
    "            print_target_hist=False,\n",
    "        )\n",
    "\n",
    "    # decay learning rate\n",
    "    learning_rate = max(0.98 * learning_rate, args.lr * 0.01)\n",
    "\n",
    "if args.save_model:\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 40 rounds of training we achieve an accuracy larger than 95% on the entire testing dataset. \n",
    "This is impressing, given that no worker has access to more than 4 digits!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (openmined)",
   "language": "python",
   "name": "openmined"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
