{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Asynchronous federated learning on MNIST\n",
    "\n",
    "This notebook will go through the steps to run a federated learning via websocket workers in an asynchronous way using [TrainConfig](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/advanced/Federated%20Learning%20with%20TrainConfig/Introduction%20to%20TrainConfig.ipynb). We will use federated averaging to join the remotely trained models.\n",
    "\n",
    "Authors:\n",
    "- Silvia - GitHub [@midokura-silvia](https://github.com/midokura-silvia)\n",
    "- Marianne Monteiro - Twitter [@hereismari](https://twitter.com/hereismari) - Github [@mari-linhares\n",
    "](https://github.com/mari-linhares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning setup\n",
    "\n",
    "For a Federated Learning setup with TrainConfig we need different participants:\n",
    "\n",
    "* _Workers_ that own datasets.\n",
    "\n",
    "* An entity that knows the workers and the dataset name that lives in each worker. We'll call this a _scheduler_.\n",
    "\n",
    "Each worker is represented by two parts, a proxy local to the scheduler (websocket client worker) and the remote instance that holds the data and performs the computations. The remote part is called a websocket server worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Start the websocket workers\n",
    "So first, we need to create the remote workers. For this, you need to run in a terminal (not possible from the notebook):\n",
    "\n",
    "```bash\n",
    "python start_websocket_servers.py\n",
    "```\n",
    "\n",
    "#### What's going on?\n",
    "\n",
    "The script will instantiate three workers, Alice, Bob and Charlie and prepare their local data. \n",
    "Each worker is set up to have a subset of the MNIST training dataset. \n",
    "Alice holds all images corresponding to the digits 0-3, \n",
    "Bob holds all images corresponding to the digits 4-6 and \n",
    "Charlie holds all images corresponding to the digits 7-9.\n",
    "\n",
    "| Worker      | Digits in local dataset | Number of samples |\n",
    "| ----------- | ----------------------- | ----------------- |\n",
    "| Alice       | 0-3                     | 24754             |\n",
    "| Bob         | 4-6                     | 17181             |\n",
    "| Charlie     | 7-9                     | 18065             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following to see the code of the function that starts a worker\n",
    "# import run_websocket_server\n",
    "\n",
    "# print(inspect.getsource(run_websocket_server.start_websocket_server_worker))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing let's first need to import dependencies, setup needed arguments and configure logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import sys\n",
    "import asyncio\n",
    "\n",
    "import syft as sy\n",
    "from syft.workers import WebsocketClientWorker\n",
    "from syft.frameworks.torch.federated import utils\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import run_websocket_client as rwc\n",
    "if torch.__version__>= \"1.0.2\":\n",
    "    raise ValueError(f\"This tutorial currently does not support torch versions >= 1.0.2, you have version {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook torch\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=32, cuda=False, federate_after_n_batches=10, lr=0.1, save_model=False, seed=1, test_batch_size=128, training_rounds=40, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# Arguments\n",
    "args = rwc.define_and_get_arguments(args=[])\n",
    "use_cuda = args.cuda and torch.cuda.is_available()\n",
    "torch.manual_seed(args.seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"run_websocket_client\")\n",
    "\n",
    "if not len(logger.handlers):\n",
    "    FORMAT = \"%(asctime)s - %(message)s\"\n",
    "    DATE_FMT = \"%H:%M:%S\"\n",
    "    formatter = logging.Formatter(FORMAT, DATE_FMT)\n",
    "    handler = logging.StreamHandler()\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "LOG_LEVEL = logging.DEBUG\n",
    "logger.setLevel(LOG_LEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's instantiate the websocket client workers, our local proxies to the remote workers.\n",
    "Note that **this step will fail, if the websocket server workers are not running**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<WebsocketClientWorker id:alice #objects local:0 #objects remote: 1>, <WebsocketClientWorker id:bob #objects local:0 #objects remote: 1>, <WebsocketClientWorker id:charlie #objects local:0 #objects remote: 1>]\n"
     ]
    }
   ],
   "source": [
    "kwargs_websocket = {\"host\": \"localhost\", \"hook\": hook, \"verbose\": args.verbose}\n",
    "alice = WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket)\n",
    "bob = WebsocketClientWorker(id=\"bob\", port=8778, **kwargs_websocket)\n",
    "charlie = WebsocketClientWorker(id=\"charlie\", port=8779, **kwargs_websocket)\n",
    "\n",
    "worker_instances = [alice, bob, charlie]\n",
    "print(worker_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Let's instantiate the machine learning model. It is a small neural network with 2 convolutional and two fully connected layers. \n",
    "It uses ReLU activations and max pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
      "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
      "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
      "        self.fc2 = nn.Linear(500, 10)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = F.relu(self.conv1(x))\n",
      "        x = F.max_pool2d(x, 2, 2)\n",
      "        x = F.relu(self.conv2(x))\n",
      "        x = F.max_pool2d(x, 2, 2)\n",
      "        x = x.view(-1, 4 * 4 * 50)\n",
      "        x = F.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return F.log_softmax(x, dim=1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(rwc.Net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
      "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = rwc.Net().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data\n",
    "\n",
    "The training data lives in each of the devices, but fefore starting the training, let's load the MNIST test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"./data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=args.test_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "(data, target) = test_loader.__iter__().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making the model serializable\n",
    "\n",
    "In order to send the model to the workers we need the model to be serializable, for this we use [`jit`](https://pytorch.org/docs/stable/jit.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start the training\n",
    "\n",
    "Now we are ready to start the federated training. We will perform training over a given number of batches separately on each worker and then calculate the federated average of the resulting model.\n",
    "\n",
    "Every 10th training round we will evaluate the performance of the models returned by the workers and of the model obtained by federated averaging. \n",
    "\n",
    "The performance will be given both as the accuracy (ratio of correct predictions) and as the histograms of predicted digits. This is of interest, as each worker only owns a subset of the digits. Therefore, in the beginning each worker will only predict their numbers and only know about the other numbers via the federated averaging process.\n",
    "\n",
    "The training is done in an asynchronous manner. This means that the scheduler just tell the workers to train and does not block to wait for the result of the training before talking to the next worker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the training are given in the arguments. \n",
    "Each worker will train on a given number of batches, given by the value of federate_after_n_batches.\n",
    "The training batch size and learning rate are also configured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Federate_after_n_batches: 10\n",
      "Batch size: 32\n",
      "Initial learning rate: 0.1\n"
     ]
    }
   ],
   "source": [
    "print(\"Federate_after_n_batches: \" + str(args.federate_after_n_batches))\n",
    "print(\"Batch size: \" + str(args.batch_size))\n",
    "print(\"Initial learning rate: \" + str(args.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:41:42 - Starting training round 1/40\n",
      "11:41:42 - Training round 1, calling fit on worker: alice, lr = 0.100\n",
      "11:41:42 - Training round 1, calling fit on worker: bob, lr = 0.100\n",
      "11:41:42 - Training round 1, calling fit on worker: charlie, lr = 0.100\n",
      "11:41:43 - Training round: 1, worker: alice, avg_loss: tensor(2.3841, grad_fn=<MeanBackward1>)\n",
      "11:41:44 - Training round: 1, worker: charlie, avg_loss: tensor(1.7610, grad_fn=<MeanBackward1>)\n",
      "11:41:45 - Training round: 1, worker: bob, avg_loss: tensor(1.4272, grad_fn=<MeanBackward1>)\n",
      "11:41:50 - Prediction hist.: [ 473  4436  5091  0  0  0  0  0  0  0]\n",
      "11:41:50 - alice: Test set: Average loss: 0.0184, Accuracy: 2261/10000 (22.61)\n",
      "11:41:52 - Prediction hist.: [ 0  0  0  0  8661  0  1339  0  0  0]\n",
      "11:41:52 - bob: Test set: Average loss: 0.0231, Accuracy: 1614/10000 (16.14)\n",
      "11:41:55 - Prediction hist.: [ 0  0  0  0  0  0  0  0  0  10000]\n",
      "11:41:55 - charlie: Test set: Average loss: 0.0224, Accuracy: 1009/10000 (10.09)\n",
      "11:41:58 - Prediction hist.: [ 9  848  13  0  270  0  780  0  211  7869]\n",
      "11:41:58 - Federated model: Test set: Average loss: 0.0178, Accuracy: 1882/10000 (18.82)\n",
      "11:41:58 - Starting training round 2/40\n",
      "11:41:58 - Training round 2, calling fit on worker: alice, lr = 0.098\n",
      "11:41:58 - Training round 2, calling fit on worker: bob, lr = 0.098\n",
      "11:41:58 - Training round 2, calling fit on worker: charlie, lr = 0.098\n",
      "11:41:59 - Training round: 2, worker: alice, avg_loss: tensor(1.2123, grad_fn=<MeanBackward1>)\n",
      "11:42:00 - Training round: 2, worker: bob, avg_loss: tensor(1.7183, grad_fn=<MeanBackward1>)\n",
      "11:42:02 - Training round: 2, worker: charlie, avg_loss: tensor(1.8563, grad_fn=<MeanBackward1>)\n",
      "11:42:03 - Starting training round 3/40\n",
      "11:42:03 - Training round 3, calling fit on worker: alice, lr = 0.096\n",
      "11:42:03 - Training round 3, calling fit on worker: bob, lr = 0.096\n",
      "11:42:03 - Training round 3, calling fit on worker: charlie, lr = 0.096\n",
      "11:42:04 - Training round: 3, worker: charlie, avg_loss: tensor(1.4111, grad_fn=<MeanBackward1>)\n",
      "11:42:05 - Training round: 3, worker: bob, avg_loss: tensor(0.9250, grad_fn=<MeanBackward1>)\n",
      "11:42:07 - Training round: 3, worker: alice, avg_loss: tensor(0.7739, grad_fn=<MeanBackward1>)\n",
      "11:42:08 - Starting training round 4/40\n",
      "11:42:08 - Training round 4, calling fit on worker: alice, lr = 0.094\n",
      "11:42:08 - Training round 4, calling fit on worker: bob, lr = 0.094\n",
      "11:42:08 - Training round 4, calling fit on worker: charlie, lr = 0.094\n",
      "11:42:09 - Training round: 4, worker: bob, avg_loss: tensor(0.2962, grad_fn=<MeanBackward1>)\n",
      "11:42:10 - Training round: 4, worker: alice, avg_loss: tensor(1.2170, grad_fn=<MeanBackward1>)\n",
      "11:42:12 - Training round: 4, worker: charlie, avg_loss: tensor(0.3557, grad_fn=<MeanBackward1>)\n",
      "11:42:13 - Starting training round 5/40\n",
      "11:42:13 - Training round 5, calling fit on worker: alice, lr = 0.092\n",
      "11:42:13 - Training round 5, calling fit on worker: bob, lr = 0.092\n",
      "11:42:13 - Training round 5, calling fit on worker: charlie, lr = 0.092\n",
      "11:42:14 - Training round: 5, worker: alice, avg_loss: tensor(0.2640, grad_fn=<MeanBackward1>)\n",
      "11:42:15 - Training round: 5, worker: charlie, avg_loss: tensor(0.3835, grad_fn=<MeanBackward1>)\n",
      "11:42:16 - Training round: 5, worker: bob, avg_loss: tensor(0.1664, grad_fn=<MeanBackward1>)\n",
      "11:42:18 - Starting training round 6/40\n",
      "11:42:18 - Training round 6, calling fit on worker: alice, lr = 0.090\n",
      "11:42:18 - Training round 6, calling fit on worker: bob, lr = 0.090\n",
      "11:42:18 - Training round 6, calling fit on worker: charlie, lr = 0.090\n",
      "11:42:18 - Training round: 6, worker: alice, avg_loss: tensor(0.1282, grad_fn=<MeanBackward1>)\n",
      "11:42:20 - Training round: 6, worker: bob, avg_loss: tensor(0.3642, grad_fn=<MeanBackward1>)\n",
      "11:42:21 - Training round: 6, worker: charlie, avg_loss: tensor(0.5357, grad_fn=<MeanBackward1>)\n",
      "11:42:22 - Starting training round 7/40\n",
      "11:42:23 - Training round 7, calling fit on worker: alice, lr = 0.089\n",
      "11:42:23 - Training round 7, calling fit on worker: bob, lr = 0.089\n",
      "11:42:23 - Training round 7, calling fit on worker: charlie, lr = 0.089\n",
      "11:42:23 - Training round: 7, worker: bob, avg_loss: tensor(0.3908, grad_fn=<MeanBackward1>)\n",
      "11:42:25 - Training round: 7, worker: alice, avg_loss: tensor(0.1498, grad_fn=<MeanBackward1>)\n",
      "11:42:26 - Training round: 7, worker: charlie, avg_loss: tensor(0.2090, grad_fn=<MeanBackward1>)\n",
      "11:42:28 - Starting training round 8/40\n",
      "11:42:28 - Training round 8, calling fit on worker: alice, lr = 0.087\n",
      "11:42:28 - Training round 8, calling fit on worker: bob, lr = 0.087\n",
      "11:42:28 - Training round 8, calling fit on worker: charlie, lr = 0.087\n",
      "11:42:28 - Training round: 8, worker: bob, avg_loss: tensor(0.2635, grad_fn=<MeanBackward1>)\n",
      "11:42:30 - Training round: 8, worker: charlie, avg_loss: tensor(0.3293, grad_fn=<MeanBackward1>)\n",
      "11:42:31 - Training round: 8, worker: alice, avg_loss: tensor(0.7170, grad_fn=<MeanBackward1>)\n",
      "11:42:33 - Starting training round 9/40\n",
      "11:42:33 - Training round 9, calling fit on worker: alice, lr = 0.085\n",
      "11:42:33 - Training round 9, calling fit on worker: bob, lr = 0.085\n",
      "11:42:33 - Training round 9, calling fit on worker: charlie, lr = 0.085\n",
      "11:42:33 - Training round: 9, worker: alice, avg_loss: tensor(0.9477, grad_fn=<MeanBackward1>)\n",
      "11:42:35 - Training round: 9, worker: charlie, avg_loss: tensor(0.6105, grad_fn=<MeanBackward1>)\n",
      "11:42:36 - Training round: 9, worker: bob, avg_loss: tensor(0.2974, grad_fn=<MeanBackward1>)\n",
      "11:42:38 - Starting training round 10/40\n",
      "11:42:38 - Training round 10, calling fit on worker: alice, lr = 0.083\n",
      "11:42:38 - Training round 10, calling fit on worker: bob, lr = 0.083\n",
      "11:42:38 - Training round 10, calling fit on worker: charlie, lr = 0.083\n",
      "11:42:39 - Training round: 10, worker: charlie, avg_loss: tensor(0.1805, grad_fn=<MeanBackward1>)\n",
      "11:42:40 - Training round: 10, worker: alice, avg_loss: tensor(0.0587, grad_fn=<MeanBackward1>)\n",
      "11:42:42 - Training round: 10, worker: bob, avg_loss: tensor(0.2199, grad_fn=<MeanBackward1>)\n",
      "11:42:43 - Starting training round 11/40\n",
      "11:42:43 - Training round 11, calling fit on worker: alice, lr = 0.082\n",
      "11:42:43 - Training round 11, calling fit on worker: bob, lr = 0.082\n",
      "11:42:43 - Training round 11, calling fit on worker: charlie, lr = 0.082\n",
      "11:42:44 - Training round: 11, worker: bob, avg_loss: tensor(0.1941, grad_fn=<MeanBackward1>)\n",
      "11:42:45 - Training round: 11, worker: alice, avg_loss: tensor(0.0456, grad_fn=<MeanBackward1>)\n",
      "11:42:46 - Training round: 11, worker: charlie, avg_loss: tensor(0.2498, grad_fn=<MeanBackward1>)\n",
      "11:42:51 - Prediction hist.: [ 1457  1475  2846  3560  230  69  59  216  24  64]\n",
      "11:42:51 - alice: Test set: Average loss: 0.0131, Accuracy: 4681/10000 (46.81)\n",
      "11:42:54 - Prediction hist.: [ 0  585  305  0  1450  6142  1234  284  0  0]\n",
      "11:42:54 - bob: Test set: Average loss: 0.0232, Accuracy: 3845/10000 (38.45)\n",
      "11:42:57 - Prediction hist.: [ 646  19  409  14  0  1  420  2042  4249  2200]\n",
      "11:42:57 - charlie: Test set: Average loss: 0.0141, Accuracy: 4301/10000 (43.01)\n",
      "11:43:00 - Prediction hist.: [ 971  1128  1121  968  1512  1144  1023  1134  804  195]\n",
      "11:43:00 - Federated model: Test set: Average loss: 0.0040, Accuracy: 8412/10000 (84.12)\n",
      "11:43:00 - Starting training round 12/40\n",
      "11:43:00 - Training round 12, calling fit on worker: alice, lr = 0.080\n",
      "11:43:00 - Training round 12, calling fit on worker: bob, lr = 0.080\n",
      "11:43:01 - Training round 12, calling fit on worker: charlie, lr = 0.080\n",
      "11:43:01 - Training round: 12, worker: bob, avg_loss: tensor(0.1388, grad_fn=<MeanBackward1>)\n",
      "11:43:03 - Training round: 12, worker: alice, avg_loss: tensor(0.3987, grad_fn=<MeanBackward1>)\n",
      "11:43:04 - Training round: 12, worker: charlie, avg_loss: tensor(0.4453, grad_fn=<MeanBackward1>)\n",
      "11:43:05 - Starting training round 13/40\n",
      "11:43:06 - Training round 13, calling fit on worker: alice, lr = 0.078\n",
      "11:43:06 - Training round 13, calling fit on worker: bob, lr = 0.078\n",
      "11:43:06 - Training round 13, calling fit on worker: charlie, lr = 0.078\n",
      "11:43:06 - Training round: 13, worker: charlie, avg_loss: tensor(0.2656, grad_fn=<MeanBackward1>)\n",
      "11:43:08 - Training round: 13, worker: alice, avg_loss: tensor(0.1475, grad_fn=<MeanBackward1>)\n",
      "11:43:09 - Training round: 13, worker: bob, avg_loss: tensor(0.1222, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:43:10 - Starting training round 14/40\n",
      "11:43:10 - Training round 14, calling fit on worker: alice, lr = 0.077\n",
      "11:43:10 - Training round 14, calling fit on worker: bob, lr = 0.077\n",
      "11:43:11 - Training round 14, calling fit on worker: charlie, lr = 0.077\n",
      "11:43:11 - Training round: 14, worker: charlie, avg_loss: tensor(0.3328, grad_fn=<MeanBackward1>)\n",
      "11:43:13 - Training round: 14, worker: bob, avg_loss: tensor(0.1389, grad_fn=<MeanBackward1>)\n",
      "11:43:14 - Training round: 14, worker: alice, avg_loss: tensor(0.2955, grad_fn=<MeanBackward1>)\n",
      "11:43:15 - Starting training round 15/40\n",
      "11:43:15 - Training round 15, calling fit on worker: alice, lr = 0.075\n",
      "11:43:16 - Training round 15, calling fit on worker: bob, lr = 0.075\n",
      "11:43:16 - Training round 15, calling fit on worker: charlie, lr = 0.075\n",
      "11:43:16 - Training round: 15, worker: charlie, avg_loss: tensor(0.1620, grad_fn=<MeanBackward1>)\n",
      "11:43:18 - Training round: 15, worker: bob, avg_loss: tensor(0.1267, grad_fn=<MeanBackward1>)\n",
      "11:43:19 - Training round: 15, worker: alice, avg_loss: tensor(0.0200, grad_fn=<MeanBackward1>)\n",
      "11:43:20 - Starting training round 16/40\n",
      "11:43:20 - Training round 16, calling fit on worker: alice, lr = 0.074\n",
      "11:43:20 - Training round 16, calling fit on worker: bob, lr = 0.074\n",
      "11:43:21 - Training round 16, calling fit on worker: charlie, lr = 0.074\n",
      "11:43:21 - Training round: 16, worker: bob, avg_loss: tensor(0.3139, grad_fn=<MeanBackward1>)\n",
      "11:43:22 - Training round: 16, worker: charlie, avg_loss: tensor(0.1488, grad_fn=<MeanBackward1>)\n",
      "11:43:24 - Training round: 16, worker: alice, avg_loss: tensor(0.0432, grad_fn=<MeanBackward1>)\n",
      "11:43:25 - Starting training round 17/40\n",
      "11:43:25 - Training round 17, calling fit on worker: alice, lr = 0.072\n",
      "11:43:25 - Training round 17, calling fit on worker: bob, lr = 0.072\n",
      "11:43:25 - Training round 17, calling fit on worker: charlie, lr = 0.072\n",
      "11:43:26 - Training round: 17, worker: alice, avg_loss: tensor(0.0657, grad_fn=<MeanBackward1>)\n",
      "11:43:28 - Training round: 17, worker: bob, avg_loss: tensor(0.0106, grad_fn=<MeanBackward1>)\n",
      "11:43:29 - Training round: 17, worker: charlie, avg_loss: tensor(0.1042, grad_fn=<MeanBackward1>)\n",
      "11:43:30 - Starting training round 18/40\n",
      "11:43:31 - Training round 18, calling fit on worker: alice, lr = 0.071\n",
      "11:43:31 - Training round 18, calling fit on worker: bob, lr = 0.071\n",
      "11:43:31 - Training round 18, calling fit on worker: charlie, lr = 0.071\n",
      "11:43:31 - Training round: 18, worker: alice, avg_loss: tensor(0.0047, grad_fn=<MeanBackward1>)\n",
      "11:43:33 - Training round: 18, worker: charlie, avg_loss: tensor(0.0304, grad_fn=<MeanBackward1>)\n",
      "11:43:34 - Training round: 18, worker: bob, avg_loss: tensor(0.0399, grad_fn=<MeanBackward1>)\n",
      "11:43:35 - Starting training round 19/40\n",
      "11:43:36 - Training round 19, calling fit on worker: alice, lr = 0.070\n",
      "11:43:36 - Training round 19, calling fit on worker: bob, lr = 0.070\n",
      "11:43:36 - Training round 19, calling fit on worker: charlie, lr = 0.070\n",
      "11:43:36 - Training round: 19, worker: alice, avg_loss: tensor(0.1650, grad_fn=<MeanBackward1>)\n",
      "11:43:38 - Training round: 19, worker: charlie, avg_loss: tensor(0.1897, grad_fn=<MeanBackward1>)\n",
      "11:43:39 - Training round: 19, worker: bob, avg_loss: tensor(0.0143, grad_fn=<MeanBackward1>)\n",
      "11:43:40 - Starting training round 20/40\n",
      "11:43:41 - Training round 20, calling fit on worker: alice, lr = 0.068\n",
      "11:43:41 - Training round 20, calling fit on worker: bob, lr = 0.068\n",
      "11:43:41 - Training round 20, calling fit on worker: charlie, lr = 0.068\n",
      "11:43:41 - Training round: 20, worker: alice, avg_loss: tensor(0.0277, grad_fn=<MeanBackward1>)\n",
      "11:43:43 - Training round: 20, worker: charlie, avg_loss: tensor(0.0224, grad_fn=<MeanBackward1>)\n",
      "11:43:44 - Training round: 20, worker: bob, avg_loss: tensor(0.0190, grad_fn=<MeanBackward1>)\n",
      "11:43:45 - Starting training round 21/40\n",
      "11:43:46 - Training round 21, calling fit on worker: alice, lr = 0.067\n",
      "11:43:46 - Training round 21, calling fit on worker: bob, lr = 0.067\n",
      "11:43:46 - Training round 21, calling fit on worker: charlie, lr = 0.067\n",
      "11:43:46 - Training round: 21, worker: alice, avg_loss: tensor(0.2845, grad_fn=<MeanBackward1>)\n",
      "11:43:48 - Training round: 21, worker: charlie, avg_loss: tensor(0.1815, grad_fn=<MeanBackward1>)\n",
      "11:43:49 - Training round: 21, worker: bob, avg_loss: tensor(0.0092, grad_fn=<MeanBackward1>)\n",
      "11:43:54 - Prediction hist.: [ 1891  1462  1350  2261  517  260  314  771  351  823]\n",
      "11:43:54 - alice: Test set: Average loss: 0.0070, Accuracy: 6893/10000 (68.93)\n",
      "11:43:57 - Prediction hist.: [ 507  1060  920  212  2324  2233  1790  825  129  0]\n",
      "11:43:57 - bob: Test set: Average loss: 0.0128, Accuracy: 6283/10000 (62.83)\n",
      "11:44:00 - Prediction hist.: [ 381  307  435  290  0  79  145  1075  4899  2389]\n",
      "11:44:00 - charlie: Test set: Average loss: 0.0147, Accuracy: 4480/10000 (44.80)\n",
      "11:44:03 - Prediction hist.: [ 1029  1152  1047  988  1171  920  981  1026  952  734]\n",
      "11:44:03 - Federated model: Test set: Average loss: 0.0022, Accuracy: 9201/10000 (92.01)\n",
      "11:44:03 - Starting training round 22/40\n",
      "11:44:04 - Training round 22, calling fit on worker: alice, lr = 0.065\n",
      "11:44:04 - Training round 22, calling fit on worker: bob, lr = 0.065\n",
      "11:44:04 - Training round 22, calling fit on worker: charlie, lr = 0.065\n",
      "11:44:04 - Training round: 22, worker: bob, avg_loss: tensor(0.0105, grad_fn=<MeanBackward1>)\n",
      "11:44:06 - Training round: 22, worker: alice, avg_loss: tensor(0.2458, grad_fn=<MeanBackward1>)\n",
      "11:44:07 - Training round: 22, worker: charlie, avg_loss: tensor(0.1962, grad_fn=<MeanBackward1>)\n",
      "11:44:08 - Starting training round 23/40\n",
      "11:44:08 - Training round 23, calling fit on worker: alice, lr = 0.064\n",
      "11:44:09 - Training round 23, calling fit on worker: bob, lr = 0.064\n",
      "11:44:09 - Training round 23, calling fit on worker: charlie, lr = 0.064\n",
      "11:44:09 - Training round: 23, worker: bob, avg_loss: tensor(0.0278, grad_fn=<MeanBackward1>)\n",
      "11:44:10 - Training round: 23, worker: alice, avg_loss: tensor(0.1663, grad_fn=<MeanBackward1>)\n",
      "11:44:12 - Training round: 23, worker: charlie, avg_loss: tensor(0.1935, grad_fn=<MeanBackward1>)\n",
      "11:44:13 - Starting training round 24/40\n",
      "11:44:13 - Training round 24, calling fit on worker: alice, lr = 0.063\n",
      "11:44:13 - Training round 24, calling fit on worker: bob, lr = 0.063\n",
      "11:44:14 - Training round 24, calling fit on worker: charlie, lr = 0.063\n",
      "11:44:14 - Training round: 24, worker: alice, avg_loss: tensor(0.1346, grad_fn=<MeanBackward1>)\n",
      "11:44:15 - Training round: 24, worker: bob, avg_loss: tensor(0.0068, grad_fn=<MeanBackward1>)\n",
      "11:44:17 - Training round: 24, worker: charlie, avg_loss: tensor(0.0788, grad_fn=<MeanBackward1>)\n",
      "11:44:18 - Starting training round 25/40\n",
      "11:44:18 - Training round 25, calling fit on worker: alice, lr = 0.062\n",
      "11:44:18 - Training round 25, calling fit on worker: bob, lr = 0.062\n",
      "11:44:18 - Training round 25, calling fit on worker: charlie, lr = 0.062\n",
      "11:44:19 - Training round: 25, worker: bob, avg_loss: tensor(0.0224, grad_fn=<MeanBackward1>)\n",
      "11:44:20 - Training round: 25, worker: alice, avg_loss: tensor(0.1537, grad_fn=<MeanBackward1>)\n",
      "11:44:22 - Training round: 25, worker: charlie, avg_loss: tensor(0.0844, grad_fn=<MeanBackward1>)\n",
      "11:44:23 - Starting training round 26/40\n",
      "11:44:23 - Training round 26, calling fit on worker: alice, lr = 0.060\n",
      "11:44:23 - Training round 26, calling fit on worker: bob, lr = 0.060\n",
      "11:44:23 - Training round 26, calling fit on worker: charlie, lr = 0.060\n",
      "11:44:24 - Training round: 26, worker: bob, avg_loss: tensor(0.0656, grad_fn=<MeanBackward1>)\n",
      "11:44:25 - Training round: 26, worker: alice, avg_loss: tensor(0.0524, grad_fn=<MeanBackward1>)\n",
      "11:44:27 - Training round: 26, worker: charlie, avg_loss: tensor(0.0771, grad_fn=<MeanBackward1>)\n",
      "11:44:28 - Starting training round 27/40\n",
      "11:44:28 - Training round 27, calling fit on worker: alice, lr = 0.059\n",
      "11:44:28 - Training round 27, calling fit on worker: bob, lr = 0.059\n",
      "11:44:28 - Training round 27, calling fit on worker: charlie, lr = 0.059\n",
      "11:44:29 - Training round: 27, worker: alice, avg_loss: tensor(0.5409, grad_fn=<MeanBackward1>)\n",
      "11:44:30 - Training round: 27, worker: bob, avg_loss: tensor(0.0198, grad_fn=<MeanBackward1>)\n",
      "11:44:32 - Training round: 27, worker: charlie, avg_loss: tensor(0.0938, grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:44:33 - Starting training round 28/40\n",
      "11:44:33 - Training round 28, calling fit on worker: alice, lr = 0.058\n",
      "11:44:33 - Training round 28, calling fit on worker: bob, lr = 0.058\n",
      "11:44:33 - Training round 28, calling fit on worker: charlie, lr = 0.058\n",
      "11:44:34 - Training round: 28, worker: bob, avg_loss: tensor(0.0163, grad_fn=<MeanBackward1>)\n",
      "11:44:35 - Training round: 28, worker: alice, avg_loss: tensor(0.0108, grad_fn=<MeanBackward1>)\n",
      "11:44:36 - Training round: 28, worker: charlie, avg_loss: tensor(0.1363, grad_fn=<MeanBackward1>)\n",
      "11:44:38 - Starting training round 29/40\n",
      "11:44:38 - Training round 29, calling fit on worker: alice, lr = 0.057\n",
      "11:44:38 - Training round 29, calling fit on worker: bob, lr = 0.057\n",
      "11:44:38 - Training round 29, calling fit on worker: charlie, lr = 0.057\n",
      "11:44:39 - Training round: 29, worker: bob, avg_loss: tensor(0.0456, grad_fn=<MeanBackward1>)\n",
      "11:44:40 - Training round: 29, worker: alice, avg_loss: tensor(0.2410, grad_fn=<MeanBackward1>)\n",
      "11:44:42 - Training round: 29, worker: charlie, avg_loss: tensor(0.0619, grad_fn=<MeanBackward1>)\n",
      "11:44:43 - Starting training round 30/40\n",
      "11:44:43 - Training round 30, calling fit on worker: alice, lr = 0.056\n",
      "11:44:43 - Training round 30, calling fit on worker: bob, lr = 0.056\n",
      "11:44:43 - Training round 30, calling fit on worker: charlie, lr = 0.056\n",
      "11:44:44 - Training round: 30, worker: bob, avg_loss: tensor(0.0273, grad_fn=<MeanBackward1>)\n",
      "11:44:45 - Training round: 30, worker: alice, avg_loss: tensor(0.0242, grad_fn=<MeanBackward1>)\n",
      "11:44:46 - Training round: 30, worker: charlie, avg_loss: tensor(0.1126, grad_fn=<MeanBackward1>)\n",
      "11:44:48 - Starting training round 31/40\n",
      "11:44:48 - Training round 31, calling fit on worker: alice, lr = 0.055\n",
      "11:44:48 - Training round 31, calling fit on worker: bob, lr = 0.055\n",
      "11:44:48 - Training round 31, calling fit on worker: charlie, lr = 0.055\n",
      "11:44:49 - Training round: 31, worker: alice, avg_loss: tensor(0.0321, grad_fn=<MeanBackward1>)\n",
      "11:44:50 - Training round: 31, worker: bob, avg_loss: tensor(0.0514, grad_fn=<MeanBackward1>)\n",
      "11:44:51 - Training round: 31, worker: charlie, avg_loss: tensor(0.0918, grad_fn=<MeanBackward1>)\n",
      "11:44:57 - Prediction hist.: [ 1305  1254  1565  1905  814  388  617  842  597  713]\n",
      "11:44:57 - alice: Test set: Average loss: 0.0046, Accuracy: 8000/10000 (80.00)\n",
      "11:45:00 - Prediction hist.: [ 363  1060  847  316  1721  2721  1796  856  320  0]\n",
      "11:45:00 - bob: Test set: Average loss: 0.0079, Accuracy: 6461/10000 (64.61)\n",
      "11:45:03 - Prediction hist.: [ 813  737  566  510  0  290  711  1388  2770  2215]\n",
      "11:45:03 - charlie: Test set: Average loss: 0.0090, Accuracy: 6528/10000 (65.28)\n",
      "11:45:07 - Prediction hist.: [ 1002  1127  1025  979  888  930  1012  1039  977  1021]\n",
      "11:45:07 - Federated model: Test set: Average loss: 0.0015, Accuracy: 9453/10000 (94.53)\n",
      "11:45:07 - Starting training round 32/40\n",
      "11:45:07 - Training round 32, calling fit on worker: alice, lr = 0.053\n",
      "11:45:07 - Training round 32, calling fit on worker: bob, lr = 0.053\n",
      "11:45:07 - Training round 32, calling fit on worker: charlie, lr = 0.053\n",
      "11:45:07 - Training round: 32, worker: bob, avg_loss: tensor(0.0165, grad_fn=<MeanBackward1>)\n",
      "11:45:09 - Training round: 32, worker: charlie, avg_loss: tensor(0.5651, grad_fn=<MeanBackward1>)\n",
      "11:45:10 - Training round: 32, worker: alice, avg_loss: tensor(0.1753, grad_fn=<MeanBackward1>)\n",
      "11:45:12 - Starting training round 33/40\n",
      "11:45:12 - Training round 33, calling fit on worker: alice, lr = 0.052\n",
      "11:45:12 - Training round 33, calling fit on worker: bob, lr = 0.052\n",
      "11:45:12 - Training round 33, calling fit on worker: charlie, lr = 0.052\n",
      "11:45:12 - Training round: 33, worker: bob, avg_loss: tensor(0.0159, grad_fn=<MeanBackward1>)\n",
      "11:45:14 - Training round: 33, worker: alice, avg_loss: tensor(0.0120, grad_fn=<MeanBackward1>)\n",
      "11:45:15 - Training round: 33, worker: charlie, avg_loss: tensor(0.3258, grad_fn=<MeanBackward1>)\n",
      "11:45:16 - Starting training round 34/40\n",
      "11:45:16 - Training round 34, calling fit on worker: alice, lr = 0.051\n",
      "11:45:17 - Training round 34, calling fit on worker: bob, lr = 0.051\n",
      "11:45:17 - Training round 34, calling fit on worker: charlie, lr = 0.051\n",
      "11:45:17 - Training round: 34, worker: alice, avg_loss: tensor(0.0035, grad_fn=<MeanBackward1>)\n",
      "11:45:18 - Training round: 34, worker: bob, avg_loss: tensor(0.0185, grad_fn=<MeanBackward1>)\n",
      "11:45:20 - Training round: 34, worker: charlie, avg_loss: tensor(0.0214, grad_fn=<MeanBackward1>)\n",
      "11:45:21 - Starting training round 35/40\n",
      "11:45:21 - Training round 35, calling fit on worker: alice, lr = 0.050\n",
      "11:45:21 - Training round 35, calling fit on worker: bob, lr = 0.050\n",
      "11:45:21 - Training round 35, calling fit on worker: charlie, lr = 0.050\n",
      "11:45:22 - Training round: 35, worker: charlie, avg_loss: tensor(0.0792, grad_fn=<MeanBackward1>)\n",
      "11:45:24 - Training round: 35, worker: bob, avg_loss: tensor(0.0441, grad_fn=<MeanBackward1>)\n",
      "11:45:25 - Training round: 35, worker: alice, avg_loss: tensor(0.1160, grad_fn=<MeanBackward1>)\n",
      "11:45:27 - Starting training round 36/40\n",
      "11:45:27 - Training round 36, calling fit on worker: alice, lr = 0.049\n",
      "11:45:27 - Training round 36, calling fit on worker: bob, lr = 0.049\n",
      "11:45:27 - Training round 36, calling fit on worker: charlie, lr = 0.049\n",
      "11:45:27 - Training round: 36, worker: alice, avg_loss: tensor(0.0172, grad_fn=<MeanBackward1>)\n",
      "11:45:29 - Training round: 36, worker: bob, avg_loss: tensor(0.0122, grad_fn=<MeanBackward1>)\n",
      "11:45:30 - Training round: 36, worker: charlie, avg_loss: tensor(0.1169, grad_fn=<MeanBackward1>)\n",
      "11:45:32 - Starting training round 37/40\n",
      "11:45:32 - Training round 37, calling fit on worker: alice, lr = 0.048\n",
      "11:45:32 - Training round 37, calling fit on worker: bob, lr = 0.048\n",
      "11:45:32 - Training round 37, calling fit on worker: charlie, lr = 0.048\n",
      "11:45:32 - Training round: 37, worker: alice, avg_loss: tensor(0.1101, grad_fn=<MeanBackward1>)\n",
      "11:45:34 - Training round: 37, worker: bob, avg_loss: tensor(0.1069, grad_fn=<MeanBackward1>)\n",
      "11:45:35 - Training round: 37, worker: charlie, avg_loss: tensor(0.0302, grad_fn=<MeanBackward1>)\n",
      "11:45:36 - Starting training round 38/40\n",
      "11:45:37 - Training round 38, calling fit on worker: alice, lr = 0.047\n",
      "11:45:37 - Training round 38, calling fit on worker: bob, lr = 0.047\n",
      "11:45:37 - Training round 38, calling fit on worker: charlie, lr = 0.047\n",
      "11:45:37 - Training round: 38, worker: alice, avg_loss: tensor(0.0398, grad_fn=<MeanBackward1>)\n",
      "11:45:39 - Training round: 38, worker: bob, avg_loss: tensor(0.0097, grad_fn=<MeanBackward1>)\n",
      "11:45:40 - Training round: 38, worker: charlie, avg_loss: tensor(0.0388, grad_fn=<MeanBackward1>)\n",
      "11:45:41 - Starting training round 39/40\n",
      "11:45:41 - Training round 39, calling fit on worker: alice, lr = 0.046\n",
      "11:45:42 - Training round 39, calling fit on worker: bob, lr = 0.046\n",
      "11:45:42 - Training round 39, calling fit on worker: charlie, lr = 0.046\n",
      "11:45:42 - Training round: 39, worker: bob, avg_loss: tensor(0.0240, grad_fn=<MeanBackward1>)\n",
      "11:45:44 - Training round: 39, worker: alice, avg_loss: tensor(0.1903, grad_fn=<MeanBackward1>)\n",
      "11:45:45 - Training round: 39, worker: charlie, avg_loss: tensor(0.1246, grad_fn=<MeanBackward1>)\n",
      "11:45:46 - Starting training round 40/40\n",
      "11:45:47 - Training round 40, calling fit on worker: alice, lr = 0.045\n",
      "11:45:47 - Training round 40, calling fit on worker: bob, lr = 0.045\n",
      "11:45:47 - Training round 40, calling fit on worker: charlie, lr = 0.045\n",
      "11:45:47 - Training round: 40, worker: alice, avg_loss: tensor(0.0402, grad_fn=<MeanBackward1>)\n",
      "11:45:49 - Training round: 40, worker: charlie, avg_loss: tensor(0.0827, grad_fn=<MeanBackward1>)\n",
      "11:45:50 - Training round: 40, worker: bob, avg_loss: tensor(0.0689, grad_fn=<MeanBackward1>)\n",
      "11:45:54 - Prediction hist.: [ 1413  1185  1406  2195  875  425  657  781  365  698]\n",
      "11:45:54 - alice: Test set: Average loss: 0.0049, Accuracy: 7843/10000 (78.43)\n",
      "11:45:57 - Prediction hist.: [ 744  1096  1012  503  1803  1906  1372  958  549  57]\n",
      "11:45:57 - bob: Test set: Average loss: 0.0048, Accuracy: 7577/10000 (75.77)\n",
      "11:46:00 - Prediction hist.: [ 865  587  481  357  0  94  728  1389  2959  2540]\n",
      "11:46:00 - charlie: Test set: Average loss: 0.0093, Accuracy: 6040/10000 (60.40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:46:03 - Prediction hist.: [ 1021  1127  1033  997  926  892  963  1036  963  1042]\n",
      "11:46:03 - Federated model: Test set: Average loss: 0.0013, Accuracy: 9536/10000 (95.36)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = args.lr\n",
    "\n",
    "for curr_round in range(1, args.training_rounds + 1):\n",
    "    global traced_model\n",
    "    \n",
    "    logger.info(\"Starting training round %s/%s\", curr_round, args.training_rounds)\n",
    "\n",
    "    # For each of the workers we ask the model to train with their part of their data\n",
    "    # in a async fashion and then we gather the results\n",
    "    results = await asyncio.gather(\n",
    "        *[\n",
    "            rwc.fit_model_on_worker(\n",
    "                worker=worker,\n",
    "                traced_model=traced_model,\n",
    "                batch_size=args.batch_size,\n",
    "                curr_round=curr_round,\n",
    "                max_nr_batches=args.federate_after_n_batches,\n",
    "                lr=learning_rate,\n",
    "            )\n",
    "            for worker in worker_instances\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    models = {}\n",
    "    loss_values = {}\n",
    "\n",
    "    # Run evaluation every 10 rounds\n",
    "    run_evaluation = (curr_round) % 10 == 1 or curr_round == args.training_rounds\n",
    "    if run_evaluation:\n",
    "        rwc.evaluate_models_on_test_data(test_loader, results)\n",
    "    \n",
    "    # Store models and loss_values for each worker\n",
    "    for worker_id, worker_model, worker_loss in results:\n",
    "        if worker_model is not None:\n",
    "            models[worker_id] = worker_model\n",
    "            loss_values[worker_id] = worker_loss\n",
    "\n",
    "    # Average model\n",
    "    avg_model = utils.federated_avg(models)\n",
    "    if run_evaluation:\n",
    "        rwc.evaluate_model(\"Federated model\", avg_model, \"cpu\", test_loader)\n",
    "\n",
    "    # decay learning rate\n",
    "    learning_rate = max(0.98 * learning_rate, args.lr * 0.01)\n",
    "    \n",
    "    # Use averaged model in the next round\n",
    "    traced_model = avg_model\n",
    "\n",
    "# Save model\n",
    "if args.save_model:\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 40 rounds of training we achieve an accuracy > 95% on the entire testing dataset. \n",
    "This is impressing, given that no worker has access to more than 4 digits!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on GitHub\n",
    "\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysyft",
   "language": "python",
   "name": "pysyft"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
