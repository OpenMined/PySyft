{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting models on Grid\n",
    "\n",
    "Grid offers both: Machine Learning as a Service and Encrypted Machine Learning as a service. In this series of tutorials we show how you can serve and query models on Grid.\n",
    "\n",
    "## 2.1 Host and query a model using a Rest API\n",
    "\n",
    "In the previous tutorial we trained a CNN for classifying images with different 2 types of skin deseases: benign keratosis and melanoma (type of skin cancer). In this tutorial we show how to serve this model on Grid using a Rest API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grid as gr\n",
    "import helper\n",
    "import syft as sy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup Config**\n",
    "\n",
    "Define Config parameters, init hook, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect with a remote worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = gr.WebsocketGridClient(hook, \"http://localhost:3000\", id=\"Bob\")\n",
    "bob.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marianne/PySyft/syft/frameworks/torch/hook/hook.py:483: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  current_tensor = hook_self.torch.native_tensor(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "df = helper.read_skin_cancer_dataset()\n",
    "train_df, valid_df, test_df = helper.split_data(df)\n",
    "\n",
    "# These values are from Part 1.\n",
    "input_size = 32\n",
    "train_mean, train_std = (torch.tensor([0.6979, 0.5445, 0.5735]), torch.tensor([0.0959, 0.1187, 0.1365]))\n",
    "\n",
    "# Create a test dataloader\n",
    "test_set = helper.Dataset(test_df, transform=helper.transform(input_size, train_mean, train_std))\n",
    "test_generator = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "# Get a data sample and a target\n",
    "data, target = next(iter(test_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model\n",
    "\n",
    "Let's load the model we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = helper.make_model()\n",
    "model.load_state_dict(torch.load(\"binary-skin-cancer-detection-model\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper.test(model, test_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a model ready to be served\n",
    "\n",
    "In order to serve the model it needs to be serializable. We support two ways of supporting serializitation for models:\n",
    "\n",
    "1. Using Jit modules. We can turn a regular torch model into a jit module. Jit modules use Torchscript.\n",
    "\n",
    "> Torchsript creates serializable and optimizable models from PyTorch code. Any code written in TorchScript can be saved from a Python process and loaded in a process where there is no Python dependency. This facility will allow us to send this model to remote workers. - jit documentation\n",
    "\n",
    "**IMPORTANT**: you'll need torch 1.0.1 in order to run this demo with jit modules due to compatibility issues between syft and torch.jit in the more recent versions.\n",
    "\n",
    "2. Using Plans. A Plan is intended to store a sequence of torch operations, just like a function, but it allows to send this sequence of operations to remote workers and to keep a reference to it. You can learn more about plans in [Syft's tutorials](https://github.com/OpenMined/PySyft/blob/dev/examples/tutorials/Part%2008%20-%20Introduction%20to%20Plans.ipynb).\n",
    "\n",
    "#### 1. Jit Module\n",
    "\n",
    "We can turn a regular module into a jit module using torch.jit.trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model = torch.jit.trace(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plan\n",
    "\n",
    "We can't turn an existing model into a Plan, in order to create a Plan we need to inherit from syft.Plan during the creation of the model. We implemented this logic at `helper.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Net Net id:convnet owner:me>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan_model = helper.make_model(is_plan=True)\n",
    "# plan_model.load_state_dict(torch.load(\"binary-skin-cancer-detection-model\"))\n",
    "plan_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before sending the plan anywhere we need to build it (similar to the trace operation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan_model.build(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serve model\n",
    "\n",
    "We can check the models served at `bob` at `bob.models`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can serve a new model at `bob` by calling `bob.serve_model(<model>, <an unique model identifier>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"success\": true}'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob.serve_model(plan_model, model_id=\"skin-cancer-model-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['skin-cancer-model-1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bob.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob.serve_model(traced_model, model_id=\"skin-cancer-model-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query model\n",
    "\n",
    "Now any one that can connect to this worker can query this model. Let's restart this notebook (so we loose connection to the worker and don't know the model anymore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "\n",
    "def restart_kernel() :\n",
    "    display_html(\"<script>Jupyter.notebook.kernel.restart()</script>\",raw=True)\n",
    "    \n",
    "restart_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's reconnect to the worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Grid nodes publish datasets online and are for EXPERIMENTAL use only.Deploy nodes at your own risk. Do not use OpenGrid with any data/models you wish to keep private.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import grid as gr\n",
    "import torch\n",
    "import syft as sy\n",
    "import helper\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "bob = gr.WebsocketGridClient(hook, \"http://localhost:3000\", id=\"Bob\")\n",
    "bob.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run inference at the host models by calling `bob.run_inference(model_id, data)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = helper.read_skin_cancer_dataset()\n",
    "train_df, valid_df, test_df = helper.split_data(df)\n",
    "\n",
    "# These values are from Part 1.\n",
    "input_size = 32\n",
    "train_mean, train_std = (torch.tensor([0.6979, 0.5445, 0.5735]), torch.tensor([0.0959, 0.1187, 0.1365]))\n",
    "\n",
    "# Create a test dataloader\n",
    "test_set = helper.Dataset(test_df, transform=helper.transform(input_size, train_mean, train_std))\n",
    "test_generator = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "# Get a data sample and a target\n",
    "data, target = next(iter(test_generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'prediction': [[0.030585136264562607, -0.015594672411680222]]}, tensor([1]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n",
      "WARNING:engineio.client:WebSocket connection was closed, aborting\n"
     ]
    }
   ],
   "source": [
    "bob.run_inference(model_id=\"skin-cancer-model-1\", data=data), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob.run_inference(model_id=\"skin-cancer-model-2\", data=data), target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other operations\n",
    "\n",
    "\n",
    "**Delete a Model**\n",
    "\n",
    "One can also delete a model if they want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deletes the remote model\n",
    "bob.delete_model(\"skin-cancer-model-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congratulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "## Star PySyft on GitHub\n",
    "The easiest way to help our community is just by starring the GitHub repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "[Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "## Join our Slack!\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at http://slack.openmined.org\n",
    "\n",
    "## Join a Code Project!\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft GitHub Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for GitHub issues marked \"good first issue\".\n",
    "\n",
    "[PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "[Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "## Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
