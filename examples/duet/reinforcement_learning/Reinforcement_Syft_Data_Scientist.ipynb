{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Syft Duet - Data Scientist ü•Å\n",
    "\n",
    "Contributed by [@Koukyosyumei](https://github.com/Koukyosyumei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Connect to a Remote Duet Server\n",
    "\n",
    "As the Data Scientist, you want to perform data science on data that is sitting in the Data Owner's Duet server in their Notebook.\n",
    "\n",
    "In order to do this, we must run the code that the Data Owner sends us, which importantly includes their Duet Session ID. The code will look like this, importantly with their real Server ID.\n",
    "\n",
    "```\n",
    "import syft as sy\n",
    "duet = sy.duet('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "```\n",
    "\n",
    "This will create a direct connection from my notebook to the remote Duet server. Once the connection is established all traffic is sent directly between the two nodes.\n",
    "\n",
    "Paste the code or Server ID that the Data Owner gives you and run it in the cell below. It will return your Client ID which you must send to the Data Owner to enter into Duet so it can pair your notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import syft as sy\n",
    "duet = sy.join_duet(loopback=True)\n",
    "sy.logger.add(sink=\"./syft_ds.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 0 : Now STOP and run the Data Owner notebook until Checkpoint 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sy.load(\"gym\")\n",
    "sy.load(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"seed\": 543,\n",
    "    \"render\": False,\n",
    "    \"log_interval\": 10,\n",
    "    \"no_cuda\": False,\n",
    "    \"log_interval\": 1,\n",
    "    \"wait_interval\": 1,\n",
    "    \"dry_run\":True,\n",
    "}\n",
    "\n",
    "remote_torch = duet.torch\n",
    "remote_torch.manual_seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cuda = False\n",
    "has_cuda_ptr = remote_torch.cuda.is_available()\n",
    "\n",
    "# lets ask to see if our Data Owner has CUDA\n",
    "has_cuda = bool(\n",
    "    has_cuda_ptr.get(\n",
    "        request_block=True,\n",
    "        reason=\"To run test and inference locally\",\n",
    "        timeout_secs=3,  # change to something slower\n",
    "    )\n",
    ")\n",
    "print(\"Is cuda available ? : \", has_cuda)\n",
    "\n",
    "\n",
    "use_cuda = not config[\"no_cuda\"] and has_cuda\n",
    "# now we can set the seed\n",
    "remote_torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "device = remote_torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# print(f\"Data Owner device is {device.type.get()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_saved_log_probs = []\n",
    "buffer_rewards = []\n",
    "\n",
    "class Policy(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(Policy, self).__init__(torch_ref=torch_ref)\n",
    "        self.affine1 = self.torch_ref.nn.Linear(4, 128)\n",
    "        self.dropout = self.torch_ref.nn.Dropout(p=0.6)\n",
    "\n",
    "        self.affine2 = self.torch_ref.nn.Linear(128, 2)\n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = remote_torch.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return remote_torch.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send our model to remote\n",
    "policy = Policy(torch)\n",
    "remote_policy = policy.send(duet)\n",
    "\n",
    "optimizer = remote_torch.optim.Adam(remote_policy.parameters(), lr=1e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have CUDA lets send our model to the GPU\n",
    "if has_cuda:\n",
    "    remote_policy.cuda(device)\n",
    "else:\n",
    "    remote_policy.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You cannot see the state\n",
    "def select_action(state):\n",
    "    global buffer_saved_log_probs\n",
    "    \n",
    "    state = remote_torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs_ptr = remote_policy(state)\n",
    "    m = remote_torch.distributions.Categorical(probs_ptr)\n",
    "    action = m.sample()\n",
    "    buffer_saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    global buffer_saved_log_probs\n",
    "    global buffer_rewards\n",
    "    \n",
    "    gamma = duet.python.Float(config[\"gamma\"])\n",
    "    \n",
    "    R = duet.python.Float(0)\n",
    "    policy_losses = duet.python.List([])\n",
    "    returns = duet.python.List([])\n",
    "    \n",
    "    for r in buffer_rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "        \n",
    "    returns = remote_torch.Tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    \n",
    "    for log_prob, R in zip(buffer_saved_log_probs, returns):\n",
    "        policy_losses.append(-log_prob * R)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = remote_torch.cat(policy_losses).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del buffer_rewards[:]\n",
    "    del buffer_saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_threshold_ptr = duet.store[\"reward_threshold\"]\n",
    "reward_threshold = reward_threshold_ptr.get(request_block=True, delete_obj=False)\n",
    "print(f\"reward_threshold is {reward_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_gym = duet.gym\n",
    "remote_env = remote_gym.make(\"CartPole-v0\")\n",
    "remote_env.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_reward = 10\n",
    "\n",
    "# run inifinitely many episodes\n",
    "for i_episode in count(1):\n",
    "\n",
    "    # reset environment and episode reward\n",
    "    state = remote_env.reset()\n",
    "    ep_reward = duet.python.Float(0)\n",
    "\n",
    "    # for each episode, only run 9999 steps so that we don't\n",
    "    # infinite loop while learning\n",
    "    for t in range(1, 10000):\n",
    "        # select action from policy\n",
    "        action = select_action(state)\n",
    "\n",
    "        # take the action\n",
    "        state, reward, done, _ = remote_env.step(action)\n",
    "\n",
    "        buffer_rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        if done.get(request_block=True):\n",
    "            break\n",
    "\n",
    "    # update cumulative reward\n",
    "    running_reward = 0.05 * ep_reward.get(request_block=True, delete_obj=False) + (1 - 0.05) * running_reward\n",
    "\n",
    "    # perform backprop\n",
    "    finish_episode()\n",
    "\n",
    "    # log results\n",
    "    if i_episode % config[\"log_interval\"] == 0:\n",
    "        print(\n",
    "                \"Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}\".format(\n",
    "                    i_episode,\n",
    "                    ep_reward.get(request_block=True, delete_obj=False),\n",
    "                    running_reward\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # check if we have \"solved\" the cart pole problem\n",
    "    if running_reward > reward_threshold:\n",
    "        print(\n",
    "                \"Solved! Running reward is now {} and \"\n",
    "                \"the last episode runs to {} time steps!\".format(running_reward, t)\n",
    "            )\n",
    "        break\n",
    "        \n",
    "    if config[\"dry_run\"]:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 1 : Now STOP and run the Data Owner notebook until Checkpoint 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
