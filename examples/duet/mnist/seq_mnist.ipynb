{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "sy.LOG_FILE = \"syft_ds.log\"\n",
    "_ = sy.logger.add(sy.LOG_FILE, enqueue=True, colorize=False, diagnose=True, backtrace=True, level=\"TRACE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¤  ðŸŽ¸  â™ªâ™ªâ™ª joining duet â™«â™«â™«  ðŸŽ»  ðŸŽ¹\n",
      "\n",
      "â™«â™«â™« >\u001b[93m DISCLAIMER\u001b[0m:\u001b[1m Duet is an experimental feature currently \n",
      "â™«â™«â™« > in alpha. Do not use this to protect real-world data.\n",
      "\u001b[0mâ™«â™«â™« >\n",
      "â™«â™«â™« > Punching through firewall to OpenGrid Network Node at network_url: \n",
      "â™«â™«â™« > http://ec2-18-216-8-163.us-east-2.compute.amazonaws.com:5000\n",
      "â™«â™«â™« >\n",
      "â™«â™«â™« > ...waiting for response from OpenGrid Network... \u001b[92mDONE!\u001b[0m\n",
      "\n",
      "â™«â™«â™« > Duet Client ID: \u001b[1ma9565988e5b9fba9dbbf055fb97a8ae8\u001b[0m\n",
      "\n",
      "â™«â™«â™« > \u001b[95mSTEP 1:\u001b[0m Send the Duet Client ID to your duet partner!\n",
      "\n",
      "â™«â™«â™« > ...waiting for partner to connect...\n",
      "â™«â™«â™« > ...using a running event loop...\n",
      "\n",
      "â™«â™«â™« > \u001b[92mCONNECTED!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "duet = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get some references to our data owners Duet torch and torchvision\n",
    "torch = duet.torch\n",
    "torchvision = duet.torchvision\n",
    "\n",
    "# these are the same as the original mnist example\n",
    "transforms = torchvision.transforms\n",
    "datasets = torchvision.datasets\n",
    "nn = torch.nn\n",
    "F = torch.nn.functional\n",
    "optim = torch.optim\n",
    "StepLR = torch.optim.lr_scheduler.StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings from original MNIST example command line args\n",
    "args = {\n",
    "    \"batch_size\": 64,\n",
    "    \"test_batch_size\": 1000,\n",
    "    \"epochs\": 14,\n",
    "    \"lr\": 1.0,\n",
    "    \"gamma\": 0.7,\n",
    "    \"no_cuda\": False,\n",
    "    \"dry_run\": False,\n",
    "    \"seed\": 42, # the meaning of life\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, 3, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Dropout2d(0.25),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(9216, 128),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout2d(0.5),\n",
    "#     nn.Linear(128, 10),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<syft.proxy.syft.lib.python._SyNonePointer at 0x7f0bda6ba7f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<syft.proxy.syft.lib.python.ListPointer object at 0x7f0bda698ca0> <class 'syft.proxy.syft.lib.python.ListPointer'>\n"
     ]
    }
   ],
   "source": [
    "# lets get our parameters for optimization\n",
    "# params_list required for remote list concatenation\n",
    "params = model.parameters(params_list=duet.syft.lib.python.List())\n",
    "print(params, type(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<syft.proxy.torch.optim.AdadeltaPointer object at 0x7f0bda6ba820> <class 'syft.proxy.torch.optim.AdadeltaPointer'>\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adadelta(params, lr=args[\"lr\"])\n",
    "print(optimizer, type(optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<syft.proxy.torch.optim.lr_scheduler.StepLRPointer object at 0x7f0bda6ab6a0> <class 'syft.proxy.torch.optim.lr_scheduler.StepLRPointer'>\n"
     ]
    }
   ],
   "source": [
    "scheduler = StepLR(optimizer, step_size=1, gamma=args[\"gamma\"])\n",
    "print(scheduler, type(scheduler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "# now can define a simple training loop very similar to the original PyTorch MNIST example\n",
    "@sy.logger.catch\n",
    "def train(args, model, train_loader, optimizer, epoch, train_data_length):\n",
    "    train_batches = round((train_data_length / args[\"batch_size\"]) + 0.5)\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        \n",
    "        data_ptr, target_ptr = data[0], data[1]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_ptr)\n",
    "        loss = F.nll_loss(output, target_ptr)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx >= 15:\n",
    "            print(\"batch_idx >= train_batches, breaking\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need some transforms for our MNIST data set\n",
    "transform_1 = torchvision.transforms.ToTensor()  # this converts PIL images to Tensors\n",
    "transform_2 = torchvision.transforms.Normalize(0.1307, 0.3081)  # this normalizes the dataset\n",
    "\n",
    "remote_list = duet.syft.lib.python.List()\n",
    "remote_list.append(transform_1)\n",
    "remote_list.append(transform_2)\n",
    "\n",
    "# compose our transforms\n",
    "transforms = torchvision.transforms.Compose(remote_list)\n",
    "\n",
    "# The DO has kindly let us initialise a DataLoader for their training set\n",
    "train_kwargs = {\n",
    "    \"batch_size\": args[\"batch_size\"],\n",
    "}\n",
    "train_data_ptr = torchvision.datasets.MNIST('../data', train=True, download=True, transform=transforms)\n",
    "train_loader_ptr = torch.utils.data.DataLoader(train_data_ptr,**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Waiting for Blocking Request\n",
      "train_size: To write the training loop.\n",
      "<UID:d753f607-4e1f-4bbc-befd-ad136670e0f8>\n",
      ".......\n",
      "> Blocking Request ACCEPTED\n",
      "Training Dataset size is: 60000\n"
     ]
    }
   ],
   "source": [
    "def get_train_length(train_data_ptr):\n",
    "    train_length_ptr = train_data_ptr.__len__()\n",
    "    train_data_length = train_length_ptr.get(\n",
    "        request_block=True,\n",
    "        name=\"train_size\",\n",
    "        reason=\"To write the training loop\"\n",
    "    )\n",
    "    return train_data_length\n",
    "\n",
    "try:\n",
    "    if train_data_length is None:\n",
    "        train_data_length = get_train_length(train_data_ptr)\n",
    "except NameError:\n",
    "        train_data_length = get_train_length(train_data_ptr)\n",
    "\n",
    "print(f\"Training Dataset size is: {train_data_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx >= train_batches, breaking\n",
      "CPU times: user 1.15 s, sys: 44.9 ms, total: 1.19 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# sequential CPU times: user 39.2 s, sys: 390 ms, total: 39.6s per epoch\n",
    "# sy.module CPU times: user 1min 28s, sys: 645 ms, total: 1min 28s per epoch\n",
    "# vanilla class CPU times: user 3min 5s, sys: 3.01 s, total: 3min 8s per epoch\n",
    "args[\"epochs\"] = 1\n",
    "sy.logger.trace(\"Start Training\")\n",
    "for epoch in range(1, args[\"epochs\"] + 1):\n",
    "    train(args, model, train_loader_ptr, optimizer, epoch, train_data_length)\n",
    "    scheduler.step()\n",
    "sy.logger.trace(\"Finish Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list_ptr = duet.syft.lib.python.List(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Waiting for Blocking Request\n",
      "Param list: Param list to see if it generalizes on my local test set.\n",
      "<UID:faf45286-3bea-4e61-acc4-01e67e1426a5>\n",
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in callback Transaction.__retry()\n",
      "handle: <TimerHandle when=20149.235858258 Transaction.__retry()>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tudor/anaconda3/envs/syft_0.3.0/lib/python3.8/site-packages/nest_asyncio.py\", line 194, in run\n",
      "    ctx.run(self._callback, *self._args)\n",
      "  File \"/home/tudor/anaconda3/envs/syft_0.3.0/lib/python3.8/site-packages/aioice/stun.py\", line 299, in __retry\n",
      "    self.__future.set_exception(TransactionTimeout())\n",
      "  File \"/home/tudor/anaconda3/envs/syft_0.3.0/lib/python3.8/asyncio/futures.py\", line 246, in set_exception\n",
      "    raise exceptions.InvalidStateError(f'{self._state}: {self!r}')\n",
      "asyncio.exceptions.InvalidStateError: FINISHED: <Future finished result=(Message(messa...\\xab\\xea\\x97'), ('192.168.20.207', 40818))>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "> Blocking Request ACCEPTED\n"
     ]
    }
   ],
   "source": [
    "param_list = param_list_ptr.get(\n",
    "    request_block=True,\n",
    "    name=\"Param list\",\n",
    "    reason=\"Param list to see if it generalizes on my local test set\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(param_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as local_torch\n",
    "import torchvision as local_torchvision\n",
    "from torchvision import transforms as local_transforms\n",
    "from torchvision import datasets as local_datasets\n",
    "from torch import nn as local_nn\n",
    "from torch.nn import functional as local_F\n",
    "from torch import optim as local_optim\n",
    "from torch.optim.lr_scheduler import StepLR as local_StepLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = local_nn.Sequential(\n",
    "    local_nn.Conv2d(1, 32, 3, 1),\n",
    "    local_nn.ReLU(),\n",
    "    local_nn.Conv2d(32, 64, 3, 1),\n",
    "    local_nn.ReLU(),\n",
    "    local_nn.MaxPool2d(2),\n",
    "    local_nn.Dropout2d(0.25),\n",
    "    local_nn.Flatten(),\n",
    "    local_nn.Linear(400, 128),\n",
    "    local_nn.ReLU(),\n",
    "    local_nn.Dropout2d(0.5),\n",
    "    local_nn.Linear(128, 10),\n",
    "    local_nn.LogSoftmax(dim=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_params = list(model.parameters())\n",
    "for i in range(len(local_params)):\n",
    "    local_params[i].data[:] = param_list[i].data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with local_torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += local_F.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "transform = local_transforms.Compose(\n",
    "    [local_transforms.ToTensor(), local_transforms.Normalize((0.1307,), (0.3081,))]\n",
    ")\n",
    "    \n",
    "dataset2 = local_datasets.MNIST(\"../data\", train=False, transform=transform)\n",
    "test_loader = local_torch.utils.data.DataLoader(dataset2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(model, \"cpu\", test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
