{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lFt2XpMqTJz"
   },
   "source": [
    "# SuperResolution - Syft Duet - Data Scientist ü•Å\n",
    "\n",
    "Contributed by [@Koukyosyumei](https://github.com/Koukyosyumei)\n",
    "\n",
    "This example trains a SuperResolution network on the BSD300 dataset with Syft.\n",
    "This notebook is mainly based on the original pytorch [example](https://github.com/OpenMined/PySyft/tree/dev/examples/duet/super_resolution/original)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Connect to a Remote Duet Server\n",
    "\n",
    "As the Data Scientist, you want to perform data science on data that is sitting in the Data Owner's Duet server in their Notebook.\n",
    "\n",
    "In order to do this, we must run the code that the Data Owner sends us, which importantly includes their Duet Session ID. The code will look like this, importantly with their real Server ID.\n",
    "\n",
    "```\n",
    "import syft as sy\n",
    "duet = sy.duet('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "```\n",
    "\n",
    "This will create a direct connection from my notebook to the remote Duet server. Once the connection is established all traffic is sent directly between the two nodes.\n",
    "\n",
    "Paste the code or Server ID that the Data Owner gives you and run it in the cell below. It will return your Client ID which you must send to the Data Owner to enter into Duet so it can pair your notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "executionInfo": {
     "elapsed": 15157,
     "status": "ok",
     "timestamp": 1613199445930,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "SDxITVktXmRM",
    "outputId": "67705052-942b-466b-8b4a-42207fd9b360"
   },
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "duet = sy.join_duet(loopback=True)\n",
    "sy.logger.add(sink=\"./syft_ds.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 0 : Now STOP and run the Data Owner notebook until Checkpoint 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4080,
     "status": "ok",
     "timestamp": 1613199475597,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "6W-d6XPGX_0A"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir, makedirs, remove\n",
    "from os.path import exists, join, basename\n",
    "import tarfile\n",
    "import subprocess\n",
    "from six.moves import urllib\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.utils.data as data\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Resize\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "try:\n",
    "    # make notebook progress bars nicer\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    print(f\"Unable to import tqdm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogAyRqtDqeB4"
   },
   "source": [
    "# Set params\n",
    "\n",
    "Following params are based on the original implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1036,
     "status": "ok",
     "timestamp": 1613199476974,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "10asMOrFiRa0"
   },
   "outputs": [],
   "source": [
    "config = {\"upscale_factor\": 2,\n",
    "          \"threads\":0,\n",
    "          \"batchSize\":1, # other size may not work\n",
    "          \"testBatchSize\":1,\n",
    "          \"lr\":0.001,\n",
    "          \"epochs\":2,\n",
    "          \"no_cuda\":True,\n",
    "          \"log_batch_size\":10,\n",
    "          \"seed\":42,\n",
    "          \"dry_run\":True,\n",
    "          \"test\":False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPgj4rQ9qcDC"
   },
   "source": [
    "# Load data\n",
    "\n",
    "You can receive the data which data owner send with following codes. Also, you need custom Dataset and collate_fn which can process tensorpointer. As of noe, tensorpointer doesn't support slice, so batch size must be one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1613199478250,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "mIA0Oj-2huzt"
   },
   "outputs": [],
   "source": [
    "remote_torch = duet.torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 1020,
     "status": "ok",
     "timestamp": 1613199479497,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "BitTaXTIhv-M",
    "outputId": "b47e6d91-8b96-4b0f-e7b4-23b406dbf8e8"
   },
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1417,
     "status": "ok",
     "timestamp": 1613199481456,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "53E99h0BikyO"
   },
   "outputs": [],
   "source": [
    "X_train = duet.store[\"X_train\"]\n",
    "y_train = duet.store[\"y_train\"]\n",
    "train_num = duet.store[\"train_num\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1160,
     "status": "ok",
     "timestamp": 1613199482720,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "7bCLz37Qj7XW"
   },
   "outputs": [],
   "source": [
    "class DatasetFromPointer(data.Dataset):\n",
    "    def __init__(self, \n",
    "                 X_tensorpointer,\n",
    "                 y_tensorpointer,\n",
    "                 datanum_pointer,\n",
    "                 ):\n",
    "        super(DatasetFromPointer, self).__init__()\n",
    "        self.X_tensorpointer = X_tensorpointer\n",
    "        self.y_tensorpointer = y_tensorpointer\n",
    "        self.datanum_pointer = datanum_pointer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input = self.X_tensorpointer[index]\n",
    "        target = self.y_tensorpointer[index]\n",
    "        return input, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum_pointer.get(\n",
    "        request_block=True,\n",
    "        reason=\"To write the training loop\",\n",
    "        timeout_secs=30,\n",
    "        delete_obj=False,\n",
    "    )\n",
    "        \n",
    "def batch_idx_fn(batch):\n",
    "    return batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1019,
     "status": "ok",
     "timestamp": 1613199484231,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "FFV0F1Jllr5e"
   },
   "outputs": [],
   "source": [
    "train_set = DatasetFromPointer(X_train, y_train, train_num)\n",
    "training_data_loader = DataLoader(dataset=train_set, \n",
    "                                  num_workers=config[\"threads\"], batch_size=config[\"batchSize\"], shuffle=True,\n",
    "                                  collate_fn=batch_idx_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3vQWcY1qkIe"
   },
   "source": [
    "# Define and create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 786,
     "status": "ok",
     "timestamp": 1613199486033,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "wkVopDoehxrZ"
   },
   "outputs": [],
   "source": [
    "class Net(sy.Module):\n",
    "    def __init__(self, torch_ref, upscale_factor):\n",
    "        super(Net, self).__init__(torch_ref = torch_ref)\n",
    "\n",
    "        self.relu = self.torch_ref.nn.ReLU()\n",
    "        self.conv1 = self.torch_ref.nn.Conv2d(1, 64, (5, 5), (1, 1), (2, 2))\n",
    "        self.conv2 = self.torch_ref.nn.Conv2d(64, 64, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv3 = self.torch_ref.nn.Conv2d(64, 32, (3, 3), (1, 1), (1, 1))\n",
    "        self.conv4 = self.torch_ref.nn.Conv2d(32, upscale_factor ** 2, (3, 3), (1, 1), (1, 1))\n",
    "        self.pixel_shuffle = self.torch_ref.nn.PixelShuffle(upscale_factor)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pixel_shuffle(self.conv4(x))\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        init.orthogonal_(self.conv1.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv2.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv3.weight, init.calculate_gain('relu'))\n",
    "        init.orthogonal_(self.conv4.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1452,
     "status": "ok",
     "timestamp": 1613199488016,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "7YQRpkkBsCG6"
   },
   "outputs": [],
   "source": [
    "local_net = Net(torch, config[\"upscale_factor\"])\n",
    "remote_net = local_net.send(duet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzvjJronqo-5"
   },
   "source": [
    "# Check cuda\n",
    "\n",
    "You should ask the data owner whether he/she has GPUs or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2154,
     "status": "ok",
     "timestamp": 1613199491670,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "HCNikEKBvvL4",
    "outputId": "b43b9147-e107-4aba-bf70-9825375842de"
   },
   "outputs": [],
   "source": [
    "has_cuda = False\n",
    "has_cuda_ptr = remote_torch.cuda.is_available()\n",
    "\n",
    "# lets ask to see if our Data Owner has CUDA\n",
    "has_cuda = bool(has_cuda_ptr.get(\n",
    "    request_block=True,\n",
    "    reason=\"To run test and inference locally\",\n",
    "    timeout_secs=3,  # change to something slower\n",
    "))\n",
    "print(\"Is cuda available ? : \", has_cuda)\n",
    "\n",
    "use_cuda = not config[\"no_cuda\"] and has_cuda\n",
    "# now we can set the seed\n",
    "remote_torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "device = remote_torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "#print(f\"Data Owner device is {device.type.get()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3987,
     "status": "ok",
     "timestamp": 1613199495894,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "kXrb9aEIvnPM"
   },
   "outputs": [],
   "source": [
    "# if we have CUDA lets send our model to the GPU\n",
    "if has_cuda:\n",
    "    remote_net.cuda(device)\n",
    "else:\n",
    "    remote_net.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrXYn_mYquDT"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 947,
     "status": "ok",
     "timestamp": 1613199497917,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "4RyjwTSjsGgf"
   },
   "outputs": [],
   "source": [
    "criterion = remote_torch.nn.MSELoss()\n",
    "optimizer = remote_torch.optim.Adam(remote_net.parameters(), lr=config[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6482,
     "status": "ok",
     "timestamp": 1613199504842,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "NdSz-fJJuqyI",
    "outputId": "41dcc767-dee6-471e-89ca-211ebce63f3a"
   },
   "outputs": [],
   "source": [
    "for epoch in range(config[\"epochs\"]):\n",
    "    \n",
    "    remote_net.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, data_pointers in enumerate(training_data_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        data_ptr, target_ptr = data_pointers[0], data_pointers[1]\n",
    "        data_ptr_reshape = remote_torch.unsqueeze(remote_torch.unsqueeze(data_ptr, 0), 0)\n",
    "        target_ptr_reshape = remote_torch.unsqueeze(remote_torch.unsqueeze(target_ptr, 0), 0)\n",
    "       \n",
    "        output_ptr = remote_net(data_ptr_reshape)\n",
    "        \n",
    "        loss = criterion(output_ptr, target_ptr_reshape)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % config[\"log_batch_size\"] == 0:\n",
    "            loss_item = loss.item().get(\n",
    "                reason=\"To evaluate training progress\",\n",
    "                request_block=True,\n",
    "                timeout_secs=3,\n",
    "                delete_obj=False,\n",
    "                verbose=False\n",
    "                )\n",
    "            print(f\"epoch {epoch}, batch_idx {batch_idx}, loss {loss_item}\")\n",
    "\n",
    "        if config[\"dry_run\"]:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hYy-CNZjoQF"
   },
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ggnh-SEGw9jc"
   },
   "outputs": [],
   "source": [
    "local_net = remote_net.get(\n",
    "    request_block=True,\n",
    "    reason=\"test evaluation\",\n",
    "    timeout_secs=5\n",
    ")\n",
    "\n",
    "local_net.save(f\"super_resolve.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgq4ylpmjsCL"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/segbench/BSDS300/html/images/plain/normal/color/12084.jpg\"\n",
    "test_img_name = basename(image_url)\n",
    "os.system(f'curl -O {image_url}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9735,
     "status": "ok",
     "timestamp": 1613199521625,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "K1KvmWx0kOdi"
   },
   "outputs": [],
   "source": [
    "output_img_name = \"output.jpg\"\n",
    "\n",
    "img = Image.open(test_img_name).convert(\"YCbCr\")\n",
    "y, cb, cr = img.split()\n",
    "img_to_tensor = ToTensor()\n",
    "input = img_to_tensor(y).view(1, -1, y.size[1], y.size[0])\n",
    "\n",
    "if has_cuda:\n",
    "    local_net = local_net.cuda()\n",
    "    input = input.cuda()\n",
    "\n",
    "out = local_net(input)\n",
    "out = out.cpu()\n",
    "out_img_y = out[0].detach().numpy()\n",
    "out_img_y *= 255.0\n",
    "out_img_y = out_img_y.clip(0, 255)\n",
    "out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode=\"L\")\n",
    "\n",
    "out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC)\n",
    "out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC)\n",
    "out_img = Image.merge(\"YCbCr\", [out_img_y, out_img_cb, out_img_cr]).convert(\"RGB\")\n",
    "\n",
    "out_img.save(output_img_name)\n",
    "print(\"output image saved to \", output_img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3hV9MgWLxCv"
   },
   "outputs": [],
   "source": [
    "original_image = Image.open(\"12084.jpg\")\n",
    "super_image = Image.open(\"output.jpg\")\n",
    "#display(original_image)\n",
    "#display(super_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 1 : Now STOP and run the Data Owner notebook until Checkpoint 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "re_SuperResolution_Syft_Data_Scientist_re.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
