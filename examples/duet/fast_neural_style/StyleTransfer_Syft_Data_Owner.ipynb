{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Style Transfer - Syft Duet - Data Owner ðŸŽ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Launch a Duet Server and Connect\n",
    "\n",
    "As a Data Owner, you want to allow someone else to perform data science on data that you own and likely want to protect.\n",
    "\n",
    "In order to do this, we must load our data into a locally running server within this notebook. We call this server a \"Duet\".\n",
    "\n",
    "To begin, you must launch Duet and help your Duet \"partner\" (a Data Scientist) connect to this server.\n",
    "\n",
    "You do this by running the code below and sending the code snippet containing your unique Server ID to your partner and following the instructions it gives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "duet = sy.launch_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "from torchvision import transforms\n",
    "\n",
    "from original.neural_style import utils\n",
    "# from original.neural_style.vgg import Vgg16\n",
    "# from original.neural_style.transformer_net import TransformerNet # redefined below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the already pre-trained model (with the style it was trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"original/download_saved_models.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNet(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(TransformerNet, self).__init__(torch_ref=torch_ref)\n",
    "        # Initial convolution layers\n",
    "        self.conv1 = ConvLayer(self.torch_ref, 3, 32, kernel_size=9, stride=1)\n",
    "        self.in1 = self.torch_ref.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.conv2 = ConvLayer(self.torch_ref, 32, 64, kernel_size=3, stride=2)\n",
    "        self.in2 = self.torch_ref.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.conv3 = ConvLayer(self.torch_ref, 64, 128, kernel_size=3, stride=2)\n",
    "        self.in3 = self.torch_ref.nn.InstanceNorm2d(128, affine=True)\n",
    "        # Residual layers\n",
    "        self.res1 = ResidualBlock(self.torch_ref, 128)\n",
    "        self.res2 = ResidualBlock(self.torch_ref, 128)\n",
    "        self.res3 = ResidualBlock(self.torch_ref, 128)\n",
    "        self.res4 = ResidualBlock(self.torch_ref, 128)\n",
    "        self.res5 = ResidualBlock(self.torch_ref, 128)\n",
    "        # Upsampling Layers\n",
    "        self.deconv1 = UpsampleConvLayer(self.torch_ref, 128, 64, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in4 = self.torch_ref.nn.InstanceNorm2d(64, affine=True)\n",
    "        self.deconv2 = UpsampleConvLayer(self.torch_ref, 64, 32, kernel_size=3, stride=1, upsample=2)\n",
    "        self.in5 = self.torch_ref.nn.InstanceNorm2d(32, affine=True)\n",
    "        self.deconv3 = ConvLayer(self.torch_ref, 32, 3, kernel_size=9, stride=1)\n",
    "        # Non-linearities\n",
    "        self.relu = self.torch_ref.nn.ReLU()\n",
    "\n",
    "    def forward(self, X):\n",
    "        y = self.relu(self.in1(self.conv1(X)))\n",
    "        y = self.relu(self.in2(self.conv2(y)))\n",
    "        y = self.relu(self.in3(self.conv3(y)))\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "        y = self.relu(self.in4(self.deconv1(y)))\n",
    "        y = self.relu(self.in5(self.deconv2(y)))\n",
    "        y = self.deconv3(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class ConvLayer(sy.Module):\n",
    "    def __init__(self, torch_ref, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__(torch_ref=torch_ref)\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = self.torch_ref.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = self.torch_ref.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResidualBlock(sy.Module):\n",
    "    \"\"\"ResidualBlock\n",
    "    introduced in: https://arxiv.org/abs/1512.03385\n",
    "    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, torch_ref, channels):\n",
    "        super(ResidualBlock, self).__init__(torch_ref=torch_ref)\n",
    "        self.conv1 = ConvLayer(self.torch_ref, channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = self.torch_ref.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvLayer(self.torch_ref, channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = self.torch_ref.nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = self.torch_ref.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.in1(self.conv1(x)))\n",
    "        out = self.in2(self.conv2(out))\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class UpsampleConvLayer(sy.Module):\n",
    "    \"\"\"UpsampleConvLayer\n",
    "    Upsamples the input and then does a convolution. This method gives better results\n",
    "    compared to ConvTranspose2d.\n",
    "    ref: http://distill.pub/2016/deconv-checkerboard/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, torch_ref, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__(torch_ref=torch_ref)\n",
    "        self.upsample = upsample\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = self.torch_ref.nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = self.torch_ref.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        if self.upsample:\n",
    "            x_in = self.torch_ref.nn.functional.interpolate(\n",
    "                x_in, mode=\"nearest\", scale_factor=self.upsample\n",
    "            )\n",
    "        out = self.reflection_pad(x_in)\n",
    "        out = self.conv2d(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model_path = \"saved_models/mosaic.pth\" \n",
    "    \n",
    "# TODO\n",
    "# load weights into the model\n",
    "with torch.no_grad():\n",
    "    style_model = TransformerNet(torch)\n",
    "    state_dict = torch.load(model_path)\n",
    "    # remove saved deprecated running_* keys in InstanceNorm from the checkpoint\n",
    "#     for k in list(state_dict.keys()):\n",
    "#         if re.search(r\"in\\d+\\.running_(mean|var)$\", k):\n",
    "#             del state_dict[k]\n",
    "#     style_model.load_state_dict(state_dict)\n",
    "    \n",
    "# sy_model = sy.Module.from_pytorch(torch, dict(style_model.named_modules()))\n",
    "print(style_model.modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# make the top level of the symodule sendable so that a pointer reference can be obtained on \n",
    "# the DS side and used to execute\n",
    "ptr = style_model.send(duet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img = utils.load_image(\"original/images/content_images/amber.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Lambda(lambda x: x.mul(255))]\n",
    "    )\n",
    "content_img = content_transform(content_img)\n",
    "content_img = content_img.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = style_model(content_img)[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to directly show the image in the notebook\n",
    "# The util function from the pytorch repository saves the image\n",
    "# def get_img_from_tensor(out):\n",
    "#     trans = transforms.ToPILImage()\n",
    "#     img = \n",
    "#     return output.detach().clamp(0, 255).numpy().astype(\"uint8\")\n",
    "\n",
    "transforms.ToPILImage()(output.detach().numpy().astype(\"uint8\").transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
