{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALSfsSFjeV1o"
   },
   "source": [
    "# Training the Boston Housing Dataset using PySyft and SocketWorker\n",
    "\n",
    "This tutorial is a 3 notebook tutorial. The partners notebooks are the notebooks entitled `SocketWorker Server Alice.ipynb` and `SocketWorker Server Bob.ipynb`. They are in the same folder as this notebook. You should execute this notebook **AFTER** you have executed the others.\n",
    "\n",
    "This tutorial is an example of training a neural network in a federated fashion on the Boston Housing dataset using socketworkers, python instances Alice and Bob running in the two other tabs you have opened in your browser.\n",
    "\n",
    "Before starting with this notebook, we recommend looking at `toy/Federated Learning Example.ipynb` which provides a basic example.\n",
    "\n",
    "Preformance: achieves ~20 MSE in 10 epochs in 25s _(Perf. measured on [colab.research.google.com/17upxC...](https://colab.research.google.com/drive/17upxCYJmJ6Zoxv0KjiJ1ZbchlJybsfhs))_\n",
    "\n",
    "_This notebook doesn't intend to provide a good prediction model and rather focuses on computation overhead due to federated learning._\n",
    "\n",
    "The base example without federated learning can be found here: [colab.research.google.com/drive/1ne4ra...](https://colab.research.google.com/drive/1ne4rap-8nD6-jABV94fkPBHvtPj-RrKY#scrollTo=i_gUp-uFfwGL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFvJUEHOfA-r"
   },
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "ZgomH7s4R5cT",
    "outputId": "83725aac-8149-4291-8bc1-8e9c299e0428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post3\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# from keras.datasets import boston_housing\n",
    "\n",
    "print(torch.__version__)\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for training (default: 8)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for testing (default: 8)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, metavar='M',\n",
    "                    help='SGD momentum (default: 0.0)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('../other/data/boston_housing.pickle','rb')\n",
    "((X, y), (X_test, y_test)) = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "caoZ_dM0wzdf",
    "outputId": "1a138f2e-f184-4b94-a93b-486617bde21a"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
    "# preprocessing\n",
    "mean = X.mean(0, keepdim=True)\n",
    "dev = X.std(0, keepdim=True)\n",
    "mean[:, 3] = 0. # the feature at column 3 is binary,\n",
    "dev[:, 3] = 1.  # so I'd rather not standardize it\n",
    "X = (X - mean) / dev\n",
    "X_test = (X_test - mean) / dev\n",
    "train = TensorDataset(X, y)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test, batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Az9PH1BrfK46"
   },
   "source": [
    "#  Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R5TyXfcOXp1w"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 32)\n",
    "        self.fc2 = nn.Linear(32, 24)\n",
    "        self.fc3 = nn.Linear(24, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8r78FvSffTp0"
   },
   "source": [
    "# Hooking into Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai5bqlEWSFgf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Socket Worker...\n",
      "Ready to receive commands...\n",
      "Attaching Pointer to Socket Worker...\n",
      "Attaching Pointer to Socket Worker...\n"
     ]
    }
   ],
   "source": [
    "import syft\n",
    "import syft as sy\n",
    "from syft.core import utils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import random\n",
    "from syft.core.frameworks.torch import utils as torch_utils\n",
    "from torch.autograd import Variable\n",
    "\n",
    "local_worker = sy.SocketWorker(id=\"local\", port=2008, hook=None, is_client_worker=False)\n",
    "hook = sy.TorchHook(local_worker=local_worker, verbose=False)\n",
    "me = hook.local_worker\n",
    "me.hook = hook\n",
    "\n",
    "bob = sy.SocketWorker(id=\"bob\", port=2006, hook=hook, is_pointer=True, is_client_worker=False)\n",
    "\n",
    "alice = sy.SocketWorker(id=\"alice\", port=2007, hook=hook, is_pointer=True, is_client_worker=False)\n",
    "\n",
    "compute_nodes = [bob, alice]\n",
    "\n",
    "me.add_workers([bob, alice])\n",
    "#bob.add_workers([me, alice])\n",
    "#alice.add_workers([me, bob])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apSGFY_fv_7z"
   },
   "source": [
    "**Send data to the worker** <br>\n",
    "Usually they would already have it, this is just for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UByFBzz2wARz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    print(batch_idx)\n",
    "    data = Variable(data)\n",
    "    target = Variable(target.float())\n",
    "    data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    train_distributed_dataset.append((data, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9vhghBr1fpPE"
   },
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cgw89i1HSU5X"
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "            \n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        pred = model(data)\n",
    "        loss = F.mse_loss(pred, target.float())\n",
    "        loss.backward()\n",
    "        model.get()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_gUp-uFfwGL"
   },
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNED3GD6Y3Va"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output, target.float(), size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmvTEpIbfzoC"
   },
   "source": [
    "# Training The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1094
    },
    "colab_type": "code",
    "id": "9JWkpGtoY48Y",
    "outputId": "4c6fbc4d-29f4-42bb-cd43-e7a76d8ea04e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/408 (0%)]\tLoss: 562.255920\n",
      "Train Epoch: 1 [80/408 (20%)]\tLoss: 551.988525\n",
      "Train Epoch: 1 [160/408 (39%)]\tLoss: 640.131042\n",
      "Train Epoch: 1 [240/408 (59%)]\tLoss: 113.095528\n",
      "Train Epoch: 1 [320/408 (78%)]\tLoss: 17.793673\n",
      "Train Epoch: 1 [400/408 (98%)]\tLoss: 36.492374\n",
      "Train Epoch: 2 [0/408 (0%)]\tLoss: 8.652925\n",
      "Train Epoch: 2 [80/408 (20%)]\tLoss: 17.827301\n",
      "Train Epoch: 2 [160/408 (39%)]\tLoss: 15.696451\n",
      "Train Epoch: 2 [240/408 (59%)]\tLoss: 16.655920\n",
      "Train Epoch: 2 [320/408 (78%)]\tLoss: 17.403423\n",
      "Train Epoch: 2 [400/408 (98%)]\tLoss: 17.845070\n",
      "Train Epoch: 3 [0/408 (0%)]\tLoss: 7.068490\n",
      "Train Epoch: 3 [80/408 (20%)]\tLoss: 12.011532\n",
      "Train Epoch: 3 [160/408 (39%)]\tLoss: 7.905748\n",
      "Train Epoch: 3 [240/408 (59%)]\tLoss: 13.399657\n",
      "Train Epoch: 3 [320/408 (78%)]\tLoss: 12.010728\n",
      "Train Epoch: 3 [400/408 (98%)]\tLoss: 10.196634\n",
      "Train Epoch: 4 [0/408 (0%)]\tLoss: 6.434999\n",
      "Train Epoch: 4 [80/408 (20%)]\tLoss: 7.154058\n",
      "Train Epoch: 4 [160/408 (39%)]\tLoss: 5.233712\n",
      "Train Epoch: 4 [240/408 (59%)]\tLoss: 11.577604\n",
      "Train Epoch: 4 [320/408 (78%)]\tLoss: 8.724213\n",
      "Train Epoch: 4 [400/408 (98%)]\tLoss: 8.727882\n",
      "Train Epoch: 5 [0/408 (0%)]\tLoss: 5.678280\n",
      "Train Epoch: 5 [80/408 (20%)]\tLoss: 4.741015\n",
      "Train Epoch: 5 [160/408 (39%)]\tLoss: 4.026522\n",
      "Train Epoch: 5 [240/408 (59%)]\tLoss: 10.108507\n",
      "Train Epoch: 5 [320/408 (78%)]\tLoss: 6.788218\n",
      "Train Epoch: 5 [400/408 (98%)]\tLoss: 9.154858\n",
      "Train Epoch: 6 [0/408 (0%)]\tLoss: 4.730551\n",
      "Train Epoch: 6 [80/408 (20%)]\tLoss: 3.897393\n",
      "Train Epoch: 6 [160/408 (39%)]\tLoss: 3.662471\n",
      "Train Epoch: 6 [240/408 (59%)]\tLoss: 8.842747\n",
      "Train Epoch: 6 [320/408 (78%)]\tLoss: 5.860409\n",
      "Train Epoch: 6 [400/408 (98%)]\tLoss: 9.349958\n",
      "Train Epoch: 7 [0/408 (0%)]\tLoss: 4.160314\n",
      "Train Epoch: 7 [80/408 (20%)]\tLoss: 3.643044\n",
      "Train Epoch: 7 [160/408 (39%)]\tLoss: 3.335357\n",
      "Train Epoch: 7 [240/408 (59%)]\tLoss: 7.844121\n",
      "Train Epoch: 7 [320/408 (78%)]\tLoss: 5.585374\n",
      "Train Epoch: 7 [400/408 (98%)]\tLoss: 8.804581\n",
      "Train Epoch: 8 [0/408 (0%)]\tLoss: 3.648461\n",
      "Train Epoch: 8 [80/408 (20%)]\tLoss: 3.807822\n",
      "Train Epoch: 8 [160/408 (39%)]\tLoss: 3.337471\n",
      "Train Epoch: 8 [240/408 (59%)]\tLoss: 7.382591\n",
      "Train Epoch: 8 [320/408 (78%)]\tLoss: 5.541363\n",
      "Train Epoch: 8 [400/408 (98%)]\tLoss: 8.410536\n",
      "Train Epoch: 9 [0/408 (0%)]\tLoss: 3.464297\n",
      "Train Epoch: 9 [80/408 (20%)]\tLoss: 3.837600\n",
      "Train Epoch: 9 [160/408 (39%)]\tLoss: 3.438318\n",
      "Train Epoch: 9 [240/408 (59%)]\tLoss: 6.993451\n",
      "Train Epoch: 9 [320/408 (78%)]\tLoss: 5.296585\n",
      "Train Epoch: 9 [400/408 (98%)]\tLoss: 8.270566\n",
      "Train Epoch: 10 [0/408 (0%)]\tLoss: 3.351248\n",
      "Train Epoch: 10 [80/408 (20%)]\tLoss: 3.812061\n",
      "Train Epoch: 10 [160/408 (39%)]\tLoss: 3.579133\n",
      "Train Epoch: 10 [240/408 (59%)]\tLoss: 6.517155\n",
      "Train Epoch: 10 [320/408 (78%)]\tLoss: 5.119539\n",
      "Train Epoch: 10 [400/408 (98%)]\tLoss: 8.209362\n",
      "CPU times: user 7.43 s, sys: 262 ms, total: 7.69 s\n",
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5nJSaJ4f9tk"
   },
   "source": [
    "# Calculating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "Il8R8NWBd-Xb",
    "outputId": "f01f338d-629e-48cd-faa9-e696d1e8b919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 20.7802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylB9_i-CVZM6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Syft) Denver_Boston_Federated_Training.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
