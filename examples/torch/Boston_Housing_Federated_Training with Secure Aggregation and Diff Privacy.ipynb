{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALSfsSFjeV1o"
   },
   "source": [
    "# Federated Learning with Secure Aggregation and Diff Privacy using PySyft\n",
    "\n",
    "This is an example of using our new Secure Multi-Party Computation tensor (SPDZTensor) to perform an encrypted average of gradients across multiple data owners.\n",
    "\n",
    "Before starting with this notebook, we recommend looking at `Boston_Housing_Federated_Training.ipynb` which is located in the same folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nFvJUEHOfA-r"
   },
   "source": [
    "# Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! URL=\"https://github.com/LaRiffle/differential-privacy.git\" && FOLDER=\"differential_privacy\" && if [ ! -d $FOLDER ]; then git clone $URL $FOLDER; else (cd $FOLDER && git pull $URL && cd ..); fi;\n",
    "#! pip install --upgrade --force-reinstall websockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for training (default: 8)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for testing (default: 8)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, metavar='M',\n",
    "                    help='SGD momentum (default: 0.0)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs ={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "caoZ_dM0wzdf",
    "outputId": "1a138f2e-f184-4b94-a93b-486617bde21a"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('../other/data/boston_housing.pickle','rb')\n",
    "((X, y), (X_test, y_test)) = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
    "# preprocessing\n",
    "mean = X.mean(0, keepdim=True)\n",
    "dev = X.std(0, keepdim=True)\n",
    "mean[:, 3] = 0. # the feature at column 3 is binary,\n",
    "dev[:, 3] = 1.  # so I'd rather not standardize it\n",
    "X = (X - mean) / dev\n",
    "X_test = (X_test - mean) / dev\n",
    "train = TensorDataset(X, y)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test, batch_size=args.test_batch_size, shuffle=True, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Az9PH1BrfK46"
   },
   "source": [
    "#  Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R5TyXfcOXp1w"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 32)\n",
    "        self.fc2 = nn.Linear(32, 24)\n",
    "        self.fc3 = nn.Linear(24, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 13)\n",
    "        print(\"WEIGHT LOCATION:\" + str(self.fc1.weight.location))\n",
    "        print(\"BIAS LOCATION:\" + str(self.fc1.bias.location))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def divide_clip_grads(self):\n",
    "        for key, param in self.named_parameters():\n",
    "            param.grad /= n_batch\n",
    "            gradient_clip(param)\n",
    "            \n",
    "    def add_noise_to_grads(self):\n",
    "        for key, param in self.named_parameters():\n",
    "            noise = 1/LOT_SIZE * gaussian_noise(param.grad)\n",
    "            param.grad += noise\n",
    "model = Net()\n",
    "model_params = list(model.parameters())\n",
    "\n",
    "bobs_model = Net()\n",
    "alices_model = Net()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8r78FvSffTp0"
   },
   "source": [
    "# Hooking into Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai5bqlEWSFgf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Worker bob already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker alice already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 0 already exists. Replacing old worker which could cause unexpected behavior\n",
      "WARNING:root:Worker 0 already exists. Replacing old worker which could cause unexpected behavior\n"
     ]
    }
   ],
   "source": [
    "import syft\n",
    "import syft as sy\n",
    "from syft.core import utils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import random\n",
    "from syft.core.frameworks.torch import utils as torch_utils\n",
    "from torch.autograd import Variable\n",
    "hook = sy.TorchHook(verbose=False)\n",
    "me = hook.local_worker\n",
    "bob = sy.VirtualWorker(id=\"bob\",hook=hook, is_client_worker=False)\n",
    "alice = sy.VirtualWorker(id=\"alice\",hook=hook, is_client_worker=False)\n",
    "me.is_client_worker = False\n",
    "\n",
    "compute_nodes = [bob, alice]\n",
    "\n",
    "me.add_workers([bob, alice])\n",
    "bob.add_workers([me, alice])\n",
    "alice.add_workers([me, bob])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apSGFY_fv_7z"
   },
   "source": [
    "**Send data to the worker** <br>\n",
    "Usually they would already have it, this is just for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UByFBzz2wARz"
   },
   "outputs": [],
   "source": [
    "remote_dataset = (list(),list())\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = Variable(data)\n",
    "    target = Variable(target.float())\n",
    "    data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    remote_dataset[batch_idx % len(compute_nodes)].append((data, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diff Privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 9.689610525210778\n",
      "The mechanism is (O(0.297030), 0.000010)-differentially private\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Inspired from Abadi et al., Deep Learning with Differential Privacy, \n",
    "    Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications\n",
    "    Security, 2016\n",
    "\"\"\"\n",
    "from differential_privacy.privacy_accountant.pytorch import accountant\n",
    "import numpy as np\n",
    "\n",
    "n_batch = 3\n",
    "NUM_TRAINING_IMAGES = X.size()[0]\n",
    "LOT_SIZE = n_batch * args.batch_size\n",
    "N_LOTS = 100\n",
    "T = N_LOTS # number of samplings\n",
    "\n",
    "bound = 10\n",
    "epsilon = 0.5\n",
    "delta = 10**(-5)\n",
    "sigma = np.sqrt(2 * np.log(1.25/delta))/epsilon \n",
    "\n",
    "def sum_batch(grads):\n",
    "    n_items = len(grads)\n",
    "    return grads.view(n_items, -1).sum(dim=1)\n",
    "\n",
    "def gradient_clip(param):\n",
    "    \"\"\"Clip gradient to ensure ||param.grad||2 < bound\"\"\"\n",
    "    nn.utils.clip_grad_norm([param], bound)\n",
    "\n",
    "def gaussian_noise(grads):\n",
    "    \"\"\"Add gaussian noise to gradients\"\"\"\n",
    "    shape = grads.shape\n",
    "    noise = Variable(torch.zeros(shape))\n",
    "    noise.data.normal_(0.0, std=bound*sigma)\n",
    "    return noise\n",
    "\n",
    "q = LOT_SIZE / NUM_TRAINING_IMAGES\n",
    "spent_epsilon = q * epsilon * np.sqrt(T)\n",
    "spent_delta = delta\n",
    "print('sigma =', sigma)\n",
    "print('The mechanism is (O(%f), %f)-differentially private' % (spent_epsilon, spent_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "priv_accountant = accountant.GaussianMomentsAccountant(NUM_TRAINING_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def select_lot(worker_dataset):\n",
    "    \"\"\"\n",
    "    Build the lot by sampling over the dataset\n",
    "    \"\"\"\n",
    "    #- select indices in worker_dataset of tensors \n",
    "    valid_ids = np.arange(len(worker_dataset)-1) \n",
    "    #- Select indices and reshape into batches\n",
    "    batches_ids = np.random.choice(valid_ids,size=LOT_SIZE, replace=False).reshape(-1, args.batch_size)\n",
    "    #- Build lot\n",
    "    lot = []\n",
    "    for batch_ids in batches_ids:\n",
    "        batch_data = []\n",
    "        batch_target = []\n",
    "        for batch_id in batch_ids:\n",
    "            data, target = worker_dataset[batch_id]\n",
    "            batch_data.append(data)\n",
    "            batch_target.append(target)\n",
    "        \n",
    "        lot.append((torch.stack(batch_data), torch.stack(batch_target)))\n",
    "    return lot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(worker_idx, model, optimizer, lot_idx):\n",
    "    # Build the lot by sampling over the dataset\n",
    "    worker_dataset = remote_dataset[worker_idx]\n",
    "    lot = select_lot(worker_dataset)\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Iterate on the lot batch per batch\n",
    "    for batch_idx, (data,target) in enumerate(lot):\n",
    "        # update the model\n",
    "        \n",
    "        #send the model to the worker\n",
    "        worker = data.location\n",
    "        print(\"DATA LOCATION: \" + str(data.location))\n",
    "        model.send(worker)\n",
    "        for param in model.parameters():\n",
    "            if not (param.id_at_location in worker._objects.keys()):\n",
    "                print(param)\n",
    "                print(\"Param Id: \" + str(param.id_at_location))\n",
    "                print(worker._objects.keys())\n",
    "            assert(param.id_at_location in worker._objects.keys())\n",
    "        pred = model(data)\n",
    "\n",
    "        loss = F.mse_loss(pred, target.float())\n",
    "        # Note that because we apply backward() several times without resetting \n",
    "        # the grads (optimizer.zero_grad()), we sum the gradients \n",
    "        loss.backward() \n",
    "        \n",
    "        if batch_idx == 0 and False:\n",
    "            loss.get()\n",
    "            print('Train Lot: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                lot_idx, batch_idx * args.batch_size, LOT_SIZE,\n",
    "                100. * batch_idx * args.batch_size / LOT_SIZE, loss.data[0]))\n",
    "            print(priv_accountant.get_privacy_spent(target_deltas=[spent_delta]))\n",
    "       \n",
    "\n",
    "    \n",
    "    optimizer.step()\n",
    "        \n",
    "    priv_accountant.accumulate_privacy_spending(bound * sigma, LOT_SIZE)\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bobs_optimizer = optim.SGD(bobs_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "alices_optimizer = optim.SGD(alices_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "models = [bobs_model, alices_model]\n",
    "#params = [list(bobs_model.parameters()), list(alices_model.parameters())]\n",
    "optimizers = [bobs_optimizer, alices_optimizer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Â Federated Learning training\n",
    "def train(lot_idx):        \n",
    "    # update remote models\n",
    "    for remote_index in range(len(compute_nodes)):\n",
    "        models[remote_index].train()\n",
    "        models[remote_index].send(compute_nodes[remote_index])\n",
    "        print(\"LOT number: \" + str (lot_idx))\n",
    "        print(\"Remote_index: \"+ str(remote_index))\n",
    "        models[remote_index] = update(remote_index, models[remote_index], optimizers[remote_index], lot_idx)\n",
    "\n",
    "    new_params = list()\n",
    "\n",
    "    for param_i in range(len(params[0])):\n",
    "\n",
    "        spdz_params = list()\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            spdz_params.append((params[remote_index][param_i].data+0).fix_precision().share(bob, alice).get())\n",
    "\n",
    "       # new_param = (spdz_params[0] + spdz_params[1]).get().decode()/2\n",
    "        new_param = (spdz_params[0] + spdz_params[1]).get().decode()/2\n",
    "        new_params.append(new_param)\n",
    "\n",
    "    for model in params:\n",
    "        for param in model:\n",
    "            param.data *= 0\n",
    "\n",
    "    for remote_index, model in enumerate(models):\n",
    "        model.get()\n",
    "        model.divide_clip_grads()\n",
    "        model.add_noise_to_grads()\n",
    "        model.send(compute_nodes[remote_index])\n",
    "\n",
    "    for remote_index in range(len(compute_nodes)):\n",
    "        for param_index in range(len(params[remote_index])):\n",
    "            params[remote_index][param_index].data.set_(new_params[param_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_gUp-uFfwGL"
   },
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TNED3GD6Y3Va"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    models[0].eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = models[0](data)\n",
    "        test_loss += F.mse_loss(output, target.float(), size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EmvTEpIbfzoC"
   },
   "source": [
    "# Training The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1094
    },
    "colab_type": "code",
    "id": "9JWkpGtoY48Y",
    "outputId": "4c6fbc4d-29f4-42bb-cd43-e7a76d8ea04e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "LOT number: 1\n",
      "Remote_index: 0\n",
      "DATA LOCATION: <syft.core.workers.virtual.VirtualWorker id:bob>\n",
      "Parameter containing:FloatTensor[_PointerTensor - id:9855202497 owner:0 loc:bob id@loc:32128155621]\n",
      "Param Id: 63272193063\n",
      "dict_keys([21101973266, 95091569294, 59185634899, 465293687, 14711960265, 94212615415, 26139528504, 95264881199, 34498290797, 67828774885, 59436795134, 2265307542, 10784322556, 66603057927, 25477858017, 44125347709, 96066567376, 54698364035, 89810002134, 48924003934, 38875813876, 18305811626, 39840684790, 64074233920, 7019772545, 51490390898, 92429788729, 54634512744, 9834648713, 40535374968, 61622406541, 21533691714, 99407225188, 71118483188, 26813111740, 33404647555, 55681704202, 44410849471, 93889544679, 45710345895, 38055847999, 10988789934, 78650271027, 62127217682, 31396629062, 51703986676, 948120620, 71234298707, 11881023616, 67425989945, 42184462052, 69760082455, 67152358869, 26338567627, 42471544588, 37603003062, 45429035420, 85995675216, 41504253643, 53106297361, 26148336688, 34328562671, 33157336577, 48455144598, 36631733270, 74618403814, 52032626598, 11771878490, 20016335689, 85785420981, 84898062640, 79206636174, 71840035247, 26406992567, 5033093688, 75620033204, 11071291025, 31034354129, 46834284393, 47975020734, 25042105658, 97455224428, 40133281930, 22666856182, 11528505907, 75338637191, 84778265101, 68578763366, 90753811086, 14610403285, 41059066369, 44971912206, 54967486574, 63619999910, 64795193632, 51593013412, 14296869815, 97032246460, 71026731023, 64996639814, 52553945504, 78368590417, 2398402518, 44435031233, 87601256215, 58225024135, 29258726424, 75664180465, 54836690509, 59244241495, 89845006964, 94724357939, 16743666831, 34822606814, 93866913946, 80125290923, 25743827205, 94469934043, 19469867789, 41103515360, 84382693264, 80235488079, 81418127192, 66399751137, 72614594281, 22495129149, 17339018032, 16194395851, 84333703121, 24219636043, 99682014547, 71159156956, 44048915951, 85262576951, 30521550490, 15134057531, 26357888082, 50646269274, 17505372776, 28265566232, 28260805751, 16613760487, 10576238668, 27259913794, 94484813990, 50997645376, 18441653646, 84603819367, 54854616221, 7709871568, 45774859973, 97548534816, 56746004687, 63338143491, 70432961828, 59805835778, 89375588490, 84552543649, 77925293812, 24323572687, 6615259570, 73904901991, 93769848573, 28311228399, 34489690685, 79503414852, 17814356063, 12505635858, 56181813032, 52200540534, 16489972584, 257655609, 78954427051, 19623381752, 72826475157, 66212792913, 63426832026, 56959711124, 95518227052, 76027253975, 504754045, 26862146643, 62108989065, 80930927885, 32380678861, 78669825606, 85907432937, 41198970193, 64074319617, 58674446026, 31104150725, 49607481245, 87446671540, 11884072830, 49235836019, 44391902153, 59498395778, 18737038900, 96425814979, 46042621014, 13775891719, 71958100238, 25875704472, 8646927120, 45899449780, 24348693111, 9899959229, 5858906841, 18121144164, 74351604473, 4466127972, 21804440475, 3473697463, 42919167036, 86723139862, 23134926245, 86852240024, 76050289789, 28527900393, 67620936413, 58653043899, 67021588294, 19437000731, 16639606173, 1379607793, 65572743456, 94378408949, 36305434550, 75312469128, 3536251592, 2675204188, 74166396557, 5290235103, 5797023188, 1901623700, 331577206, 9927144216, 4158538696, 6950451994, 1064888341, 3481858574, 8396938755, 7892940765, 8448369999, 1486079200, 4537132947, 4437769158, 1291284863, 5784094043, 1400270208, 1187559106, 2413508961, 9559161456, 6703408788, 2115417637, 333424740])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9833a601e930>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(lot_idx)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LOT number: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlot_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Remote_index: \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremote_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlot_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mnew_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-2c831563a711>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(worker_idx, model, optimizer, lot_idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Param Id: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_at_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_at_location\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_objects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(epoch)\n",
    "    for lot_idx in range(1, N_LOTS):\n",
    "        train(lot_idx)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Syft) Denver_Boston_Federated_Training.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
