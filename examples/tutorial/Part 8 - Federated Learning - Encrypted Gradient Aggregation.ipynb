{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 8: Federated Learning with Encrypted Gradient Aggregation\n",
    "\n",
    "In the last few sections, we've been learning about encrypted computation by building several simple programs (including a database and a ledger). In this section, we're going to return to machine learning by using our new tools for encrypted computation for Federated Learning.\n",
    "\n",
    "In the last Federated Learning Demo (Part 4), we had a \"trusted aggregator\" who was responsible for averaging the model updates from multiple workers. This is less than ideal because it assumes that we can find someone trustworthy enough to have access to this sensitive information. This is not always the case.\n",
    "\n",
    "Thus, in this notebook, we will show how one can use SMPC to perform aggregation such that we don't need a \"trusted aggregator\".\n",
    "\n",
    "Authors:\n",
    "- Andrew Trask - Twitter: [@iamtrask](https://twitter.com/iamtrask)\n",
    "- Theo Ryffel - Github: [LaRiffle](https://github.com/LaRiffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Section 1: Normal Federated Learning\n",
    "\n",
    "First I'll show some code which performs normal federated learning on the Boston Housing Dataset. This section of code is broken into several sections.\n",
    "\n",
    "### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(torch.__version__)\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for training (default: 8)')\n",
    "parser.add_argument('--test-batch-size', type=int, default=8, metavar='N',\n",
    "                    help='input batch size for testing (default: 8)')\n",
    "parser.add_argument('--epochs', type=int, default=10, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                    help='learning rate (default: 0.001)')\n",
    "parser.add_argument('--momentum', type=float, default=0.0, metavar='M',\n",
    "                    help='SGD momentum (default: 0.0)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "args = parser.parse_args([])\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../other/data/boston_housing.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-512e60360dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../other/data/boston_housing.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../other/data/boston_housing.pickle'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "f = open('../other/data/boston_housing.pickle','rb')\n",
    "((X, y), (X_test, y_test)) = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "y = torch.from_numpy(y).type(torch.FloatTensor)\n",
    "X_test = torch.from_numpy(X_test).type(torch.FloatTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.FloatTensor)\n",
    "# preprocessing\n",
    "mean = X.mean(0, keepdim=True)\n",
    "dev = X.std(0, keepdim=True)\n",
    "mean[:, 3] = 0. # the feature at column 3 is binary,\n",
    "dev[:, 3] = 1.  # so I'd rather not standardize it\n",
    "X = (X - mean) / dev\n",
    "X_test = (X_test - mean) / dev\n",
    "train = TensorDataset(X, y)\n",
    "test = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train, batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "test_loader = DataLoader(test, batch_size=args.test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(13, 32)\n",
    "        self.fc2 = nn.Linear(32, 24)\n",
    "        self.fc3 = nn.Linear(24, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 13)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hookinfg PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft\n",
    "import syft as sy\n",
    "from syft.core import utils\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import random\n",
    "from syft.core.frameworks.torch import utils as torch_utils\n",
    "from torch.autograd import Variable\n",
    "hook = sy.TorchHook(verbose=False)\n",
    "me = hook.local_worker\n",
    "bob = sy.VirtualWorker(id=\"bob\",hook=hook, is_client_worker=False)\n",
    "alice = sy.VirtualWorker(id=\"alice\",hook=hook, is_client_worker=False)\n",
    "me.is_client_worker = False\n",
    "\n",
    "compute_nodes = [bob, alice]\n",
    "\n",
    "bob.add_workers([alice])\n",
    "alice.add_workers([bob])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Send data to the worker** <br>\n",
    "Usually they would already have it, this is just for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_distributed_dataset = []\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = Variable(data)\n",
    "    target = Variable(target.float())\n",
    "    data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    train_distributed_dataset.append((data, target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(train_distributed_dataset):\n",
    "            \n",
    "        worker = data.location\n",
    "        model.send(worker)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # update the model\n",
    "        pred = model(data)\n",
    "        loss = F.mse_loss(pred, target.float())\n",
    "        loss.backward()\n",
    "        model.get()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss.get()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output, target.float(), size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/408 (0%)]\tLoss: 562.255920\n",
      "Train Epoch: 1 [80/408 (20%)]\tLoss: 551.988525\n",
      "Train Epoch: 1 [160/408 (39%)]\tLoss: 640.131042\n",
      "Train Epoch: 1 [240/408 (59%)]\tLoss: 113.095512\n",
      "Train Epoch: 1 [320/408 (78%)]\tLoss: 17.793676\n",
      "Train Epoch: 1 [400/408 (98%)]\tLoss: 36.492382\n",
      "Train Epoch: 2 [0/408 (0%)]\tLoss: 8.652936\n",
      "Train Epoch: 2 [80/408 (20%)]\tLoss: 17.827301\n",
      "Train Epoch: 2 [160/408 (39%)]\tLoss: 15.696460\n",
      "Train Epoch: 2 [240/408 (59%)]\tLoss: 16.655910\n",
      "Train Epoch: 2 [320/408 (78%)]\tLoss: 17.403427\n",
      "Train Epoch: 2 [400/408 (98%)]\tLoss: 17.845072\n",
      "Train Epoch: 3 [0/408 (0%)]\tLoss: 7.068494\n",
      "Train Epoch: 3 [80/408 (20%)]\tLoss: 12.011536\n",
      "Train Epoch: 3 [160/408 (39%)]\tLoss: 7.905757\n",
      "Train Epoch: 3 [240/408 (59%)]\tLoss: 13.399664\n",
      "Train Epoch: 3 [320/408 (78%)]\tLoss: 12.010727\n",
      "Train Epoch: 3 [400/408 (98%)]\tLoss: 10.196638\n",
      "Train Epoch: 4 [0/408 (0%)]\tLoss: 6.434998\n",
      "Train Epoch: 4 [80/408 (20%)]\tLoss: 7.154055\n",
      "Train Epoch: 4 [160/408 (39%)]\tLoss: 5.233717\n",
      "Train Epoch: 4 [240/408 (59%)]\tLoss: 11.577619\n",
      "Train Epoch: 4 [320/408 (78%)]\tLoss: 8.724215\n",
      "Train Epoch: 4 [400/408 (98%)]\tLoss: 8.727889\n",
      "Train Epoch: 5 [0/408 (0%)]\tLoss: 5.678280\n",
      "Train Epoch: 5 [80/408 (20%)]\tLoss: 4.741013\n",
      "Train Epoch: 5 [160/408 (39%)]\tLoss: 4.026522\n",
      "Train Epoch: 5 [240/408 (59%)]\tLoss: 10.108504\n",
      "Train Epoch: 5 [320/408 (78%)]\tLoss: 6.788220\n",
      "Train Epoch: 5 [400/408 (98%)]\tLoss: 9.154861\n",
      "Train Epoch: 6 [0/408 (0%)]\tLoss: 4.730562\n",
      "Train Epoch: 6 [80/408 (20%)]\tLoss: 3.897393\n",
      "Train Epoch: 6 [160/408 (39%)]\tLoss: 3.662481\n",
      "Train Epoch: 6 [240/408 (59%)]\tLoss: 8.842749\n",
      "Train Epoch: 6 [320/408 (78%)]\tLoss: 5.860407\n",
      "Train Epoch: 6 [400/408 (98%)]\tLoss: 9.349962\n",
      "Train Epoch: 7 [0/408 (0%)]\tLoss: 4.160307\n",
      "Train Epoch: 7 [80/408 (20%)]\tLoss: 3.643047\n",
      "Train Epoch: 7 [160/408 (39%)]\tLoss: 3.335356\n",
      "Train Epoch: 7 [240/408 (59%)]\tLoss: 7.844127\n",
      "Train Epoch: 7 [320/408 (78%)]\tLoss: 5.585377\n",
      "Train Epoch: 7 [400/408 (98%)]\tLoss: 8.804581\n",
      "Train Epoch: 8 [0/408 (0%)]\tLoss: 3.648460\n",
      "Train Epoch: 8 [80/408 (20%)]\tLoss: 3.807822\n",
      "Train Epoch: 8 [160/408 (39%)]\tLoss: 3.337467\n",
      "Train Epoch: 8 [240/408 (59%)]\tLoss: 7.382596\n",
      "Train Epoch: 8 [320/408 (78%)]\tLoss: 5.541367\n",
      "Train Epoch: 8 [400/408 (98%)]\tLoss: 8.410532\n",
      "Train Epoch: 9 [0/408 (0%)]\tLoss: 3.464303\n",
      "Train Epoch: 9 [80/408 (20%)]\tLoss: 3.837596\n",
      "Train Epoch: 9 [160/408 (39%)]\tLoss: 3.438318\n",
      "Train Epoch: 9 [240/408 (59%)]\tLoss: 6.993455\n",
      "Train Epoch: 9 [320/408 (78%)]\tLoss: 5.296587\n",
      "Train Epoch: 9 [400/408 (98%)]\tLoss: 8.270564\n",
      "Train Epoch: 10 [0/408 (0%)]\tLoss: 3.351252\n",
      "Train Epoch: 10 [80/408 (20%)]\tLoss: 3.812066\n",
      "Train Epoch: 10 [160/408 (39%)]\tLoss: 3.579137\n",
      "Train Epoch: 10 [240/408 (59%)]\tLoss: 6.517159\n",
      "Train Epoch: 10 [320/408 (78%)]\tLoss: 5.119540\n",
      "Train Epoch: 10 [400/408 (98%)]\tLoss: 8.209358\n",
      "Encoding 0 s 0.0 %\n",
      "Handling 0 s 0.0 %\n",
      "Execute call 0 s 0.0 %\n",
      "Total 33.35 s\n",
      "CPU times: user 19.9 s, sys: 2.93 s, total: 22.9 s\n",
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = time.time()\n",
    "args.epochs = 10\n",
    "torch.encode_timer = 0\n",
    "torch.handle_call_timer = 0\n",
    "torch.execute_call_timer = 0\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Encoding', round(torch.encode_timer, 2), 's', round(torch.encode_timer/total_time*100, 2), '%')\n",
    "print('Handling', round(torch.handle_call_timer, 2), 's',  round(torch.handle_call_timer/total_time*100, 2), '%')\n",
    "print('Execute call', round(torch.execute_call_timer, 2), 's',  round(torch.execute_call_timer/total_time*100, 2), '%')\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 20.7802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Adding Encrypted Aggregation\n",
    "\n",
    "Now we're going to modify this example slightly to aggregate gradients using encryption. The main piece that's different is really 1 or 2 lines of code in the train() function, which I'll point out. For the moment, let's re-preocess our data and initialize a model for bob and alice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_dataset = (list(),list())\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(train_loader):\n",
    "    data = Variable(data)\n",
    "    target = Variable(target.float())\n",
    "    data.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    target.send(compute_nodes[batch_idx % len(compute_nodes)])\n",
    "    remote_dataset[batch_idx % len(compute_nodes)].append((data, target))\n",
    "\n",
    "def update(data, target, model, optimizer):\n",
    "    model.send(data.location)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = F.mse_loss(pred, target.float())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return model\n",
    "\n",
    "bobs_model = Net()\n",
    "alices_model = Net()\n",
    "\n",
    "bobs_optimizer = optim.SGD(bobs_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "alices_optimizer = optim.SGD(alices_model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "models = [bobs_model, alices_model]\n",
    "params = [list(bobs_model.parameters()), list(alices_model.parameters())]\n",
    "optimizers = [bobs_optimizer, alices_optimizer]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our Training Logic\n",
    "\n",
    "The only **real** difference is inside of this train method. Let's walk through it step-by-step.\n",
    "\n",
    "### Part A: Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is selecting which batch to train on\n",
    "data_index = 0\n",
    "\n",
    "\n",
    "# update remote models\n",
    "# we could iterate this multiple times before proceeding, but we're only iterating once per worker here\n",
    "for remote_index in range(len(compute_nodes)):\n",
    "    data, target = remote_dataset[remote_index][data_index]\n",
    "    models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Encrypted Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list where we'll deposit our encrypted model average\n",
    "new_params = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each parameter\n",
    "for param_i in range(len(params[0])):\n",
    "\n",
    "    # for each worker\n",
    "    spdz_params = list()\n",
    "    for remote_index in range(len(compute_nodes)):\n",
    "        \n",
    "        # select the identical parameter from each worker and copy it\n",
    "        copy_of_parameter = params[remote_index][param_i].data+0\n",
    "        \n",
    "        # since SMPC can only work with integers (not floats), we need\n",
    "        # to use Integers to store decimal information. In other words,\n",
    "        # we need to use \"Fixed Precision\" encoding.\n",
    "        # fix it's precision (read more about Fixed Precision encodings)\n",
    "        fixed_precision_param = copy_of_parameter.fix_precision()\n",
    "        \n",
    "        # now we encrypt it on the remote machine. Note that \n",
    "        # fixed_precision_param is ALREADY a pointer. Thus, when\n",
    "        # we call share, it actually encrpyts the data that the\n",
    "        # data is pointing TO. This returns a POINTER to  the \n",
    "        # MPC Shared object, which we need to fetch.\n",
    "        encrypted_param = fixed_precision_param.share(bob, alice)\n",
    "        \n",
    "        # now we fetch the pointer to the MPC shared value\n",
    "        param = encrypted_param.get()\n",
    "        \n",
    "        # save the parameter so we can average it with the same parameter\n",
    "        # from the other workers\n",
    "        spdz_params.append(param)\n",
    "\n",
    "    # average params from multiple workers, fetch them to the local machine\n",
    "    # decrypt and decode (from fixed precision) back into a floaing point number\n",
    "    new_param = (spdz_params[0] + spdz_params[1]).get().decode()/2\n",
    "    \n",
    "    # save the new averaged parameter\n",
    "    new_params.append(new_param)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in params:\n",
    "    for param in model:\n",
    "        param.data *= 0\n",
    "\n",
    "for model in models:\n",
    "    model.get()\n",
    "\n",
    "for remote_index in range(len(compute_nodes)):\n",
    "    for param_index in range(len(params[remote_index])):\n",
    "        params[remote_index][param_index].data.set_(new_params[param_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's put it all Together!!\n",
    "\n",
    "And now that we know each step, we can put it all together into one training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "\n",
    "    for data_index in range(len(remote_dataset[0])-1):\n",
    "        # update remote models\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            data, target = remote_dataset[remote_index][data_index]\n",
    "            models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "\n",
    "        new_params = list()\n",
    "\n",
    "        for param_i in range(len(params[0])):\n",
    "\n",
    "            spdz_params = list()\n",
    "            for remote_index in range(len(compute_nodes)):\n",
    "                spdz_params.append((params[remote_index][param_i].data+0).fix_precision().share(bob, alice).get())\n",
    "\n",
    "            new_param = (spdz_params[0] + spdz_params[1]).get().decode()/2\n",
    "            new_params.append(new_param)\n",
    "\n",
    "        for model in params:\n",
    "            for param in model:\n",
    "                param.data *= 0\n",
    "\n",
    "        for model in models:\n",
    "            model.get()\n",
    "\n",
    "        for remote_index in range(len(compute_nodes)):\n",
    "            for param_index in range(len(params[remote_index])):\n",
    "                params[remote_index][param_index].data.set_(new_params[param_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    models[0].eval()\n",
    "    test_loss = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = models[0](data)\n",
    "        test_loss += F.mse_loss(output, target.float(), size_average=False).data[0] # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "\n",
      "Test set: Average loss: 555.1237\n",
      "\n",
      "2\n",
      "\n",
      "Test set: Average loss: 153.6630\n",
      "\n",
      "3\n",
      "\n",
      "Test set: Average loss: 26.8698\n",
      "\n",
      "4\n",
      "\n",
      "Test set: Average loss: 21.4207\n",
      "\n",
      "5\n",
      "\n",
      "Test set: Average loss: 19.2581\n",
      "\n",
      "6\n",
      "\n",
      "Test set: Average loss: 17.9186\n",
      "\n",
      "7\n",
      "\n",
      "Test set: Average loss: 17.1278\n",
      "\n",
      "8\n",
      "\n",
      "Test set: Average loss: 16.8299\n",
      "\n",
      "9\n",
      "\n",
      "Test set: Average loss: 16.6942\n",
      "\n",
      "10\n",
      "\n",
      "Test set: Average loss: 16.7554\n",
      "\n",
      "Encoding 0 s 0.0 %\n",
      "Handling 0 s 0.0 %\n",
      "Execute call 0 s 0.0 %\n",
      "Total 121.25 s\n",
      "CPU times: user 1min 14s, sys: 9.97 s, total: 1min 24s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t = time.time()\n",
    "args.epochs = 10\n",
    "torch.encode_timer = 0\n",
    "torch.handle_call_timer = 0\n",
    "torch.execute_call_timer = 0\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    print(epoch)\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Encoding', round(torch.encode_timer, 2), 's', round(torch.encode_timer/total_time*100, 2), '%')\n",
    "print('Handling', round(torch.handle_call_timer, 2), 's',  round(torch.handle_call_timer/total_time*100, 2), '%')\n",
    "print('Execute call', round(torch.execute_call_timer, 2), 's',  round(torch.execute_call_timer/total_time*100, 2), '%')\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!!! - Time to Join the Community!\n",
    "\n",
    "Congraulations on completing this notebook tutorial! If you enjoyed this and would like to join the movement toward privacy preserving, decentralized ownership of AI and the AI supply chain (data), you can do so in the following ways!\n",
    "\n",
    "### Star PySyft on Github\n",
    "\n",
    "The easiest way to help our community is just by starring the Repos! This helps raise awareness of the cool tools we're building.\n",
    "\n",
    "- [Star PySyft](https://github.com/OpenMined/PySyft)\n",
    "\n",
    "### Join our Slack!\n",
    "\n",
    "The best way to keep up to date on the latest advancements is to join our community! You can do so by filling out the form at [http://slack.openmined.org](http://slack.openmined.org)\n",
    "\n",
    "### Join a Code Project!\n",
    "\n",
    "The best way to contribute to our community is to become a code contributor! At any time you can go to PySyft Github Issues page and filter for \"Projects\". This will show you all the top level Tickets giving an overview of what projects you can join! If you don't want to join a project, but you would like to do a bit of coding, you can also look for more \"one off\" mini-projects by searching for github issues marked \"good first issue\".\n",
    "\n",
    "- [PySyft Projects](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3AProject)\n",
    "- [Good First Issue Tickets](https://github.com/OpenMined/PySyft/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)\n",
    "\n",
    "### Donate\n",
    "\n",
    "If you don't have time to contribute to our codebase, but would still like to lend support, you can also become a Backer on our Open Collective. All donations go toward our web hosting and other community expenses such as hackathons and meetups!\n",
    "\n",
    "[OpenMined's Open Collective Page](https://opencollective.com/openmined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
