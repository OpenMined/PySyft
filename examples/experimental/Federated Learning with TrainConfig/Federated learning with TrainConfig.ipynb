{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Federated learning with TrainConfig\n",
    "\n",
    "This notebook will go through the steps to run a federated learning via websocket workers. We will use federated averaging to join the remotely trained models. \n",
    "\n",
    "Authors:\n",
    "- [mari-linhares](https://github.com/mari-linhares)\n",
    "- [midokura-silvia](https://github.com/midokura-silvia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: start the websocket server workers\n",
    "\n",
    "Each worker is represented by two parts, a local handle (websocket client worker) and the remote instance that holds the data and performs the computations. The remote part is called a websocket server worker.\n",
    "\n",
    "So first, we need to create the remote workers. For this, you need to run in a terminal (not possible from the notebook):\n",
    "\n",
    "```bash\n",
    "python examples/experimental/Federated\\ Learning\\ with\\ TrainConfig/run_websocket_server.py --port 8777 --id alice\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the websocket client workers (all these cells are from the FL websocket tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to perform the imports and setup some arguments and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import syft as sy\n",
    "from syft import workers\n",
    "from syft.frameworks.torch import pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_cuda = th.cuda.is_available()\n",
    "th.manual_seed(1)\n",
    "device = th.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's instantiate the websocket client workers, our local access point to the remote workers.\n",
    "Note that **this step will fail, if the websocket server workers are not running**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(th)\n",
    "me = hook.local_worker\n",
    "\n",
    "kwargs_websocket = {\"host\": \"localhost\", \"hook\": hook, \"verbose\": False}\n",
    "alice = workers.WebsocketClientWorker(id=\"alice\", port=8777, **kwargs_websocket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Star is (almost) born: TrainConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/silvia/git/PySyft/syft/frameworks/torch/hook/hook.py:753: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  current_tensor = hook_self.torch.native_tensor(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<weakref at 0x7fe8dc212e08; to 'TrainConfig' at 0x7fe8dc2186a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss function\n",
    "@th.jit.script\n",
    "def loss_fn(real, pred):\n",
    "    return ((real - pred) ** 2).mean()\n",
    "\n",
    "# Model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 3)\n",
    "        self.fc2 = nn.Linear(3, 2)\n",
    "        self.fc3 = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "data = th.tensor(th.tensor([[-10, -2.0], [1, 1.1], [11, 22.1], [-10, 1.2]]))\n",
    "\n",
    "traced_model = th.jit.trace(model, data)\n",
    "\n",
    "model_with_id = pointers.ObjectWrapper(id=sy.ID_PROVIDER.pop(), obj=traced_model)\n",
    "loss_fn_with_id = pointers.ObjectWrapper(id=sy.ID_PROVIDER.pop(), obj=loss_fn)\n",
    "\n",
    "model_ptr = me.send(model_with_id, alice)\n",
    "loss_fn_ptr = me.send(loss_fn_with_id, alice)\n",
    "\n",
    "# Create and send train config\n",
    "train_config = sy.TrainConfig(\n",
    "    model_id=model_ptr.id_at_location, loss_plan_id=loss_fn_ptr.id_at_location, batch_size=2\n",
    ")\n",
    "train_config.send(alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation before training\n",
      "Loss: 0.328726589679718\n"
     ]
    }
   ],
   "source": [
    "data = th.tensor([[-1, 2.0], [0, 1.1], [-1, 2.1], [0, 1.2]], requires_grad=True)\n",
    "target = th.tensor([[1.0], [0.0], [1.0], [0.0]], requires_grad=True)\n",
    "\n",
    "print(\"\\nEvaluation before training\")\n",
    "pred = model(data)\n",
    "loss = loss_fn(real=target, pred=pred)\n",
    "print(\"Loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run training on data available at the worker and evaluate again. The last loss notified by the worker will be equal to the loss evaluated with the new model, as the content of the data available on the proxy is equal to the remote data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Iteration 0: alice's loss: tensor(0.2991, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Iteration 1: alice's loss: tensor(0.2680, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Iteration 2: alice's loss: tensor(0.2549, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Iteration 3: alice's loss: tensor(0.2489, requires_grad=True)\n",
      "--------------------------------------------------\n",
      "Iteration 4: alice's loss: tensor(0.2458, requires_grad=True)\n",
      "\n",
      "Evaluation after training:\n",
      "Loss: 0.24460731446743011\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    loss = alice.fit(dataset_key=\"vectors\", return_id=88 + epoch)\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Iteration %s: alice's loss: %s\" % (epoch, loss))\n",
    "\n",
    "new_model = model_ptr.get()\n",
    "\n",
    "print(\"\\nEvaluation after training:\")\n",
    "pred = new_model(data)\n",
    "loss = loss_fn(real=target, pred=pred)\n",
    "print(\"Loss: {}\".format(loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
