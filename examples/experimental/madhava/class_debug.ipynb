{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import inspect\n",
    "from enum import Enum\n",
    "\n",
    "import torch\n",
    "# torch.set_printoptions(profile=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is based off the docs here and the file native_functions.yaml in the pytorch github repo\n",
    "# https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  267k  100  267k    0     0  3220k      0 --:--:-- --:--:-- --:--:-- 3181k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/pytorch/pytorch/1.6/aten/src/ATen/native/native_functions.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"native_functions.yaml\") as file:\n",
    "    native_functions = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'func': '_cast_Byte(Tensor self, bool non_blocking=False) -> Tensor', 'use_c10_dispatcher': 'full', 'variants': 'function'}\n",
      "('_cast_Byte', 'Tensor self, bool non_blocking=False', 'Tensor')\n"
     ]
    }
   ],
   "source": [
    "# example structure\n",
    "# - func: add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)\n",
    "#   variants: function\n",
    "#   dispatch:\n",
    "#     CPU: add_relu_out\n",
    "\n",
    "def process_yaml_entry(native_func):\n",
    "    return_type = native_func[\"func\"].split(\"->\")[-1].strip()\n",
    "\n",
    "    func_name = native_func[\"func\"].split(\"(\")[0].strip()\n",
    "    args_parts = native_func[\"func\"].split(\"->\")[0].strip().split(\"(\")\n",
    "    args = \"\".join(args_parts[1:]).strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    return func_name, args, return_type\n",
    "\n",
    "print(native_functions[0])\n",
    "print(process_yaml_entry(native_functions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tensor', 'Tensor']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_type_to_python(return_type):\n",
    "    return_type = return_type.strip()\n",
    "    if \"()\" == return_type:\n",
    "        return [\"None\"]\n",
    "\n",
    "    return_type = return_type.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    parts = return_type.split(\",\")\n",
    "    clean_parts = []\n",
    "    for part in parts:\n",
    "        clean_part = part.strip()\n",
    "        if clean_part.startswith(\"Tensor\"):\n",
    "            clean_part = \"Tensor\"\n",
    "        # according to the docs Scalar is any kind of numeric in python or a unit tensor\n",
    "        if clean_part == \"Scalar\":\n",
    "            clean_part = \"Union[int, float, complex, Tensor]\"\n",
    "        # it seems like these are the pytorch dtype types\n",
    "        if clean_part == \"ScalarType\":\n",
    "            clean_part = \"torch.dtype\"\n",
    "        if \"[]\" in part:\n",
    "            clean_part = f\"List[{clean_part}]\"\n",
    "        \n",
    "        clean_parts.append(clean_part)\n",
    "    return clean_parts\n",
    "    \n",
    "    if any(prim == return_type for prim in [\"int\", \"float\", \"bool\"]):\n",
    "        return return_type\n",
    "    \n",
    "return_type_to_python(\"(Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end: Union[int, float, complex, Tensor]',\n",
       " 'dtype: Optional[torch.dtype] = None',\n",
       " 'layout: Optional[Layout] = None',\n",
       " 'device: Optional[Device] = None',\n",
       " 'pin_memory: Optional[bool] = None']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def signature_type_to_python(signature):\n",
    "    signature = signature.strip()\n",
    "\n",
    "    signature = signature.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    clean_parts = []\n",
    "\n",
    "    if signature == \"\":\n",
    "        return [\"None\"]\n",
    "    # default int[] value [0,1] should not add space after comma, since native_parse.py uses ', ' to split args\n",
    "    for part in signature.split(\", \"):\n",
    "        name = None\n",
    "        default = None\n",
    "        clean_part = part.strip()\n",
    "        if clean_part == \"*\":\n",
    "            # all args after this must be kwargs\n",
    "            # not sure we care right now\n",
    "#             clean_parts.append(clean_part)\n",
    "            continue\n",
    "        subpart = clean_part.split(\" \")\n",
    "        if len(subpart) > 1:\n",
    "            subsub = subpart[1].split(\"=\")\n",
    "            name = subsub[0]\n",
    "            if len(subsub) > 1:\n",
    "                default = subsub[1]\n",
    "\n",
    "        if subpart[0].startswith(\"Tensor\"):\n",
    "            clean_part = \"Tensor\"\n",
    "        else:\n",
    "            clean_part = subpart[0]\n",
    "\n",
    "        if clean_part.startswith(\"ScalarType\"):\n",
    "            clean_part = \"torch.dtype\"\n",
    "            \n",
    "        if clean_part.startswith(\"Scalar\"):\n",
    "            clean_part = \"Union[int, float, complex, Tensor]\"\n",
    "\n",
    "        if \"[\" in subpart[0]:\n",
    "            t = clean_part.split(\"[\")[0]\n",
    "            clean_part = f\"List[{t}]\"\n",
    "\n",
    "        if \"?\" in part:\n",
    "            clean_part = f\"Optional[{clean_part}]\".replace(\"?\", \"\")\n",
    "            \n",
    "        if name is not None:\n",
    "            clean_part = f\"{name}: {clean_part}\"\n",
    "            \n",
    "        if default is not None:\n",
    "            clean_part = f\"{clean_part} = {default}\"\n",
    "\n",
    "        clean_parts.append(clean_part)\n",
    "        \n",
    "    if len(clean_parts) == 0:\n",
    "        return [\"None\"]\n",
    "    return clean_parts\n",
    "\n",
    "signature_type_to_python( 'Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func_yaml(native_functions):\n",
    "    unique_return_types = set()\n",
    "    unique_function_args = set()\n",
    "    func_dict = {}\n",
    "    for native_func in native_functions:\n",
    "        func_name, args, return_type = process_yaml_entry(native_func)\n",
    "        unique_return_types.add(return_type)\n",
    "        unique_function_args.add(args)\n",
    "\n",
    "        if func_name in func_dict:\n",
    "            print(f\"Error duplicate func_name: {func_name}\")\n",
    "        func_dict[func_name] = {\"args\": signature_type_to_python(args), \"return_type\": return_type_to_python(return_type)}\n",
    "\n",
    "    return func_dict, unique_return_types, unique_function_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1527\n"
     ]
    }
   ],
   "source": [
    "native_func_dict, unique_return_types, unique_function_args = process_func_yaml(native_functions)\n",
    "print(len(native_func_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args': ['self: Tensor', 'other: Tensor'], 'return_type': ['Tensor']}\n"
     ]
    }
   ],
   "source": [
    "print(native_func_dict[\"div_.Tensor\"])\n",
    "\n",
    "type_json = json.dumps(native_func_dict)\n",
    "with open(\"torch_types.json\", \"w\") as f:\n",
    "    f.write(type_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'()',\n",
       " '(Tensor Q, Tensor R)',\n",
       " '(Tensor U, Tensor S, Tensor V)',\n",
       " '(Tensor a, Tensor tau)',\n",
       " '(Tensor eigenvalues, Tensor eigenvectors)',\n",
       " '(Tensor grad_input, Tensor grad_weight)',\n",
       " '(Tensor grad_input, Tensor grad_weight, Tensor grad_bias)',\n",
       " '(Tensor grad_self, Tensor grad_grid)',\n",
       " '(Tensor output, Tensor buffer)',\n",
       " '(Tensor output, Tensor finput, Tensor fgrad_input)',\n",
       " '(Tensor output, Tensor is_target)',\n",
       " '(Tensor output, Tensor total_weight)',\n",
       " '(Tensor sign, Tensor logabsdet)',\n",
       " '(Tensor solution, Tensor LU)',\n",
       " '(Tensor solution, Tensor QR)',\n",
       " '(Tensor solution, Tensor cloned_coefficient)',\n",
       " '(Tensor values, Tensor indices)',\n",
       " '(Tensor(a!) Q, Tensor(b!) R)',\n",
       " '(Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)',\n",
       " '(Tensor(a!) a, Tensor(b!) tau)',\n",
       " '(Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)',\n",
       " '(Tensor(a!) solution, Tensor(b!) LU)',\n",
       " '(Tensor(a!) solution, Tensor(b!) QR)',\n",
       " '(Tensor(a!) solution, Tensor(b!) cloned_coefficient)',\n",
       " '(Tensor(a!) values, Tensor(b!) indices)',\n",
       " '(Tensor(a!), Tensor(b!))',\n",
       " '(Tensor(a!), Tensor(b!), Tensor(c!))',\n",
       " '(Tensor, Tensor)',\n",
       " '(Tensor, Tensor, Tensor)',\n",
       " '(Tensor, Tensor, Tensor, Tensor)',\n",
       " '(Tensor, Tensor, Tensor, Tensor, Tensor)',\n",
       " '(Tensor, Tensor, Tensor, Tensor, int)',\n",
       " '(Tensor, Tensor, Tensor, Tensor[])',\n",
       " '(Tensor, Tensor, float, int)',\n",
       " '(float, int)',\n",
       " 'QScheme',\n",
       " 'Scalar',\n",
       " 'ScalarType',\n",
       " 'Tensor',\n",
       " 'Tensor grad_theta',\n",
       " 'Tensor grid',\n",
       " 'Tensor output',\n",
       " 'Tensor(a!)',\n",
       " 'Tensor(a)',\n",
       " 'Tensor(a)[]',\n",
       " 'Tensor[]',\n",
       " 'bool',\n",
       " 'float',\n",
       " 'int'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_return_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('List[Tensor]',),\n",
       " ('None',),\n",
       " ('QScheme',),\n",
       " ('Tensor',),\n",
       " ('Tensor', 'Tensor'),\n",
       " ('Tensor', 'Tensor', 'Tensor'),\n",
       " ('Tensor', 'Tensor', 'Tensor', 'List[Tensor]'),\n",
       " ('Tensor', 'Tensor', 'Tensor', 'Tensor'),\n",
       " ('Tensor', 'Tensor', 'Tensor', 'Tensor', 'Tensor'),\n",
       " ('Tensor', 'Tensor', 'Tensor', 'Tensor', 'int'),\n",
       " ('Tensor', 'Tensor', 'float', 'int'),\n",
       " ('Union[int, float, complex, Tensor]',),\n",
       " ('bool',),\n",
       " ('float',),\n",
       " ('float', 'int'),\n",
       " ('int',),\n",
       " ('torch.dtype',)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_python_return_types = set()\n",
    "for line in [return_type_to_python(t) for t in unique_return_types]:\n",
    "    unique_python_return_types.add(tuple(line))\n",
    "\n",
    "unique_python_return_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Tensora! self, Tensor end, Tensor weight',\n",
       " 'Tensor grad_output, Tensor self, Tensor target, int reduction',\n",
       " 'Tensor self, int dim, *, ScalarType? dtype=None',\n",
       " 'Tensor grad, Tensor self, float scale, int zero_point, int quant_min, int quant_max',\n",
       " 'Tensor self, Tensor target, Scalar p=1, Scalar margin=1, Tensor? weight=None, int reduction=Mean',\n",
       " 'Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor mean_dy, Tensor mean_dy_xmu',\n",
       " 'Tensor self, int[1] output_size',\n",
       " 'Tensor self, int dim0, int dim1',\n",
       " 'Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensora! grad_input']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(unique_function_args)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('self: Tensor',\n",
       "  'eigenvectors: bool = False',\n",
       "  'upper: bool = True',\n",
       "  'e: Tensor',\n",
       "  'V: Tensor'),\n",
       " ('self: Tensor',\n",
       "  'dim: Dimname',\n",
       "  'keepdim: bool = False',\n",
       "  'min: Tensor',\n",
       "  'min_indices: Tensor'),\n",
       " ('self: Tensor', 'pivot: bool = True', 'check_errors: bool = True'),\n",
       " ('self: Tensor', 'dim: List[int]', 'keepdim: bool = False'),\n",
       " ('input: Tensor',),\n",
       " ('self: Tensor',\n",
       "  'output_size: List[int]',\n",
       "  'kernel_size: List[int]',\n",
       "  'dilation: List[int]',\n",
       "  'padding: List[int]',\n",
       "  'stride: List[int]'),\n",
       " ('self: Tensor',\n",
       "  'dim: int',\n",
       "  'sorted: bool = True',\n",
       "  'return_inverse: bool = False',\n",
       "  'return_counts: bool = False'),\n",
       " ('input: Tensor', 'coefficients: Tensor'),\n",
       " ('n: int', 'm: int', 'out: Tensor'),\n",
       " ('self: Tensor',\n",
       "  'dim: int',\n",
       "  'keepdim: bool = False',\n",
       "  'min: Tensor',\n",
       "  'min_indices: Tensor')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_python_arg_signatures = set()\n",
    "for line in [signature_type_to_python(t) for t in unique_function_args]:\n",
    "    unique_python_arg_signatures.add(tuple(line))\n",
    "    \n",
    "print(len(unique_python_arg_signatures))\n",
    "\n",
    "list(unique_python_arg_signatures)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Tensor', 'Tensor', 'bool'),\n",
       " ('Tensor', 'Dimname', 'Tensor', 'bool'),\n",
       " ('Tensor', 'Tensor', 'Optional[float]', 'Tensor'),\n",
       " ('Tensor', 'Tensor', 'Optional[Generator]', 'Tensor'),\n",
       " ('Tensor', 'Tensor', 'Tensor', 'Optional[Tensor]', 'int'),\n",
       " ('Tensor', 'Tensor', 'Tensor', 'Tensor', 'int'),\n",
       " ('Tensor', 'int', 'List[int]', 'Optional[List[Dimname]]'),\n",
       " ('Union[int, float, complex, Tensor]', 'Tensor', 'Tensor'),\n",
       " ('Tensor', 'Dimname', 'Tensor', 'Tensor'),\n",
       " ('Tensor',\n",
       "  'List[Tensor]',\n",
       "  'int',\n",
       "  'Tensor',\n",
       "  'Tensor',\n",
       "  'Optional[Tensor]',\n",
       "  'Tensor',\n",
       "  'Optional[Tensor]',\n",
       "  'Optional[Tensor]',\n",
       "  'Optional[Tensor]',\n",
       "  'int',\n",
       "  'int',\n",
       "  'int',\n",
       "  'bool',\n",
       "  'float',\n",
       "  'bool',\n",
       "  'bool',\n",
       "  'List[int]',\n",
       "  'Optional[Tensor]',\n",
       "  'Tensor',\n",
       "  'List[bool]')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_signatures_no_names = set()\n",
    "for sig in unique_python_arg_signatures:\n",
    "    new_parts = []\n",
    "    for part in sig:\n",
    "        new_part = part.split(\"=\")[0]\n",
    "        new_part = new_part.split(\":\")[-1]\n",
    "        new_part = new_part.strip()\n",
    "        new_parts.append(new_part)\n",
    "    arg_signatures_no_names.add(tuple(new_parts))\n",
    "    \n",
    "print(len(arg_signatures_no_names))\n",
    "\n",
    "list(arg_signatures_no_names)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ConstQuantizerPtr',\n",
       " 'Device',\n",
       " 'Dimname',\n",
       " 'List[Dimname]',\n",
       " 'List[Tensor]',\n",
       " 'List[bool]',\n",
       " 'List[int]',\n",
       " 'MemoryFormat',\n",
       " 'None',\n",
       " 'Optional[Device]',\n",
       " 'Optional[Generator]',\n",
       " 'Optional[Layout]',\n",
       " 'Optional[List[Dimname]]',\n",
       " 'Optional[List[Tensor]]',\n",
       " 'Optional[List[float]]',\n",
       " 'Optional[List[int]]',\n",
       " 'Optional[MemoryFormat]',\n",
       " 'Optional[Tensor]',\n",
       " 'Optional[Union[int, float, complex, Tensor]]',\n",
       " 'Optional[bool]',\n",
       " 'Optional[float]',\n",
       " 'Optional[int]',\n",
       " 'Optional[torch.dtype]',\n",
       " 'QScheme',\n",
       " 'Storage',\n",
       " 'Tensor',\n",
       " 'Union[int, float, complex, Tensor]',\n",
       " 'bool',\n",
       " 'float',\n",
       " 'int',\n",
       " 'str',\n",
       " 'torch.dtype'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_types = set()\n",
    "for sigs in arg_signatures_no_names:\n",
    "    for part in sigs:\n",
    "        all_types.add(part)\n",
    "        \n",
    "for return_types in unique_python_return_types:\n",
    "    for part in return_types:\n",
    "        all_types.add(part)\n",
    "        \n",
    "print(len(all_types))\n",
    "all_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_name_with_qualname(klass: type) -> str:\n",
    "    return f'{klass.__module__}.{klass.__qualname__}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whatis(thing, optional_key: Optional[str] = None):\n",
    "    name = getattr(thing, \"__name__\", \"\")\n",
    "    name = f\"{thing} {type(thing)} {name}\"\n",
    "    if optional_key is not None:\n",
    "        name = f\".{optional_key} == {name}\"\n",
    "\n",
    "    print(name)\n",
    "    \n",
    "    mro = getattr(thing, \"mro\", None)\n",
    "    if mro is not None:\n",
    "        hierarchy = mro()\n",
    "        print(f\"Hierarchy: {hierarchy}\")\n",
    "    \n",
    "\n",
    "    test_types = [\n",
    "        \"ismodule\", \"isclass\", \"ismethod\", \"ismethoddescriptor\", \"isfunction\", \"isgeneratorfunction\", \"isgenerator\",\n",
    "        \"isbuiltin\", \"isroutine\", \"isdatadescriptor\", \"isgetsetdescriptor\", \"ismemberdescriptor\"\n",
    "    ]\n",
    "    result_ttypes = []\n",
    "    for ttype in test_types:\n",
    "        istype = getattr(inspect, ttype)(thing)\n",
    "        if istype is True:\n",
    "            result_ttypes.append(ttype)\n",
    "\n",
    "    if callable(thing):\n",
    "        result_ttypes.append(\"callable\")\n",
    "       \n",
    "    if issubclass(type(thing), Enum):\n",
    "        result_ttype.append(\"ispyenum\")\n",
    "\n",
    "    members = getattr(thing, \"__members__\", None)\n",
    "    if members is not None:\n",
    "        print(\"members: \", members)\n",
    "        result_ttypes.append(\"iscenum\")\n",
    "    \n",
    "    if \"pybind11_type\" in str(type(thing)):\n",
    "        result_ttypes.append(\"ispybind11\")\n",
    "        \n",
    "    if len(result_ttypes) == 0:\n",
    "        if type(thing) in [type(None), bool, int, float, complex, str]:\n",
    "            result_ttypes.append(\"isproperty\")\n",
    "\n",
    "    return result_ttypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signature(thing):\n",
    "    return inspect.getfullargspec(thing.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c_class_signature(thing):\n",
    "    signature = \"\"\n",
    "    try:\n",
    "        thing()\n",
    "        signature = f\"1. {thing}()\"\n",
    "    except Exception as e:\n",
    "        if \"No constructor defined\" in str(e):\n",
    "            return None\n",
    "\n",
    "        for line in str(e).splitlines():\n",
    "            if any(parts in line.strip() for parts in [\"TypeError\", \"incompatible\", \"Invoked with:\"]):\n",
    "                continue\n",
    "            else:\n",
    "                signature += f\"{line}\\n\"\n",
    "        signature = signature.strip()\n",
    "\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_native_signature(key):\n",
    "    if key in native_func_dict:\n",
    "        # print(f\"Found {key} in native_func_dict\")\n",
    "        return native_func_dict[key][\"args\"]\n",
    "    else:\n",
    "        print(f\"Cant find {key} in native_func_dict\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_native_return_type(key):\n",
    "    if key in native_func_dict:\n",
    "        # print(f\"Found {key} in native_func_dict\")\n",
    "        return native_func_dict[key][\"return_type\"]\n",
    "    else:\n",
    "        print(f\"Cant find {key} in native_func_dict\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".AVG == AggregationType.AVG <class 'torch._C.AggregationType'> \n",
      "members:  {'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG}\n",
      ".AggregationType == <class 'torch._C.AggregationType'> <class 'pybind11_builtins.pybind11_type'> AggregationType\n",
      "Hierarchy: [<class 'torch._C.AggregationType'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "members:  {'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG}\n",
      ".AnyType == <class 'torch._C.AnyType'> <class 'pybind11_builtins.pybind11_type'> AnyType\n",
      "Hierarchy: [<class 'torch._C.AnyType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Argument == <class 'torch._C.Argument'> <class 'pybind11_builtins.pybind11_type'> Argument\n",
      "Hierarchy: [<class 'torch._C.Argument'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ArgumentSpec == <class 'torch._C.ArgumentSpec'> <class 'pybind11_builtins.pybind11_type'> ArgumentSpec\n",
      "Hierarchy: [<class 'torch._C.ArgumentSpec'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".BFloat16Storage == <class 'torch.BFloat16Storage'> <class 'type'> BFloat16Storage\n",
      "Hierarchy: [<class 'torch.BFloat16Storage'>, <class 'torch._C.BFloat16StorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".BFloat16Tensor == <class 'torch.BFloat16Tensor'> <class 'torch.tensortype'> BFloat16Tensor\n",
      "Hierarchy: [<class 'torch.BFloat16Tensor'>, <class 'object'>]\n",
      ".BenchmarkConfig == <class 'torch._C.BenchmarkConfig'> <class 'pybind11_builtins.pybind11_type'> BenchmarkConfig\n",
      "Hierarchy: [<class 'torch._C.BenchmarkConfig'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".BenchmarkExecutionStats == <class 'torch._C.BenchmarkExecutionStats'> <class 'pybind11_builtins.pybind11_type'> BenchmarkExecutionStats\n",
      "Hierarchy: [<class 'torch._C.BenchmarkExecutionStats'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Block == <class 'torch._C.Block'> <class 'pybind11_builtins.pybind11_type'> Block\n",
      "Hierarchy: [<class 'torch._C.Block'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".BoolStorage == <class 'torch.BoolStorage'> <class 'type'> BoolStorage\n",
      "Hierarchy: [<class 'torch.BoolStorage'>, <class 'torch._C.BoolStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".BoolTensor == <class 'torch.BoolTensor'> <class 'torch.tensortype'> BoolTensor\n",
      "Hierarchy: [<class 'torch.BoolTensor'>, <class 'object'>]\n",
      ".BoolType == <class 'torch._C.BoolType'> <class 'pybind11_builtins.pybind11_type'> BoolType\n",
      "Hierarchy: [<class 'torch._C.BoolType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".BufferDict == <class 'torch._C.BufferDict'> <class 'pybind11_builtins.pybind11_type'> BufferDict\n",
      "Hierarchy: [<class 'torch._C.BufferDict'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ByteStorage == <class 'torch.ByteStorage'> <class 'type'> ByteStorage\n",
      "Hierarchy: [<class 'torch.ByteStorage'>, <class 'torch._C.ByteStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".ByteTensor == <class 'torch.ByteTensor'> <class 'torch.tensortype'> ByteTensor\n",
      "Hierarchy: [<class 'torch.ByteTensor'>, <class 'object'>]\n",
      ".CONV_BN_FUSION == MobileOptimizerType.CONV_BN_FUSION <class 'torch._C.MobileOptimizerType'> \n",
      "members:  {'CONV_BN_FUSION': MobileOptimizerType.CONV_BN_FUSION, 'INSERT_FOLD_PREPACK_OPS': MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, 'REMOVE_DROPOUT': MobileOptimizerType.REMOVE_DROPOUT}\n",
      ".CallStack == <class 'torch._C.CallStack'> <class 'pybind11_builtins.pybind11_type'> CallStack\n",
      "Hierarchy: [<class 'torch._C.CallStack'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Capsule == <class 'torch._C.Capsule'> <class 'pybind11_builtins.pybind11_type'> Capsule\n",
      "Hierarchy: [<class 'torch._C.Capsule'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".CharStorage == <class 'torch.CharStorage'> <class 'type'> CharStorage\n",
      "Hierarchy: [<class 'torch.CharStorage'>, <class 'torch._C.CharStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".CharTensor == <class 'torch.CharTensor'> <class 'torch.tensortype'> CharTensor\n",
      "Hierarchy: [<class 'torch.CharTensor'>, <class 'object'>]\n",
      ".ClassType == <class 'torch._C.ClassType'> <class 'pybind11_builtins.pybind11_type'> ClassType\n",
      "Hierarchy: [<class 'torch._C.ClassType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Code == <class 'torch._C.Code'> <class 'pybind11_builtins.pybind11_type'> Code\n",
      "Hierarchy: [<class 'torch._C.Code'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".CompilationUnit == <class 'torch._C.CompilationUnit'> <class 'pybind11_builtins.pybind11_type'> CompilationUnit\n",
      "Hierarchy: [<class 'torch._C.CompilationUnit'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".CompleteArgumentSpec == <class 'torch._C.CompleteArgumentSpec'> <class 'pybind11_builtins.pybind11_type'> CompleteArgumentSpec\n",
      "Hierarchy: [<class 'torch._C.CompleteArgumentSpec'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ComplexDoubleStorage == <class 'torch.ComplexDoubleStorage'> <class 'type'> ComplexDoubleStorage\n",
      "Hierarchy: [<class 'torch.ComplexDoubleStorage'>, <class 'torch._C.ComplexDoubleStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".ComplexFloatStorage == <class 'torch.ComplexFloatStorage'> <class 'type'> ComplexFloatStorage\n",
      "Hierarchy: [<class 'torch.ComplexFloatStorage'>, <class 'torch._C.ComplexFloatStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".ConcreteModuleType == <class 'torch._C.ConcreteModuleType'> <class 'pybind11_builtins.pybind11_type'> ConcreteModuleType\n",
      "Hierarchy: [<class 'torch._C.ConcreteModuleType'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ConcreteModuleTypeBuilder == <class 'torch._C.ConcreteModuleTypeBuilder'> <class 'pybind11_builtins.pybind11_type'> ConcreteModuleTypeBuilder\n",
      "Hierarchy: [<class 'torch._C.ConcreteModuleTypeBuilder'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".DeepCopyMemoTable == <class 'torch._C.DeepCopyMemoTable'> <class 'pybind11_builtins.pybind11_type'> DeepCopyMemoTable\n",
      "Hierarchy: [<class 'torch._C.DeepCopyMemoTable'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".DeviceObjType == <class 'torch._C.DeviceObjType'> <class 'pybind11_builtins.pybind11_type'> DeviceObjType\n",
      "Hierarchy: [<class 'torch._C.DeviceObjType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".DictType == <class 'torch._C.DictType'> <class 'pybind11_builtins.pybind11_type'> DictType\n",
      "Hierarchy: [<class 'torch._C.DictType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".DoubleStorage == <class 'torch.DoubleStorage'> <class 'type'> DoubleStorage\n",
      "Hierarchy: [<class 'torch.DoubleStorage'>, <class 'torch._C.DoubleStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".DoubleTensor == <class 'torch.DoubleTensor'> <class 'torch.tensortype'> DoubleTensor\n",
      "Hierarchy: [<class 'torch.DoubleTensor'>, <class 'object'>]\n",
      ".ErrorReport == <class 'torch._C.ErrorReport'> <class 'pybind11_builtins.pybind11_type'> ErrorReport\n",
      "Hierarchy: [<class 'torch._C.ErrorReport'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ExecutionPlan == <class 'torch._C.ExecutionPlan'> <class 'pybind11_builtins.pybind11_type'> ExecutionPlan\n",
      "Hierarchy: [<class 'torch._C.ExecutionPlan'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ExtraFilesMap == <class 'torch._C.ExtraFilesMap'> <class 'pybind11_builtins.pybind11_type'> ExtraFilesMap\n",
      "Hierarchy: [<class 'torch._C.ExtraFilesMap'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".FatalError == <class 'torch.FatalError'> <class 'type'> FatalError\n",
      "Hierarchy: [<class 'torch.FatalError'>, <class 'Exception'>, <class 'BaseException'>, <class 'object'>]\n",
      ".FileCheck == <class 'torch._C.FileCheck'> <class 'pybind11_builtins.pybind11_type'> FileCheck\n",
      "Hierarchy: [<class 'torch._C.FileCheck'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".FloatStorage == <class 'torch.FloatStorage'> <class 'type'> FloatStorage\n",
      "Hierarchy: [<class 'torch.FloatStorage'>, <class 'torch._C.FloatStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".FloatTensor == <class 'torch.FloatTensor'> <class 'torch.tensortype'> FloatTensor\n",
      "Hierarchy: [<class 'torch.FloatTensor'>, <class 'object'>]\n",
      ".FloatType == <class 'torch._C.FloatType'> <class 'pybind11_builtins.pybind11_type'> FloatType\n",
      "Hierarchy: [<class 'torch._C.FloatType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".FunctionSchema == <class 'torch._C.FunctionSchema'> <class 'pybind11_builtins.pybind11_type'> FunctionSchema\n",
      "Hierarchy: [<class 'torch._C.FunctionSchema'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Future == <class 'torch._C.Future'> <class 'pybind11_builtins.pybind11_type'> Future\n",
      "Hierarchy: [<class 'torch._C.Future'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".FutureType == <class 'torch._C.FutureType'> <class 'pybind11_builtins.pybind11_type'> FutureType\n",
      "Hierarchy: [<class 'torch._C.FutureType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Generator == <class 'torch._C.Generator'> <class 'type'> Generator\n",
      "Hierarchy: [<class 'torch._C.Generator'>, <class 'object'>]\n",
      ".Gradient == <class 'torch._C.Gradient'> <class 'pybind11_builtins.pybind11_type'> Gradient\n",
      "Hierarchy: [<class 'torch._C.Gradient'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Graph == <class 'torch._C.Graph'> <class 'pybind11_builtins.pybind11_type'> Graph\n",
      "Hierarchy: [<class 'torch._C.Graph'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".GraphExecutorState == <class 'torch._C.GraphExecutorState'> <class 'pybind11_builtins.pybind11_type'> GraphExecutorState\n",
      "Hierarchy: [<class 'torch._C.GraphExecutorState'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".HalfStorage == <class 'torch.HalfStorage'> <class 'type'> HalfStorage\n",
      "Hierarchy: [<class 'torch.HalfStorage'>, <class 'torch._C.HalfStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".HalfStorageBase == <class 'torch._C.HalfStorageBase'> <class 'type'> HalfStorageBase\n",
      "Hierarchy: [<class 'torch._C.HalfStorageBase'>, <class 'object'>]\n",
      ".HalfTensor == <class 'torch.HalfTensor'> <class 'torch.tensortype'> HalfTensor\n",
      "Hierarchy: [<class 'torch.HalfTensor'>, <class 'object'>]\n",
      ".INSERT_FOLD_PREPACK_OPS == MobileOptimizerType.INSERT_FOLD_PREPACK_OPS <class 'torch._C.MobileOptimizerType'> \n",
      "members:  {'CONV_BN_FUSION': MobileOptimizerType.CONV_BN_FUSION, 'INSERT_FOLD_PREPACK_OPS': MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, 'REMOVE_DROPOUT': MobileOptimizerType.REMOVE_DROPOUT}\n",
      ".IODescriptor == <class 'torch._C.IODescriptor'> <class 'pybind11_builtins.pybind11_type'> IODescriptor\n",
      "Hierarchy: [<class 'torch._C.IODescriptor'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".IntStorage == <class 'torch.IntStorage'> <class 'type'> IntStorage\n",
      "Hierarchy: [<class 'torch.IntStorage'>, <class 'torch._C.IntStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".IntTensor == <class 'torch.IntTensor'> <class 'torch.tensortype'> IntTensor\n",
      "Hierarchy: [<class 'torch.IntTensor'>, <class 'object'>]\n",
      ".IntType == <class 'torch._C.IntType'> <class 'pybind11_builtins.pybind11_type'> IntType\n",
      "Hierarchy: [<class 'torch._C.IntType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".InterfaceType == <class 'torch._C.InterfaceType'> <class 'pybind11_builtins.pybind11_type'> InterfaceType\n",
      "Hierarchy: [<class 'torch._C.InterfaceType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".JITException == <class 'torch.jit.Error'> <class 'type'> Error\n",
      "Hierarchy: [<class 'torch.jit.Error'>, <class 'Exception'>, <class 'BaseException'>, <class 'object'>]\n",
      ".ListType == <class 'torch._C.ListType'> <class 'pybind11_builtins.pybind11_type'> ListType\n",
      "Hierarchy: [<class 'torch._C.ListType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".LiteScriptModule == <class 'torch._C.LiteScriptModule'> <class 'pybind11_builtins.pybind11_type'> LiteScriptModule\n",
      "Hierarchy: [<class 'torch._C.LiteScriptModule'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".LockingLogger == <class 'torch._C.LockingLogger'> <class 'pybind11_builtins.pybind11_type'> LockingLogger\n",
      "Hierarchy: [<class 'torch._C.LockingLogger'>, <class 'torch._C.LoggerBase'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".LoggerBase == <class 'torch._C.LoggerBase'> <class 'pybind11_builtins.pybind11_type'> LoggerBase\n",
      "Hierarchy: [<class 'torch._C.LoggerBase'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".LongStorage == <class 'torch.LongStorage'> <class 'type'> LongStorage\n",
      "Hierarchy: [<class 'torch.LongStorage'>, <class 'torch._C.LongStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".LongTensor == <class 'torch.LongTensor'> <class 'torch.tensortype'> LongTensor\n",
      "Hierarchy: [<class 'torch.LongTensor'>, <class 'object'>]\n",
      ".MobileOptimizerType == <class 'torch._C.MobileOptimizerType'> <class 'pybind11_builtins.pybind11_type'> MobileOptimizerType\n",
      "Hierarchy: [<class 'torch._C.MobileOptimizerType'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "members:  {'CONV_BN_FUSION': MobileOptimizerType.CONV_BN_FUSION, 'INSERT_FOLD_PREPACK_OPS': MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, 'REMOVE_DROPOUT': MobileOptimizerType.REMOVE_DROPOUT}\n",
      ".ModuleDict == <class 'torch._C.ModuleDict'> <class 'pybind11_builtins.pybind11_type'> ModuleDict\n",
      "Hierarchy: [<class 'torch._C.ModuleDict'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Node == <class 'torch._C.Node'> <class 'pybind11_builtins.pybind11_type'> Node\n",
      "Hierarchy: [<class 'torch._C.Node'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".NoneType == <class 'torch._C.NoneType'> <class 'pybind11_builtins.pybind11_type'> NoneType\n",
      "Hierarchy: [<class 'torch._C.NoneType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".NoopLogger == <class 'torch._C.NoopLogger'> <class 'pybind11_builtins.pybind11_type'> NoopLogger\n",
      "Hierarchy: [<class 'torch._C.NoopLogger'>, <class 'torch._C.LoggerBase'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".NumberType == <class 'torch._C.NumberType'> <class 'pybind11_builtins.pybind11_type'> NumberType\n",
      "Hierarchy: [<class 'torch._C.NumberType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".OptionalType == <class 'torch._C.OptionalType'> <class 'pybind11_builtins.pybind11_type'> OptionalType\n",
      "Hierarchy: [<class 'torch._C.OptionalType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ParameterDict == <class 'torch._C.ParameterDict'> <class 'pybind11_builtins.pybind11_type'> ParameterDict\n",
      "Hierarchy: [<class 'torch._C.ParameterDict'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".PyObjectType == <class 'torch._C.PyObjectType'> <class 'pybind11_builtins.pybind11_type'> PyObjectType\n",
      "Hierarchy: [<class 'torch._C.PyObjectType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".PyTorchFileReader == <class 'torch._C.PyTorchFileReader'> <class 'pybind11_builtins.pybind11_type'> PyTorchFileReader\n",
      "Hierarchy: [<class 'torch._C.PyTorchFileReader'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".PyTorchFileWriter == <class 'torch._C.PyTorchFileWriter'> <class 'pybind11_builtins.pybind11_type'> PyTorchFileWriter\n",
      "Hierarchy: [<class 'torch._C.PyTorchFileWriter'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".QInt32Storage == <class 'torch.QInt32Storage'> <class 'type'> QInt32Storage\n",
      "Hierarchy: [<class 'torch.QInt32Storage'>, <class 'torch._C.QInt32StorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".QInt32StorageBase == <class 'torch._C.QInt32StorageBase'> <class 'type'> QInt32StorageBase\n",
      "Hierarchy: [<class 'torch._C.QInt32StorageBase'>, <class 'object'>]\n",
      ".QInt8Storage == <class 'torch.QInt8Storage'> <class 'type'> QInt8Storage\n",
      "Hierarchy: [<class 'torch.QInt8Storage'>, <class 'torch._C.QInt8StorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".QInt8StorageBase == <class 'torch._C.QInt8StorageBase'> <class 'type'> QInt8StorageBase\n",
      "Hierarchy: [<class 'torch._C.QInt8StorageBase'>, <class 'object'>]\n",
      ".QUInt8Storage == <class 'torch.QUInt8Storage'> <class 'type'> QUInt8Storage\n",
      "Hierarchy: [<class 'torch.QUInt8Storage'>, <class 'torch._C.QUInt8StorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".REMOVE_DROPOUT == MobileOptimizerType.REMOVE_DROPOUT <class 'torch._C.MobileOptimizerType'> \n",
      "members:  {'CONV_BN_FUSION': MobileOptimizerType.CONV_BN_FUSION, 'INSERT_FOLD_PREPACK_OPS': MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, 'REMOVE_DROPOUT': MobileOptimizerType.REMOVE_DROPOUT}\n",
      ".RRefType == <class 'torch._C.RRefType'> <class 'pybind11_builtins.pybind11_type'> RRefType\n",
      "Hierarchy: [<class 'torch._C.RRefType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".SUM == AggregationType.SUM <class 'torch._C.AggregationType'> \n",
      "members:  {'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG}\n",
      ".ScriptClass == <class 'torch._C.ScriptClass'> <class 'pybind11_builtins.pybind11_type'> ScriptClass\n",
      "Hierarchy: [<class 'torch._C.ScriptClass'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ScriptFunction == <class 'torch.jit.ScriptFunction'> <class 'pybind11_builtins.pybind11_type'> ScriptFunction\n",
      "Hierarchy: [<class 'torch.jit.ScriptFunction'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ScriptMethod == <class 'torch._C.ScriptMethod'> <class 'pybind11_builtins.pybind11_type'> ScriptMethod\n",
      "Hierarchy: [<class 'torch._C.ScriptMethod'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ScriptModule == <class 'torch._C.ScriptModule'> <class 'pybind11_builtins.pybind11_type'> ScriptModule\n",
      "Hierarchy: [<class 'torch._C.ScriptModule'>, <class 'torch._C.ScriptObject'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ScriptObject == <class 'torch._C.ScriptObject'> <class 'pybind11_builtins.pybind11_type'> ScriptObject\n",
      "Hierarchy: [<class 'torch._C.ScriptObject'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Set == typing.Set <class 'typing.GenericMeta'> Set\n",
      "Hierarchy: [typing.Set, <class 'set'>, typing.MutableSet, <class 'collections.abc.MutableSet'>, typing.AbstractSet, <class 'collections.abc.Set'>, typing.Collection, <class 'collections.abc.Collection'>, <class 'collections.abc.Sized'>, typing.Iterable, <class 'collections.abc.Iterable'>, typing.Container, <class 'collections.abc.Container'>, typing.Generic, <class 'object'>]\n",
      ".ShortStorage == <class 'torch.ShortStorage'> <class 'type'> ShortStorage\n",
      "Hierarchy: [<class 'torch.ShortStorage'>, <class 'torch._C.ShortStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".ShortTensor == <class 'torch.ShortTensor'> <class 'torch.tensortype'> ShortTensor\n",
      "Hierarchy: [<class 'torch.ShortTensor'>, <class 'object'>]\n",
      ".Size == <class 'torch.Size'> <class 'type'> Size\n",
      "Hierarchy: [<class 'torch.Size'>, <class 'tuple'>, <class 'object'>]\n",
      ".Storage == <class 'torch.FloatStorage'> <class 'type'> FloatStorage\n",
      "Hierarchy: [<class 'torch.FloatStorage'>, <class 'torch._C.FloatStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      ".StringType == <class 'torch._C.StringType'> <class 'pybind11_builtins.pybind11_type'> StringType\n",
      "Hierarchy: [<class 'torch._C.StringType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Tensor == <class 'torch.Tensor'> <class 'type'> Tensor\n",
      "Hierarchy: [<class 'torch.Tensor'>, <class 'torch._C._TensorBase'>, <class 'object'>]\n",
      ".TensorType == <class 'torch._C.TensorType'> <class 'pybind11_builtins.pybind11_type'> TensorType\n",
      "Hierarchy: [<class 'torch._C.TensorType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".ThroughputBenchmark == <class 'torch._C.ThroughputBenchmark'> <class 'pybind11_builtins.pybind11_type'> ThroughputBenchmark\n",
      "Hierarchy: [<class 'torch._C.ThroughputBenchmark'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".TracingState == <class 'torch._C.TracingState'> <class 'pybind11_builtins.pybind11_type'> TracingState\n",
      "Hierarchy: [<class 'torch._C.TracingState'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".TupleType == <class 'torch._C.TupleType'> <class 'pybind11_builtins.pybind11_type'> TupleType\n",
      "Hierarchy: [<class 'torch._C.TupleType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Type == <class 'torch._C.Type'> <class 'pybind11_builtins.pybind11_type'> Type\n",
      "Hierarchy: [<class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".USE_GLOBAL_DEPS == True <class 'bool'> \n",
      ".USE_RTLD_GLOBAL_WITH_LIBTORCH == False <class 'bool'> \n",
      ".Use == <class 'torch._C.Use'> <class 'pybind11_builtins.pybind11_type'> Use\n",
      "Hierarchy: [<class 'torch._C.Use'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      ".Value == <class 'torch._C.Value'> <class 'pybind11_builtins.pybind11_type'> Value\n",
      "Hierarchy: [<class 'torch._C.Value'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "._C == <module 'torch._C' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so'> <class 'module'> torch._C\n",
      "._StorageBase == <class 'torch.storage._StorageBase'> <class 'type'> _StorageBase\n",
      "Hierarchy: [<class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "._VF == <module 'torch._VF'> <class 'torch._VF.VFModule'> torch._VF\n",
      "._adaptive_avg_pool2d == <built-in method _adaptive_avg_pool2d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _adaptive_avg_pool2d\n",
      "._addmv_impl_ == <built-in method _addmv_impl_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _addmv_impl_\n",
      "._addr == <built-in method _addr of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _addr\n",
      "._addr_ == <built-in method _addr_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _addr_\n",
      "._amp_non_finite_check_and_unscale_ == <built-in method _amp_non_finite_check_and_unscale_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _amp_non_finite_check_and_unscale_\n",
      "._amp_update_scale == <built-in method _amp_update_scale of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _amp_update_scale\n",
      "._baddbmm_mkl_ == <built-in method _baddbmm_mkl_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _baddbmm_mkl_\n",
      "._batch_norm_impl_index == <built-in method _batch_norm_impl_index of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _batch_norm_impl_index\n",
      "._bmm == <built-in method _bmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _bmm\n",
      "._cast_Byte == <built-in method _cast_Byte of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cast_Byte\n",
      "._cast_Char == <built-in method _cast_Char of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cast_Char\n",
      "._cast_Double == <built-in method _cast_Double of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cast_Double\n",
      "._cast_Float == <built-in method _cast_Float of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cast_Float\n",
      "._cast_Half == <built-in method _cast_Half of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cast_Half\n",
      "._cast_Int == <built-in method _cast_Int of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cast_Int\n",
      "._cast_Long == <built-in method _cast_Long of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cast_Long\n",
      "._cast_Short == <built-in method _cast_Short of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cast_Short\n",
      "._cat == <built-in method _cat of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cat\n",
      "._choose_qparams_per_tensor == <built-in method _choose_qparams_per_tensor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _choose_qparams_per_tensor\n",
      "._classes == <module 'torch._classes' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_classes.py'> <class 'module'> torch._classes\n",
      "._convolution == <built-in method _convolution of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _convolution\n",
      "._convolution_nogroup == <built-in method _convolution_nogroup of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _convolution_nogroup\n",
      "._copy_from == <built-in method _copy_from of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _copy_from\n",
      "._ctc_loss == <built-in method _ctc_loss of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _ctc_loss\n",
      "._cudnn_ctc_loss == <built-in method _cudnn_ctc_loss of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cudnn_ctc_loss\n",
      "._cudnn_init_dropout_state == <built-in method _cudnn_init_dropout_state of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cudnn_init_dropout_state\n",
      "._cudnn_rnn == <built-in method _cudnn_rnn of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cudnn_rnn\n",
      "._cudnn_rnn_flatten_weight == <built-in method _cudnn_rnn_flatten_weight of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cudnn_rnn_flatten_weight\n",
      "._cufft_clear_plan_cache == <built-in method _cufft_clear_plan_cache of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cufft_clear_plan_cache\n",
      "._cufft_get_plan_cache_max_size == <built-in method _cufft_get_plan_cache_max_size of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cufft_get_plan_cache_max_size\n",
      "._cufft_get_plan_cache_size == <built-in method _cufft_get_plan_cache_size of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cufft_get_plan_cache_size\n",
      "._cufft_set_plan_cache_max_size == <built-in method _cufft_set_plan_cache_max_size of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cufft_set_plan_cache_max_size\n",
      "._cummax_helper == <built-in method _cummax_helper of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cummax_helper\n",
      "._cummin_helper == <built-in method _cummin_helper of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _cummin_helper\n",
      "._debug_has_internal_overlap == <built-in method _debug_has_internal_overlap of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _debug_has_internal_overlap\n",
      "._dim_arange == <built-in method _dim_arange of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _dim_arange\n",
      "._dirichlet_grad == <built-in method _dirichlet_grad of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _dirichlet_grad\n",
      "._embedding_bag == <built-in method _embedding_bag of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _embedding_bag\n",
      "._empty_affine_quantized == <built-in method _empty_affine_quantized of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _empty_affine_quantized\n",
      "._empty_per_channel_affine_quantized == <built-in method _empty_per_channel_affine_quantized of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _empty_per_channel_affine_quantized\n",
      "._euclidean_dist == <built-in method _euclidean_dist of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _euclidean_dist\n",
      "._fft_with_size == <built-in method _fft_with_size of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _fft_with_size\n",
      "._fused_dropout == <built-in method _fused_dropout of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _fused_dropout\n",
      "._has_compatible_shallow_copy_type == <built-in method _has_compatible_shallow_copy_type of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _has_compatible_shallow_copy_type\n",
      "._import_dotted_name == <function _import_dotted_name at 0x10a1181e0> <class 'function'> _import_dotted_name\n",
      "._index_copy_ == <built-in method _index_copy_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _index_copy_\n",
      "._index_put_impl_ == <built-in method _index_put_impl_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _index_put_impl_\n",
      "._is_deterministic == <function _is_deterministic at 0x12e209b70> <class 'function'> _is_deterministic\n",
      "._jit_internal == <module 'torch._jit_internal' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_jit_internal.py'> <class 'module'> torch._jit_internal\n",
      "._linalg_utils == <module 'torch._linalg_utils' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_linalg_utils.py'> <class 'module'> torch._linalg_utils\n",
      "._load_global_deps == <function _load_global_deps at 0x10a107ae8> <class 'function'> _load_global_deps\n",
      "._lobpcg == <module 'torch._lobpcg' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_lobpcg.py'> <class 'module'> torch._lobpcg\n",
      "._log_softmax == <built-in method _log_softmax of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _log_softmax\n",
      "._log_softmax_backward_data == <built-in method _log_softmax_backward_data of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _log_softmax_backward_data\n",
      "._logcumsumexp == <built-in method _logcumsumexp of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _logcumsumexp\n",
      "._lowrank == <module 'torch._lowrank' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_lowrank.py'> <class 'module'> torch._lowrank\n",
      "._lu_solve_helper == <built-in method _lu_solve_helper of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _lu_solve_helper\n",
      "._lu_with_info == <built-in method _lu_with_info of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _lu_with_info\n",
      "._make_per_channel_quantized_tensor == <built-in method _make_per_channel_quantized_tensor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _make_per_channel_quantized_tensor\n",
      "._make_per_tensor_quantized_tensor == <built-in method _make_per_tensor_quantized_tensor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _make_per_tensor_quantized_tensor\n",
      "._masked_scale == <built-in method _masked_scale of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _masked_scale\n",
      "._mkldnn == torch._mkldnn <class 'torch.layout'> \n",
      "._mkldnn_reshape == <built-in method _mkldnn_reshape of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _mkldnn_reshape\n",
      "._mkldnn_transpose == <built-in method _mkldnn_transpose of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _mkldnn_transpose\n",
      "._mkldnn_transpose_ == <built-in method _mkldnn_transpose_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _mkldnn_transpose_\n",
      "._mode == <built-in method _mode of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _mode\n",
      "._multinomial_alias_draw == <built-in method _multinomial_alias_draw of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _multinomial_alias_draw\n",
      "._multinomial_alias_setup == <built-in method _multinomial_alias_setup of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _multinomial_alias_setup\n",
      "._namedtensor_internals == <module 'torch._namedtensor_internals' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_namedtensor_internals.py'> <class 'module'> torch._namedtensor_internals\n",
      "._nnpack_available == <built-in method _nnpack_available of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _nnpack_available\n",
      "._nnpack_spatial_convolution == <built-in method _nnpack_spatial_convolution of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _nnpack_spatial_convolution\n",
      "._ops == <module 'torch._ops' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_ops.py'> <class 'module'> torch._ops\n",
      "._overrides == <module 'torch._overrides' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_overrides.py'> <class 'module'> torch._overrides\n",
      "._pack_padded_sequence == <built-in method _pack_padded_sequence of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _pack_padded_sequence\n",
      "._pad_packed_sequence == <built-in method _pad_packed_sequence of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _pad_packed_sequence\n",
      "._reshape_from_tensor == <built-in method _reshape_from_tensor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _reshape_from_tensor\n",
      "._s_where == <built-in method _s_where of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _s_where\n",
      "._sample_dirichlet == <built-in method _sample_dirichlet of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sample_dirichlet\n",
      "._set_deterministic == <function _set_deterministic at 0x12e1a2158> <class 'function'> _set_deterministic\n",
      "._shape_as_tensor == <built-in method _shape_as_tensor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _shape_as_tensor\n",
      "._six == <module 'torch._six' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_six.py'> <class 'module'> torch._six\n",
      "._sobol_engine_draw == <built-in method _sobol_engine_draw of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sobol_engine_draw\n",
      "._sobol_engine_ff_ == <built-in method _sobol_engine_ff_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sobol_engine_ff_\n",
      "._sobol_engine_initialize_state_ == <built-in method _sobol_engine_initialize_state_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sobol_engine_initialize_state_\n",
      "._sobol_engine_scramble_ == <built-in method _sobol_engine_scramble_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sobol_engine_scramble_\n",
      "._softmax == <built-in method _softmax of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _softmax\n",
      "._softmax_backward_data == <built-in method _softmax_backward_data of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _softmax_backward_data\n",
      "._sparse_addmm == <built-in method _sparse_addmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sparse_addmm\n",
      "._sparse_log_softmax == <built-in method _sparse_log_softmax of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sparse_log_softmax\n",
      "._sparse_log_softmax_backward_data == <built-in method _sparse_log_softmax_backward_data of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sparse_log_softmax_backward_data\n",
      "._sparse_mm == <built-in method _sparse_mm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sparse_mm\n",
      "._sparse_softmax == <built-in method _sparse_softmax of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sparse_softmax\n",
      "._sparse_softmax_backward_data == <built-in method _sparse_softmax_backward_data of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sparse_softmax_backward_data\n",
      "._sparse_sum == <built-in method _sparse_sum of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _sparse_sum\n",
      "._standard_gamma == <built-in method _standard_gamma of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _standard_gamma\n",
      "._standard_gamma_grad == <built-in method _standard_gamma_grad of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _standard_gamma_grad\n",
      "._storage_classes == {<class 'torch.cuda.CharStorage'>, <class 'torch.QUInt8Storage'>, <class 'torch.DoubleStorage'>, <class 'torch.cuda.IntStorage'>, <class 'torch.ComplexDoubleStorage'>, <class 'torch.cuda.FloatStorage'>, <class 'torch.BoolStorage'>, <class 'torch.cuda.ComplexDoubleStorage'>, <class 'torch.CharStorage'>, <class 'torch.cuda.BoolStorage'>, <class 'torch.IntStorage'>, <class 'torch.cuda.ByteStorage'>, <class 'torch.QInt8Storage'>, <class 'torch.HalfStorage'>, <class 'torch.cuda.ShortStorage'>, <class 'torch.ComplexFloatStorage'>, <class 'torch.cuda.LongStorage'>, <class 'torch.BFloat16Storage'>, <class 'torch.cuda.ComplexFloatStorage'>, <class 'torch.cuda.DoubleStorage'>, <class 'torch.ByteStorage'>, <class 'torch.FloatStorage'>, <class 'torch.cuda.BFloat16Storage'>, <class 'torch.ShortStorage'>, <class 'torch.cuda.HalfStorage'>, <class 'torch.QInt32Storage'>, <class 'torch.LongStorage'>} <class 'set'> \n",
      "._string_classes == (<class 'str'>, <class 'bytes'>) <class 'tuple'> \n",
      "._tensor_classes == {<class 'torch.cuda.sparse.CharTensor'>, <class 'torch.sparse.ShortTensor'>, <class 'torch.sparse.DoubleTensor'>, <class 'torch.cuda.BoolTensor'>, <class 'torch.cuda.IntTensor'>, <class 'torch.cuda.ByteTensor'>, <class 'torch.ShortTensor'>, <class 'torch.cuda.sparse.ShortTensor'>, <class 'torch.DoubleTensor'>, <class 'torch.cuda.sparse.DoubleTensor'>, <class 'torch.sparse.HalfTensor'>, <class 'torch.sparse.FloatTensor'>, <class 'torch.cuda.BFloat16Tensor'>, <class 'torch.cuda.LongTensor'>, <class 'torch.cuda.CharTensor'>, <class 'torch.HalfTensor'>, <class 'torch.cuda.sparse.HalfTensor'>, <class 'torch.FloatTensor'>, <class 'torch.cuda.sparse.FloatTensor'>, <class 'torch.sparse.BFloat16Tensor'>, <class 'torch.sparse.IntTensor'>, <class 'torch.sparse.ByteTensor'>, <class 'torch.cuda.ShortTensor'>, <class 'torch.cuda.DoubleTensor'>, <class 'torch.BoolTensor'>, <class 'torch.cuda.sparse.BFloat16Tensor'>, <class 'torch.IntTensor'>, <class 'torch.cuda.sparse.IntTensor'>, <class 'torch.ByteTensor'>, <class 'torch.cuda.sparse.ByteTensor'>, <class 'torch.sparse.LongTensor'>, <class 'torch.sparse.CharTensor'>, <class 'torch.cuda.HalfTensor'>, <class 'torch.cuda.FloatTensor'>, <class 'torch.BFloat16Tensor'>, <class 'torch.LongTensor'>, <class 'torch.cuda.sparse.LongTensor'>, <class 'torch.CharTensor'>} <class 'set'> \n",
      "._tensor_str == <module 'torch._tensor_str' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_tensor_str.py'> <class 'module'> torch._tensor_str\n",
      "._test_serialization_subcmul == <built-in method _test_serialization_subcmul of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _test_serialization_subcmul\n",
      "._trilinear == <built-in method _trilinear of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _trilinear\n",
      "._unique == <built-in method _unique of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _unique\n",
      "._unique2 == <built-in method _unique2 of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _unique2\n",
      "._use_cudnn_ctc_loss == <built-in method _use_cudnn_ctc_loss of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _use_cudnn_ctc_loss\n",
      "._use_cudnn_rnn_flatten_weight == <built-in method _use_cudnn_rnn_flatten_weight of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _use_cudnn_rnn_flatten_weight\n",
      "._utils == <module 'torch._utils' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_utils.py'> <class 'module'> torch._utils\n",
      "._utils_internal == <module 'torch._utils_internal' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_utils_internal.py'> <class 'module'> torch._utils_internal\n",
      "._weight_norm == <built-in method _weight_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _weight_norm\n",
      "._weight_norm_cuda_interface == <built-in method _weight_norm_cuda_interface of type object at 0x10e2335b0> <class 'builtin_function_or_method'> _weight_norm_cuda_interface\n",
      ".abs == <built-in method abs of type object at 0x10e2335b0> <class 'builtin_function_or_method'> abs\n",
      ".abs_ == <built-in method abs_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> abs_\n",
      ".absolute == <built-in method absolute of type object at 0x10e2335b0> <class 'builtin_function_or_method'> absolute\n",
      ".absolute_ == <built-in method absolute_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> absolute_\n",
      ".acos == <built-in method acos of type object at 0x10e2335b0> <class 'builtin_function_or_method'> acos\n",
      ".acos_ == <built-in method acos_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> acos_\n",
      ".acosh == <built-in method acosh of type object at 0x10e2335b0> <class 'builtin_function_or_method'> acosh\n",
      ".acosh_ == <built-in method acosh_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> acosh_\n",
      ".adaptive_avg_pool1d == <built-in method adaptive_avg_pool1d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> adaptive_avg_pool1d\n",
      ".adaptive_max_pool1d == <built-in method adaptive_max_pool1d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> adaptive_max_pool1d\n",
      ".add == <built-in method add of type object at 0x10e2335b0> <class 'builtin_function_or_method'> add\n",
      "Cant find add in native_func_dict\n",
      "Cant find add in native_func_dict\n",
      "Cant find add in native_func_dict\n",
      ".addbmm == <built-in method addbmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> addbmm\n",
      ".addcdiv == <built-in method addcdiv of type object at 0x10e2335b0> <class 'builtin_function_or_method'> addcdiv\n",
      ".addcmul == <built-in method addcmul of type object at 0x10e2335b0> <class 'builtin_function_or_method'> addcmul\n",
      ".addmm == <built-in method addmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> addmm\n",
      ".addmv == <built-in method addmv of type object at 0x10e2335b0> <class 'builtin_function_or_method'> addmv\n",
      ".addmv_ == <built-in method addmv_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> addmv_\n",
      ".addr == <built-in method addr of type object at 0x10e2335b0> <class 'builtin_function_or_method'> addr\n",
      ".affine_grid_generator == <built-in method affine_grid_generator of type object at 0x10e2335b0> <class 'builtin_function_or_method'> affine_grid_generator\n",
      ".align_tensors == <function align_tensors at 0x130650950> <class 'function'> align_tensors\n",
      ".all == <built-in method all of type object at 0x10e2335b0> <class 'builtin_function_or_method'> all\n",
      ".allclose == <built-in method allclose of type object at 0x10e2335b0> <class 'builtin_function_or_method'> allclose\n",
      ".alpha_dropout == <built-in method alpha_dropout of type object at 0x10e2335b0> <class 'builtin_function_or_method'> alpha_dropout\n",
      ".alpha_dropout_ == <built-in method alpha_dropout_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> alpha_dropout_\n",
      ".angle == <built-in method angle of type object at 0x10e2335b0> <class 'builtin_function_or_method'> angle\n",
      ".any == <built-in method any of type object at 0x10e2335b0> <class 'builtin_function_or_method'> any\n",
      ".arange == <built-in method arange of type object at 0x10e2335b0> <class 'builtin_function_or_method'> arange\n",
      ".argmax == <built-in method argmax of type object at 0x10e2335b0> <class 'builtin_function_or_method'> argmax\n",
      ".argmin == <built-in method argmin of type object at 0x10e2335b0> <class 'builtin_function_or_method'> argmin\n",
      ".argsort == <built-in method argsort of type object at 0x10e2335b0> <class 'builtin_function_or_method'> argsort\n",
      ".as_strided == <built-in method as_strided of type object at 0x10e2335b0> <class 'builtin_function_or_method'> as_strided\n",
      ".as_strided_ == <built-in method as_strided_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> as_strided_\n",
      ".as_tensor == <built-in method as_tensor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> as_tensor\n",
      "Cant find as_tensor in native_func_dict\n",
      "Cant find as_tensor in native_func_dict\n",
      "Cant find as_tensor in native_func_dict\n",
      ".asin == <built-in method asin of type object at 0x10e2335b0> <class 'builtin_function_or_method'> asin\n",
      ".asin_ == <built-in method asin_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> asin_\n",
      ".asinh == <built-in method asinh of type object at 0x10e2335b0> <class 'builtin_function_or_method'> asinh\n",
      ".asinh_ == <built-in method asinh_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> asinh_\n",
      ".atan == <built-in method atan of type object at 0x10e2335b0> <class 'builtin_function_or_method'> atan\n",
      ".atan2 == <built-in method atan2 of type object at 0x10e2335b0> <class 'builtin_function_or_method'> atan2\n",
      ".atan_ == <built-in method atan_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> atan_\n",
      ".atanh == <built-in method atanh of type object at 0x10e2335b0> <class 'builtin_function_or_method'> atanh\n",
      ".atanh_ == <built-in method atanh_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> atanh_\n",
      ".autocast_decrement_nesting == <built-in function autocast_decrement_nesting> <class 'builtin_function_or_method'> autocast_decrement_nesting\n",
      "Cant find autocast_decrement_nesting in native_func_dict\n",
      "Cant find autocast_decrement_nesting in native_func_dict\n",
      "Cant find autocast_decrement_nesting in native_func_dict\n",
      ".autocast_increment_nesting == <built-in function autocast_increment_nesting> <class 'builtin_function_or_method'> autocast_increment_nesting\n",
      "Cant find autocast_increment_nesting in native_func_dict\n",
      "Cant find autocast_increment_nesting in native_func_dict\n",
      "Cant find autocast_increment_nesting in native_func_dict\n",
      ".autograd == <module 'torch.autograd' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/autograd/__init__.py'> <class 'module'> torch.autograd\n",
      ".avg_pool1d == <built-in method avg_pool1d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> avg_pool1d\n",
      ".backends == <module 'torch.backends' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/backends/__init__.py'> <class 'module'> torch.backends\n",
      ".baddbmm == <built-in method baddbmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> baddbmm\n",
      ".bartlett_window == <built-in method bartlett_window of type object at 0x10e2335b0> <class 'builtin_function_or_method'> bartlett_window\n",
      ".batch_norm == <built-in method batch_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> batch_norm\n",
      ".batch_norm_backward_elemt == <built-in method batch_norm_backward_elemt of type object at 0x10e2335b0> <class 'builtin_function_or_method'> batch_norm_backward_elemt\n",
      ".batch_norm_backward_reduce == <built-in method batch_norm_backward_reduce of type object at 0x10e2335b0> <class 'builtin_function_or_method'> batch_norm_backward_reduce\n",
      ".batch_norm_elemt == <built-in method batch_norm_elemt of type object at 0x10e2335b0> <class 'builtin_function_or_method'> batch_norm_elemt\n",
      ".batch_norm_gather_stats == <built-in method batch_norm_gather_stats of type object at 0x10e2335b0> <class 'builtin_function_or_method'> batch_norm_gather_stats\n",
      ".batch_norm_gather_stats_with_counts == <built-in method batch_norm_gather_stats_with_counts of type object at 0x10e2335b0> <class 'builtin_function_or_method'> batch_norm_gather_stats_with_counts\n",
      ".batch_norm_stats == <built-in method batch_norm_stats of type object at 0x10e2335b0> <class 'builtin_function_or_method'> batch_norm_stats\n",
      ".batch_norm_update_stats == <built-in method batch_norm_update_stats of type object at 0x10e2335b0> <class 'builtin_function_or_method'> batch_norm_update_stats\n",
      ".bernoulli == <built-in method bernoulli of type object at 0x10e2335b0> <class 'builtin_function_or_method'> bernoulli\n",
      ".bfloat16 == torch.bfloat16 <class 'torch.dtype'> \n",
      ".bilinear == <built-in method bilinear of type object at 0x10e2335b0> <class 'builtin_function_or_method'> bilinear\n",
      ".binary_cross_entropy_with_logits == <built-in method binary_cross_entropy_with_logits of type object at 0x10e2335b0> <class 'builtin_function_or_method'> binary_cross_entropy_with_logits\n",
      ".bincount == <built-in method bincount of type object at 0x10e2335b0> <class 'builtin_function_or_method'> bincount\n",
      ".binomial == <built-in method binomial of type object at 0x10e2335b0> <class 'builtin_function_or_method'> binomial\n",
      ".bitwise_and == <built-in method bitwise_and of type object at 0x10e2335b0> <class 'builtin_function_or_method'> bitwise_and\n",
      "Cant find bitwise_and in native_func_dict\n",
      "Cant find bitwise_and in native_func_dict\n",
      "Cant find bitwise_and in native_func_dict\n",
      ".bitwise_not == <built-in method bitwise_not of type object at 0x10e2335b0> <class 'builtin_function_or_method'> bitwise_not\n",
      ".bitwise_or == <built-in method bitwise_or of type object at 0x10e2335b0> <class 'builtin_function_or_method'> bitwise_or\n",
      "Cant find bitwise_or in native_func_dict\n",
      "Cant find bitwise_or in native_func_dict\n",
      "Cant find bitwise_or in native_func_dict\n",
      ".bitwise_xor == <built-in method bitwise_xor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> bitwise_xor\n",
      "Cant find bitwise_xor in native_func_dict\n",
      "Cant find bitwise_xor in native_func_dict\n",
      "Cant find bitwise_xor in native_func_dict\n",
      ".blackman_window == <built-in method blackman_window of type object at 0x10e2335b0> <class 'builtin_function_or_method'> blackman_window\n",
      ".block_diag == <function block_diag at 0x130650268> <class 'function'> block_diag\n",
      ".bmm == <built-in method bmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> bmm\n",
      ".bool == torch.bool <class 'torch.dtype'> \n",
      ".broadcast_tensors == <function broadcast_tensors at 0x1303c2378> <class 'function'> broadcast_tensors\n",
      ".bucketize == <built-in method bucketize of type object at 0x10e2335b0> <class 'builtin_function_or_method'> bucketize\n",
      "Cant find bucketize in native_func_dict\n",
      "Cant find bucketize in native_func_dict\n",
      "Cant find bucketize in native_func_dict\n",
      ".can_cast == <built-in method can_cast of type object at 0x10e2335b0> <class 'builtin_function_or_method'> can_cast\n",
      ".cartesian_prod == <function cartesian_prod at 0x1306501e0> <class 'function'> cartesian_prod\n",
      ".cat == <built-in method cat of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cat\n",
      ".cdist == <function cdist at 0x1306502f0> <class 'function'> cdist\n",
      ".cdouble == torch.complex128 <class 'torch.dtype'> \n",
      ".ceil == <built-in method ceil of type object at 0x10e2335b0> <class 'builtin_function_or_method'> ceil\n",
      ".ceil_ == <built-in method ceil_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> ceil_\n",
      ".celu == <built-in method celu of type object at 0x10e2335b0> <class 'builtin_function_or_method'> celu\n",
      ".celu_ == <built-in method celu_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> celu_\n",
      ".cfloat == torch.complex64 <class 'torch.dtype'> \n",
      ".chain_matmul == <function chain_matmul at 0x130650620> <class 'function'> chain_matmul\n",
      ".channel_shuffle == <built-in method channel_shuffle of type object at 0x10e2335b0> <class 'builtin_function_or_method'> channel_shuffle\n",
      ".channels_last == torch.channels_last <class 'torch.memory_format'> \n",
      ".channels_last_3d == torch.channels_last_3d <class 'torch.memory_format'> \n",
      ".cholesky == <built-in method cholesky of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cholesky\n",
      ".cholesky_inverse == <built-in method cholesky_inverse of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cholesky_inverse\n",
      ".cholesky_solve == <built-in method cholesky_solve of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cholesky_solve\n",
      ".chunk == <built-in method chunk of type object at 0x10e2335b0> <class 'builtin_function_or_method'> chunk\n",
      ".clamp == <built-in method clamp of type object at 0x10e2335b0> <class 'builtin_function_or_method'> clamp\n",
      ".clamp_ == <built-in method clamp_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> clamp_\n",
      ".clamp_max == <built-in method clamp_max of type object at 0x10e2335b0> <class 'builtin_function_or_method'> clamp_max\n",
      ".clamp_max_ == <built-in method clamp_max_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> clamp_max_\n",
      ".clamp_min == <built-in method clamp_min of type object at 0x10e2335b0> <class 'builtin_function_or_method'> clamp_min\n",
      ".clamp_min_ == <built-in method clamp_min_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> clamp_min_\n",
      "Exception with classes. Tried to instantiate class '__file__.__file__', but it does not exist! Ensure that it is registered via torch::jit::class_\n",
      ".clear_autocast_cache == <built-in function clear_autocast_cache> <class 'builtin_function_or_method'> clear_autocast_cache\n",
      "Cant find clear_autocast_cache in native_func_dict\n",
      "Cant find clear_autocast_cache in native_func_dict\n",
      "Cant find clear_autocast_cache in native_func_dict\n",
      ".clone == <built-in method clone of type object at 0x10e2335b0> <class 'builtin_function_or_method'> clone\n",
      ".combinations == <built-in method combinations of type object at 0x10e2335b0> <class 'builtin_function_or_method'> combinations\n",
      ".compiled_with_cxx11_abi == <function compiled_with_cxx11_abi at 0x12e209bf8> <class 'function'> compiled_with_cxx11_abi\n",
      ".complex128 == torch.complex128 <class 'torch.dtype'> \n",
      ".complex32 == torch.complex32 <class 'torch.dtype'> \n",
      ".complex64 == torch.complex64 <class 'torch.dtype'> \n",
      ".conj == <built-in method conj of type object at 0x10e2335b0> <class 'builtin_function_or_method'> conj\n",
      ".constant_pad_nd == <built-in method constant_pad_nd of type object at 0x10e2335b0> <class 'builtin_function_or_method'> constant_pad_nd\n",
      ".contiguous_format == torch.contiguous_format <class 'torch.memory_format'> \n",
      ".conv1d == <built-in method conv1d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> conv1d\n",
      ".conv2d == <built-in method conv2d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> conv2d\n",
      ".conv3d == <built-in method conv3d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> conv3d\n",
      ".conv_tbc == <built-in method conv_tbc of type object at 0x10e2335b0> <class 'builtin_function_or_method'> conv_tbc\n",
      ".conv_transpose1d == <built-in method conv_transpose1d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> conv_transpose1d\n",
      ".conv_transpose2d == <built-in method conv_transpose2d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> conv_transpose2d\n",
      "Cant find conv_transpose2d in native_func_dict\n",
      "Cant find conv_transpose2d in native_func_dict\n",
      "Cant find conv_transpose2d in native_func_dict\n",
      ".conv_transpose3d == <built-in method conv_transpose3d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> conv_transpose3d\n",
      "Cant find conv_transpose3d in native_func_dict\n",
      "Cant find conv_transpose3d in native_func_dict\n",
      "Cant find conv_transpose3d in native_func_dict\n",
      ".convolution == <built-in method convolution of type object at 0x10e2335b0> <class 'builtin_function_or_method'> convolution\n",
      ".cos == <built-in method cos of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cos\n",
      ".cos_ == <built-in method cos_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cos_\n",
      ".cosh == <built-in method cosh of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cosh\n",
      ".cosh_ == <built-in method cosh_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cosh_\n",
      ".cosine_embedding_loss == <built-in method cosine_embedding_loss of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cosine_embedding_loss\n",
      ".cosine_similarity == <built-in method cosine_similarity of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cosine_similarity\n",
      ".cpp == <module 'torch._C.cpp'> <class 'module'> torch._C.cpp\n",
      ".cross == <built-in method cross of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cross\n",
      ".ctc_loss == <built-in method ctc_loss of type object at 0x10e2335b0> <class 'builtin_function_or_method'> ctc_loss\n",
      "Cant find ctc_loss in native_func_dict\n",
      "Cant find ctc_loss in native_func_dict\n",
      "Cant find ctc_loss in native_func_dict\n",
      ".ctypes == <module 'ctypes' from '/Users/madhavajay/.pyenv/versions/3.6.11/lib/python3.6/ctypes/__init__.py'> <class 'module'> ctypes\n",
      ".cuda == <module 'torch.cuda' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/cuda/__init__.py'> <class 'module'> torch.cuda\n",
      ".cudnn_affine_grid_generator == <built-in method cudnn_affine_grid_generator of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cudnn_affine_grid_generator\n",
      ".cudnn_batch_norm == <built-in method cudnn_batch_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cudnn_batch_norm\n",
      ".cudnn_convolution == <built-in method cudnn_convolution of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cudnn_convolution\n",
      ".cudnn_convolution_transpose == <built-in method cudnn_convolution_transpose of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cudnn_convolution_transpose\n",
      ".cudnn_grid_sampler == <built-in method cudnn_grid_sampler of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cudnn_grid_sampler\n",
      ".cudnn_is_acceptable == <built-in method cudnn_is_acceptable of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cudnn_is_acceptable\n",
      ".cummax == <built-in method cummax of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cummax\n",
      ".cummin == <built-in method cummin of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cummin\n",
      ".cumprod == <built-in method cumprod of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cumprod\n",
      ".cumsum == <built-in method cumsum of type object at 0x10e2335b0> <class 'builtin_function_or_method'> cumsum\n",
      ".default_generator == <torch._C.Generator object at 0x109631e58> <class 'torch._C.Generator'> \n",
      ".deg2rad == <built-in method deg2rad of type object at 0x10e2335b0> <class 'builtin_function_or_method'> deg2rad\n",
      ".deg2rad_ == <built-in method deg2rad_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> deg2rad_\n",
      ".dequantize == <built-in method dequantize of type object at 0x10e2335b0> <class 'builtin_function_or_method'> dequantize\n",
      "Cant find dequantize in native_func_dict\n",
      "Cant find dequantize in native_func_dict\n",
      "Cant find dequantize in native_func_dict\n",
      ".det == <built-in method det of type object at 0x10e2335b0> <class 'builtin_function_or_method'> det\n",
      ".detach == <built-in method detach of type object at 0x10e2335b0> <class 'builtin_function_or_method'> detach\n",
      ".detach_ == <built-in method detach_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> detach_\n",
      ".device == <class 'torch.device'> <class 'type'> device\n",
      "Hierarchy: [<class 'torch.device'>, <class 'object'>]\n",
      ".diag == <built-in method diag of type object at 0x10e2335b0> <class 'builtin_function_or_method'> diag\n",
      ".diag_embed == <built-in method diag_embed of type object at 0x10e2335b0> <class 'builtin_function_or_method'> diag_embed\n",
      ".diagflat == <built-in method diagflat of type object at 0x10e2335b0> <class 'builtin_function_or_method'> diagflat\n",
      ".diagonal == <built-in method diagonal of type object at 0x10e2335b0> <class 'builtin_function_or_method'> diagonal\n",
      ".digamma == <built-in method digamma of type object at 0x10e2335b0> <class 'builtin_function_or_method'> digamma\n",
      ".dist == <built-in method dist of type object at 0x10e2335b0> <class 'builtin_function_or_method'> dist\n",
      ".distributed == <module 'torch.distributed' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/distributed/__init__.py'> <class 'module'> torch.distributed\n",
      ".distributions == <module 'torch.distributions' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/distributions/__init__.py'> <class 'module'> torch.distributions\n",
      ".div == <built-in method div of type object at 0x10e2335b0> <class 'builtin_function_or_method'> div\n",
      "Cant find div in native_func_dict\n",
      "Cant find div in native_func_dict\n",
      "Cant find div in native_func_dict\n",
      ".dot == <built-in method dot of type object at 0x10e2335b0> <class 'builtin_function_or_method'> dot\n",
      ".double == torch.float64 <class 'torch.dtype'> \n",
      ".dropout == <built-in method dropout of type object at 0x10e2335b0> <class 'builtin_function_or_method'> dropout\n",
      ".dropout_ == <built-in method dropout_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> dropout_\n",
      ".dsmm == <built-in method dsmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> dsmm\n",
      "Cant find dsmm in native_func_dict\n",
      "Cant find dsmm in native_func_dict\n",
      "Cant find dsmm in native_func_dict\n",
      ".dtype == <class 'torch.dtype'> <class 'type'> dtype\n",
      "Hierarchy: [<class 'torch.dtype'>, <class 'object'>]\n",
      ".eig == <built-in method eig of type object at 0x10e2335b0> <class 'builtin_function_or_method'> eig\n",
      ".einsum == <function einsum at 0x13064d730> <class 'function'> einsum\n",
      ".embedding == <built-in method embedding of type object at 0x10e2335b0> <class 'builtin_function_or_method'> embedding\n",
      ".embedding_bag == <built-in method embedding_bag of type object at 0x10e2335b0> <class 'builtin_function_or_method'> embedding_bag\n",
      ".embedding_renorm_ == <built-in method embedding_renorm_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> embedding_renorm_\n",
      ".empty == <built-in method empty of type object at 0x10e2335b0> <class 'builtin_function_or_method'> empty\n",
      "Cant find empty in native_func_dict\n",
      "Cant find empty in native_func_dict\n",
      "Cant find empty in native_func_dict\n",
      ".empty_like == <built-in method empty_like of type object at 0x10e2335b0> <class 'builtin_function_or_method'> empty_like\n",
      ".empty_meta == <built-in method empty_meta of type object at 0x10e2335b0> <class 'builtin_function_or_method'> empty_meta\n",
      ".empty_quantized == <built-in method empty_quantized of type object at 0x10e2335b0> <class 'builtin_function_or_method'> empty_quantized\n",
      ".empty_strided == <built-in method empty_strided of type object at 0x10e2335b0> <class 'builtin_function_or_method'> empty_strided\n",
      ".enable_grad == <class 'torch.autograd.grad_mode.enable_grad'> <class 'type'> enable_grad\n",
      "Hierarchy: [<class 'torch.autograd.grad_mode.enable_grad'>, <class 'torch.autograd.grad_mode._DecoratorContextManager'>, <class 'object'>]\n",
      ".eq == <built-in method eq of type object at 0x10e2335b0> <class 'builtin_function_or_method'> eq\n",
      "Cant find eq in native_func_dict\n",
      "Cant find eq in native_func_dict\n",
      "Cant find eq in native_func_dict\n",
      ".equal == <built-in method equal of type object at 0x10e2335b0> <class 'builtin_function_or_method'> equal\n",
      ".erf == <built-in method erf of type object at 0x10e2335b0> <class 'builtin_function_or_method'> erf\n",
      ".erf_ == <built-in method erf_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> erf_\n",
      ".erfc == <built-in method erfc of type object at 0x10e2335b0> <class 'builtin_function_or_method'> erfc\n",
      ".erfc_ == <built-in method erfc_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> erfc_\n",
      ".erfinv == <built-in method erfinv of type object at 0x10e2335b0> <class 'builtin_function_or_method'> erfinv\n",
      ".exp == <built-in method exp of type object at 0x10e2335b0> <class 'builtin_function_or_method'> exp\n",
      ".exp_ == <built-in method exp_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> exp_\n",
      ".expm1 == <built-in method expm1 of type object at 0x10e2335b0> <class 'builtin_function_or_method'> expm1\n",
      ".expm1_ == <built-in method expm1_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> expm1_\n",
      ".eye == <built-in method eye of type object at 0x10e2335b0> <class 'builtin_function_or_method'> eye\n",
      ".fake_quantize_per_channel_affine == <built-in method fake_quantize_per_channel_affine of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fake_quantize_per_channel_affine\n",
      ".fake_quantize_per_tensor_affine == <built-in method fake_quantize_per_tensor_affine of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fake_quantize_per_tensor_affine\n",
      ".fbgemm_linear_fp16_weight == <built-in method fbgemm_linear_fp16_weight of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fbgemm_linear_fp16_weight\n",
      ".fbgemm_linear_fp16_weight_fp32_activation == <built-in method fbgemm_linear_fp16_weight_fp32_activation of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fbgemm_linear_fp16_weight_fp32_activation\n",
      ".fbgemm_linear_int8_weight == <built-in method fbgemm_linear_int8_weight of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fbgemm_linear_int8_weight\n",
      ".fbgemm_linear_int8_weight_fp32_activation == <built-in method fbgemm_linear_int8_weight_fp32_activation of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fbgemm_linear_int8_weight_fp32_activation\n",
      ".fbgemm_linear_quantize_weight == <built-in method fbgemm_linear_quantize_weight of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fbgemm_linear_quantize_weight\n",
      ".fbgemm_pack_gemm_matrix_fp16 == <built-in method fbgemm_pack_gemm_matrix_fp16 of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fbgemm_pack_gemm_matrix_fp16\n",
      ".fbgemm_pack_quantized_matrix == <built-in method fbgemm_pack_quantized_matrix of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fbgemm_pack_quantized_matrix\n",
      ".feature_alpha_dropout == <built-in method feature_alpha_dropout of type object at 0x10e2335b0> <class 'builtin_function_or_method'> feature_alpha_dropout\n",
      ".feature_alpha_dropout_ == <built-in method feature_alpha_dropout_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> feature_alpha_dropout_\n",
      ".feature_dropout == <built-in method feature_dropout of type object at 0x10e2335b0> <class 'builtin_function_or_method'> feature_dropout\n",
      ".feature_dropout_ == <built-in method feature_dropout_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> feature_dropout_\n",
      ".fft == <built-in method fft of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fft\n",
      ".fill_ == <built-in method fill_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fill_\n",
      "Cant find fill_ in native_func_dict\n",
      "Cant find fill_ in native_func_dict\n",
      "Cant find fill_ in native_func_dict\n",
      ".finfo == <class 'torch.finfo'> <class 'type'> finfo\n",
      "Hierarchy: [<class 'torch.finfo'>, <class 'object'>]\n",
      ".flatten == <built-in method flatten of type object at 0x10e2335b0> <class 'builtin_function_or_method'> flatten\n",
      "Cant find flatten in native_func_dict\n",
      "Cant find flatten in native_func_dict\n",
      "Cant find flatten in native_func_dict\n",
      ".flip == <built-in method flip of type object at 0x10e2335b0> <class 'builtin_function_or_method'> flip\n",
      ".fliplr == <built-in method fliplr of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fliplr\n",
      ".flipud == <built-in method flipud of type object at 0x10e2335b0> <class 'builtin_function_or_method'> flipud\n",
      ".float == torch.float32 <class 'torch.dtype'> \n",
      ".float16 == torch.float16 <class 'torch.dtype'> \n",
      ".float32 == torch.float32 <class 'torch.dtype'> \n",
      ".float64 == torch.float64 <class 'torch.dtype'> \n",
      ".floor == <built-in method floor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> floor\n",
      ".floor_ == <built-in method floor_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> floor_\n",
      ".floor_divide == <built-in method floor_divide of type object at 0x10e2335b0> <class 'builtin_function_or_method'> floor_divide\n",
      ".fmod == <built-in method fmod of type object at 0x10e2335b0> <class 'builtin_function_or_method'> fmod\n",
      "Cant find fmod in native_func_dict\n",
      "Cant find fmod in native_func_dict\n",
      "Cant find fmod in native_func_dict\n",
      ".fork == <built-in method fork of PyCapsule object at 0x10a739fc0> <class 'builtin_function_or_method'> fork\n",
      "Cant find fork in native_func_dict\n",
      "Cant find fork in native_func_dict\n",
      "Cant find fork in native_func_dict\n",
      ".frac == <built-in method frac of type object at 0x10e2335b0> <class 'builtin_function_or_method'> frac\n",
      ".frac_ == <built-in method frac_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> frac_\n",
      ".frobenius_norm == <built-in method frobenius_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> frobenius_norm\n",
      ".from_file == <built-in method from_file of type object at 0x10e2335b0> <class 'builtin_function_or_method'> from_file\n",
      ".from_numpy == <built-in method from_numpy of type object at 0x10e2335b0> <class 'builtin_function_or_method'> from_numpy\n",
      "Cant find from_numpy in native_func_dict\n",
      "Cant find from_numpy in native_func_dict\n",
      "Cant find from_numpy in native_func_dict\n",
      ".full == <built-in method full of type object at 0x10e2335b0> <class 'builtin_function_or_method'> full\n",
      ".full_like == <built-in method full_like of type object at 0x10e2335b0> <class 'builtin_function_or_method'> full_like\n",
      ".functional == <module 'torch.functional' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/functional.py'> <class 'module'> torch.functional\n",
      ".futures == <module 'torch.futures' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/futures/__init__.py'> <class 'module'> torch.futures\n",
      ".gather == <built-in method gather of type object at 0x10e2335b0> <class 'builtin_function_or_method'> gather\n",
      ".ge == <built-in method ge of type object at 0x10e2335b0> <class 'builtin_function_or_method'> ge\n",
      "Cant find ge in native_func_dict\n",
      "Cant find ge in native_func_dict\n",
      "Cant find ge in native_func_dict\n",
      ".geqrf == <built-in method geqrf of type object at 0x10e2335b0> <class 'builtin_function_or_method'> geqrf\n",
      ".ger == <built-in method ger of type object at 0x10e2335b0> <class 'builtin_function_or_method'> ger\n",
      ".get_default_dtype == <built-in function get_default_dtype> <class 'builtin_function_or_method'> get_default_dtype\n",
      "Cant find get_default_dtype in native_func_dict\n",
      "Cant find get_default_dtype in native_func_dict\n",
      "Cant find get_default_dtype in native_func_dict\n",
      ".get_device == <built-in method get_device of type object at 0x10e2335b0> <class 'builtin_function_or_method'> get_device\n",
      "Cant find get_device in native_func_dict\n",
      "Cant find get_device in native_func_dict\n",
      "Cant find get_device in native_func_dict\n",
      ".get_file_path == <function get_file_path at 0x10a118730> <class 'function'> get_file_path\n",
      ".get_num_interop_threads == <built-in function get_num_interop_threads> <class 'builtin_function_or_method'> get_num_interop_threads\n",
      "Cant find get_num_interop_threads in native_func_dict\n",
      "Cant find get_num_interop_threads in native_func_dict\n",
      "Cant find get_num_interop_threads in native_func_dict\n",
      ".get_num_threads == <built-in function get_num_threads> <class 'builtin_function_or_method'> get_num_threads\n",
      "Cant find get_num_threads in native_func_dict\n",
      "Cant find get_num_threads in native_func_dict\n",
      "Cant find get_num_threads in native_func_dict\n",
      ".get_rng_state == <function get_rng_state at 0x12e209d90> <class 'function'> get_rng_state\n",
      ".grid_sampler == <built-in method grid_sampler of type object at 0x10e2335b0> <class 'builtin_function_or_method'> grid_sampler\n",
      ".grid_sampler_2d == <built-in method grid_sampler_2d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> grid_sampler_2d\n",
      ".grid_sampler_3d == <built-in method grid_sampler_3d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> grid_sampler_3d\n",
      ".group_norm == <built-in method group_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> group_norm\n",
      ".gru == <built-in method gru of type object at 0x10e2335b0> <class 'builtin_function_or_method'> gru\n",
      "Cant find gru in native_func_dict\n",
      "Cant find gru in native_func_dict\n",
      "Cant find gru in native_func_dict\n",
      ".gru_cell == <built-in method gru_cell of type object at 0x10e2335b0> <class 'builtin_function_or_method'> gru_cell\n",
      ".gt == <built-in method gt of type object at 0x10e2335b0> <class 'builtin_function_or_method'> gt\n",
      "Cant find gt in native_func_dict\n",
      "Cant find gt in native_func_dict\n",
      "Cant find gt in native_func_dict\n",
      ".half == torch.float16 <class 'torch.dtype'> \n",
      ".hamming_window == <built-in method hamming_window of type object at 0x10e2335b0> <class 'builtin_function_or_method'> hamming_window\n",
      ".hann_window == <built-in method hann_window of type object at 0x10e2335b0> <class 'builtin_function_or_method'> hann_window\n",
      ".hardshrink == <built-in method hardshrink of type object at 0x10e2335b0> <class 'builtin_function_or_method'> hardshrink\n",
      ".has_cuda == False <class 'bool'> \n",
      ".has_cudnn == False <class 'bool'> \n",
      ".has_lapack == True <class 'bool'> \n",
      ".has_mkl == True <class 'bool'> \n",
      ".has_mkldnn == True <class 'bool'> \n",
      ".has_openmp == False <class 'bool'> \n",
      ".hinge_embedding_loss == <built-in method hinge_embedding_loss of type object at 0x10e2335b0> <class 'builtin_function_or_method'> hinge_embedding_loss\n",
      ".histc == <built-in method histc of type object at 0x10e2335b0> <class 'builtin_function_or_method'> histc\n",
      ".hsmm == <built-in method hsmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> hsmm\n",
      "Cant find hsmm in native_func_dict\n",
      "Cant find hsmm in native_func_dict\n",
      "Cant find hsmm in native_func_dict\n",
      ".hspmm == <built-in method hspmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> hspmm\n",
      ".hub == <module 'torch.hub' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/hub.py'> <class 'module'> torch.hub\n",
      ".ifft == <built-in method ifft of type object at 0x10e2335b0> <class 'builtin_function_or_method'> ifft\n",
      ".iinfo == <class 'torch.iinfo'> <class 'type'> iinfo\n",
      "Hierarchy: [<class 'torch.iinfo'>, <class 'object'>]\n",
      ".imag == <built-in method imag of type object at 0x10e2335b0> <class 'builtin_function_or_method'> imag\n",
      ".import_ir_module == <built-in method import_ir_module of PyCapsule object at 0x10a75a600> <class 'builtin_function_or_method'> import_ir_module\n",
      "Cant find import_ir_module in native_func_dict\n",
      "Cant find import_ir_module in native_func_dict\n",
      "Cant find import_ir_module in native_func_dict\n",
      ".import_ir_module_from_buffer == <built-in method import_ir_module_from_buffer of PyCapsule object at 0x10a75a630> <class 'builtin_function_or_method'> import_ir_module_from_buffer\n",
      "Cant find import_ir_module_from_buffer in native_func_dict\n",
      "Cant find import_ir_module_from_buffer in native_func_dict\n",
      "Cant find import_ir_module_from_buffer in native_func_dict\n",
      ".index_add == <built-in method index_add of type object at 0x10e2335b0> <class 'builtin_function_or_method'> index_add\n",
      ".index_copy == <built-in method index_copy of type object at 0x10e2335b0> <class 'builtin_function_or_method'> index_copy\n",
      ".index_fill == <built-in method index_fill of type object at 0x10e2335b0> <class 'builtin_function_or_method'> index_fill\n",
      "Cant find index_fill in native_func_dict\n",
      "Cant find index_fill in native_func_dict\n",
      "Cant find index_fill in native_func_dict\n",
      ".index_put == <built-in method index_put of type object at 0x10e2335b0> <class 'builtin_function_or_method'> index_put\n",
      ".index_put_ == <built-in method index_put_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> index_put_\n",
      ".index_select == <built-in method index_select of type object at 0x10e2335b0> <class 'builtin_function_or_method'> index_select\n",
      ".init_num_threads == <built-in method init_num_threads of PyCapsule object at 0x10a767360> <class 'builtin_function_or_method'> init_num_threads\n",
      "Cant find init_num_threads in native_func_dict\n",
      "Cant find init_num_threads in native_func_dict\n",
      "Cant find init_num_threads in native_func_dict\n",
      ".initial_seed == <function initial_seed at 0x12e209f28> <class 'function'> initial_seed\n",
      ".instance_norm == <built-in method instance_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> instance_norm\n",
      ".int == torch.int32 <class 'torch.dtype'> \n",
      ".int16 == torch.int16 <class 'torch.dtype'> \n",
      ".int32 == torch.int32 <class 'torch.dtype'> \n",
      ".int64 == torch.int64 <class 'torch.dtype'> \n",
      ".int8 == torch.int8 <class 'torch.dtype'> \n",
      ".int_repr == <built-in method int_repr of type object at 0x10e2335b0> <class 'builtin_function_or_method'> int_repr\n",
      ".inverse == <built-in method inverse of type object at 0x10e2335b0> <class 'builtin_function_or_method'> inverse\n",
      ".irfft == <built-in method irfft of type object at 0x10e2335b0> <class 'builtin_function_or_method'> irfft\n",
      ".is_anomaly_enabled == <built-in function is_anomaly_enabled> <class 'builtin_function_or_method'> is_anomaly_enabled\n",
      "Cant find is_anomaly_enabled in native_func_dict\n",
      "Cant find is_anomaly_enabled in native_func_dict\n",
      "Cant find is_anomaly_enabled in native_func_dict\n",
      ".is_autocast_enabled == <built-in function is_autocast_enabled> <class 'builtin_function_or_method'> is_autocast_enabled\n",
      "Cant find is_autocast_enabled in native_func_dict\n",
      "Cant find is_autocast_enabled in native_func_dict\n",
      "Cant find is_autocast_enabled in native_func_dict\n",
      ".is_complex == <built-in method is_complex of type object at 0x10e2335b0> <class 'builtin_function_or_method'> is_complex\n",
      ".is_distributed == <built-in method is_distributed of type object at 0x10e2335b0> <class 'builtin_function_or_method'> is_distributed\n",
      ".is_floating_point == <built-in method is_floating_point of type object at 0x10e2335b0> <class 'builtin_function_or_method'> is_floating_point\n",
      ".is_grad_enabled == <built-in function is_grad_enabled> <class 'builtin_function_or_method'> is_grad_enabled\n",
      "Cant find is_grad_enabled in native_func_dict\n",
      "Cant find is_grad_enabled in native_func_dict\n",
      "Cant find is_grad_enabled in native_func_dict\n",
      ".is_nonzero == <built-in method is_nonzero of type object at 0x10e2335b0> <class 'builtin_function_or_method'> is_nonzero\n",
      ".is_same_size == <built-in method is_same_size of type object at 0x10e2335b0> <class 'builtin_function_or_method'> is_same_size\n",
      ".is_signed == <built-in method is_signed of type object at 0x10e2335b0> <class 'builtin_function_or_method'> is_signed\n",
      ".is_storage == <function is_storage at 0x10a775048> <class 'function'> is_storage\n",
      ".is_tensor == <function is_tensor at 0x10810fe18> <class 'function'> is_tensor\n",
      ".is_vulkan_available == <built-in method is_vulkan_available of type object at 0x10e2335b0> <class 'builtin_function_or_method'> is_vulkan_available\n",
      ".isclose == <built-in method isclose of type object at 0x10e2335b0> <class 'builtin_function_or_method'> isclose\n",
      ".isfinite == <built-in method isfinite of type object at 0x10e2335b0> <class 'builtin_function_or_method'> isfinite\n",
      ".isinf == <built-in method isinf of type object at 0x10e2335b0> <class 'builtin_function_or_method'> isinf\n",
      ".isnan == <built-in method isnan of type object at 0x10e2335b0> <class 'builtin_function_or_method'> isnan\n",
      ".istft == <function istft at 0x13064d8c8> <class 'function'> istft\n",
      ".jit == <module 'torch.jit' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/jit/__init__.py'> <class 'module'> torch.jit\n",
      ".kl_div == <built-in method kl_div of type object at 0x10e2335b0> <class 'builtin_function_or_method'> kl_div\n",
      ".kthvalue == <built-in method kthvalue of type object at 0x10e2335b0> <class 'builtin_function_or_method'> kthvalue\n",
      ".layer_norm == <built-in method layer_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> layer_norm\n",
      ".layout == <class 'torch.layout'> <class 'type'> layout\n",
      "Hierarchy: [<class 'torch.layout'>, <class 'object'>]\n",
      ".le == <built-in method le of type object at 0x10e2335b0> <class 'builtin_function_or_method'> le\n",
      "Cant find le in native_func_dict\n",
      "Cant find le in native_func_dict\n",
      "Cant find le in native_func_dict\n",
      ".legacy_contiguous_format == torch.contiguous_format <class 'torch.memory_format'> \n",
      ".lerp == <built-in method lerp of type object at 0x10e2335b0> <class 'builtin_function_or_method'> lerp\n",
      "Cant find lerp in native_func_dict\n",
      "Cant find lerp in native_func_dict\n",
      "Cant find lerp in native_func_dict\n",
      ".lgamma == <built-in method lgamma of type object at 0x10e2335b0> <class 'builtin_function_or_method'> lgamma\n",
      ".linspace == <built-in method linspace of type object at 0x10e2335b0> <class 'builtin_function_or_method'> linspace\n",
      ".load == <function load at 0x1302511e0> <class 'function'> load\n",
      ".lobpcg == <function lobpcg at 0x1308c9c80> <class 'function'> lobpcg\n",
      ".log == <built-in method log of type object at 0x10e2335b0> <class 'builtin_function_or_method'> log\n",
      ".log10 == <built-in method log10 of type object at 0x10e2335b0> <class 'builtin_function_or_method'> log10\n",
      ".log10_ == <built-in method log10_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> log10_\n",
      ".log1p == <built-in method log1p of type object at 0x10e2335b0> <class 'builtin_function_or_method'> log1p\n",
      ".log1p_ == <built-in method log1p_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> log1p_\n",
      ".log2 == <built-in method log2 of type object at 0x10e2335b0> <class 'builtin_function_or_method'> log2\n",
      ".log2_ == <built-in method log2_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> log2_\n",
      ".log_ == <built-in method log_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> log_\n",
      ".log_softmax == <built-in method log_softmax of type object at 0x10e2335b0> <class 'builtin_function_or_method'> log_softmax\n",
      "Cant find log_softmax in native_func_dict\n",
      "Cant find log_softmax in native_func_dict\n",
      "Cant find log_softmax in native_func_dict\n",
      ".logaddexp == <built-in method logaddexp of type object at 0x10e2335b0> <class 'builtin_function_or_method'> logaddexp\n",
      ".logaddexp2 == <built-in method logaddexp2 of type object at 0x10e2335b0> <class 'builtin_function_or_method'> logaddexp2\n",
      ".logcumsumexp == <built-in method logcumsumexp of type object at 0x10e2335b0> <class 'builtin_function_or_method'> logcumsumexp\n",
      ".logdet == <built-in method logdet of type object at 0x10e2335b0> <class 'builtin_function_or_method'> logdet\n",
      ".logical_and == <built-in method logical_and of type object at 0x10e2335b0> <class 'builtin_function_or_method'> logical_and\n",
      ".logical_not == <built-in method logical_not of type object at 0x10e2335b0> <class 'builtin_function_or_method'> logical_not\n",
      ".logical_or == <built-in method logical_or of type object at 0x10e2335b0> <class 'builtin_function_or_method'> logical_or\n",
      ".logical_xor == <built-in method logical_xor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> logical_xor\n",
      ".logspace == <built-in method logspace of type object at 0x10e2335b0> <class 'builtin_function_or_method'> logspace\n",
      ".logsumexp == <built-in method logsumexp of type object at 0x10e2335b0> <class 'builtin_function_or_method'> logsumexp\n",
      ".long == torch.int64 <class 'torch.dtype'> \n",
      ".lstm == <built-in method lstm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> lstm\n",
      "Cant find lstm in native_func_dict\n",
      "Cant find lstm in native_func_dict\n",
      "Cant find lstm in native_func_dict\n",
      ".lstm_cell == <built-in method lstm_cell of type object at 0x10e2335b0> <class 'builtin_function_or_method'> lstm_cell\n",
      ".lstsq == <built-in method lstsq of type object at 0x10e2335b0> <class 'builtin_function_or_method'> lstsq\n",
      ".lt == <built-in method lt of type object at 0x10e2335b0> <class 'builtin_function_or_method'> lt\n",
      "Cant find lt in native_func_dict\n",
      "Cant find lt in native_func_dict\n",
      "Cant find lt in native_func_dict\n",
      ".lu == <function boolean_dispatch.<locals>.fn at 0x1306508c8> <class 'function'> lu\n",
      ".lu_solve == <built-in method lu_solve of type object at 0x10e2335b0> <class 'builtin_function_or_method'> lu_solve\n",
      ".lu_unpack == <function lu_unpack at 0x13064d6a8> <class 'function'> lu_unpack\n",
      ".manual_seed == <function manual_seed at 0x12e209e18> <class 'function'> manual_seed\n",
      ".margin_ranking_loss == <built-in method margin_ranking_loss of type object at 0x10e2335b0> <class 'builtin_function_or_method'> margin_ranking_loss\n",
      ".masked_fill == <built-in method masked_fill of type object at 0x10e2335b0> <class 'builtin_function_or_method'> masked_fill\n",
      "Cant find masked_fill in native_func_dict\n",
      "Cant find masked_fill in native_func_dict\n",
      "Cant find masked_fill in native_func_dict\n",
      ".masked_scatter == <built-in method masked_scatter of type object at 0x10e2335b0> <class 'builtin_function_or_method'> masked_scatter\n",
      ".masked_select == <built-in method masked_select of type object at 0x10e2335b0> <class 'builtin_function_or_method'> masked_select\n",
      ".matmul == <built-in method matmul of type object at 0x10e2335b0> <class 'builtin_function_or_method'> matmul\n",
      ".matrix_power == <built-in method matrix_power of type object at 0x10e2335b0> <class 'builtin_function_or_method'> matrix_power\n",
      ".matrix_rank == <built-in method matrix_rank of type object at 0x10e2335b0> <class 'builtin_function_or_method'> matrix_rank\n",
      ".max == <built-in method max of type object at 0x10e2335b0> <class 'builtin_function_or_method'> max\n",
      ".max_pool1d == <built-in method max_pool1d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> max_pool1d\n",
      ".max_pool1d_with_indices == <built-in method max_pool1d_with_indices of type object at 0x10e2335b0> <class 'builtin_function_or_method'> max_pool1d_with_indices\n",
      ".max_pool2d == <built-in method max_pool2d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> max_pool2d\n",
      ".max_pool3d == <built-in method max_pool3d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> max_pool3d\n",
      ".mean == <built-in method mean of type object at 0x10e2335b0> <class 'builtin_function_or_method'> mean\n",
      ".median == <built-in method median of type object at 0x10e2335b0> <class 'builtin_function_or_method'> median\n",
      ".memory_format == <class 'torch.memory_format'> <class 'type'> memory_format\n",
      "Hierarchy: [<class 'torch.memory_format'>, <class 'object'>]\n",
      ".merge_type_from_type_comment == <built-in method merge_type_from_type_comment of PyCapsule object at 0x10a75a5d0> <class 'builtin_function_or_method'> merge_type_from_type_comment\n",
      "Cant find merge_type_from_type_comment in native_func_dict\n",
      "Cant find merge_type_from_type_comment in native_func_dict\n",
      "Cant find merge_type_from_type_comment in native_func_dict\n",
      ".meshgrid == <function meshgrid at 0x13064d7b8> <class 'function'> meshgrid\n",
      ".min == <built-in method min of type object at 0x10e2335b0> <class 'builtin_function_or_method'> min\n",
      ".miopen_batch_norm == <built-in method miopen_batch_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> miopen_batch_norm\n",
      ".miopen_convolution == <built-in method miopen_convolution of type object at 0x10e2335b0> <class 'builtin_function_or_method'> miopen_convolution\n",
      ".miopen_convolution_transpose == <built-in method miopen_convolution_transpose of type object at 0x10e2335b0> <class 'builtin_function_or_method'> miopen_convolution_transpose\n",
      ".miopen_depthwise_convolution == <built-in method miopen_depthwise_convolution of type object at 0x10e2335b0> <class 'builtin_function_or_method'> miopen_depthwise_convolution\n",
      ".miopen_rnn == <built-in method miopen_rnn of type object at 0x10e2335b0> <class 'builtin_function_or_method'> miopen_rnn\n",
      ".mkldnn_adaptive_avg_pool2d == <built-in method mkldnn_adaptive_avg_pool2d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> mkldnn_adaptive_avg_pool2d\n",
      ".mkldnn_convolution == <built-in method mkldnn_convolution of type object at 0x10e2335b0> <class 'builtin_function_or_method'> mkldnn_convolution\n",
      ".mkldnn_convolution_backward_weights == <built-in method mkldnn_convolution_backward_weights of type object at 0x10e2335b0> <class 'builtin_function_or_method'> mkldnn_convolution_backward_weights\n",
      ".mkldnn_max_pool2d == <built-in method mkldnn_max_pool2d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> mkldnn_max_pool2d\n",
      ".mm == <built-in method mm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> mm\n",
      ".mode == <built-in method mode of type object at 0x10e2335b0> <class 'builtin_function_or_method'> mode\n",
      ".mul == <built-in method mul of type object at 0x10e2335b0> <class 'builtin_function_or_method'> mul\n",
      "Cant find mul in native_func_dict\n",
      "Cant find mul in native_func_dict\n",
      "Cant find mul in native_func_dict\n",
      ".multinomial == <built-in method multinomial of type object at 0x10e2335b0> <class 'builtin_function_or_method'> multinomial\n",
      ".multiprocessing == <module 'torch.multiprocessing' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/multiprocessing/__init__.py'> <class 'module'> torch.multiprocessing\n",
      ".mv == <built-in method mv of type object at 0x10e2335b0> <class 'builtin_function_or_method'> mv\n",
      ".mvlgamma == <built-in method mvlgamma of type object at 0x10e2335b0> <class 'builtin_function_or_method'> mvlgamma\n",
      ".name == zeros_like <class 'str'> \n",
      ".narrow == <built-in method narrow of type object at 0x10e2335b0> <class 'builtin_function_or_method'> narrow\n",
      ".native_batch_norm == <built-in method native_batch_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> native_batch_norm\n",
      ".native_group_norm == <built-in method native_group_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> native_group_norm\n",
      ".native_layer_norm == <built-in method native_layer_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> native_layer_norm\n",
      ".native_norm == <built-in method native_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> native_norm\n",
      ".ne == <built-in method ne of type object at 0x10e2335b0> <class 'builtin_function_or_method'> ne\n",
      "Cant find ne in native_func_dict\n",
      "Cant find ne in native_func_dict\n",
      "Cant find ne in native_func_dict\n",
      ".neg == <built-in method neg of type object at 0x10e2335b0> <class 'builtin_function_or_method'> neg\n",
      ".neg_ == <built-in method neg_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> neg_\n",
      ".nn == <module 'torch.nn' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/nn/__init__.py'> <class 'module'> torch.nn\n",
      ".no_grad == <class 'torch.autograd.grad_mode.no_grad'> <class 'type'> no_grad\n",
      "Hierarchy: [<class 'torch.autograd.grad_mode.no_grad'>, <class 'torch.autograd.grad_mode._DecoratorContextManager'>, <class 'object'>]\n",
      ".nonzero == <built-in method nonzero of type object at 0x10e2335b0> <class 'builtin_function_or_method'> nonzero\n",
      ".norm == <function norm at 0x130650598> <class 'function'> norm\n",
      ".norm_except_dim == <built-in method norm_except_dim of type object at 0x10e2335b0> <class 'builtin_function_or_method'> norm_except_dim\n",
      ".normal == <built-in method normal of type object at 0x10e2335b0> <class 'builtin_function_or_method'> normal\n",
      "Cant find normal in native_func_dict\n",
      "Cant find normal in native_func_dict\n",
      "Cant find normal in native_func_dict\n",
      ".nuclear_norm == <built-in method nuclear_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> nuclear_norm\n",
      ".numel == <built-in method numel of type object at 0x10e2335b0> <class 'builtin_function_or_method'> numel\n",
      "Cant find numel in native_func_dict\n",
      "Cant find numel in native_func_dict\n",
      "Cant find numel in native_func_dict\n",
      ".ones == <built-in method ones of type object at 0x10e2335b0> <class 'builtin_function_or_method'> ones\n",
      ".ones_like == <built-in method ones_like of type object at 0x10e2335b0> <class 'builtin_function_or_method'> ones_like\n",
      ".onnx == <module 'torch.onnx' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/onnx/__init__.py'> <class 'module'> torch.onnx\n",
      ".ops == <module 'torch.ops' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_ops.py'> <class 'torch._ops._Ops'> torch.ops\n",
      "Exception with ops. '_OpNamespace' object is not callable\n",
      ".optim == <module 'torch.optim' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/optim/__init__.py'> <class 'module'> torch.optim\n",
      ".orgqr == <built-in method orgqr of type object at 0x10e2335b0> <class 'builtin_function_or_method'> orgqr\n",
      ".ormqr == <built-in method ormqr of type object at 0x10e2335b0> <class 'builtin_function_or_method'> ormqr\n",
      ".os == <module 'os' from '/Users/madhavajay/.pyenv/versions/3.6.11/lib/python3.6/os.py'> <class 'module'> os\n",
      ".pairwise_distance == <built-in method pairwise_distance of type object at 0x10e2335b0> <class 'builtin_function_or_method'> pairwise_distance\n",
      ".parse_ir == <built-in method parse_ir of PyCapsule object at 0x10a739960> <class 'builtin_function_or_method'> parse_ir\n",
      "Cant find parse_ir in native_func_dict\n",
      "Cant find parse_ir in native_func_dict\n",
      "Cant find parse_ir in native_func_dict\n",
      ".parse_schema == <built-in method parse_schema of PyCapsule object at 0x10a739990> <class 'builtin_function_or_method'> parse_schema\n",
      "Cant find parse_schema in native_func_dict\n",
      "Cant find parse_schema in native_func_dict\n",
      "Cant find parse_schema in native_func_dict\n",
      ".parse_type_comment == <built-in method parse_type_comment of PyCapsule object at 0x10a75a5a0> <class 'builtin_function_or_method'> parse_type_comment\n",
      "Cant find parse_type_comment in native_func_dict\n",
      "Cant find parse_type_comment in native_func_dict\n",
      "Cant find parse_type_comment in native_func_dict\n",
      ".pca_lowrank == <function pca_lowrank at 0x13064d488> <class 'function'> pca_lowrank\n",
      ".pdist == <built-in method pdist of type object at 0x10e2335b0> <class 'builtin_function_or_method'> pdist\n",
      ".per_channel_affine == torch.per_channel_affine <class 'torch.qscheme'> \n",
      ".per_channel_symmetric == torch.per_channel_symmetric <class 'torch.qscheme'> \n",
      ".per_tensor_affine == torch.per_tensor_affine <class 'torch.qscheme'> \n",
      ".per_tensor_symmetric == torch.per_tensor_symmetric <class 'torch.qscheme'> \n",
      ".pinverse == <built-in method pinverse of type object at 0x10e2335b0> <class 'builtin_function_or_method'> pinverse\n",
      ".pixel_shuffle == <built-in method pixel_shuffle of type object at 0x10e2335b0> <class 'builtin_function_or_method'> pixel_shuffle\n",
      ".platform == <module 'platform' from '/Users/madhavajay/.pyenv/versions/3.6.11/lib/python3.6/platform.py'> <class 'module'> platform\n",
      ".poisson == <built-in method poisson of type object at 0x10e2335b0> <class 'builtin_function_or_method'> poisson\n",
      ".poisson_nll_loss == <built-in method poisson_nll_loss of type object at 0x10e2335b0> <class 'builtin_function_or_method'> poisson_nll_loss\n",
      ".polygamma == <built-in method polygamma of type object at 0x10e2335b0> <class 'builtin_function_or_method'> polygamma\n",
      ".pow == <built-in method pow of type object at 0x10e2335b0> <class 'builtin_function_or_method'> pow\n",
      "Cant find pow in native_func_dict\n",
      "Cant find pow in native_func_dict\n",
      "Cant find pow in native_func_dict\n",
      ".prelu == <built-in method prelu of type object at 0x10e2335b0> <class 'builtin_function_or_method'> prelu\n",
      ".prepare_multiprocessing_environment == <function prepare_multiprocessing_environment at 0x10a118ae8> <class 'function'> prepare_multiprocessing_environment\n",
      ".preserve_format == torch.preserve_format <class 'torch.memory_format'> \n",
      ".prod == <built-in method prod of type object at 0x10e2335b0> <class 'builtin_function_or_method'> prod\n",
      ".promote_types == <built-in method promote_types of type object at 0x10e2335b0> <class 'builtin_function_or_method'> promote_types\n",
      ".q_per_channel_axis == <built-in method q_per_channel_axis of type object at 0x10e2335b0> <class 'builtin_function_or_method'> q_per_channel_axis\n",
      ".q_per_channel_scales == <built-in method q_per_channel_scales of type object at 0x10e2335b0> <class 'builtin_function_or_method'> q_per_channel_scales\n",
      ".q_per_channel_zero_points == <built-in method q_per_channel_zero_points of type object at 0x10e2335b0> <class 'builtin_function_or_method'> q_per_channel_zero_points\n",
      ".q_scale == <built-in method q_scale of type object at 0x10e2335b0> <class 'builtin_function_or_method'> q_scale\n",
      ".q_zero_point == <built-in method q_zero_point of type object at 0x10e2335b0> <class 'builtin_function_or_method'> q_zero_point\n",
      ".qint32 == torch.qint32 <class 'torch.dtype'> \n",
      ".qint8 == torch.qint8 <class 'torch.dtype'> \n",
      ".qr == <built-in method qr of type object at 0x10e2335b0> <class 'builtin_function_or_method'> qr\n",
      ".qscheme == <class 'torch.qscheme'> <class 'type'> qscheme\n",
      "Hierarchy: [<class 'torch.qscheme'>, <class 'object'>]\n",
      ".quantization == <module 'torch.quantization' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/quantization/__init__.py'> <class 'module'> torch.quantization\n",
      ".quantize_per_channel == <built-in method quantize_per_channel of type object at 0x10e2335b0> <class 'builtin_function_or_method'> quantize_per_channel\n",
      ".quantize_per_tensor == <built-in method quantize_per_tensor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> quantize_per_tensor\n",
      ".quantized_batch_norm == <built-in method quantized_batch_norm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> quantized_batch_norm\n",
      ".quantized_gru == <built-in method quantized_gru of PyCapsule object at 0x130891cf0> <class 'builtin_function_or_method'> quantized_gru\n",
      "Cant find quantized_gru in native_func_dict\n",
      "Cant find quantized_gru in native_func_dict\n",
      "Cant find quantized_gru in native_func_dict\n",
      ".quantized_gru_cell == <built-in method quantized_gru_cell of type object at 0x10e2335b0> <class 'builtin_function_or_method'> quantized_gru_cell\n",
      ".quantized_lstm == <built-in method quantized_lstm of PyCapsule object at 0x130891cc0> <class 'builtin_function_or_method'> quantized_lstm\n",
      "Cant find quantized_lstm in native_func_dict\n",
      "Cant find quantized_lstm in native_func_dict\n",
      "Cant find quantized_lstm in native_func_dict\n",
      ".quantized_lstm_cell == <built-in method quantized_lstm_cell of type object at 0x10e2335b0> <class 'builtin_function_or_method'> quantized_lstm_cell\n",
      ".quantized_max_pool2d == <built-in method quantized_max_pool2d of type object at 0x10e2335b0> <class 'builtin_function_or_method'> quantized_max_pool2d\n",
      ".quantized_rnn_relu_cell == <built-in method quantized_rnn_relu_cell of type object at 0x10e2335b0> <class 'builtin_function_or_method'> quantized_rnn_relu_cell\n",
      ".quantized_rnn_tanh_cell == <built-in method quantized_rnn_tanh_cell of type object at 0x10e2335b0> <class 'builtin_function_or_method'> quantized_rnn_tanh_cell\n",
      ".quasirandom == <module 'torch.quasirandom' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/quasirandom.py'> <class 'module'> torch.quasirandom\n",
      ".quint8 == torch.quint8 <class 'torch.dtype'> \n",
      ".rad2deg == <built-in method rad2deg of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rad2deg\n",
      ".rad2deg_ == <built-in method rad2deg_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rad2deg_\n",
      ".rand == <built-in method rand of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rand\n",
      ".rand_like == <built-in method rand_like of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rand_like\n",
      ".randint == <built-in method randint of type object at 0x10e2335b0> <class 'builtin_function_or_method'> randint\n",
      ".randint_like == <built-in method randint_like of type object at 0x10e2335b0> <class 'builtin_function_or_method'> randint_like\n",
      ".randn == <built-in method randn of type object at 0x10e2335b0> <class 'builtin_function_or_method'> randn\n",
      ".randn_like == <built-in method randn_like of type object at 0x10e2335b0> <class 'builtin_function_or_method'> randn_like\n",
      ".random == <module 'torch.random' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/random.py'> <class 'module'> torch.random\n",
      ".randperm == <built-in method randperm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> randperm\n",
      ".range == <built-in method range of type object at 0x10e2335b0> <class 'builtin_function_or_method'> range\n",
      ".real == <built-in method real of type object at 0x10e2335b0> <class 'builtin_function_or_method'> real\n",
      ".reciprocal == <built-in method reciprocal of type object at 0x10e2335b0> <class 'builtin_function_or_method'> reciprocal\n",
      ".reciprocal_ == <built-in method reciprocal_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> reciprocal_\n",
      ".relu == <built-in method relu of type object at 0x10e2335b0> <class 'builtin_function_or_method'> relu\n",
      ".relu_ == <built-in method relu_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> relu_\n",
      ".remainder == <built-in method remainder of type object at 0x10e2335b0> <class 'builtin_function_or_method'> remainder\n",
      "Cant find remainder in native_func_dict\n",
      "Cant find remainder in native_func_dict\n",
      "Cant find remainder in native_func_dict\n",
      ".renorm == <built-in method renorm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> renorm\n",
      ".repeat_interleave == <built-in method repeat_interleave of type object at 0x10e2335b0> <class 'builtin_function_or_method'> repeat_interleave\n",
      "Cant find repeat_interleave in native_func_dict\n",
      "Cant find repeat_interleave in native_func_dict\n",
      "Cant find repeat_interleave in native_func_dict\n",
      ".reshape == <built-in method reshape of type object at 0x10e2335b0> <class 'builtin_function_or_method'> reshape\n",
      ".resize_as_ == <built-in method resize_as_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> resize_as_\n",
      ".result_type == <built-in method result_type of type object at 0x10e2335b0> <class 'builtin_function_or_method'> result_type\n",
      "Cant find result_type in native_func_dict\n",
      "Cant find result_type in native_func_dict\n",
      "Cant find result_type in native_func_dict\n",
      ".rfft == <built-in method rfft of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rfft\n",
      ".rnn_relu == <built-in method rnn_relu of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rnn_relu\n",
      "Cant find rnn_relu in native_func_dict\n",
      "Cant find rnn_relu in native_func_dict\n",
      "Cant find rnn_relu in native_func_dict\n",
      ".rnn_relu_cell == <built-in method rnn_relu_cell of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rnn_relu_cell\n",
      ".rnn_tanh == <built-in method rnn_tanh of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rnn_tanh\n",
      "Cant find rnn_tanh in native_func_dict\n",
      "Cant find rnn_tanh in native_func_dict\n",
      "Cant find rnn_tanh in native_func_dict\n",
      ".rnn_tanh_cell == <built-in method rnn_tanh_cell of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rnn_tanh_cell\n",
      ".roll == <built-in method roll of type object at 0x10e2335b0> <class 'builtin_function_or_method'> roll\n",
      ".rot90 == <built-in method rot90 of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rot90\n",
      ".round == <built-in method round of type object at 0x10e2335b0> <class 'builtin_function_or_method'> round\n",
      ".round_ == <built-in method round_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> round_\n",
      ".rrelu == <built-in method rrelu of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rrelu\n",
      ".rrelu_ == <built-in method rrelu_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rrelu_\n",
      ".rsqrt == <built-in method rsqrt of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rsqrt\n",
      ".rsqrt_ == <built-in method rsqrt_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rsqrt_\n",
      ".rsub == <built-in method rsub of type object at 0x10e2335b0> <class 'builtin_function_or_method'> rsub\n",
      "Cant find rsub in native_func_dict\n",
      "Cant find rsub in native_func_dict\n",
      "Cant find rsub in native_func_dict\n",
      ".saddmm == <built-in method saddmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> saddmm\n",
      "Cant find saddmm in native_func_dict\n",
      "Cant find saddmm in native_func_dict\n",
      "Cant find saddmm in native_func_dict\n",
      ".save == <function save at 0x130251048> <class 'function'> save\n",
      ".scalar_tensor == <built-in method scalar_tensor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> scalar_tensor\n",
      ".scatter == <built-in method scatter of type object at 0x10e2335b0> <class 'builtin_function_or_method'> scatter\n",
      "Cant find scatter in native_func_dict\n",
      "Cant find scatter in native_func_dict\n",
      "Cant find scatter in native_func_dict\n",
      ".scatter_add == <built-in method scatter_add of type object at 0x10e2335b0> <class 'builtin_function_or_method'> scatter_add\n",
      ".searchsorted == <built-in method searchsorted of type object at 0x10e2335b0> <class 'builtin_function_or_method'> searchsorted\n",
      "Cant find searchsorted in native_func_dict\n",
      "Cant find searchsorted in native_func_dict\n",
      "Cant find searchsorted in native_func_dict\n",
      ".seed == <function seed at 0x12e209ea0> <class 'function'> seed\n",
      ".select == <built-in method select of type object at 0x10e2335b0> <class 'builtin_function_or_method'> select\n",
      "Cant find select in native_func_dict\n",
      "Cant find select in native_func_dict\n",
      "Cant find select in native_func_dict\n",
      ".selu == <built-in method selu of type object at 0x10e2335b0> <class 'builtin_function_or_method'> selu\n",
      ".selu_ == <built-in method selu_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> selu_\n",
      ".serialization == <module 'torch.serialization' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/serialization.py'> <class 'module'> torch.serialization\n",
      ".set_anomaly_enabled == <built-in function set_anomaly_enabled> <class 'builtin_function_or_method'> set_anomaly_enabled\n",
      "Cant find set_anomaly_enabled in native_func_dict\n",
      "Cant find set_anomaly_enabled in native_func_dict\n",
      "Cant find set_anomaly_enabled in native_func_dict\n",
      ".set_autocast_enabled == <built-in function set_autocast_enabled> <class 'builtin_function_or_method'> set_autocast_enabled\n",
      "Cant find set_autocast_enabled in native_func_dict\n",
      "Cant find set_autocast_enabled in native_func_dict\n",
      "Cant find set_autocast_enabled in native_func_dict\n",
      ".set_default_dtype == <function set_default_dtype at 0x12de6ee18> <class 'function'> set_default_dtype\n",
      ".set_default_tensor_type == <function set_default_tensor_type at 0x10a7750d0> <class 'function'> set_default_tensor_type\n",
      ".set_flush_denormal == <built-in function set_flush_denormal> <class 'builtin_function_or_method'> set_flush_denormal\n",
      "Cant find set_flush_denormal in native_func_dict\n",
      "Cant find set_flush_denormal in native_func_dict\n",
      "Cant find set_flush_denormal in native_func_dict\n",
      ".set_grad_enabled == <class 'torch.autograd.grad_mode.set_grad_enabled'> <class 'type'> set_grad_enabled\n",
      "Hierarchy: [<class 'torch.autograd.grad_mode.set_grad_enabled'>, <class 'object'>]\n",
      ".set_num_interop_threads == <built-in function set_num_interop_threads> <class 'builtin_function_or_method'> set_num_interop_threads\n",
      "Cant find set_num_interop_threads in native_func_dict\n",
      "Cant find set_num_interop_threads in native_func_dict\n",
      "Cant find set_num_interop_threads in native_func_dict\n",
      ".set_num_threads == <built-in function set_num_threads> <class 'builtin_function_or_method'> set_num_threads\n",
      "Cant find set_num_threads in native_func_dict\n",
      "Cant find set_num_threads in native_func_dict\n",
      "Cant find set_num_threads in native_func_dict\n",
      ".set_printoptions == <function set_printoptions at 0x130251620> <class 'function'> set_printoptions\n",
      ".set_rng_state == <function set_rng_state at 0x12e209d08> <class 'function'> set_rng_state\n",
      ".short == torch.int16 <class 'torch.dtype'> \n",
      ".sigmoid == <built-in method sigmoid of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sigmoid\n",
      ".sigmoid_ == <built-in method sigmoid_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sigmoid_\n",
      ".sign == <built-in method sign of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sign\n",
      ".sin == <built-in method sin of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sin\n",
      ".sin_ == <built-in method sin_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sin_\n",
      ".sinh == <built-in method sinh of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sinh\n",
      ".sinh_ == <built-in method sinh_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sinh_\n",
      ".slogdet == <built-in method slogdet of type object at 0x10e2335b0> <class 'builtin_function_or_method'> slogdet\n",
      ".smm == <built-in method smm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> smm\n",
      ".softmax == <built-in method softmax of type object at 0x10e2335b0> <class 'builtin_function_or_method'> softmax\n",
      "Cant find softmax in native_func_dict\n",
      "Cant find softmax in native_func_dict\n",
      "Cant find softmax in native_func_dict\n",
      ".solve == <built-in method solve of type object at 0x10e2335b0> <class 'builtin_function_or_method'> solve\n",
      ".sort == <built-in method sort of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sort\n",
      ".sparse == <module 'torch.sparse' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/sparse/__init__.py'> <class 'module'> torch.sparse\n",
      ".sparse_coo == torch.sparse_coo <class 'torch.layout'> \n",
      ".sparse_coo_tensor == <built-in method sparse_coo_tensor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sparse_coo_tensor\n",
      "Cant find sparse_coo_tensor in native_func_dict\n",
      "Cant find sparse_coo_tensor in native_func_dict\n",
      "Cant find sparse_coo_tensor in native_func_dict\n",
      ".split == <function split at 0x13064d510> <class 'function'> split\n",
      ".split_with_sizes == <built-in method split_with_sizes of type object at 0x10e2335b0> <class 'builtin_function_or_method'> split_with_sizes\n",
      ".spmm == <built-in method spmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> spmm\n",
      "Cant find spmm in native_func_dict\n",
      "Cant find spmm in native_func_dict\n",
      "Cant find spmm in native_func_dict\n",
      ".sqrt == <built-in method sqrt of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sqrt\n",
      ".sqrt_ == <built-in method sqrt_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sqrt_\n",
      ".square == <built-in method square of type object at 0x10e2335b0> <class 'builtin_function_or_method'> square\n",
      ".square_ == <built-in method square_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> square_\n",
      ".squeeze == <built-in method squeeze of type object at 0x10e2335b0> <class 'builtin_function_or_method'> squeeze\n",
      ".sspaddmm == <built-in method sspaddmm of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sspaddmm\n",
      ".stack == <built-in method stack of type object at 0x10e2335b0> <class 'builtin_function_or_method'> stack\n",
      ".std == <built-in method std of type object at 0x10e2335b0> <class 'builtin_function_or_method'> std\n",
      ".std_mean == <built-in method std_mean of type object at 0x10e2335b0> <class 'builtin_function_or_method'> std_mean\n",
      ".stft == <function stft at 0x13064d840> <class 'function'> stft\n",
      ".storage == <module 'torch.storage' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/storage.py'> <class 'module'> torch.storage\n",
      ".strided == torch.strided <class 'torch.layout'> \n",
      ".sub == <built-in method sub of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sub\n",
      "Cant find sub in native_func_dict\n",
      "Cant find sub in native_func_dict\n",
      "Cant find sub in native_func_dict\n",
      ".sum == <built-in method sum of type object at 0x10e2335b0> <class 'builtin_function_or_method'> sum\n",
      ".svd == <built-in method svd of type object at 0x10e2335b0> <class 'builtin_function_or_method'> svd\n",
      ".svd_lowrank == <function svd_lowrank at 0x13064d378> <class 'function'> svd_lowrank\n",
      ".symeig == <built-in method symeig of type object at 0x10e2335b0> <class 'builtin_function_or_method'> symeig\n",
      ".sys == <module 'sys' (built-in)> <class 'module'> sys\n",
      ".t == <built-in method t of type object at 0x10e2335b0> <class 'builtin_function_or_method'> t\n",
      ".take == <built-in method take of type object at 0x10e2335b0> <class 'builtin_function_or_method'> take\n",
      ".tan == <built-in method tan of type object at 0x10e2335b0> <class 'builtin_function_or_method'> tan\n",
      ".tan_ == <built-in method tan_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> tan_\n",
      ".tanh == <built-in method tanh of type object at 0x10e2335b0> <class 'builtin_function_or_method'> tanh\n",
      ".tanh_ == <built-in method tanh_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> tanh_\n",
      ".tensor == <built-in method tensor of type object at 0x10e2335b0> <class 'builtin_function_or_method'> tensor\n",
      "Cant find tensor in native_func_dict\n",
      "Cant find tensor in native_func_dict\n",
      "Cant find tensor in native_func_dict\n",
      ".tensordot == <function tensordot at 0x130650158> <class 'function'> tensordot\n",
      ".testing == <module 'torch.testing' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/testing/__init__.py'> <class 'module'> torch.testing\n",
      ".threshold == <built-in method threshold of type object at 0x10e2335b0> <class 'builtin_function_or_method'> threshold\n",
      ".threshold_ == <built-in method threshold_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> threshold_\n",
      ".topk == <built-in method topk of type object at 0x10e2335b0> <class 'builtin_function_or_method'> topk\n",
      ".torch == <module 'torch' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/__init__.py'> <class 'module'> torch\n",
      ".trace == <built-in method trace of type object at 0x10e2335b0> <class 'builtin_function_or_method'> trace\n",
      ".transpose == <built-in method transpose of type object at 0x10e2335b0> <class 'builtin_function_or_method'> transpose\n",
      "Cant find transpose in native_func_dict\n",
      "Cant find transpose in native_func_dict\n",
      "Cant find transpose in native_func_dict\n",
      ".trapz == <built-in method trapz of type object at 0x10e2335b0> <class 'builtin_function_or_method'> trapz\n",
      "Cant find trapz in native_func_dict\n",
      "Cant find trapz in native_func_dict\n",
      "Cant find trapz in native_func_dict\n",
      ".triangular_solve == <built-in method triangular_solve of type object at 0x10e2335b0> <class 'builtin_function_or_method'> triangular_solve\n",
      ".tril == <built-in method tril of type object at 0x10e2335b0> <class 'builtin_function_or_method'> tril\n",
      ".tril_indices == <built-in method tril_indices of type object at 0x10e2335b0> <class 'builtin_function_or_method'> tril_indices\n",
      ".triplet_margin_loss == <built-in method triplet_margin_loss of type object at 0x10e2335b0> <class 'builtin_function_or_method'> triplet_margin_loss\n",
      ".triu == <built-in method triu of type object at 0x10e2335b0> <class 'builtin_function_or_method'> triu\n",
      ".triu_indices == <built-in method triu_indices of type object at 0x10e2335b0> <class 'builtin_function_or_method'> triu_indices\n",
      ".true_divide == <built-in method true_divide of type object at 0x10e2335b0> <class 'builtin_function_or_method'> true_divide\n",
      "Cant find true_divide in native_func_dict\n",
      "Cant find true_divide in native_func_dict\n",
      "Cant find true_divide in native_func_dict\n",
      ".trunc == <built-in method trunc of type object at 0x10e2335b0> <class 'builtin_function_or_method'> trunc\n",
      ".trunc_ == <built-in method trunc_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> trunc_\n",
      ".typename == <function typename at 0x10a118f28> <class 'function'> typename\n",
      ".types == <module 'torch.types' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/types.py'> <class 'module'> torch.types\n",
      ".uint8 == torch.uint8 <class 'torch.dtype'> \n",
      ".unbind == <built-in method unbind of type object at 0x10e2335b0> <class 'builtin_function_or_method'> unbind\n",
      "Cant find unbind in native_func_dict\n",
      "Cant find unbind in native_func_dict\n",
      "Cant find unbind in native_func_dict\n",
      ".unique == <function boolean_dispatch.<locals>.fn at 0x13064dd08> <class 'function'> unique\n",
      ".unique_consecutive == <function boolean_dispatch.<locals>.fn at 0x1306500d0> <class 'function'> unique_consecutive\n",
      ".unsqueeze == <built-in method unsqueeze of type object at 0x10e2335b0> <class 'builtin_function_or_method'> unsqueeze\n",
      ".utils == <module 'torch.utils' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/utils/__init__.py'> <class 'module'> torch.utils\n",
      ".vander == <built-in method vander of type object at 0x10e2335b0> <class 'builtin_function_or_method'> vander\n",
      ".var == <built-in method var of type object at 0x10e2335b0> <class 'builtin_function_or_method'> var\n",
      ".var_mean == <built-in method var_mean of type object at 0x10e2335b0> <class 'builtin_function_or_method'> var_mean\n",
      ".version == <module 'torch.version' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/version.py'> <class 'module'> torch.version\n",
      ".view_as_complex == <built-in method view_as_complex of type object at 0x10e2335b0> <class 'builtin_function_or_method'> view_as_complex\n",
      ".view_as_real == <built-in method view_as_real of type object at 0x10e2335b0> <class 'builtin_function_or_method'> view_as_real\n",
      ".wait == <built-in method wait of PyCapsule object at 0x10a73d030> <class 'builtin_function_or_method'> wait\n",
      "Cant find wait in native_func_dict\n",
      "Cant find wait in native_func_dict\n",
      "Cant find wait in native_func_dict\n",
      ".where == <built-in method where of type object at 0x10e2335b0> <class 'builtin_function_or_method'> where\n",
      ".zero_ == <built-in method zero_ of type object at 0x10e2335b0> <class 'builtin_function_or_method'> zero_\n",
      ".zeros == <built-in method zeros of type object at 0x10e2335b0> <class 'builtin_function_or_method'> zeros\n",
      ".zeros_like == <built-in method zeros_like of type object at 0x10e2335b0> <class 'builtin_function_or_method'> zeros_like\n"
     ]
    }
   ],
   "source": [
    "missing_attr_matches = []\n",
    "def detect_attrs(obj) -> dict:\n",
    "    path_dict = {}\n",
    "    attr_dict = {\"module\":{}, \"enum\": {}, \"class\": {}, \"method\":{}, \"function\":{}, \"property\":{}, \"builtin\": {}}\n",
    "    \n",
    "    count = 0\n",
    "    for key in dir(obj):\n",
    "        try:\n",
    "            if key.startswith(\"__\"):\n",
    "                continue\n",
    "            count += 1\n",
    "    #         if count > 20:\n",
    "    #             break\n",
    "            attr = getattr(obj, key)\n",
    "            prop_types = whatis(attr, key)\n",
    "            if \"iscenum\" in prop_types and \"ispybind11\" not in prop_types:\n",
    "                attr_dict[\"enum\"][type(attr)] = {\"members\":type(attr).__members__}\n",
    "                path_dict[key] = {\"type\": \"enum\", \"members\":type(attr).__members__}\n",
    "            if \"isclass\" in prop_types and \"ispybind11\" not in prop_types:\n",
    "                attr_dict[\"class\"][key] = get_signature(attr)\n",
    "                path_dict[key] = {\"type\": \"class\", \"init\":get_signature(attr)}\n",
    "            if \"isclass\" in prop_types and \"ispybind11\" in prop_types:\n",
    "                attr_dict[\"class\"][key] = get_c_class_signature(attr)\n",
    "                path_dict[key] = {\"type\": \"class\", \"init\":get_c_class_signature(attr)}\n",
    "\n",
    "            if \"isbuiltin\" in prop_types and \"isroutine\" in prop_types:\n",
    "                if get_native_signature(key) is None or get_native_return_type(key) is None:\n",
    "                    missing_attr_matches.append(key)\n",
    "                path_dict[key] = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"signature\": get_native_signature(key),\n",
    "                    \"return_type\": get_native_return_type(key)\n",
    "                }\n",
    "            if \"isproperty\" in prop_types:\n",
    "                attr_dict[\"property\"][key] = type(attr)\n",
    "                path_dict[key] = {\"type\": \"property\", \"return_type\":type(attr)}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Exception with {key}. {e}\")\n",
    "    return path_dict\n",
    "\n",
    "attrs = detect_attrs(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add',\n",
       " 'as_tensor',\n",
       " 'autocast_decrement_nesting',\n",
       " 'autocast_increment_nesting',\n",
       " 'bitwise_and',\n",
       " 'bitwise_or',\n",
       " 'bitwise_xor',\n",
       " 'bucketize',\n",
       " 'clear_autocast_cache',\n",
       " 'conv_transpose2d',\n",
       " 'conv_transpose3d',\n",
       " 'ctc_loss',\n",
       " 'dequantize',\n",
       " 'div',\n",
       " 'dsmm',\n",
       " 'empty',\n",
       " 'eq',\n",
       " 'fill_',\n",
       " 'flatten',\n",
       " 'fmod',\n",
       " 'fork',\n",
       " 'from_numpy',\n",
       " 'ge',\n",
       " 'get_default_dtype',\n",
       " 'get_device',\n",
       " 'get_num_interop_threads',\n",
       " 'get_num_threads',\n",
       " 'gru',\n",
       " 'gt',\n",
       " 'hsmm',\n",
       " 'import_ir_module',\n",
       " 'import_ir_module_from_buffer',\n",
       " 'index_fill',\n",
       " 'init_num_threads',\n",
       " 'is_anomaly_enabled',\n",
       " 'is_autocast_enabled',\n",
       " 'is_grad_enabled',\n",
       " 'le',\n",
       " 'lerp',\n",
       " 'log_softmax',\n",
       " 'lstm',\n",
       " 'lt',\n",
       " 'masked_fill',\n",
       " 'merge_type_from_type_comment',\n",
       " 'mul',\n",
       " 'ne',\n",
       " 'normal',\n",
       " 'numel',\n",
       " 'parse_ir',\n",
       " 'parse_schema',\n",
       " 'parse_type_comment',\n",
       " 'pow',\n",
       " 'quantized_gru',\n",
       " 'quantized_lstm',\n",
       " 'remainder',\n",
       " 'repeat_interleave',\n",
       " 'result_type',\n",
       " 'rnn_relu',\n",
       " 'rnn_tanh',\n",
       " 'rsub',\n",
       " 'saddmm',\n",
       " 'scatter',\n",
       " 'searchsorted',\n",
       " 'select',\n",
       " 'set_anomaly_enabled',\n",
       " 'set_autocast_enabled',\n",
       " 'set_flush_denormal',\n",
       " 'set_num_interop_threads',\n",
       " 'set_num_threads',\n",
       " 'softmax',\n",
       " 'sparse_coo_tensor',\n",
       " 'spmm',\n",
       " 'sub',\n",
       " 'tensor',\n",
       " 'transpose',\n",
       " 'trapz',\n",
       " 'true_divide',\n",
       " 'unbind',\n",
       " 'wait']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_attr_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_attrs(attrs, key: Optional[str] = None):\n",
    "    for attr, meta in attrs.items():\n",
    "        if meta[\"type\"] == \"enum\":\n",
    "            meta_info = meta[\"members\"]\n",
    "        if meta[\"type\"] == \"class\":\n",
    "            meta_info = meta[\"init\"]\n",
    "        if meta[\"type\"] == \"function\":\n",
    "            sig = meta[\"signature\"]\n",
    "            return_type = meta[\"return_type\"]\n",
    "            meta_info = f\"{sig} => {return_type}\"\n",
    "        if meta[\"type\"] == \"property\":\n",
    "            return_type = meta[\"return_type\"]\n",
    "            meta_info = f\"=> {return_type}\"\n",
    "        t = meta[\"type\"]\n",
    "        path = attr\n",
    "        if key is not None:\n",
    "            path = f\"{key}.{path}\"\n",
    "        print(f\"torch.{path} - {t} ({meta_info})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConstQuantizerPtr\n",
    "# Device\n",
    "# Dimname\n",
    "# DONE Generator\n",
    "# Layout\n",
    "# MemoryFormat\n",
    "# QScheme\n",
    "# torch.Storage\n",
    "# torch.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".index == 1 <class 'int'> \n",
      ".type == cuda <class 'str'> \n",
      "torch.device.index - property (=> <class 'int'>)\n",
      "torch.device.type - property (=> <class 'str'>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.device('cuda', 1)\n",
    "d = detect_attrs(a)\n",
    "print_attrs(d, \"device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issubclass(type(torch.AggregationType), Enum)\n",
    "print(torch.AggregationType)\n",
    "print(torch.AggregationType.SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.Tensor.set_.__doc__)\n",
    "print(get_native_signature(\"set_\"))\n",
    "print(get_native_return_type(\"set_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatis(torch.SUM)\n",
    "torch.SUM.__members__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch._C.AggregationType(0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.AVG.__members__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whatis(torch._C.AggregationType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.AVG.__members__)\n",
    "# print(torch.AVG.mro())\n",
    "\n",
    "#callable(torch.AVG)\n",
    "#inspect.getmro(torch.AVG)\n",
    "id(torch.AVG)\n",
    "id(torch.SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.device('cpu')\n",
    "thing = getattr(a, \"index\")\n",
    "print(type(thing))\n",
    "print(f\"ismodule: {inspect.ismodule(thing)}\")\n",
    "print(f\"isclass: {inspect.isclass(thing)}\")\n",
    "print(f\"ismethod: {inspect.ismethod(thing)}\")\n",
    "print(f\"ismethoddescriptor: {inspect.ismethoddescriptor(thing)}\")\n",
    "print(f\"isfunction: {inspect.isfunction(thing)}\")\n",
    "print(f\"isgeneratorfunction: {inspect.isgeneratorfunction(thing)}\")\n",
    "print(f\"isgenerator: {inspect.isgenerator(thing)}\")\n",
    "print(f\"isbuiltin: {inspect.isbuiltin(thing)}\")\n",
    "print(f\"isroutine: {inspect.isroutine(thing)}\")\n",
    "print(f\"isdatadescriptor: {inspect.isroutine(thing)}\")\n",
    "print(f\"isgetsetdescriptor: {inspect.isroutine(thing)}\")\n",
    "print(f\"ismemberdescriptor: {inspect.isroutine(thing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_class_w_instance(klass, inst):\n",
    "    a = set(list(dir(klass)))\n",
    "    b = set(list(dir(inst)))\n",
    "\n",
    "    print(a.difference(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_class_w_instance(torch.Tensor, torch.Tensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
