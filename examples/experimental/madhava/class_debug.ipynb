{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import inspect\n",
    "from enum import Enum\n",
    "\n",
    "import torch\n",
    "# torch.set_printoptions(profile=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is based off the docs here and the file native_functions.yaml in the pytorch github repo\n",
    "# https://github.com/pytorch/pytorch/tree/master/aten/src/ATen/native"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  267k  100  267k    0     0   492k      0 --:--:-- --:--:-- --:--:--  491k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/pytorch/pytorch/1.6/aten/src/ATen/native/native_functions.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"native_functions.yaml\") as file:\n",
    "    native_functions = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'func': '_cast_Byte(Tensor self, bool non_blocking=False) -> Tensor', 'use_c10_dispatcher': 'full', 'variants': 'function'}\n",
      "('_cast_Byte', 'Tensor self, bool non_blocking=False', 'Tensor')\n"
     ]
    }
   ],
   "source": [
    "# example structure\n",
    "# - func: add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)\n",
    "#   variants: function\n",
    "#   dispatch:\n",
    "#     CPU: add_relu_out\n",
    "\n",
    "def process_yaml_entry(native_func):\n",
    "    return_type = native_func[\"func\"].split(\"->\")[-1].strip()\n",
    "\n",
    "    func_name = native_func[\"func\"].split(\"(\")[0].strip()\n",
    "    args_parts = native_func[\"func\"].split(\"->\")[0].strip().split(\"(\")\n",
    "    args = \"\".join(args_parts[1:]).strip().replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    return func_name, args, return_type\n",
    "\n",
    "print(native_functions[0])\n",
    "print(process_yaml_entry(native_functions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tensor', 'Tensor']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_type_to_python(return_type):\n",
    "    return_type = return_type.strip()\n",
    "    if \"()\" == return_type:\n",
    "        return [\"None\"]\n",
    "\n",
    "    return_type = return_type.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    parts = return_type.split(\",\")\n",
    "    clean_parts = []\n",
    "    for part in parts:\n",
    "        clean_part = part.strip()\n",
    "        if clean_part.startswith(\"Tensor\"):\n",
    "            clean_part = \"Tensor\"\n",
    "        # according to the docs Scalar is any kind of numeric in python or a unit tensor\n",
    "        if clean_part == \"Scalar\":\n",
    "            clean_part = \"Union[int, float, complex, Tensor]\"\n",
    "        # it seems like these are the pytorch dtype types\n",
    "        if clean_part == \"ScalarType\":\n",
    "            clean_part = \"torch.dtype\"\n",
    "        if \"[]\" in part:\n",
    "            clean_part = f\"List[{clean_part}]\"\n",
    "        \n",
    "        clean_parts.append(clean_part)\n",
    "    return clean_parts\n",
    "    \n",
    "    if any(prim == return_type for prim in [\"int\", \"float\", \"bool\"]):\n",
    "        return return_type\n",
    "    \n",
    "return_type_to_python(\"(Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['end: Union[int, float, complex, Tensor]',\n",
       " 'dtype: Optional[torch.dtype] = None',\n",
       " 'layout: Optional[Layout] = None',\n",
       " 'device: Optional[Device] = None',\n",
       " 'pin_memory: Optional[bool] = None']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def signature_type_to_python(signature):\n",
    "    signature = signature.strip()\n",
    "\n",
    "    signature = signature.replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    clean_parts = []\n",
    "\n",
    "    if signature == \"\":\n",
    "        return [\"None\"]\n",
    "    # default int[] value [0,1] should not add space after comma, since native_parse.py uses ', ' to split args\n",
    "    for part in signature.split(\", \"):\n",
    "        name = None\n",
    "        default = None\n",
    "        clean_part = part.strip()\n",
    "        if clean_part == \"*\":\n",
    "            # all args after this must be kwargs\n",
    "            # not sure we care right now\n",
    "#             clean_parts.append(clean_part)\n",
    "            continue\n",
    "        subpart = clean_part.split(\" \")\n",
    "        if len(subpart) > 1:\n",
    "            subsub = subpart[1].split(\"=\")\n",
    "            name = subsub[0]\n",
    "            if len(subsub) > 1:\n",
    "                default = subsub[1]\n",
    "\n",
    "        if subpart[0].startswith(\"Tensor\"):\n",
    "            clean_part = \"Tensor\"\n",
    "        else:\n",
    "            clean_part = subpart[0]\n",
    "\n",
    "        if clean_part.startswith(\"ScalarType\"):\n",
    "            clean_part = \"torch.dtype\"\n",
    "            \n",
    "        if clean_part.startswith(\"Scalar\"):\n",
    "            clean_part = \"Union[int, float, complex, Tensor]\"\n",
    "\n",
    "        if \"[\" in subpart[0]:\n",
    "            t = clean_part.split(\"[\")[0]\n",
    "            clean_part = f\"List[{t}]\"\n",
    "\n",
    "        if \"?\" in part:\n",
    "            clean_part = f\"Optional[{clean_part}]\".replace(\"?\", \"\")\n",
    "            \n",
    "        if name is not None:\n",
    "            clean_part = f\"{name}: {clean_part}\"\n",
    "            \n",
    "        if default is not None:\n",
    "            clean_part = f\"{clean_part} = {default}\"\n",
    "\n",
    "        clean_parts.append(clean_part)\n",
    "        \n",
    "    if len(clean_parts) == 0:\n",
    "        return [\"None\"]\n",
    "    return clean_parts\n",
    "\n",
    "signature_type_to_python( 'Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func_yaml(native_functions):\n",
    "    unique_return_types = set()\n",
    "    unique_function_args = set()\n",
    "    func_dict = {}\n",
    "    for native_func in native_functions:\n",
    "        func_name, args, return_type = process_yaml_entry(native_func)\n",
    "        unique_return_types.add(return_type)\n",
    "        unique_function_args.add(args)\n",
    "\n",
    "        if func_name in func_dict:\n",
    "            print(f\"Error duplicate func_name: {func_name}\")\n",
    "        func_dict[func_name] = {\"args\": signature_type_to_python(args), \"return_type\": return_type_to_python(return_type)}\n",
    "\n",
    "    return func_dict, unique_return_types, unique_function_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1527\n"
     ]
    }
   ],
   "source": [
    "native_func_dict, unique_return_types, unique_function_args = process_func_yaml(native_functions)\n",
    "print(len(native_func_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'args': ['self: Tensor', 'other: Tensor'], 'return_type': ['Tensor']}\n"
     ]
    }
   ],
   "source": [
    "print(native_func_dict[\"div_.Tensor\"])\n",
    "\n",
    "type_json = json.dumps(native_func_dict)\n",
    "with open(\"torch_types.json\", \"w\") as f:\n",
    "    f.write(type_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'()',\n",
       " '(Tensor Q, Tensor R)',\n",
       " '(Tensor U, Tensor S, Tensor V)',\n",
       " '(Tensor a, Tensor tau)',\n",
       " '(Tensor eigenvalues, Tensor eigenvectors)',\n",
       " '(Tensor grad_input, Tensor grad_weight)',\n",
       " '(Tensor grad_input, Tensor grad_weight, Tensor grad_bias)',\n",
       " '(Tensor grad_self, Tensor grad_grid)',\n",
       " '(Tensor output, Tensor buffer)',\n",
       " '(Tensor output, Tensor finput, Tensor fgrad_input)',\n",
       " '(Tensor output, Tensor is_target)',\n",
       " '(Tensor output, Tensor total_weight)',\n",
       " '(Tensor sign, Tensor logabsdet)',\n",
       " '(Tensor solution, Tensor LU)',\n",
       " '(Tensor solution, Tensor QR)',\n",
       " '(Tensor solution, Tensor cloned_coefficient)',\n",
       " '(Tensor values, Tensor indices)',\n",
       " '(Tensor(a!) Q, Tensor(b!) R)',\n",
       " '(Tensor(a!) U, Tensor(b!) S, Tensor(c!) V)',\n",
       " '(Tensor(a!) a, Tensor(b!) tau)',\n",
       " '(Tensor(a!) eigenvalues, Tensor(b!) eigenvectors)',\n",
       " '(Tensor(a!) solution, Tensor(b!) LU)',\n",
       " '(Tensor(a!) solution, Tensor(b!) QR)',\n",
       " '(Tensor(a!) solution, Tensor(b!) cloned_coefficient)',\n",
       " '(Tensor(a!) values, Tensor(b!) indices)',\n",
       " '(Tensor(a!), Tensor(b!))',\n",
       " '(Tensor(a!), Tensor(b!), Tensor(c!))',\n",
       " '(Tensor, Tensor)',\n",
       " '(Tensor, Tensor, Tensor)',\n",
       " '(Tensor, Tensor, Tensor, Tensor)',\n",
       " '(Tensor, Tensor, Tensor, Tensor, Tensor)',\n",
       " '(Tensor, Tensor, Tensor, Tensor, int)',\n",
       " '(Tensor, Tensor, Tensor, Tensor[])',\n",
       " '(Tensor, Tensor, float, int)',\n",
       " '(float, int)',\n",
       " 'QScheme',\n",
       " 'Scalar',\n",
       " 'ScalarType',\n",
       " 'Tensor',\n",
       " 'Tensor grad_theta',\n",
       " 'Tensor grid',\n",
       " 'Tensor output',\n",
       " 'Tensor(a!)',\n",
       " 'Tensor(a)',\n",
       " 'Tensor(a)[]',\n",
       " 'Tensor[]',\n",
       " 'bool',\n",
       " 'float',\n",
       " 'int'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_return_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('List[Tensor]',),\n",
       " ('None',),\n",
       " ('QScheme',),\n",
       " ('Tensor',),\n",
       " ('Tensor', 'Tensor'),\n",
       " ('Tensor', 'Tensor', 'Tensor'),\n",
       " ('Tensor', 'Tensor', 'Tensor', 'List[Tensor]'),\n",
       " ('Tensor', 'Tensor', 'Tensor', 'Tensor'),\n",
       " ('Tensor', 'Tensor', 'Tensor', 'Tensor', 'Tensor'),\n",
       " ('Tensor', 'Tensor', 'Tensor', 'Tensor', 'int'),\n",
       " ('Tensor', 'Tensor', 'float', 'int'),\n",
       " ('Union[int, float, complex, Tensor]',),\n",
       " ('bool',),\n",
       " ('float',),\n",
       " ('float', 'int'),\n",
       " ('int',),\n",
       " ('torch.dtype',)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_python_return_types = set()\n",
    "for line in [return_type_to_python(t) for t in unique_return_types]:\n",
    "    unique_python_return_types.add(tuple(line))\n",
    "\n",
    "unique_python_return_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensora! out',\n",
       " 'int sparse_dim, int dense_dim, int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=False',\n",
       " 'Tensor self, int[2] padding, *, Tensora! out',\n",
       " 'int n, Tensor self, *, Tensora! out',\n",
       " 'Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1',\n",
       " 'Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean',\n",
       " 'Tensor self, int dim, Tensor index',\n",
       " 'Tensor self, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool benchmark, bool deterministic',\n",
       " 'Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(unique_function_args)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('self: Tensor', 'repeats: List[int]'),\n",
       " ('self: Tensor',\n",
       "  'kernel_size: List[int]',\n",
       "  'stride: List[int] = []',\n",
       "  'padding: List[int] = 0',\n",
       "  'dilation: List[int] = 1',\n",
       "  'ceil_mode: bool = False'),\n",
       " ('self: Tensor', 'scale: Tensor', 'zero_point: Tensor', 'axis: int'),\n",
       " ('self: Tensor',\n",
       "  'batch1: Tensor',\n",
       "  'batch2: Tensor',\n",
       "  'beta: Union[int, float, complex, Tensor] = 1',\n",
       "  'alpha: Union[int, float, complex, Tensor] = 1'),\n",
       " ('self: Tensor',\n",
       "  'dim: List[Dimname]',\n",
       "  'unbiased: bool = True',\n",
       "  'keepdim: bool = False'),\n",
       " ('low: int',\n",
       "  'high: int',\n",
       "  'size: List[int]',\n",
       "  'generator: Optional[Generator]',\n",
       "  'out: Tensor'),\n",
       " ('self: Tensor',\n",
       "  'output_size: List[int]',\n",
       "  'align_corners: bool',\n",
       "  'scales_h: Optional[float] = None',\n",
       "  'scales_w: Optional[float] = None'),\n",
       " ('self: Tensor', 'exponent: Union[int, float, complex, Tensor]'),\n",
       " ('start: Union[int, float, complex, Tensor]',\n",
       "  'end: Union[int, float, complex, Tensor]',\n",
       "  'steps: int = 100',\n",
       "  'out: Tensor'),\n",
       " ('self: Tensor', 'lambd: Union[int, float, complex, Tensor] = 0.5')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_python_arg_signatures = set()\n",
    "for line in [signature_type_to_python(t) for t in unique_function_args]:\n",
    "    unique_python_arg_signatures.add(tuple(line))\n",
    "    \n",
    "print(len(unique_python_arg_signatures))\n",
    "\n",
    "list(unique_python_arg_signatures)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Tensor', 'Tensor', 'List[int]', 'List[int]', 'int', 'int', 'bool'),\n",
       " ('Tensor',\n",
       "  'Tensor',\n",
       "  'List[Tensor]',\n",
       "  'bool',\n",
       "  'int',\n",
       "  'float',\n",
       "  'bool',\n",
       "  'bool',\n",
       "  'bool'),\n",
       " ('Tensor', 'int', 'List[int]'),\n",
       " ('Tensor',\n",
       "  'Tensor',\n",
       "  'Tensor',\n",
       "  'Optional[Tensor]',\n",
       "  'Optional[Tensor]',\n",
       "  'float',\n",
       "  'float',\n",
       "  'Tensor'),\n",
       " ('Union[int, float, complex, Tensor]',\n",
       "  'Union[int, float, complex, Tensor]',\n",
       "  'int',\n",
       "  'float',\n",
       "  'Optional[torch.dtype]',\n",
       "  'Optional[Layout]',\n",
       "  'Optional[Device]',\n",
       "  'Optional[bool]'),\n",
       " ('Tensor', 'List[int]', 'bool', 'Optional[float]'),\n",
       " ('Tensor', 'ConstQuantizerPtr'),\n",
       " ('Tensor', 'Tensor', 'Tensor', 'int'),\n",
       " ('List[int]',\n",
       "  'Optional[torch.dtype]',\n",
       "  'Optional[Layout]',\n",
       "  'Optional[Device]',\n",
       "  'Optional[bool]'),\n",
       " ('Tensor', 'bool', 'bool', 'Tensor', 'Tensor')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg_signatures_no_names = set()\n",
    "for sig in unique_python_arg_signatures:\n",
    "    new_parts = []\n",
    "    for part in sig:\n",
    "        new_part = part.split(\"=\")[0]\n",
    "        new_part = new_part.split(\":\")[-1]\n",
    "        new_part = new_part.strip()\n",
    "        new_parts.append(new_part)\n",
    "    arg_signatures_no_names.add(tuple(new_parts))\n",
    "    \n",
    "print(len(arg_signatures_no_names))\n",
    "\n",
    "list(arg_signatures_no_names)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ConstQuantizerPtr',\n",
       " 'Device',\n",
       " 'Dimname',\n",
       " 'List[Dimname]',\n",
       " 'List[Tensor]',\n",
       " 'List[bool]',\n",
       " 'List[int]',\n",
       " 'MemoryFormat',\n",
       " 'None',\n",
       " 'Optional[Device]',\n",
       " 'Optional[Generator]',\n",
       " 'Optional[Layout]',\n",
       " 'Optional[List[Dimname]]',\n",
       " 'Optional[List[Tensor]]',\n",
       " 'Optional[List[float]]',\n",
       " 'Optional[List[int]]',\n",
       " 'Optional[MemoryFormat]',\n",
       " 'Optional[Tensor]',\n",
       " 'Optional[Union[int, float, complex, Tensor]]',\n",
       " 'Optional[bool]',\n",
       " 'Optional[float]',\n",
       " 'Optional[int]',\n",
       " 'Optional[torch.dtype]',\n",
       " 'QScheme',\n",
       " 'Storage',\n",
       " 'Tensor',\n",
       " 'Union[int, float, complex, Tensor]',\n",
       " 'bool',\n",
       " 'float',\n",
       " 'int',\n",
       " 'str',\n",
       " 'torch.dtype'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_types = set()\n",
    "for sigs in arg_signatures_no_names:\n",
    "    for part in sigs:\n",
    "        all_types.add(part)\n",
    "        \n",
    "for return_types in unique_python_return_types:\n",
    "    for part in return_types:\n",
    "        all_types.add(part)\n",
    "        \n",
    "print(len(all_types))\n",
    "all_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_name_with_qualname(klass: type) -> str:\n",
    "    return f'{klass.__module__}.{klass.__qualname__}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whatis(thing):\n",
    "    name = getattr(thing, \"__name__\", \"\")\n",
    "    print(f\"{thing} {type(thing)} {name}\")\n",
    "    \n",
    "    mro = getattr(thing, \"mro\", None)\n",
    "    if mro is not None:\n",
    "        hierarchy = mro()\n",
    "        print(f\"Hierarchy: {hierarchy}\")\n",
    "    \n",
    "\n",
    "    test_types = [\n",
    "        \"ismodule\", \"isclass\", \"ismethod\", \"ismethoddescriptor\", \"isfunction\", \"isgeneratorfunction\", \"isgenerator\",\n",
    "        \"isbuiltin\", \"isroutine\", \"isdatadescriptor\", \"isgetsetdescriptor\", \"ismemberdescriptor\"\n",
    "    ]\n",
    "    result_ttypes = []\n",
    "    for ttype in test_types:\n",
    "        istype = getattr(inspect, ttype)(thing)\n",
    "        if istype is True:\n",
    "            result_ttypes.append(ttype)\n",
    "\n",
    "    if callable(thing):\n",
    "        result_ttypes.append(\"callable\")\n",
    "       \n",
    "    if issubclass(type(thing), Enum):\n",
    "        result_ttype.append(\"ispyenum\")\n",
    "\n",
    "    members = getattr(thing, \"__members__\", None)\n",
    "    if members is not None:\n",
    "        print(\"members: \", members)\n",
    "        result_ttypes.append(\"iscenum\")\n",
    "    \n",
    "    if \"pybind11_type\" in str(type(thing)):\n",
    "        result_ttypes.append(\"ispybind11\")\n",
    "\n",
    "    return result_ttypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signature(thing):\n",
    "    return inspect.getfullargspec(thing.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_c_class_signature(thing):\n",
    "    signature = \"\"\n",
    "    try:\n",
    "        thing()\n",
    "        signature = f\"1. {thing}()\"\n",
    "    except Exception as e:\n",
    "        if \"No constructor defined\" in str(e):\n",
    "            return None\n",
    "\n",
    "        for line in str(e).splitlines():\n",
    "            if any(parts in line.strip() for parts in [\"TypeError\", \"incompatible\", \"Invoked with:\"]):\n",
    "                continue\n",
    "            else:\n",
    "                signature += f\"{line}\\n\"\n",
    "        signature = signature.strip()\n",
    "\n",
    "    return signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_native_signature(key):\n",
    "    if key in native_func_dict:\n",
    "        # print(f\"Found {key} in native_func_dict\")\n",
    "        return native_func_dict[key][\"args\"]\n",
    "    else:\n",
    "        print(f\"Cant find {key} in native_func_dict\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_native_return_type(key):\n",
    "    if key in native_func_dict:\n",
    "        # print(f\"Found {key} in native_func_dict\")\n",
    "        return native_func_dict[key][\"return_type\"]\n",
    "    else:\n",
    "        print(f\"Cant find {key} in native_func_dict\")\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking AVG\n",
      "AggregationType.AVG <class 'torch._C.AggregationType'> \n",
      "members:  {'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG}\n",
      "AVG ['iscenum']\n",
      "checking AggregationType\n",
      "<class 'torch._C.AggregationType'> <class 'pybind11_builtins.pybind11_type'> AggregationType\n",
      "Hierarchy: [<class 'torch._C.AggregationType'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "members:  {'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG}\n",
      "AggregationType ['isclass', 'callable', 'iscenum', 'ispybind11']\n",
      "checking AnyType\n",
      "<class 'torch._C.AnyType'> <class 'pybind11_builtins.pybind11_type'> AnyType\n",
      "Hierarchy: [<class 'torch._C.AnyType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "AnyType ['isclass', 'callable', 'ispybind11']\n",
      "checking Argument\n",
      "<class 'torch._C.Argument'> <class 'pybind11_builtins.pybind11_type'> Argument\n",
      "Hierarchy: [<class 'torch._C.Argument'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "Argument ['isclass', 'callable', 'ispybind11']\n",
      "checking ArgumentSpec\n",
      "<class 'torch._C.ArgumentSpec'> <class 'pybind11_builtins.pybind11_type'> ArgumentSpec\n",
      "Hierarchy: [<class 'torch._C.ArgumentSpec'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ArgumentSpec ['isclass', 'callable', 'ispybind11']\n",
      "checking BFloat16Storage\n",
      "<class 'torch.BFloat16Storage'> <class 'type'> BFloat16Storage\n",
      "Hierarchy: [<class 'torch.BFloat16Storage'>, <class 'torch._C.BFloat16StorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "BFloat16Storage ['isclass', 'callable']\n",
      "checking BFloat16Tensor\n",
      "<class 'torch.BFloat16Tensor'> <class 'torch.tensortype'> BFloat16Tensor\n",
      "Hierarchy: [<class 'torch.BFloat16Tensor'>, <class 'object'>]\n",
      "BFloat16Tensor ['isclass', 'callable']\n",
      "checking BenchmarkConfig\n",
      "<class 'torch._C.BenchmarkConfig'> <class 'pybind11_builtins.pybind11_type'> BenchmarkConfig\n",
      "Hierarchy: [<class 'torch._C.BenchmarkConfig'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "BenchmarkConfig ['isclass', 'callable', 'ispybind11']\n",
      "checking BenchmarkExecutionStats\n",
      "<class 'torch._C.BenchmarkExecutionStats'> <class 'pybind11_builtins.pybind11_type'> BenchmarkExecutionStats\n",
      "Hierarchy: [<class 'torch._C.BenchmarkExecutionStats'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "BenchmarkExecutionStats ['isclass', 'callable', 'ispybind11']\n",
      "checking Block\n",
      "<class 'torch._C.Block'> <class 'pybind11_builtins.pybind11_type'> Block\n",
      "Hierarchy: [<class 'torch._C.Block'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "Block ['isclass', 'callable', 'ispybind11']\n",
      "checking BoolStorage\n",
      "<class 'torch.BoolStorage'> <class 'type'> BoolStorage\n",
      "Hierarchy: [<class 'torch.BoolStorage'>, <class 'torch._C.BoolStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "BoolStorage ['isclass', 'callable']\n",
      "checking BoolTensor\n",
      "<class 'torch.BoolTensor'> <class 'torch.tensortype'> BoolTensor\n",
      "Hierarchy: [<class 'torch.BoolTensor'>, <class 'object'>]\n",
      "BoolTensor ['isclass', 'callable']\n",
      "checking BoolType\n",
      "<class 'torch._C.BoolType'> <class 'pybind11_builtins.pybind11_type'> BoolType\n",
      "Hierarchy: [<class 'torch._C.BoolType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "BoolType ['isclass', 'callable', 'ispybind11']\n",
      "checking BufferDict\n",
      "<class 'torch._C.BufferDict'> <class 'pybind11_builtins.pybind11_type'> BufferDict\n",
      "Hierarchy: [<class 'torch._C.BufferDict'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "BufferDict ['isclass', 'callable', 'ispybind11']\n",
      "checking ByteStorage\n",
      "<class 'torch.ByteStorage'> <class 'type'> ByteStorage\n",
      "Hierarchy: [<class 'torch.ByteStorage'>, <class 'torch._C.ByteStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "ByteStorage ['isclass', 'callable']\n",
      "checking ByteTensor\n",
      "<class 'torch.ByteTensor'> <class 'torch.tensortype'> ByteTensor\n",
      "Hierarchy: [<class 'torch.ByteTensor'>, <class 'object'>]\n",
      "ByteTensor ['isclass', 'callable']\n",
      "checking CONV_BN_FUSION\n",
      "MobileOptimizerType.CONV_BN_FUSION <class 'torch._C.MobileOptimizerType'> \n",
      "members:  {'CONV_BN_FUSION': MobileOptimizerType.CONV_BN_FUSION, 'INSERT_FOLD_PREPACK_OPS': MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, 'REMOVE_DROPOUT': MobileOptimizerType.REMOVE_DROPOUT}\n",
      "CONV_BN_FUSION ['iscenum']\n",
      "checking CallStack\n",
      "<class 'torch._C.CallStack'> <class 'pybind11_builtins.pybind11_type'> CallStack\n",
      "Hierarchy: [<class 'torch._C.CallStack'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "CallStack ['isclass', 'callable', 'ispybind11']\n",
      "checking Capsule\n",
      "<class 'torch._C.Capsule'> <class 'pybind11_builtins.pybind11_type'> Capsule\n",
      "Hierarchy: [<class 'torch._C.Capsule'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "Capsule ['isclass', 'callable', 'ispybind11']\n",
      "checking CharStorage\n",
      "<class 'torch.CharStorage'> <class 'type'> CharStorage\n",
      "Hierarchy: [<class 'torch.CharStorage'>, <class 'torch._C.CharStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "CharStorage ['isclass', 'callable']\n",
      "checking CharTensor\n",
      "<class 'torch.CharTensor'> <class 'torch.tensortype'> CharTensor\n",
      "Hierarchy: [<class 'torch.CharTensor'>, <class 'object'>]\n",
      "CharTensor ['isclass', 'callable']\n",
      "checking ClassType\n",
      "<class 'torch._C.ClassType'> <class 'pybind11_builtins.pybind11_type'> ClassType\n",
      "Hierarchy: [<class 'torch._C.ClassType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ClassType ['isclass', 'callable', 'ispybind11']\n",
      "checking Code\n",
      "<class 'torch._C.Code'> <class 'pybind11_builtins.pybind11_type'> Code\n",
      "Hierarchy: [<class 'torch._C.Code'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "Code ['isclass', 'callable', 'ispybind11']\n",
      "checking CompilationUnit\n",
      "<class 'torch._C.CompilationUnit'> <class 'pybind11_builtins.pybind11_type'> CompilationUnit\n",
      "Hierarchy: [<class 'torch._C.CompilationUnit'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "CompilationUnit ['isclass', 'callable', 'ispybind11']\n",
      "checking CompleteArgumentSpec\n",
      "<class 'torch._C.CompleteArgumentSpec'> <class 'pybind11_builtins.pybind11_type'> CompleteArgumentSpec\n",
      "Hierarchy: [<class 'torch._C.CompleteArgumentSpec'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "CompleteArgumentSpec ['isclass', 'callable', 'ispybind11']\n",
      "checking ComplexDoubleStorage\n",
      "<class 'torch.ComplexDoubleStorage'> <class 'type'> ComplexDoubleStorage\n",
      "Hierarchy: [<class 'torch.ComplexDoubleStorage'>, <class 'torch._C.ComplexDoubleStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "ComplexDoubleStorage ['isclass', 'callable']\n",
      "checking ComplexFloatStorage\n",
      "<class 'torch.ComplexFloatStorage'> <class 'type'> ComplexFloatStorage\n",
      "Hierarchy: [<class 'torch.ComplexFloatStorage'>, <class 'torch._C.ComplexFloatStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "ComplexFloatStorage ['isclass', 'callable']\n",
      "checking ConcreteModuleType\n",
      "<class 'torch._C.ConcreteModuleType'> <class 'pybind11_builtins.pybind11_type'> ConcreteModuleType\n",
      "Hierarchy: [<class 'torch._C.ConcreteModuleType'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ConcreteModuleType ['isclass', 'callable', 'ispybind11']\n",
      "checking ConcreteModuleTypeBuilder\n",
      "<class 'torch._C.ConcreteModuleTypeBuilder'> <class 'pybind11_builtins.pybind11_type'> ConcreteModuleTypeBuilder\n",
      "Hierarchy: [<class 'torch._C.ConcreteModuleTypeBuilder'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ConcreteModuleTypeBuilder ['isclass', 'callable', 'ispybind11']\n",
      "checking DeepCopyMemoTable\n",
      "<class 'torch._C.DeepCopyMemoTable'> <class 'pybind11_builtins.pybind11_type'> DeepCopyMemoTable\n",
      "Hierarchy: [<class 'torch._C.DeepCopyMemoTable'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "DeepCopyMemoTable ['isclass', 'callable', 'ispybind11']\n",
      "checking DeviceObjType\n",
      "<class 'torch._C.DeviceObjType'> <class 'pybind11_builtins.pybind11_type'> DeviceObjType\n",
      "Hierarchy: [<class 'torch._C.DeviceObjType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "DeviceObjType ['isclass', 'callable', 'ispybind11']\n",
      "checking DictType\n",
      "<class 'torch._C.DictType'> <class 'pybind11_builtins.pybind11_type'> DictType\n",
      "Hierarchy: [<class 'torch._C.DictType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "DictType ['isclass', 'callable', 'ispybind11']\n",
      "checking DoubleStorage\n",
      "<class 'torch.DoubleStorage'> <class 'type'> DoubleStorage\n",
      "Hierarchy: [<class 'torch.DoubleStorage'>, <class 'torch._C.DoubleStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "DoubleStorage ['isclass', 'callable']\n",
      "checking DoubleTensor\n",
      "<class 'torch.DoubleTensor'> <class 'torch.tensortype'> DoubleTensor\n",
      "Hierarchy: [<class 'torch.DoubleTensor'>, <class 'object'>]\n",
      "DoubleTensor ['isclass', 'callable']\n",
      "checking ErrorReport\n",
      "<class 'torch._C.ErrorReport'> <class 'pybind11_builtins.pybind11_type'> ErrorReport\n",
      "Hierarchy: [<class 'torch._C.ErrorReport'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ErrorReport ['isclass', 'callable', 'ispybind11']\n",
      "checking ExecutionPlan\n",
      "<class 'torch._C.ExecutionPlan'> <class 'pybind11_builtins.pybind11_type'> ExecutionPlan\n",
      "Hierarchy: [<class 'torch._C.ExecutionPlan'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ExecutionPlan ['isclass', 'callable', 'ispybind11']\n",
      "checking ExtraFilesMap\n",
      "<class 'torch._C.ExtraFilesMap'> <class 'pybind11_builtins.pybind11_type'> ExtraFilesMap\n",
      "Hierarchy: [<class 'torch._C.ExtraFilesMap'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ExtraFilesMap ['isclass', 'callable', 'ispybind11']\n",
      "checking FatalError\n",
      "<class 'torch.FatalError'> <class 'type'> FatalError\n",
      "Hierarchy: [<class 'torch.FatalError'>, <class 'Exception'>, <class 'BaseException'>, <class 'object'>]\n",
      "FatalError ['isclass', 'callable']\n",
      "checking FileCheck\n",
      "<class 'torch._C.FileCheck'> <class 'pybind11_builtins.pybind11_type'> FileCheck\n",
      "Hierarchy: [<class 'torch._C.FileCheck'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "FileCheck ['isclass', 'callable', 'ispybind11']\n",
      "checking FloatStorage\n",
      "<class 'torch.FloatStorage'> <class 'type'> FloatStorage\n",
      "Hierarchy: [<class 'torch.FloatStorage'>, <class 'torch._C.FloatStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "FloatStorage ['isclass', 'callable']\n",
      "checking FloatTensor\n",
      "<class 'torch.FloatTensor'> <class 'torch.tensortype'> FloatTensor\n",
      "Hierarchy: [<class 'torch.FloatTensor'>, <class 'object'>]\n",
      "FloatTensor ['isclass', 'callable']\n",
      "checking FloatType\n",
      "<class 'torch._C.FloatType'> <class 'pybind11_builtins.pybind11_type'> FloatType\n",
      "Hierarchy: [<class 'torch._C.FloatType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "FloatType ['isclass', 'callable', 'ispybind11']\n",
      "checking FunctionSchema\n",
      "<class 'torch._C.FunctionSchema'> <class 'pybind11_builtins.pybind11_type'> FunctionSchema\n",
      "Hierarchy: [<class 'torch._C.FunctionSchema'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "FunctionSchema ['isclass', 'callable', 'ispybind11']\n",
      "checking Future\n",
      "<class 'torch._C.Future'> <class 'pybind11_builtins.pybind11_type'> Future\n",
      "Hierarchy: [<class 'torch._C.Future'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "Future ['isclass', 'callable', 'ispybind11']\n",
      "checking FutureType\n",
      "<class 'torch._C.FutureType'> <class 'pybind11_builtins.pybind11_type'> FutureType\n",
      "Hierarchy: [<class 'torch._C.FutureType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "FutureType ['isclass', 'callable', 'ispybind11']\n",
      "checking Generator\n",
      "<class 'torch._C.Generator'> <class 'type'> Generator\n",
      "Hierarchy: [<class 'torch._C.Generator'>, <class 'object'>]\n",
      "Generator ['isclass', 'callable']\n",
      "checking Gradient\n",
      "<class 'torch._C.Gradient'> <class 'pybind11_builtins.pybind11_type'> Gradient\n",
      "Hierarchy: [<class 'torch._C.Gradient'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "Gradient ['isclass', 'callable', 'ispybind11']\n",
      "checking Graph\n",
      "<class 'torch._C.Graph'> <class 'pybind11_builtins.pybind11_type'> Graph\n",
      "Hierarchy: [<class 'torch._C.Graph'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "Graph ['isclass', 'callable', 'ispybind11']\n",
      "checking GraphExecutorState\n",
      "<class 'torch._C.GraphExecutorState'> <class 'pybind11_builtins.pybind11_type'> GraphExecutorState\n",
      "Hierarchy: [<class 'torch._C.GraphExecutorState'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "GraphExecutorState ['isclass', 'callable', 'ispybind11']\n",
      "checking HalfStorage\n",
      "<class 'torch.HalfStorage'> <class 'type'> HalfStorage\n",
      "Hierarchy: [<class 'torch.HalfStorage'>, <class 'torch._C.HalfStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "HalfStorage ['isclass', 'callable']\n",
      "checking HalfStorageBase\n",
      "<class 'torch._C.HalfStorageBase'> <class 'type'> HalfStorageBase\n",
      "Hierarchy: [<class 'torch._C.HalfStorageBase'>, <class 'object'>]\n",
      "HalfStorageBase ['isclass', 'callable']\n",
      "checking HalfTensor\n",
      "<class 'torch.HalfTensor'> <class 'torch.tensortype'> HalfTensor\n",
      "Hierarchy: [<class 'torch.HalfTensor'>, <class 'object'>]\n",
      "HalfTensor ['isclass', 'callable']\n",
      "checking INSERT_FOLD_PREPACK_OPS\n",
      "MobileOptimizerType.INSERT_FOLD_PREPACK_OPS <class 'torch._C.MobileOptimizerType'> \n",
      "members:  {'CONV_BN_FUSION': MobileOptimizerType.CONV_BN_FUSION, 'INSERT_FOLD_PREPACK_OPS': MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, 'REMOVE_DROPOUT': MobileOptimizerType.REMOVE_DROPOUT}\n",
      "INSERT_FOLD_PREPACK_OPS ['iscenum']\n",
      "checking IODescriptor\n",
      "<class 'torch._C.IODescriptor'> <class 'pybind11_builtins.pybind11_type'> IODescriptor\n",
      "Hierarchy: [<class 'torch._C.IODescriptor'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "IODescriptor ['isclass', 'callable', 'ispybind11']\n",
      "checking IntStorage\n",
      "<class 'torch.IntStorage'> <class 'type'> IntStorage\n",
      "Hierarchy: [<class 'torch.IntStorage'>, <class 'torch._C.IntStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "IntStorage ['isclass', 'callable']\n",
      "checking IntTensor\n",
      "<class 'torch.IntTensor'> <class 'torch.tensortype'> IntTensor\n",
      "Hierarchy: [<class 'torch.IntTensor'>, <class 'object'>]\n",
      "IntTensor ['isclass', 'callable']\n",
      "checking IntType\n",
      "<class 'torch._C.IntType'> <class 'pybind11_builtins.pybind11_type'> IntType\n",
      "Hierarchy: [<class 'torch._C.IntType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "IntType ['isclass', 'callable', 'ispybind11']\n",
      "checking InterfaceType\n",
      "<class 'torch._C.InterfaceType'> <class 'pybind11_builtins.pybind11_type'> InterfaceType\n",
      "Hierarchy: [<class 'torch._C.InterfaceType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "InterfaceType ['isclass', 'callable', 'ispybind11']\n",
      "checking JITException\n",
      "<class 'torch.jit.Error'> <class 'type'> Error\n",
      "Hierarchy: [<class 'torch.jit.Error'>, <class 'Exception'>, <class 'BaseException'>, <class 'object'>]\n",
      "JITException ['isclass', 'callable']\n",
      "checking ListType\n",
      "<class 'torch._C.ListType'> <class 'pybind11_builtins.pybind11_type'> ListType\n",
      "Hierarchy: [<class 'torch._C.ListType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ListType ['isclass', 'callable', 'ispybind11']\n",
      "checking LiteScriptModule\n",
      "<class 'torch._C.LiteScriptModule'> <class 'pybind11_builtins.pybind11_type'> LiteScriptModule\n",
      "Hierarchy: [<class 'torch._C.LiteScriptModule'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "LiteScriptModule ['isclass', 'callable', 'ispybind11']\n",
      "checking LockingLogger\n",
      "<class 'torch._C.LockingLogger'> <class 'pybind11_builtins.pybind11_type'> LockingLogger\n",
      "Hierarchy: [<class 'torch._C.LockingLogger'>, <class 'torch._C.LoggerBase'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "LockingLogger ['isclass', 'callable', 'ispybind11']\n",
      "checking LoggerBase\n",
      "<class 'torch._C.LoggerBase'> <class 'pybind11_builtins.pybind11_type'> LoggerBase\n",
      "Hierarchy: [<class 'torch._C.LoggerBase'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "LoggerBase ['isclass', 'callable', 'ispybind11']\n",
      "checking LongStorage\n",
      "<class 'torch.LongStorage'> <class 'type'> LongStorage\n",
      "Hierarchy: [<class 'torch.LongStorage'>, <class 'torch._C.LongStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "LongStorage ['isclass', 'callable']\n",
      "checking LongTensor\n",
      "<class 'torch.LongTensor'> <class 'torch.tensortype'> LongTensor\n",
      "Hierarchy: [<class 'torch.LongTensor'>, <class 'object'>]\n",
      "LongTensor ['isclass', 'callable']\n",
      "checking MobileOptimizerType\n",
      "<class 'torch._C.MobileOptimizerType'> <class 'pybind11_builtins.pybind11_type'> MobileOptimizerType\n",
      "Hierarchy: [<class 'torch._C.MobileOptimizerType'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "members:  {'CONV_BN_FUSION': MobileOptimizerType.CONV_BN_FUSION, 'INSERT_FOLD_PREPACK_OPS': MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, 'REMOVE_DROPOUT': MobileOptimizerType.REMOVE_DROPOUT}\n",
      "MobileOptimizerType ['isclass', 'callable', 'iscenum', 'ispybind11']\n",
      "checking ModuleDict\n",
      "<class 'torch._C.ModuleDict'> <class 'pybind11_builtins.pybind11_type'> ModuleDict\n",
      "Hierarchy: [<class 'torch._C.ModuleDict'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ModuleDict ['isclass', 'callable', 'ispybind11']\n",
      "checking Node\n",
      "<class 'torch._C.Node'> <class 'pybind11_builtins.pybind11_type'> Node\n",
      "Hierarchy: [<class 'torch._C.Node'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "Node ['isclass', 'callable', 'ispybind11']\n",
      "checking NoneType\n",
      "<class 'torch._C.NoneType'> <class 'pybind11_builtins.pybind11_type'> NoneType\n",
      "Hierarchy: [<class 'torch._C.NoneType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "NoneType ['isclass', 'callable', 'ispybind11']\n",
      "checking NoopLogger\n",
      "<class 'torch._C.NoopLogger'> <class 'pybind11_builtins.pybind11_type'> NoopLogger\n",
      "Hierarchy: [<class 'torch._C.NoopLogger'>, <class 'torch._C.LoggerBase'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "NoopLogger ['isclass', 'callable', 'ispybind11']\n",
      "checking NumberType\n",
      "<class 'torch._C.NumberType'> <class 'pybind11_builtins.pybind11_type'> NumberType\n",
      "Hierarchy: [<class 'torch._C.NumberType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "NumberType ['isclass', 'callable', 'ispybind11']\n",
      "checking OptionalType\n",
      "<class 'torch._C.OptionalType'> <class 'pybind11_builtins.pybind11_type'> OptionalType\n",
      "Hierarchy: [<class 'torch._C.OptionalType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "OptionalType ['isclass', 'callable', 'ispybind11']\n",
      "checking ParameterDict\n",
      "<class 'torch._C.ParameterDict'> <class 'pybind11_builtins.pybind11_type'> ParameterDict\n",
      "Hierarchy: [<class 'torch._C.ParameterDict'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ParameterDict ['isclass', 'callable', 'ispybind11']\n",
      "checking PyObjectType\n",
      "<class 'torch._C.PyObjectType'> <class 'pybind11_builtins.pybind11_type'> PyObjectType\n",
      "Hierarchy: [<class 'torch._C.PyObjectType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "PyObjectType ['isclass', 'callable', 'ispybind11']\n",
      "checking PyTorchFileReader\n",
      "<class 'torch._C.PyTorchFileReader'> <class 'pybind11_builtins.pybind11_type'> PyTorchFileReader\n",
      "Hierarchy: [<class 'torch._C.PyTorchFileReader'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "PyTorchFileReader ['isclass', 'callable', 'ispybind11']\n",
      "checking PyTorchFileWriter\n",
      "<class 'torch._C.PyTorchFileWriter'> <class 'pybind11_builtins.pybind11_type'> PyTorchFileWriter\n",
      "Hierarchy: [<class 'torch._C.PyTorchFileWriter'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "PyTorchFileWriter ['isclass', 'callable', 'ispybind11']\n",
      "checking QInt32Storage\n",
      "<class 'torch.QInt32Storage'> <class 'type'> QInt32Storage\n",
      "Hierarchy: [<class 'torch.QInt32Storage'>, <class 'torch._C.QInt32StorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "QInt32Storage ['isclass', 'callable']\n",
      "checking QInt32StorageBase\n",
      "<class 'torch._C.QInt32StorageBase'> <class 'type'> QInt32StorageBase\n",
      "Hierarchy: [<class 'torch._C.QInt32StorageBase'>, <class 'object'>]\n",
      "QInt32StorageBase ['isclass', 'callable']\n",
      "checking QInt8Storage\n",
      "<class 'torch.QInt8Storage'> <class 'type'> QInt8Storage\n",
      "Hierarchy: [<class 'torch.QInt8Storage'>, <class 'torch._C.QInt8StorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "QInt8Storage ['isclass', 'callable']\n",
      "checking QInt8StorageBase\n",
      "<class 'torch._C.QInt8StorageBase'> <class 'type'> QInt8StorageBase\n",
      "Hierarchy: [<class 'torch._C.QInt8StorageBase'>, <class 'object'>]\n",
      "QInt8StorageBase ['isclass', 'callable']\n",
      "checking QUInt8Storage\n",
      "<class 'torch.QUInt8Storage'> <class 'type'> QUInt8Storage\n",
      "Hierarchy: [<class 'torch.QUInt8Storage'>, <class 'torch._C.QUInt8StorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "QUInt8Storage ['isclass', 'callable']\n",
      "checking REMOVE_DROPOUT\n",
      "MobileOptimizerType.REMOVE_DROPOUT <class 'torch._C.MobileOptimizerType'> \n",
      "members:  {'CONV_BN_FUSION': MobileOptimizerType.CONV_BN_FUSION, 'INSERT_FOLD_PREPACK_OPS': MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, 'REMOVE_DROPOUT': MobileOptimizerType.REMOVE_DROPOUT}\n",
      "REMOVE_DROPOUT ['iscenum']\n",
      "checking RRefType\n",
      "<class 'torch._C.RRefType'> <class 'pybind11_builtins.pybind11_type'> RRefType\n",
      "Hierarchy: [<class 'torch._C.RRefType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "RRefType ['isclass', 'callable', 'ispybind11']\n",
      "checking SUM\n",
      "AggregationType.SUM <class 'torch._C.AggregationType'> \n",
      "members:  {'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG}\n",
      "SUM ['iscenum']\n",
      "checking ScriptClass\n",
      "<class 'torch._C.ScriptClass'> <class 'pybind11_builtins.pybind11_type'> ScriptClass\n",
      "Hierarchy: [<class 'torch._C.ScriptClass'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ScriptClass ['isclass', 'callable', 'ispybind11']\n",
      "checking ScriptFunction\n",
      "<class 'torch.jit.ScriptFunction'> <class 'pybind11_builtins.pybind11_type'> ScriptFunction\n",
      "Hierarchy: [<class 'torch.jit.ScriptFunction'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ScriptFunction ['isclass', 'callable', 'ispybind11']\n",
      "checking ScriptMethod\n",
      "<class 'torch._C.ScriptMethod'> <class 'pybind11_builtins.pybind11_type'> ScriptMethod\n",
      "Hierarchy: [<class 'torch._C.ScriptMethod'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ScriptMethod ['isclass', 'callable', 'ispybind11']\n",
      "checking ScriptModule\n",
      "<class 'torch._C.ScriptModule'> <class 'pybind11_builtins.pybind11_type'> ScriptModule\n",
      "Hierarchy: [<class 'torch._C.ScriptModule'>, <class 'torch._C.ScriptObject'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ScriptModule ['isclass', 'callable', 'ispybind11']\n",
      "checking ScriptObject\n",
      "<class 'torch._C.ScriptObject'> <class 'pybind11_builtins.pybind11_type'> ScriptObject\n",
      "Hierarchy: [<class 'torch._C.ScriptObject'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ScriptObject ['isclass', 'callable', 'ispybind11']\n",
      "checking Set\n",
      "typing.Set <class 'typing.GenericMeta'> Set\n",
      "Hierarchy: [typing.Set, <class 'set'>, typing.MutableSet, <class 'collections.abc.MutableSet'>, typing.AbstractSet, <class 'collections.abc.Set'>, typing.Collection, <class 'collections.abc.Collection'>, <class 'collections.abc.Sized'>, typing.Iterable, <class 'collections.abc.Iterable'>, typing.Container, <class 'collections.abc.Container'>, typing.Generic, <class 'object'>]\n",
      "Set ['isclass', 'callable']\n",
      "checking ShortStorage\n",
      "<class 'torch.ShortStorage'> <class 'type'> ShortStorage\n",
      "Hierarchy: [<class 'torch.ShortStorage'>, <class 'torch._C.ShortStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "ShortStorage ['isclass', 'callable']\n",
      "checking ShortTensor\n",
      "<class 'torch.ShortTensor'> <class 'torch.tensortype'> ShortTensor\n",
      "Hierarchy: [<class 'torch.ShortTensor'>, <class 'object'>]\n",
      "ShortTensor ['isclass', 'callable']\n",
      "checking Size\n",
      "<class 'torch.Size'> <class 'type'> Size\n",
      "Hierarchy: [<class 'torch.Size'>, <class 'tuple'>, <class 'object'>]\n",
      "Size ['isclass', 'callable']\n",
      "checking Storage\n",
      "<class 'torch.FloatStorage'> <class 'type'> FloatStorage\n",
      "Hierarchy: [<class 'torch.FloatStorage'>, <class 'torch._C.FloatStorageBase'>, <class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "Storage ['isclass', 'callable']\n",
      "checking StringType\n",
      "<class 'torch._C.StringType'> <class 'pybind11_builtins.pybind11_type'> StringType\n",
      "Hierarchy: [<class 'torch._C.StringType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "StringType ['isclass', 'callable', 'ispybind11']\n",
      "checking Tensor\n",
      "<class 'torch.Tensor'> <class 'type'> Tensor\n",
      "Hierarchy: [<class 'torch.Tensor'>, <class 'torch._C._TensorBase'>, <class 'object'>]\n",
      "Tensor ['isclass', 'callable']\n",
      "checking TensorType\n",
      "<class 'torch._C.TensorType'> <class 'pybind11_builtins.pybind11_type'> TensorType\n",
      "Hierarchy: [<class 'torch._C.TensorType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "TensorType ['isclass', 'callable', 'ispybind11']\n",
      "checking ThroughputBenchmark\n",
      "<class 'torch._C.ThroughputBenchmark'> <class 'pybind11_builtins.pybind11_type'> ThroughputBenchmark\n",
      "Hierarchy: [<class 'torch._C.ThroughputBenchmark'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "ThroughputBenchmark ['isclass', 'callable', 'ispybind11']\n",
      "checking TracingState\n",
      "<class 'torch._C.TracingState'> <class 'pybind11_builtins.pybind11_type'> TracingState\n",
      "Hierarchy: [<class 'torch._C.TracingState'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "TracingState ['isclass', 'callable', 'ispybind11']\n",
      "checking TupleType\n",
      "<class 'torch._C.TupleType'> <class 'pybind11_builtins.pybind11_type'> TupleType\n",
      "Hierarchy: [<class 'torch._C.TupleType'>, <class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "TupleType ['isclass', 'callable', 'ispybind11']\n",
      "checking Type\n",
      "<class 'torch._C.Type'> <class 'pybind11_builtins.pybind11_type'> Type\n",
      "Hierarchy: [<class 'torch._C.Type'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "Type ['isclass', 'callable', 'ispybind11']\n",
      "checking USE_GLOBAL_DEPS\n",
      "True <class 'bool'> \n",
      "USE_GLOBAL_DEPS []\n",
      "checking USE_RTLD_GLOBAL_WITH_LIBTORCH\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False <class 'bool'> \n",
      "USE_RTLD_GLOBAL_WITH_LIBTORCH []\n",
      "checking Use\n",
      "<class 'torch._C.Use'> <class 'pybind11_builtins.pybind11_type'> Use\n",
      "Hierarchy: [<class 'torch._C.Use'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "Use ['isclass', 'callable', 'ispybind11']\n",
      "checking Value\n",
      "<class 'torch._C.Value'> <class 'pybind11_builtins.pybind11_type'> Value\n",
      "Hierarchy: [<class 'torch._C.Value'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "Value ['isclass', 'callable', 'ispybind11']\n",
      "checking _C\n",
      "<module 'torch._C' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so'> <class 'module'> torch._C\n",
      "_C ['ismodule']\n",
      "checking _StorageBase\n",
      "<class 'torch.storage._StorageBase'> <class 'type'> _StorageBase\n",
      "Hierarchy: [<class 'torch.storage._StorageBase'>, <class 'object'>]\n",
      "_StorageBase ['isclass', 'callable']\n",
      "checking _VF\n",
      "<module 'torch._VF'> <class 'torch._VF.VFModule'> torch._VF\n",
      "_VF ['ismodule']\n",
      "checking _adaptive_avg_pool2d\n",
      "<built-in method _adaptive_avg_pool2d of type object at 0x1144465b0> <class 'builtin_function_or_method'> _adaptive_avg_pool2d\n",
      "_adaptive_avg_pool2d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _addmv_impl_\n",
      "<built-in method _addmv_impl_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> _addmv_impl_\n",
      "_addmv_impl_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _addr\n",
      "<built-in method _addr of type object at 0x1144465b0> <class 'builtin_function_or_method'> _addr\n",
      "_addr ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _addr_\n",
      "<built-in method _addr_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> _addr_\n",
      "_addr_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _amp_non_finite_check_and_unscale_\n",
      "<built-in method _amp_non_finite_check_and_unscale_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> _amp_non_finite_check_and_unscale_\n",
      "_amp_non_finite_check_and_unscale_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _amp_update_scale\n",
      "<built-in method _amp_update_scale of type object at 0x1144465b0> <class 'builtin_function_or_method'> _amp_update_scale\n",
      "_amp_update_scale ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _baddbmm_mkl_\n",
      "<built-in method _baddbmm_mkl_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> _baddbmm_mkl_\n",
      "_baddbmm_mkl_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _batch_norm_impl_index\n",
      "<built-in method _batch_norm_impl_index of type object at 0x1144465b0> <class 'builtin_function_or_method'> _batch_norm_impl_index\n",
      "_batch_norm_impl_index ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _bmm\n",
      "<built-in method _bmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> _bmm\n",
      "_bmm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cast_Byte\n",
      "<built-in method _cast_Byte of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cast_Byte\n",
      "_cast_Byte ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cast_Char\n",
      "<built-in method _cast_Char of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cast_Char\n",
      "_cast_Char ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cast_Double\n",
      "<built-in method _cast_Double of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cast_Double\n",
      "_cast_Double ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cast_Float\n",
      "<built-in method _cast_Float of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cast_Float\n",
      "_cast_Float ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cast_Half\n",
      "<built-in method _cast_Half of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cast_Half\n",
      "_cast_Half ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cast_Int\n",
      "<built-in method _cast_Int of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cast_Int\n",
      "_cast_Int ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cast_Long\n",
      "<built-in method _cast_Long of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cast_Long\n",
      "_cast_Long ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cast_Short\n",
      "<built-in method _cast_Short of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cast_Short\n",
      "_cast_Short ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cat\n",
      "<built-in method _cat of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cat\n",
      "_cat ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _choose_qparams_per_tensor\n",
      "<built-in method _choose_qparams_per_tensor of type object at 0x1144465b0> <class 'builtin_function_or_method'> _choose_qparams_per_tensor\n",
      "_choose_qparams_per_tensor ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _classes\n",
      "<module 'torch._classes' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_classes.py'> <class 'module'> torch._classes\n",
      "_classes ['ismodule']\n",
      "checking _convolution\n",
      "<built-in method _convolution of type object at 0x1144465b0> <class 'builtin_function_or_method'> _convolution\n",
      "_convolution ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _convolution_nogroup\n",
      "<built-in method _convolution_nogroup of type object at 0x1144465b0> <class 'builtin_function_or_method'> _convolution_nogroup\n",
      "_convolution_nogroup ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _copy_from\n",
      "<built-in method _copy_from of type object at 0x1144465b0> <class 'builtin_function_or_method'> _copy_from\n",
      "_copy_from ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _ctc_loss\n",
      "<built-in method _ctc_loss of type object at 0x1144465b0> <class 'builtin_function_or_method'> _ctc_loss\n",
      "_ctc_loss ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cudnn_ctc_loss\n",
      "<built-in method _cudnn_ctc_loss of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cudnn_ctc_loss\n",
      "_cudnn_ctc_loss ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cudnn_init_dropout_state\n",
      "<built-in method _cudnn_init_dropout_state of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cudnn_init_dropout_state\n",
      "_cudnn_init_dropout_state ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cudnn_rnn\n",
      "<built-in method _cudnn_rnn of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cudnn_rnn\n",
      "_cudnn_rnn ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cudnn_rnn_flatten_weight\n",
      "<built-in method _cudnn_rnn_flatten_weight of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cudnn_rnn_flatten_weight\n",
      "_cudnn_rnn_flatten_weight ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cufft_clear_plan_cache\n",
      "<built-in method _cufft_clear_plan_cache of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cufft_clear_plan_cache\n",
      "_cufft_clear_plan_cache ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cufft_get_plan_cache_max_size\n",
      "<built-in method _cufft_get_plan_cache_max_size of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cufft_get_plan_cache_max_size\n",
      "_cufft_get_plan_cache_max_size ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cufft_get_plan_cache_size\n",
      "<built-in method _cufft_get_plan_cache_size of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cufft_get_plan_cache_size\n",
      "_cufft_get_plan_cache_size ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cufft_set_plan_cache_max_size\n",
      "<built-in method _cufft_set_plan_cache_max_size of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cufft_set_plan_cache_max_size\n",
      "_cufft_set_plan_cache_max_size ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cummax_helper\n",
      "<built-in method _cummax_helper of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cummax_helper\n",
      "_cummax_helper ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _cummin_helper\n",
      "<built-in method _cummin_helper of type object at 0x1144465b0> <class 'builtin_function_or_method'> _cummin_helper\n",
      "_cummin_helper ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _debug_has_internal_overlap\n",
      "<built-in method _debug_has_internal_overlap of type object at 0x1144465b0> <class 'builtin_function_or_method'> _debug_has_internal_overlap\n",
      "_debug_has_internal_overlap ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _dim_arange\n",
      "<built-in method _dim_arange of type object at 0x1144465b0> <class 'builtin_function_or_method'> _dim_arange\n",
      "_dim_arange ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _dirichlet_grad\n",
      "<built-in method _dirichlet_grad of type object at 0x1144465b0> <class 'builtin_function_or_method'> _dirichlet_grad\n",
      "_dirichlet_grad ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _embedding_bag\n",
      "<built-in method _embedding_bag of type object at 0x1144465b0> <class 'builtin_function_or_method'> _embedding_bag\n",
      "_embedding_bag ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _empty_affine_quantized\n",
      "<built-in method _empty_affine_quantized of type object at 0x1144465b0> <class 'builtin_function_or_method'> _empty_affine_quantized\n",
      "_empty_affine_quantized ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _empty_per_channel_affine_quantized\n",
      "<built-in method _empty_per_channel_affine_quantized of type object at 0x1144465b0> <class 'builtin_function_or_method'> _empty_per_channel_affine_quantized\n",
      "_empty_per_channel_affine_quantized ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _euclidean_dist\n",
      "<built-in method _euclidean_dist of type object at 0x1144465b0> <class 'builtin_function_or_method'> _euclidean_dist\n",
      "_euclidean_dist ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _fft_with_size\n",
      "<built-in method _fft_with_size of type object at 0x1144465b0> <class 'builtin_function_or_method'> _fft_with_size\n",
      "_fft_with_size ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _fused_dropout\n",
      "<built-in method _fused_dropout of type object at 0x1144465b0> <class 'builtin_function_or_method'> _fused_dropout\n",
      "_fused_dropout ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _has_compatible_shallow_copy_type\n",
      "<built-in method _has_compatible_shallow_copy_type of type object at 0x1144465b0> <class 'builtin_function_or_method'> _has_compatible_shallow_copy_type\n",
      "_has_compatible_shallow_copy_type ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _import_dotted_name\n",
      "<function _import_dotted_name at 0x1111a21e0> <class 'function'> _import_dotted_name\n",
      "_import_dotted_name ['isfunction', 'isroutine', 'callable']\n",
      "checking _index_copy_\n",
      "<built-in method _index_copy_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> _index_copy_\n",
      "_index_copy_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _index_put_impl_\n",
      "<built-in method _index_put_impl_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> _index_put_impl_\n",
      "_index_put_impl_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _is_deterministic\n",
      "<function _is_deterministic at 0x115a3fb70> <class 'function'> _is_deterministic\n",
      "_is_deterministic ['isfunction', 'isroutine', 'callable']\n",
      "checking _jit_internal\n",
      "<module 'torch._jit_internal' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_jit_internal.py'> <class 'module'> torch._jit_internal\n",
      "_jit_internal ['ismodule']\n",
      "checking _linalg_utils\n",
      "<module 'torch._linalg_utils' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_linalg_utils.py'> <class 'module'> torch._linalg_utils\n",
      "_linalg_utils ['ismodule']\n",
      "checking _load_global_deps\n",
      "<function _load_global_deps at 0x111191ae8> <class 'function'> _load_global_deps\n",
      "_load_global_deps ['isfunction', 'isroutine', 'callable']\n",
      "checking _lobpcg\n",
      "<module 'torch._lobpcg' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_lobpcg.py'> <class 'module'> torch._lobpcg\n",
      "_lobpcg ['ismodule']\n",
      "checking _log_softmax\n",
      "<built-in method _log_softmax of type object at 0x1144465b0> <class 'builtin_function_or_method'> _log_softmax\n",
      "_log_softmax ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _log_softmax_backward_data\n",
      "<built-in method _log_softmax_backward_data of type object at 0x1144465b0> <class 'builtin_function_or_method'> _log_softmax_backward_data\n",
      "_log_softmax_backward_data ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _logcumsumexp\n",
      "<built-in method _logcumsumexp of type object at 0x1144465b0> <class 'builtin_function_or_method'> _logcumsumexp\n",
      "_logcumsumexp ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _lowrank\n",
      "<module 'torch._lowrank' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_lowrank.py'> <class 'module'> torch._lowrank\n",
      "_lowrank ['ismodule']\n",
      "checking _lu_solve_helper\n",
      "<built-in method _lu_solve_helper of type object at 0x1144465b0> <class 'builtin_function_or_method'> _lu_solve_helper\n",
      "_lu_solve_helper ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _lu_with_info\n",
      "<built-in method _lu_with_info of type object at 0x1144465b0> <class 'builtin_function_or_method'> _lu_with_info\n",
      "_lu_with_info ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _make_per_channel_quantized_tensor\n",
      "<built-in method _make_per_channel_quantized_tensor of type object at 0x1144465b0> <class 'builtin_function_or_method'> _make_per_channel_quantized_tensor\n",
      "_make_per_channel_quantized_tensor ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _make_per_tensor_quantized_tensor\n",
      "<built-in method _make_per_tensor_quantized_tensor of type object at 0x1144465b0> <class 'builtin_function_or_method'> _make_per_tensor_quantized_tensor\n",
      "_make_per_tensor_quantized_tensor ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _masked_scale\n",
      "<built-in method _masked_scale of type object at 0x1144465b0> <class 'builtin_function_or_method'> _masked_scale\n",
      "_masked_scale ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _mkldnn\n",
      "torch._mkldnn <class 'torch.layout'> \n",
      "_mkldnn []\n",
      "checking _mkldnn_reshape\n",
      "<built-in method _mkldnn_reshape of type object at 0x1144465b0> <class 'builtin_function_or_method'> _mkldnn_reshape\n",
      "_mkldnn_reshape ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _mkldnn_transpose\n",
      "<built-in method _mkldnn_transpose of type object at 0x1144465b0> <class 'builtin_function_or_method'> _mkldnn_transpose\n",
      "_mkldnn_transpose ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _mkldnn_transpose_\n",
      "<built-in method _mkldnn_transpose_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> _mkldnn_transpose_\n",
      "_mkldnn_transpose_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _mode\n",
      "<built-in method _mode of type object at 0x1144465b0> <class 'builtin_function_or_method'> _mode\n",
      "_mode ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _multinomial_alias_draw\n",
      "<built-in method _multinomial_alias_draw of type object at 0x1144465b0> <class 'builtin_function_or_method'> _multinomial_alias_draw\n",
      "_multinomial_alias_draw ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _multinomial_alias_setup\n",
      "<built-in method _multinomial_alias_setup of type object at 0x1144465b0> <class 'builtin_function_or_method'> _multinomial_alias_setup\n",
      "_multinomial_alias_setup ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _namedtensor_internals\n",
      "<module 'torch._namedtensor_internals' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_namedtensor_internals.py'> <class 'module'> torch._namedtensor_internals\n",
      "_namedtensor_internals ['ismodule']\n",
      "checking _nnpack_available\n",
      "<built-in method _nnpack_available of type object at 0x1144465b0> <class 'builtin_function_or_method'> _nnpack_available\n",
      "_nnpack_available ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _nnpack_spatial_convolution\n",
      "<built-in method _nnpack_spatial_convolution of type object at 0x1144465b0> <class 'builtin_function_or_method'> _nnpack_spatial_convolution\n",
      "_nnpack_spatial_convolution ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _ops\n",
      "<module 'torch._ops' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_ops.py'> <class 'module'> torch._ops\n",
      "_ops ['ismodule']\n",
      "checking _overrides\n",
      "<module 'torch._overrides' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_overrides.py'> <class 'module'> torch._overrides\n",
      "_overrides ['ismodule']\n",
      "checking _pack_padded_sequence\n",
      "<built-in method _pack_padded_sequence of type object at 0x1144465b0> <class 'builtin_function_or_method'> _pack_padded_sequence\n",
      "_pack_padded_sequence ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _pad_packed_sequence\n",
      "<built-in method _pad_packed_sequence of type object at 0x1144465b0> <class 'builtin_function_or_method'> _pad_packed_sequence\n",
      "_pad_packed_sequence ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _reshape_from_tensor\n",
      "<built-in method _reshape_from_tensor of type object at 0x1144465b0> <class 'builtin_function_or_method'> _reshape_from_tensor\n",
      "_reshape_from_tensor ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _s_where\n",
      "<built-in method _s_where of type object at 0x1144465b0> <class 'builtin_function_or_method'> _s_where\n",
      "_s_where ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _sample_dirichlet\n",
      "<built-in method _sample_dirichlet of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sample_dirichlet\n",
      "_sample_dirichlet ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _set_deterministic\n",
      "<function _set_deterministic at 0x1159d7158> <class 'function'> _set_deterministic\n",
      "_set_deterministic ['isfunction', 'isroutine', 'callable']\n",
      "checking _shape_as_tensor\n",
      "<built-in method _shape_as_tensor of type object at 0x1144465b0> <class 'builtin_function_or_method'> _shape_as_tensor\n",
      "_shape_as_tensor ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _six\n",
      "<module 'torch._six' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_six.py'> <class 'module'> torch._six\n",
      "_six ['ismodule']\n",
      "checking _sobol_engine_draw\n",
      "<built-in method _sobol_engine_draw of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sobol_engine_draw\n",
      "_sobol_engine_draw ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _sobol_engine_ff_\n",
      "<built-in method _sobol_engine_ff_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sobol_engine_ff_\n",
      "_sobol_engine_ff_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _sobol_engine_initialize_state_\n",
      "<built-in method _sobol_engine_initialize_state_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sobol_engine_initialize_state_\n",
      "_sobol_engine_initialize_state_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _sobol_engine_scramble_\n",
      "<built-in method _sobol_engine_scramble_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sobol_engine_scramble_\n",
      "_sobol_engine_scramble_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _softmax\n",
      "<built-in method _softmax of type object at 0x1144465b0> <class 'builtin_function_or_method'> _softmax\n",
      "_softmax ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _softmax_backward_data\n",
      "<built-in method _softmax_backward_data of type object at 0x1144465b0> <class 'builtin_function_or_method'> _softmax_backward_data\n",
      "_softmax_backward_data ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _sparse_addmm\n",
      "<built-in method _sparse_addmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sparse_addmm\n",
      "_sparse_addmm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _sparse_log_softmax\n",
      "<built-in method _sparse_log_softmax of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sparse_log_softmax\n",
      "_sparse_log_softmax ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _sparse_log_softmax_backward_data\n",
      "<built-in method _sparse_log_softmax_backward_data of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sparse_log_softmax_backward_data\n",
      "_sparse_log_softmax_backward_data ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _sparse_mm\n",
      "<built-in method _sparse_mm of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sparse_mm\n",
      "_sparse_mm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _sparse_softmax\n",
      "<built-in method _sparse_softmax of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sparse_softmax\n",
      "_sparse_softmax ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _sparse_softmax_backward_data\n",
      "<built-in method _sparse_softmax_backward_data of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sparse_softmax_backward_data\n",
      "_sparse_softmax_backward_data ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _sparse_sum\n",
      "<built-in method _sparse_sum of type object at 0x1144465b0> <class 'builtin_function_or_method'> _sparse_sum\n",
      "_sparse_sum ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _standard_gamma\n",
      "<built-in method _standard_gamma of type object at 0x1144465b0> <class 'builtin_function_or_method'> _standard_gamma\n",
      "_standard_gamma ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _standard_gamma_grad\n",
      "<built-in method _standard_gamma_grad of type object at 0x1144465b0> <class 'builtin_function_or_method'> _standard_gamma_grad\n",
      "_standard_gamma_grad ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _storage_classes\n",
      "{<class 'torch.cuda.ByteStorage'>, <class 'torch.CharStorage'>, <class 'torch.cuda.ShortStorage'>, <class 'torch.IntStorage'>, <class 'torch.QInt8Storage'>, <class 'torch.cuda.LongStorage'>, <class 'torch.HalfStorage'>, <class 'torch.cuda.ComplexFloatStorage'>, <class 'torch.ComplexFloatStorage'>, <class 'torch.cuda.DoubleStorage'>, <class 'torch.DoubleStorage'>, <class 'torch.cuda.BFloat16Storage'>, <class 'torch.BFloat16Storage'>, <class 'torch.cuda.HalfStorage'>, <class 'torch.ByteStorage'>, <class 'torch.cuda.CharStorage'>, <class 'torch.ShortStorage'>, <class 'torch.QInt32Storage'>, <class 'torch.cuda.IntStorage'>, <class 'torch.LongStorage'>, <class 'torch.QUInt8Storage'>, <class 'torch.cuda.FloatStorage'>, <class 'torch.FloatStorage'>, <class 'torch.cuda.ComplexDoubleStorage'>, <class 'torch.ComplexDoubleStorage'>, <class 'torch.cuda.BoolStorage'>, <class 'torch.BoolStorage'>} <class 'set'> \n",
      "_storage_classes []\n",
      "checking _string_classes\n",
      "(<class 'str'>, <class 'bytes'>) <class 'tuple'> \n",
      "_string_classes []\n",
      "checking _tensor_classes\n",
      "{<class 'torch.cuda.sparse.DoubleTensor'>, <class 'torch.sparse.HalfTensor'>, <class 'torch.sparse.FloatTensor'>, <class 'torch.cuda.BFloat16Tensor'>, <class 'torch.cuda.LongTensor'>, <class 'torch.cuda.CharTensor'>, <class 'torch.HalfTensor'>, <class 'torch.cuda.sparse.HalfTensor'>, <class 'torch.FloatTensor'>, <class 'torch.cuda.sparse.FloatTensor'>, <class 'torch.sparse.BFloat16Tensor'>, <class 'torch.sparse.IntTensor'>, <class 'torch.sparse.ByteTensor'>, <class 'torch.cuda.ShortTensor'>, <class 'torch.cuda.DoubleTensor'>, <class 'torch.BoolTensor'>, <class 'torch.cuda.sparse.BFloat16Tensor'>, <class 'torch.IntTensor'>, <class 'torch.cuda.sparse.IntTensor'>, <class 'torch.ByteTensor'>, <class 'torch.cuda.sparse.ByteTensor'>, <class 'torch.sparse.LongTensor'>, <class 'torch.sparse.CharTensor'>, <class 'torch.cuda.HalfTensor'>, <class 'torch.cuda.FloatTensor'>, <class 'torch.BFloat16Tensor'>, <class 'torch.LongTensor'>, <class 'torch.cuda.sparse.LongTensor'>, <class 'torch.CharTensor'>, <class 'torch.cuda.sparse.CharTensor'>, <class 'torch.sparse.ShortTensor'>, <class 'torch.sparse.DoubleTensor'>, <class 'torch.cuda.BoolTensor'>, <class 'torch.cuda.IntTensor'>, <class 'torch.cuda.ByteTensor'>, <class 'torch.ShortTensor'>, <class 'torch.cuda.sparse.ShortTensor'>, <class 'torch.DoubleTensor'>} <class 'set'> \n",
      "_tensor_classes []\n",
      "checking _tensor_str\n",
      "<module 'torch._tensor_str' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_tensor_str.py'> <class 'module'> torch._tensor_str\n",
      "_tensor_str ['ismodule']\n",
      "checking _test_serialization_subcmul\n",
      "<built-in method _test_serialization_subcmul of type object at 0x1144465b0> <class 'builtin_function_or_method'> _test_serialization_subcmul\n",
      "_test_serialization_subcmul ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _trilinear\n",
      "<built-in method _trilinear of type object at 0x1144465b0> <class 'builtin_function_or_method'> _trilinear\n",
      "_trilinear ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _unique\n",
      "<built-in method _unique of type object at 0x1144465b0> <class 'builtin_function_or_method'> _unique\n",
      "_unique ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _unique2\n",
      "<built-in method _unique2 of type object at 0x1144465b0> <class 'builtin_function_or_method'> _unique2\n",
      "_unique2 ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _use_cudnn_ctc_loss\n",
      "<built-in method _use_cudnn_ctc_loss of type object at 0x1144465b0> <class 'builtin_function_or_method'> _use_cudnn_ctc_loss\n",
      "_use_cudnn_ctc_loss ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _use_cudnn_rnn_flatten_weight\n",
      "<built-in method _use_cudnn_rnn_flatten_weight of type object at 0x1144465b0> <class 'builtin_function_or_method'> _use_cudnn_rnn_flatten_weight\n",
      "_use_cudnn_rnn_flatten_weight ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _utils\n",
      "<module 'torch._utils' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_utils.py'> <class 'module'> torch._utils\n",
      "_utils ['ismodule']\n",
      "checking _utils_internal\n",
      "<module 'torch._utils_internal' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_utils_internal.py'> <class 'module'> torch._utils_internal\n",
      "_utils_internal ['ismodule']\n",
      "checking _weight_norm\n",
      "<built-in method _weight_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> _weight_norm\n",
      "_weight_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking _weight_norm_cuda_interface\n",
      "<built-in method _weight_norm_cuda_interface of type object at 0x1144465b0> <class 'builtin_function_or_method'> _weight_norm_cuda_interface\n",
      "_weight_norm_cuda_interface ['isbuiltin', 'isroutine', 'callable']\n",
      "checking abs\n",
      "<built-in method abs of type object at 0x1144465b0> <class 'builtin_function_or_method'> abs\n",
      "abs ['isbuiltin', 'isroutine', 'callable']\n",
      "checking abs_\n",
      "<built-in method abs_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> abs_\n",
      "abs_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking absolute\n",
      "<built-in method absolute of type object at 0x1144465b0> <class 'builtin_function_or_method'> absolute\n",
      "absolute ['isbuiltin', 'isroutine', 'callable']\n",
      "checking absolute_\n",
      "<built-in method absolute_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> absolute_\n",
      "absolute_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking acos\n",
      "<built-in method acos of type object at 0x1144465b0> <class 'builtin_function_or_method'> acos\n",
      "acos ['isbuiltin', 'isroutine', 'callable']\n",
      "checking acos_\n",
      "<built-in method acos_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> acos_\n",
      "acos_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking acosh\n",
      "<built-in method acosh of type object at 0x1144465b0> <class 'builtin_function_or_method'> acosh\n",
      "acosh ['isbuiltin', 'isroutine', 'callable']\n",
      "checking acosh_\n",
      "<built-in method acosh_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> acosh_\n",
      "acosh_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking adaptive_avg_pool1d\n",
      "<built-in method adaptive_avg_pool1d of type object at 0x1144465b0> <class 'builtin_function_or_method'> adaptive_avg_pool1d\n",
      "adaptive_avg_pool1d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking adaptive_max_pool1d\n",
      "<built-in method adaptive_max_pool1d of type object at 0x1144465b0> <class 'builtin_function_or_method'> adaptive_max_pool1d\n",
      "adaptive_max_pool1d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking add\n",
      "<built-in method add of type object at 0x1144465b0> <class 'builtin_function_or_method'> add\n",
      "add ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find add in native_func_dict\n",
      "Cant find add in native_func_dict\n",
      "Cant find add in native_func_dict\n",
      "checking addbmm\n",
      "<built-in method addbmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> addbmm\n",
      "addbmm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking addcdiv\n",
      "<built-in method addcdiv of type object at 0x1144465b0> <class 'builtin_function_or_method'> addcdiv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addcdiv ['isbuiltin', 'isroutine', 'callable']\n",
      "checking addcmul\n",
      "<built-in method addcmul of type object at 0x1144465b0> <class 'builtin_function_or_method'> addcmul\n",
      "addcmul ['isbuiltin', 'isroutine', 'callable']\n",
      "checking addmm\n",
      "<built-in method addmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> addmm\n",
      "addmm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking addmv\n",
      "<built-in method addmv of type object at 0x1144465b0> <class 'builtin_function_or_method'> addmv\n",
      "addmv ['isbuiltin', 'isroutine', 'callable']\n",
      "checking addmv_\n",
      "<built-in method addmv_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> addmv_\n",
      "addmv_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking addr\n",
      "<built-in method addr of type object at 0x1144465b0> <class 'builtin_function_or_method'> addr\n",
      "addr ['isbuiltin', 'isroutine', 'callable']\n",
      "checking affine_grid_generator\n",
      "<built-in method affine_grid_generator of type object at 0x1144465b0> <class 'builtin_function_or_method'> affine_grid_generator\n",
      "affine_grid_generator ['isbuiltin', 'isroutine', 'callable']\n",
      "checking align_tensors\n",
      "<function align_tensors at 0x137712950> <class 'function'> align_tensors\n",
      "align_tensors ['isfunction', 'isroutine', 'callable']\n",
      "checking all\n",
      "<built-in method all of type object at 0x1144465b0> <class 'builtin_function_or_method'> all\n",
      "all ['isbuiltin', 'isroutine', 'callable']\n",
      "checking allclose\n",
      "<built-in method allclose of type object at 0x1144465b0> <class 'builtin_function_or_method'> allclose\n",
      "allclose ['isbuiltin', 'isroutine', 'callable']\n",
      "checking alpha_dropout\n",
      "<built-in method alpha_dropout of type object at 0x1144465b0> <class 'builtin_function_or_method'> alpha_dropout\n",
      "alpha_dropout ['isbuiltin', 'isroutine', 'callable']\n",
      "checking alpha_dropout_\n",
      "<built-in method alpha_dropout_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> alpha_dropout_\n",
      "alpha_dropout_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking angle\n",
      "<built-in method angle of type object at 0x1144465b0> <class 'builtin_function_or_method'> angle\n",
      "angle ['isbuiltin', 'isroutine', 'callable']\n",
      "checking any\n",
      "<built-in method any of type object at 0x1144465b0> <class 'builtin_function_or_method'> any\n",
      "any ['isbuiltin', 'isroutine', 'callable']\n",
      "checking arange\n",
      "<built-in method arange of type object at 0x1144465b0> <class 'builtin_function_or_method'> arange\n",
      "arange ['isbuiltin', 'isroutine', 'callable']\n",
      "checking argmax\n",
      "<built-in method argmax of type object at 0x1144465b0> <class 'builtin_function_or_method'> argmax\n",
      "argmax ['isbuiltin', 'isroutine', 'callable']\n",
      "checking argmin\n",
      "<built-in method argmin of type object at 0x1144465b0> <class 'builtin_function_or_method'> argmin\n",
      "argmin ['isbuiltin', 'isroutine', 'callable']\n",
      "checking argsort\n",
      "<built-in method argsort of type object at 0x1144465b0> <class 'builtin_function_or_method'> argsort\n",
      "argsort ['isbuiltin', 'isroutine', 'callable']\n",
      "checking as_strided\n",
      "<built-in method as_strided of type object at 0x1144465b0> <class 'builtin_function_or_method'> as_strided\n",
      "as_strided ['isbuiltin', 'isroutine', 'callable']\n",
      "checking as_strided_\n",
      "<built-in method as_strided_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> as_strided_\n",
      "as_strided_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking as_tensor\n",
      "<built-in method as_tensor of type object at 0x1144465b0> <class 'builtin_function_or_method'> as_tensor\n",
      "as_tensor ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find as_tensor in native_func_dict\n",
      "Cant find as_tensor in native_func_dict\n",
      "Cant find as_tensor in native_func_dict\n",
      "checking asin\n",
      "<built-in method asin of type object at 0x1144465b0> <class 'builtin_function_or_method'> asin\n",
      "asin ['isbuiltin', 'isroutine', 'callable']\n",
      "checking asin_\n",
      "<built-in method asin_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> asin_\n",
      "asin_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking asinh\n",
      "<built-in method asinh of type object at 0x1144465b0> <class 'builtin_function_or_method'> asinh\n",
      "asinh ['isbuiltin', 'isroutine', 'callable']\n",
      "checking asinh_\n",
      "<built-in method asinh_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> asinh_\n",
      "asinh_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking atan\n",
      "<built-in method atan of type object at 0x1144465b0> <class 'builtin_function_or_method'> atan\n",
      "atan ['isbuiltin', 'isroutine', 'callable']\n",
      "checking atan2\n",
      "<built-in method atan2 of type object at 0x1144465b0> <class 'builtin_function_or_method'> atan2\n",
      "atan2 ['isbuiltin', 'isroutine', 'callable']\n",
      "checking atan_\n",
      "<built-in method atan_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> atan_\n",
      "atan_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking atanh\n",
      "<built-in method atanh of type object at 0x1144465b0> <class 'builtin_function_or_method'> atanh\n",
      "atanh ['isbuiltin', 'isroutine', 'callable']\n",
      "checking atanh_\n",
      "<built-in method atanh_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> atanh_\n",
      "atanh_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking autocast_decrement_nesting\n",
      "<built-in function autocast_decrement_nesting> <class 'builtin_function_or_method'> autocast_decrement_nesting\n",
      "autocast_decrement_nesting ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find autocast_decrement_nesting in native_func_dict\n",
      "Cant find autocast_decrement_nesting in native_func_dict\n",
      "Cant find autocast_decrement_nesting in native_func_dict\n",
      "checking autocast_increment_nesting\n",
      "<built-in function autocast_increment_nesting> <class 'builtin_function_or_method'> autocast_increment_nesting\n",
      "autocast_increment_nesting ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find autocast_increment_nesting in native_func_dict\n",
      "Cant find autocast_increment_nesting in native_func_dict\n",
      "Cant find autocast_increment_nesting in native_func_dict\n",
      "checking autograd\n",
      "<module 'torch.autograd' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/autograd/__init__.py'> <class 'module'> torch.autograd\n",
      "autograd ['ismodule']\n",
      "checking avg_pool1d\n",
      "<built-in method avg_pool1d of type object at 0x1144465b0> <class 'builtin_function_or_method'> avg_pool1d\n",
      "avg_pool1d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking backends\n",
      "<module 'torch.backends' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/backends/__init__.py'> <class 'module'> torch.backends\n",
      "backends ['ismodule']\n",
      "checking baddbmm\n",
      "<built-in method baddbmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> baddbmm\n",
      "baddbmm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking bartlett_window\n",
      "<built-in method bartlett_window of type object at 0x1144465b0> <class 'builtin_function_or_method'> bartlett_window\n",
      "bartlett_window ['isbuiltin', 'isroutine', 'callable']\n",
      "checking batch_norm\n",
      "<built-in method batch_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> batch_norm\n",
      "batch_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking batch_norm_backward_elemt\n",
      "<built-in method batch_norm_backward_elemt of type object at 0x1144465b0> <class 'builtin_function_or_method'> batch_norm_backward_elemt\n",
      "batch_norm_backward_elemt ['isbuiltin', 'isroutine', 'callable']\n",
      "checking batch_norm_backward_reduce\n",
      "<built-in method batch_norm_backward_reduce of type object at 0x1144465b0> <class 'builtin_function_or_method'> batch_norm_backward_reduce\n",
      "batch_norm_backward_reduce ['isbuiltin', 'isroutine', 'callable']\n",
      "checking batch_norm_elemt\n",
      "<built-in method batch_norm_elemt of type object at 0x1144465b0> <class 'builtin_function_or_method'> batch_norm_elemt\n",
      "batch_norm_elemt ['isbuiltin', 'isroutine', 'callable']\n",
      "checking batch_norm_gather_stats\n",
      "<built-in method batch_norm_gather_stats of type object at 0x1144465b0> <class 'builtin_function_or_method'> batch_norm_gather_stats\n",
      "batch_norm_gather_stats ['isbuiltin', 'isroutine', 'callable']\n",
      "checking batch_norm_gather_stats_with_counts\n",
      "<built-in method batch_norm_gather_stats_with_counts of type object at 0x1144465b0> <class 'builtin_function_or_method'> batch_norm_gather_stats_with_counts\n",
      "batch_norm_gather_stats_with_counts ['isbuiltin', 'isroutine', 'callable']\n",
      "checking batch_norm_stats\n",
      "<built-in method batch_norm_stats of type object at 0x1144465b0> <class 'builtin_function_or_method'> batch_norm_stats\n",
      "batch_norm_stats ['isbuiltin', 'isroutine', 'callable']\n",
      "checking batch_norm_update_stats\n",
      "<built-in method batch_norm_update_stats of type object at 0x1144465b0> <class 'builtin_function_or_method'> batch_norm_update_stats\n",
      "batch_norm_update_stats ['isbuiltin', 'isroutine', 'callable']\n",
      "checking bernoulli\n",
      "<built-in method bernoulli of type object at 0x1144465b0> <class 'builtin_function_or_method'> bernoulli\n",
      "bernoulli ['isbuiltin', 'isroutine', 'callable']\n",
      "checking bfloat16\n",
      "torch.bfloat16 <class 'torch.dtype'> \n",
      "bfloat16 []\n",
      "checking bilinear\n",
      "<built-in method bilinear of type object at 0x1144465b0> <class 'builtin_function_or_method'> bilinear\n",
      "bilinear ['isbuiltin', 'isroutine', 'callable']\n",
      "checking binary_cross_entropy_with_logits\n",
      "<built-in method binary_cross_entropy_with_logits of type object at 0x1144465b0> <class 'builtin_function_or_method'> binary_cross_entropy_with_logits\n",
      "binary_cross_entropy_with_logits ['isbuiltin', 'isroutine', 'callable']\n",
      "checking bincount\n",
      "<built-in method bincount of type object at 0x1144465b0> <class 'builtin_function_or_method'> bincount\n",
      "bincount ['isbuiltin', 'isroutine', 'callable']\n",
      "checking binomial\n",
      "<built-in method binomial of type object at 0x1144465b0> <class 'builtin_function_or_method'> binomial\n",
      "binomial ['isbuiltin', 'isroutine', 'callable']\n",
      "checking bitwise_and\n",
      "<built-in method bitwise_and of type object at 0x1144465b0> <class 'builtin_function_or_method'> bitwise_and\n",
      "bitwise_and ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find bitwise_and in native_func_dict\n",
      "Cant find bitwise_and in native_func_dict\n",
      "Cant find bitwise_and in native_func_dict\n",
      "checking bitwise_not\n",
      "<built-in method bitwise_not of type object at 0x1144465b0> <class 'builtin_function_or_method'> bitwise_not\n",
      "bitwise_not ['isbuiltin', 'isroutine', 'callable']\n",
      "checking bitwise_or\n",
      "<built-in method bitwise_or of type object at 0x1144465b0> <class 'builtin_function_or_method'> bitwise_or\n",
      "bitwise_or ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find bitwise_or in native_func_dict\n",
      "Cant find bitwise_or in native_func_dict\n",
      "Cant find bitwise_or in native_func_dict\n",
      "checking bitwise_xor\n",
      "<built-in method bitwise_xor of type object at 0x1144465b0> <class 'builtin_function_or_method'> bitwise_xor\n",
      "bitwise_xor ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find bitwise_xor in native_func_dict\n",
      "Cant find bitwise_xor in native_func_dict\n",
      "Cant find bitwise_xor in native_func_dict\n",
      "checking blackman_window\n",
      "<built-in method blackman_window of type object at 0x1144465b0> <class 'builtin_function_or_method'> blackman_window\n",
      "blackman_window ['isbuiltin', 'isroutine', 'callable']\n",
      "checking block_diag\n",
      "<function block_diag at 0x137712268> <class 'function'> block_diag\n",
      "block_diag ['isfunction', 'isroutine', 'callable']\n",
      "checking bmm\n",
      "<built-in method bmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> bmm\n",
      "bmm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking bool\n",
      "torch.bool <class 'torch.dtype'> \n",
      "bool []\n",
      "checking broadcast_tensors\n",
      "<function broadcast_tensors at 0x115bf8378> <class 'function'> broadcast_tensors\n",
      "broadcast_tensors ['isfunction', 'isroutine', 'callable']\n",
      "checking bucketize\n",
      "<built-in method bucketize of type object at 0x1144465b0> <class 'builtin_function_or_method'> bucketize\n",
      "bucketize ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find bucketize in native_func_dict\n",
      "Cant find bucketize in native_func_dict\n",
      "Cant find bucketize in native_func_dict\n",
      "checking can_cast\n",
      "<built-in method can_cast of type object at 0x1144465b0> <class 'builtin_function_or_method'> can_cast\n",
      "can_cast ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cartesian_prod\n",
      "<function cartesian_prod at 0x1377121e0> <class 'function'> cartesian_prod\n",
      "cartesian_prod ['isfunction', 'isroutine', 'callable']\n",
      "checking cat\n",
      "<built-in method cat of type object at 0x1144465b0> <class 'builtin_function_or_method'> cat\n",
      "cat ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cdist\n",
      "<function cdist at 0x1377122f0> <class 'function'> cdist\n",
      "cdist ['isfunction', 'isroutine', 'callable']\n",
      "checking cdouble\n",
      "torch.complex128 <class 'torch.dtype'> \n",
      "cdouble []\n",
      "checking ceil\n",
      "<built-in method ceil of type object at 0x1144465b0> <class 'builtin_function_or_method'> ceil\n",
      "ceil ['isbuiltin', 'isroutine', 'callable']\n",
      "checking ceil_\n",
      "<built-in method ceil_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> ceil_\n",
      "ceil_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking celu\n",
      "<built-in method celu of type object at 0x1144465b0> <class 'builtin_function_or_method'> celu\n",
      "celu ['isbuiltin', 'isroutine', 'callable']\n",
      "checking celu_\n",
      "<built-in method celu_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> celu_\n",
      "celu_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cfloat\n",
      "torch.complex64 <class 'torch.dtype'> \n",
      "cfloat []\n",
      "checking chain_matmul\n",
      "<function chain_matmul at 0x137712620> <class 'function'> chain_matmul\n",
      "chain_matmul ['isfunction', 'isroutine', 'callable']\n",
      "checking channel_shuffle\n",
      "<built-in method channel_shuffle of type object at 0x1144465b0> <class 'builtin_function_or_method'> channel_shuffle\n",
      "channel_shuffle ['isbuiltin', 'isroutine', 'callable']\n",
      "checking channels_last\n",
      "torch.channels_last <class 'torch.memory_format'> \n",
      "channels_last []\n",
      "checking channels_last_3d\n",
      "torch.channels_last_3d <class 'torch.memory_format'> \n",
      "channels_last_3d []\n",
      "checking cholesky\n",
      "<built-in method cholesky of type object at 0x1144465b0> <class 'builtin_function_or_method'> cholesky\n",
      "cholesky ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cholesky_inverse\n",
      "<built-in method cholesky_inverse of type object at 0x1144465b0> <class 'builtin_function_or_method'> cholesky_inverse\n",
      "cholesky_inverse ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cholesky_solve\n",
      "<built-in method cholesky_solve of type object at 0x1144465b0> <class 'builtin_function_or_method'> cholesky_solve\n",
      "cholesky_solve ['isbuiltin', 'isroutine', 'callable']\n",
      "checking chunk\n",
      "<built-in method chunk of type object at 0x1144465b0> <class 'builtin_function_or_method'> chunk\n",
      "chunk ['isbuiltin', 'isroutine', 'callable']\n",
      "checking clamp\n",
      "<built-in method clamp of type object at 0x1144465b0> <class 'builtin_function_or_method'> clamp\n",
      "clamp ['isbuiltin', 'isroutine', 'callable']\n",
      "checking clamp_\n",
      "<built-in method clamp_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> clamp_\n",
      "clamp_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking clamp_max\n",
      "<built-in method clamp_max of type object at 0x1144465b0> <class 'builtin_function_or_method'> clamp_max\n",
      "clamp_max ['isbuiltin', 'isroutine', 'callable']\n",
      "checking clamp_max_\n",
      "<built-in method clamp_max_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> clamp_max_\n",
      "clamp_max_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking clamp_min\n",
      "<built-in method clamp_min of type object at 0x1144465b0> <class 'builtin_function_or_method'> clamp_min\n",
      "clamp_min ['isbuiltin', 'isroutine', 'callable']\n",
      "checking clamp_min_\n",
      "<built-in method clamp_min_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> clamp_min_\n",
      "clamp_min_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking classes\n",
      "Exception with classes. Tried to instantiate class '__file__.__file__', but it does not exist! Ensure that it is registered via torch::jit::class_\n",
      "checking clear_autocast_cache\n",
      "<built-in function clear_autocast_cache> <class 'builtin_function_or_method'> clear_autocast_cache\n",
      "clear_autocast_cache ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find clear_autocast_cache in native_func_dict\n",
      "Cant find clear_autocast_cache in native_func_dict\n",
      "Cant find clear_autocast_cache in native_func_dict\n",
      "checking clone\n",
      "<built-in method clone of type object at 0x1144465b0> <class 'builtin_function_or_method'> clone\n",
      "clone ['isbuiltin', 'isroutine', 'callable']\n",
      "checking combinations\n",
      "<built-in method combinations of type object at 0x1144465b0> <class 'builtin_function_or_method'> combinations\n",
      "combinations ['isbuiltin', 'isroutine', 'callable']\n",
      "checking compiled_with_cxx11_abi\n",
      "<function compiled_with_cxx11_abi at 0x115a3fbf8> <class 'function'> compiled_with_cxx11_abi\n",
      "compiled_with_cxx11_abi ['isfunction', 'isroutine', 'callable']\n",
      "checking complex128\n",
      "torch.complex128 <class 'torch.dtype'> \n",
      "complex128 []\n",
      "checking complex32\n",
      "torch.complex32 <class 'torch.dtype'> \n",
      "complex32 []\n",
      "checking complex64\n",
      "torch.complex64 <class 'torch.dtype'> \n",
      "complex64 []\n",
      "checking conj\n",
      "<built-in method conj of type object at 0x1144465b0> <class 'builtin_function_or_method'> conj\n",
      "conj ['isbuiltin', 'isroutine', 'callable']\n",
      "checking constant_pad_nd\n",
      "<built-in method constant_pad_nd of type object at 0x1144465b0> <class 'builtin_function_or_method'> constant_pad_nd\n",
      "constant_pad_nd ['isbuiltin', 'isroutine', 'callable']\n",
      "checking contiguous_format\n",
      "torch.contiguous_format <class 'torch.memory_format'> \n",
      "contiguous_format []\n",
      "checking conv1d\n",
      "<built-in method conv1d of type object at 0x1144465b0> <class 'builtin_function_or_method'> conv1d\n",
      "conv1d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking conv2d\n",
      "<built-in method conv2d of type object at 0x1144465b0> <class 'builtin_function_or_method'> conv2d\n",
      "conv2d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking conv3d\n",
      "<built-in method conv3d of type object at 0x1144465b0> <class 'builtin_function_or_method'> conv3d\n",
      "conv3d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking conv_tbc\n",
      "<built-in method conv_tbc of type object at 0x1144465b0> <class 'builtin_function_or_method'> conv_tbc\n",
      "conv_tbc ['isbuiltin', 'isroutine', 'callable']\n",
      "checking conv_transpose1d\n",
      "<built-in method conv_transpose1d of type object at 0x1144465b0> <class 'builtin_function_or_method'> conv_transpose1d\n",
      "conv_transpose1d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking conv_transpose2d\n",
      "<built-in method conv_transpose2d of type object at 0x1144465b0> <class 'builtin_function_or_method'> conv_transpose2d\n",
      "conv_transpose2d ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find conv_transpose2d in native_func_dict\n",
      "Cant find conv_transpose2d in native_func_dict\n",
      "Cant find conv_transpose2d in native_func_dict\n",
      "checking conv_transpose3d\n",
      "<built-in method conv_transpose3d of type object at 0x1144465b0> <class 'builtin_function_or_method'> conv_transpose3d\n",
      "conv_transpose3d ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find conv_transpose3d in native_func_dict\n",
      "Cant find conv_transpose3d in native_func_dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cant find conv_transpose3d in native_func_dict\n",
      "checking convolution\n",
      "<built-in method convolution of type object at 0x1144465b0> <class 'builtin_function_or_method'> convolution\n",
      "convolution ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cos\n",
      "<built-in method cos of type object at 0x1144465b0> <class 'builtin_function_or_method'> cos\n",
      "cos ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cos_\n",
      "<built-in method cos_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> cos_\n",
      "cos_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cosh\n",
      "<built-in method cosh of type object at 0x1144465b0> <class 'builtin_function_or_method'> cosh\n",
      "cosh ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cosh_\n",
      "<built-in method cosh_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> cosh_\n",
      "cosh_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cosine_embedding_loss\n",
      "<built-in method cosine_embedding_loss of type object at 0x1144465b0> <class 'builtin_function_or_method'> cosine_embedding_loss\n",
      "cosine_embedding_loss ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cosine_similarity\n",
      "<built-in method cosine_similarity of type object at 0x1144465b0> <class 'builtin_function_or_method'> cosine_similarity\n",
      "cosine_similarity ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cpp\n",
      "<module 'torch._C.cpp'> <class 'module'> torch._C.cpp\n",
      "cpp ['ismodule']\n",
      "checking cross\n",
      "<built-in method cross of type object at 0x1144465b0> <class 'builtin_function_or_method'> cross\n",
      "cross ['isbuiltin', 'isroutine', 'callable']\n",
      "checking ctc_loss\n",
      "<built-in method ctc_loss of type object at 0x1144465b0> <class 'builtin_function_or_method'> ctc_loss\n",
      "ctc_loss ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find ctc_loss in native_func_dict\n",
      "Cant find ctc_loss in native_func_dict\n",
      "Cant find ctc_loss in native_func_dict\n",
      "checking ctypes\n",
      "<module 'ctypes' from '/Users/madhavajay/.pyenv/versions/3.6.11/lib/python3.6/ctypes/__init__.py'> <class 'module'> ctypes\n",
      "ctypes ['ismodule']\n",
      "checking cuda\n",
      "<module 'torch.cuda' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/cuda/__init__.py'> <class 'module'> torch.cuda\n",
      "cuda ['ismodule']\n",
      "checking cudnn_affine_grid_generator\n",
      "<built-in method cudnn_affine_grid_generator of type object at 0x1144465b0> <class 'builtin_function_or_method'> cudnn_affine_grid_generator\n",
      "cudnn_affine_grid_generator ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cudnn_batch_norm\n",
      "<built-in method cudnn_batch_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> cudnn_batch_norm\n",
      "cudnn_batch_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cudnn_convolution\n",
      "<built-in method cudnn_convolution of type object at 0x1144465b0> <class 'builtin_function_or_method'> cudnn_convolution\n",
      "cudnn_convolution ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cudnn_convolution_transpose\n",
      "<built-in method cudnn_convolution_transpose of type object at 0x1144465b0> <class 'builtin_function_or_method'> cudnn_convolution_transpose\n",
      "cudnn_convolution_transpose ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cudnn_grid_sampler\n",
      "<built-in method cudnn_grid_sampler of type object at 0x1144465b0> <class 'builtin_function_or_method'> cudnn_grid_sampler\n",
      "cudnn_grid_sampler ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cudnn_is_acceptable\n",
      "<built-in method cudnn_is_acceptable of type object at 0x1144465b0> <class 'builtin_function_or_method'> cudnn_is_acceptable\n",
      "cudnn_is_acceptable ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cummax\n",
      "<built-in method cummax of type object at 0x1144465b0> <class 'builtin_function_or_method'> cummax\n",
      "cummax ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cummin\n",
      "<built-in method cummin of type object at 0x1144465b0> <class 'builtin_function_or_method'> cummin\n",
      "cummin ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cumprod\n",
      "<built-in method cumprod of type object at 0x1144465b0> <class 'builtin_function_or_method'> cumprod\n",
      "cumprod ['isbuiltin', 'isroutine', 'callable']\n",
      "checking cumsum\n",
      "<built-in method cumsum of type object at 0x1144465b0> <class 'builtin_function_or_method'> cumsum\n",
      "cumsum ['isbuiltin', 'isroutine', 'callable']\n",
      "checking default_generator\n",
      "<torch._C.Generator object at 0x1106bde58> <class 'torch._C.Generator'> \n",
      "default_generator []\n",
      "checking deg2rad\n",
      "<built-in method deg2rad of type object at 0x1144465b0> <class 'builtin_function_or_method'> deg2rad\n",
      "deg2rad ['isbuiltin', 'isroutine', 'callable']\n",
      "checking deg2rad_\n",
      "<built-in method deg2rad_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> deg2rad_\n",
      "deg2rad_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking dequantize\n",
      "<built-in method dequantize of type object at 0x1144465b0> <class 'builtin_function_or_method'> dequantize\n",
      "dequantize ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find dequantize in native_func_dict\n",
      "Cant find dequantize in native_func_dict\n",
      "Cant find dequantize in native_func_dict\n",
      "checking det\n",
      "<built-in method det of type object at 0x1144465b0> <class 'builtin_function_or_method'> det\n",
      "det ['isbuiltin', 'isroutine', 'callable']\n",
      "checking detach\n",
      "<built-in method detach of type object at 0x1144465b0> <class 'builtin_function_or_method'> detach\n",
      "detach ['isbuiltin', 'isroutine', 'callable']\n",
      "checking detach_\n",
      "<built-in method detach_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> detach_\n",
      "detach_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking device\n",
      "<class 'torch.device'> <class 'type'> device\n",
      "Hierarchy: [<class 'torch.device'>, <class 'object'>]\n",
      "device ['isclass', 'callable']\n",
      "checking diag\n",
      "<built-in method diag of type object at 0x1144465b0> <class 'builtin_function_or_method'> diag\n",
      "diag ['isbuiltin', 'isroutine', 'callable']\n",
      "checking diag_embed\n",
      "<built-in method diag_embed of type object at 0x1144465b0> <class 'builtin_function_or_method'> diag_embed\n",
      "diag_embed ['isbuiltin', 'isroutine', 'callable']\n",
      "checking diagflat\n",
      "<built-in method diagflat of type object at 0x1144465b0> <class 'builtin_function_or_method'> diagflat\n",
      "diagflat ['isbuiltin', 'isroutine', 'callable']\n",
      "checking diagonal\n",
      "<built-in method diagonal of type object at 0x1144465b0> <class 'builtin_function_or_method'> diagonal\n",
      "diagonal ['isbuiltin', 'isroutine', 'callable']\n",
      "checking digamma\n",
      "<built-in method digamma of type object at 0x1144465b0> <class 'builtin_function_or_method'> digamma\n",
      "digamma ['isbuiltin', 'isroutine', 'callable']\n",
      "checking dist\n",
      "<built-in method dist of type object at 0x1144465b0> <class 'builtin_function_or_method'> dist\n",
      "dist ['isbuiltin', 'isroutine', 'callable']\n",
      "checking distributed\n",
      "<module 'torch.distributed' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/distributed/__init__.py'> <class 'module'> torch.distributed\n",
      "distributed ['ismodule']\n",
      "checking distributions\n",
      "<module 'torch.distributions' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/distributions/__init__.py'> <class 'module'> torch.distributions\n",
      "distributions ['ismodule']\n",
      "checking div\n",
      "<built-in method div of type object at 0x1144465b0> <class 'builtin_function_or_method'> div\n",
      "div ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find div in native_func_dict\n",
      "Cant find div in native_func_dict\n",
      "Cant find div in native_func_dict\n",
      "checking dot\n",
      "<built-in method dot of type object at 0x1144465b0> <class 'builtin_function_or_method'> dot\n",
      "dot ['isbuiltin', 'isroutine', 'callable']\n",
      "checking double\n",
      "torch.float64 <class 'torch.dtype'> \n",
      "double []\n",
      "checking dropout\n",
      "<built-in method dropout of type object at 0x1144465b0> <class 'builtin_function_or_method'> dropout\n",
      "dropout ['isbuiltin', 'isroutine', 'callable']\n",
      "checking dropout_\n",
      "<built-in method dropout_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> dropout_\n",
      "dropout_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking dsmm\n",
      "<built-in method dsmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> dsmm\n",
      "dsmm ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find dsmm in native_func_dict\n",
      "Cant find dsmm in native_func_dict\n",
      "Cant find dsmm in native_func_dict\n",
      "checking dtype\n",
      "<class 'torch.dtype'> <class 'type'> dtype\n",
      "Hierarchy: [<class 'torch.dtype'>, <class 'object'>]\n",
      "dtype ['isclass', 'callable']\n",
      "checking eig\n",
      "<built-in method eig of type object at 0x1144465b0> <class 'builtin_function_or_method'> eig\n",
      "eig ['isbuiltin', 'isroutine', 'callable']\n",
      "checking einsum\n",
      "<function einsum at 0x13770f730> <class 'function'> einsum\n",
      "einsum ['isfunction', 'isroutine', 'callable']\n",
      "checking embedding\n",
      "<built-in method embedding of type object at 0x1144465b0> <class 'builtin_function_or_method'> embedding\n",
      "embedding ['isbuiltin', 'isroutine', 'callable']\n",
      "checking embedding_bag\n",
      "<built-in method embedding_bag of type object at 0x1144465b0> <class 'builtin_function_or_method'> embedding_bag\n",
      "embedding_bag ['isbuiltin', 'isroutine', 'callable']\n",
      "checking embedding_renorm_\n",
      "<built-in method embedding_renorm_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> embedding_renorm_\n",
      "embedding_renorm_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking empty\n",
      "<built-in method empty of type object at 0x1144465b0> <class 'builtin_function_or_method'> empty\n",
      "empty ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find empty in native_func_dict\n",
      "Cant find empty in native_func_dict\n",
      "Cant find empty in native_func_dict\n",
      "checking empty_like\n",
      "<built-in method empty_like of type object at 0x1144465b0> <class 'builtin_function_or_method'> empty_like\n",
      "empty_like ['isbuiltin', 'isroutine', 'callable']\n",
      "checking empty_meta\n",
      "<built-in method empty_meta of type object at 0x1144465b0> <class 'builtin_function_or_method'> empty_meta\n",
      "empty_meta ['isbuiltin', 'isroutine', 'callable']\n",
      "checking empty_quantized\n",
      "<built-in method empty_quantized of type object at 0x1144465b0> <class 'builtin_function_or_method'> empty_quantized\n",
      "empty_quantized ['isbuiltin', 'isroutine', 'callable']\n",
      "checking empty_strided\n",
      "<built-in method empty_strided of type object at 0x1144465b0> <class 'builtin_function_or_method'> empty_strided\n",
      "empty_strided ['isbuiltin', 'isroutine', 'callable']\n",
      "checking enable_grad\n",
      "<class 'torch.autograd.grad_mode.enable_grad'> <class 'type'> enable_grad\n",
      "Hierarchy: [<class 'torch.autograd.grad_mode.enable_grad'>, <class 'torch.autograd.grad_mode._DecoratorContextManager'>, <class 'object'>]\n",
      "enable_grad ['isclass', 'callable']\n",
      "checking eq\n",
      "<built-in method eq of type object at 0x1144465b0> <class 'builtin_function_or_method'> eq\n",
      "eq ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find eq in native_func_dict\n",
      "Cant find eq in native_func_dict\n",
      "Cant find eq in native_func_dict\n",
      "checking equal\n",
      "<built-in method equal of type object at 0x1144465b0> <class 'builtin_function_or_method'> equal\n",
      "equal ['isbuiltin', 'isroutine', 'callable']\n",
      "checking erf\n",
      "<built-in method erf of type object at 0x1144465b0> <class 'builtin_function_or_method'> erf\n",
      "erf ['isbuiltin', 'isroutine', 'callable']\n",
      "checking erf_\n",
      "<built-in method erf_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> erf_\n",
      "erf_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking erfc\n",
      "<built-in method erfc of type object at 0x1144465b0> <class 'builtin_function_or_method'> erfc\n",
      "erfc ['isbuiltin', 'isroutine', 'callable']\n",
      "checking erfc_\n",
      "<built-in method erfc_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> erfc_\n",
      "erfc_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking erfinv\n",
      "<built-in method erfinv of type object at 0x1144465b0> <class 'builtin_function_or_method'> erfinv\n",
      "erfinv ['isbuiltin', 'isroutine', 'callable']\n",
      "checking exp\n",
      "<built-in method exp of type object at 0x1144465b0> <class 'builtin_function_or_method'> exp\n",
      "exp ['isbuiltin', 'isroutine', 'callable']\n",
      "checking exp_\n",
      "<built-in method exp_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> exp_\n",
      "exp_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking expm1\n",
      "<built-in method expm1 of type object at 0x1144465b0> <class 'builtin_function_or_method'> expm1\n",
      "expm1 ['isbuiltin', 'isroutine', 'callable']\n",
      "checking expm1_\n",
      "<built-in method expm1_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> expm1_\n",
      "expm1_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking eye\n",
      "<built-in method eye of type object at 0x1144465b0> <class 'builtin_function_or_method'> eye\n",
      "eye ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fake_quantize_per_channel_affine\n",
      "<built-in method fake_quantize_per_channel_affine of type object at 0x1144465b0> <class 'builtin_function_or_method'> fake_quantize_per_channel_affine\n",
      "fake_quantize_per_channel_affine ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fake_quantize_per_tensor_affine\n",
      "<built-in method fake_quantize_per_tensor_affine of type object at 0x1144465b0> <class 'builtin_function_or_method'> fake_quantize_per_tensor_affine\n",
      "fake_quantize_per_tensor_affine ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fbgemm_linear_fp16_weight\n",
      "<built-in method fbgemm_linear_fp16_weight of type object at 0x1144465b0> <class 'builtin_function_or_method'> fbgemm_linear_fp16_weight\n",
      "fbgemm_linear_fp16_weight ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fbgemm_linear_fp16_weight_fp32_activation\n",
      "<built-in method fbgemm_linear_fp16_weight_fp32_activation of type object at 0x1144465b0> <class 'builtin_function_or_method'> fbgemm_linear_fp16_weight_fp32_activation\n",
      "fbgemm_linear_fp16_weight_fp32_activation ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fbgemm_linear_int8_weight\n",
      "<built-in method fbgemm_linear_int8_weight of type object at 0x1144465b0> <class 'builtin_function_or_method'> fbgemm_linear_int8_weight\n",
      "fbgemm_linear_int8_weight ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fbgemm_linear_int8_weight_fp32_activation\n",
      "<built-in method fbgemm_linear_int8_weight_fp32_activation of type object at 0x1144465b0> <class 'builtin_function_or_method'> fbgemm_linear_int8_weight_fp32_activation\n",
      "fbgemm_linear_int8_weight_fp32_activation ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fbgemm_linear_quantize_weight\n",
      "<built-in method fbgemm_linear_quantize_weight of type object at 0x1144465b0> <class 'builtin_function_or_method'> fbgemm_linear_quantize_weight\n",
      "fbgemm_linear_quantize_weight ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fbgemm_pack_gemm_matrix_fp16\n",
      "<built-in method fbgemm_pack_gemm_matrix_fp16 of type object at 0x1144465b0> <class 'builtin_function_or_method'> fbgemm_pack_gemm_matrix_fp16\n",
      "fbgemm_pack_gemm_matrix_fp16 ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fbgemm_pack_quantized_matrix\n",
      "<built-in method fbgemm_pack_quantized_matrix of type object at 0x1144465b0> <class 'builtin_function_or_method'> fbgemm_pack_quantized_matrix\n",
      "fbgemm_pack_quantized_matrix ['isbuiltin', 'isroutine', 'callable']\n",
      "checking feature_alpha_dropout\n",
      "<built-in method feature_alpha_dropout of type object at 0x1144465b0> <class 'builtin_function_or_method'> feature_alpha_dropout\n",
      "feature_alpha_dropout ['isbuiltin', 'isroutine', 'callable']\n",
      "checking feature_alpha_dropout_\n",
      "<built-in method feature_alpha_dropout_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> feature_alpha_dropout_\n",
      "feature_alpha_dropout_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking feature_dropout\n",
      "<built-in method feature_dropout of type object at 0x1144465b0> <class 'builtin_function_or_method'> feature_dropout\n",
      "feature_dropout ['isbuiltin', 'isroutine', 'callable']\n",
      "checking feature_dropout_\n",
      "<built-in method feature_dropout_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> feature_dropout_\n",
      "feature_dropout_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fft\n",
      "<built-in method fft of type object at 0x1144465b0> <class 'builtin_function_or_method'> fft\n",
      "fft ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fill_\n",
      "<built-in method fill_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> fill_\n",
      "fill_ ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find fill_ in native_func_dict\n",
      "Cant find fill_ in native_func_dict\n",
      "Cant find fill_ in native_func_dict\n",
      "checking finfo\n",
      "<class 'torch.finfo'> <class 'type'> finfo\n",
      "Hierarchy: [<class 'torch.finfo'>, <class 'object'>]\n",
      "finfo ['isclass', 'callable']\n",
      "checking flatten\n",
      "<built-in method flatten of type object at 0x1144465b0> <class 'builtin_function_or_method'> flatten\n",
      "flatten ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find flatten in native_func_dict\n",
      "Cant find flatten in native_func_dict\n",
      "Cant find flatten in native_func_dict\n",
      "checking flip\n",
      "<built-in method flip of type object at 0x1144465b0> <class 'builtin_function_or_method'> flip\n",
      "flip ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fliplr\n",
      "<built-in method fliplr of type object at 0x1144465b0> <class 'builtin_function_or_method'> fliplr\n",
      "fliplr ['isbuiltin', 'isroutine', 'callable']\n",
      "checking flipud\n",
      "<built-in method flipud of type object at 0x1144465b0> <class 'builtin_function_or_method'> flipud\n",
      "flipud ['isbuiltin', 'isroutine', 'callable']\n",
      "checking float\n",
      "torch.float32 <class 'torch.dtype'> \n",
      "float []\n",
      "checking float16\n",
      "torch.float16 <class 'torch.dtype'> \n",
      "float16 []\n",
      "checking float32\n",
      "torch.float32 <class 'torch.dtype'> \n",
      "float32 []\n",
      "checking float64\n",
      "torch.float64 <class 'torch.dtype'> \n",
      "float64 []\n",
      "checking floor\n",
      "<built-in method floor of type object at 0x1144465b0> <class 'builtin_function_or_method'> floor\n",
      "floor ['isbuiltin', 'isroutine', 'callable']\n",
      "checking floor_\n",
      "<built-in method floor_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> floor_\n",
      "floor_ ['isbuiltin', 'isroutine', 'callable']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking floor_divide\n",
      "<built-in method floor_divide of type object at 0x1144465b0> <class 'builtin_function_or_method'> floor_divide\n",
      "floor_divide ['isbuiltin', 'isroutine', 'callable']\n",
      "checking fmod\n",
      "<built-in method fmod of type object at 0x1144465b0> <class 'builtin_function_or_method'> fmod\n",
      "fmod ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find fmod in native_func_dict\n",
      "Cant find fmod in native_func_dict\n",
      "Cant find fmod in native_func_dict\n",
      "checking fork\n",
      "<built-in method fork of PyCapsule object at 0x114ca6030> <class 'builtin_function_or_method'> fork\n",
      "fork ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find fork in native_func_dict\n",
      "Cant find fork in native_func_dict\n",
      "Cant find fork in native_func_dict\n",
      "checking frac\n",
      "<built-in method frac of type object at 0x1144465b0> <class 'builtin_function_or_method'> frac\n",
      "frac ['isbuiltin', 'isroutine', 'callable']\n",
      "checking frac_\n",
      "<built-in method frac_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> frac_\n",
      "frac_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking frobenius_norm\n",
      "<built-in method frobenius_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> frobenius_norm\n",
      "frobenius_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking from_file\n",
      "<built-in method from_file of type object at 0x1144465b0> <class 'builtin_function_or_method'> from_file\n",
      "from_file ['isbuiltin', 'isroutine', 'callable']\n",
      "checking from_numpy\n",
      "<built-in method from_numpy of type object at 0x1144465b0> <class 'builtin_function_or_method'> from_numpy\n",
      "from_numpy ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find from_numpy in native_func_dict\n",
      "Cant find from_numpy in native_func_dict\n",
      "Cant find from_numpy in native_func_dict\n",
      "checking full\n",
      "<built-in method full of type object at 0x1144465b0> <class 'builtin_function_or_method'> full\n",
      "full ['isbuiltin', 'isroutine', 'callable']\n",
      "checking full_like\n",
      "<built-in method full_like of type object at 0x1144465b0> <class 'builtin_function_or_method'> full_like\n",
      "full_like ['isbuiltin', 'isroutine', 'callable']\n",
      "checking functional\n",
      "<module 'torch.functional' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/functional.py'> <class 'module'> torch.functional\n",
      "functional ['ismodule']\n",
      "checking futures\n",
      "<module 'torch.futures' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/futures/__init__.py'> <class 'module'> torch.futures\n",
      "futures ['ismodule']\n",
      "checking gather\n",
      "<built-in method gather of type object at 0x1144465b0> <class 'builtin_function_or_method'> gather\n",
      "gather ['isbuiltin', 'isroutine', 'callable']\n",
      "checking ge\n",
      "<built-in method ge of type object at 0x1144465b0> <class 'builtin_function_or_method'> ge\n",
      "ge ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find ge in native_func_dict\n",
      "Cant find ge in native_func_dict\n",
      "Cant find ge in native_func_dict\n",
      "checking geqrf\n",
      "<built-in method geqrf of type object at 0x1144465b0> <class 'builtin_function_or_method'> geqrf\n",
      "geqrf ['isbuiltin', 'isroutine', 'callable']\n",
      "checking ger\n",
      "<built-in method ger of type object at 0x1144465b0> <class 'builtin_function_or_method'> ger\n",
      "ger ['isbuiltin', 'isroutine', 'callable']\n",
      "checking get_default_dtype\n",
      "<built-in function get_default_dtype> <class 'builtin_function_or_method'> get_default_dtype\n",
      "get_default_dtype ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find get_default_dtype in native_func_dict\n",
      "Cant find get_default_dtype in native_func_dict\n",
      "Cant find get_default_dtype in native_func_dict\n",
      "checking get_device\n",
      "<built-in method get_device of type object at 0x1144465b0> <class 'builtin_function_or_method'> get_device\n",
      "get_device ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find get_device in native_func_dict\n",
      "Cant find get_device in native_func_dict\n",
      "Cant find get_device in native_func_dict\n",
      "checking get_file_path\n",
      "<function get_file_path at 0x1111a2730> <class 'function'> get_file_path\n",
      "get_file_path ['isfunction', 'isroutine', 'callable']\n",
      "checking get_num_interop_threads\n",
      "<built-in function get_num_interop_threads> <class 'builtin_function_or_method'> get_num_interop_threads\n",
      "get_num_interop_threads ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find get_num_interop_threads in native_func_dict\n",
      "Cant find get_num_interop_threads in native_func_dict\n",
      "Cant find get_num_interop_threads in native_func_dict\n",
      "checking get_num_threads\n",
      "<built-in function get_num_threads> <class 'builtin_function_or_method'> get_num_threads\n",
      "get_num_threads ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find get_num_threads in native_func_dict\n",
      "Cant find get_num_threads in native_func_dict\n",
      "Cant find get_num_threads in native_func_dict\n",
      "checking get_rng_state\n",
      "<function get_rng_state at 0x115a3fd90> <class 'function'> get_rng_state\n",
      "get_rng_state ['isfunction', 'isroutine', 'callable']\n",
      "checking grid_sampler\n",
      "<built-in method grid_sampler of type object at 0x1144465b0> <class 'builtin_function_or_method'> grid_sampler\n",
      "grid_sampler ['isbuiltin', 'isroutine', 'callable']\n",
      "checking grid_sampler_2d\n",
      "<built-in method grid_sampler_2d of type object at 0x1144465b0> <class 'builtin_function_or_method'> grid_sampler_2d\n",
      "grid_sampler_2d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking grid_sampler_3d\n",
      "<built-in method grid_sampler_3d of type object at 0x1144465b0> <class 'builtin_function_or_method'> grid_sampler_3d\n",
      "grid_sampler_3d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking group_norm\n",
      "<built-in method group_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> group_norm\n",
      "group_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking gru\n",
      "<built-in method gru of type object at 0x1144465b0> <class 'builtin_function_or_method'> gru\n",
      "gru ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find gru in native_func_dict\n",
      "Cant find gru in native_func_dict\n",
      "Cant find gru in native_func_dict\n",
      "checking gru_cell\n",
      "<built-in method gru_cell of type object at 0x1144465b0> <class 'builtin_function_or_method'> gru_cell\n",
      "gru_cell ['isbuiltin', 'isroutine', 'callable']\n",
      "checking gt\n",
      "<built-in method gt of type object at 0x1144465b0> <class 'builtin_function_or_method'> gt\n",
      "gt ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find gt in native_func_dict\n",
      "Cant find gt in native_func_dict\n",
      "Cant find gt in native_func_dict\n",
      "checking half\n",
      "torch.float16 <class 'torch.dtype'> \n",
      "half []\n",
      "checking hamming_window\n",
      "<built-in method hamming_window of type object at 0x1144465b0> <class 'builtin_function_or_method'> hamming_window\n",
      "hamming_window ['isbuiltin', 'isroutine', 'callable']\n",
      "checking hann_window\n",
      "<built-in method hann_window of type object at 0x1144465b0> <class 'builtin_function_or_method'> hann_window\n",
      "hann_window ['isbuiltin', 'isroutine', 'callable']\n",
      "checking hardshrink\n",
      "<built-in method hardshrink of type object at 0x1144465b0> <class 'builtin_function_or_method'> hardshrink\n",
      "hardshrink ['isbuiltin', 'isroutine', 'callable']\n",
      "checking has_cuda\n",
      "False <class 'bool'> \n",
      "has_cuda []\n",
      "checking has_cudnn\n",
      "False <class 'bool'> \n",
      "has_cudnn []\n",
      "checking has_lapack\n",
      "True <class 'bool'> \n",
      "has_lapack []\n",
      "checking has_mkl\n",
      "True <class 'bool'> \n",
      "has_mkl []\n",
      "checking has_mkldnn\n",
      "True <class 'bool'> \n",
      "has_mkldnn []\n",
      "checking has_openmp\n",
      "False <class 'bool'> \n",
      "has_openmp []\n",
      "checking hinge_embedding_loss\n",
      "<built-in method hinge_embedding_loss of type object at 0x1144465b0> <class 'builtin_function_or_method'> hinge_embedding_loss\n",
      "hinge_embedding_loss ['isbuiltin', 'isroutine', 'callable']\n",
      "checking histc\n",
      "<built-in method histc of type object at 0x1144465b0> <class 'builtin_function_or_method'> histc\n",
      "histc ['isbuiltin', 'isroutine', 'callable']\n",
      "checking hsmm\n",
      "<built-in method hsmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> hsmm\n",
      "hsmm ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find hsmm in native_func_dict\n",
      "Cant find hsmm in native_func_dict\n",
      "Cant find hsmm in native_func_dict\n",
      "checking hspmm\n",
      "<built-in method hspmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> hspmm\n",
      "hspmm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking hub\n",
      "<module 'torch.hub' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/hub.py'> <class 'module'> torch.hub\n",
      "hub ['ismodule']\n",
      "checking ifft\n",
      "<built-in method ifft of type object at 0x1144465b0> <class 'builtin_function_or_method'> ifft\n",
      "ifft ['isbuiltin', 'isroutine', 'callable']\n",
      "checking iinfo\n",
      "<class 'torch.iinfo'> <class 'type'> iinfo\n",
      "Hierarchy: [<class 'torch.iinfo'>, <class 'object'>]\n",
      "iinfo ['isclass', 'callable']\n",
      "checking imag\n",
      "<built-in method imag of type object at 0x1144465b0> <class 'builtin_function_or_method'> imag\n",
      "imag ['isbuiltin', 'isroutine', 'callable']\n",
      "checking import_ir_module\n",
      "<built-in method import_ir_module of PyCapsule object at 0x114cc1630> <class 'builtin_function_or_method'> import_ir_module\n",
      "import_ir_module ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find import_ir_module in native_func_dict\n",
      "Cant find import_ir_module in native_func_dict\n",
      "Cant find import_ir_module in native_func_dict\n",
      "checking import_ir_module_from_buffer\n",
      "<built-in method import_ir_module_from_buffer of PyCapsule object at 0x114cc1660> <class 'builtin_function_or_method'> import_ir_module_from_buffer\n",
      "import_ir_module_from_buffer ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find import_ir_module_from_buffer in native_func_dict\n",
      "Cant find import_ir_module_from_buffer in native_func_dict\n",
      "Cant find import_ir_module_from_buffer in native_func_dict\n",
      "checking index_add\n",
      "<built-in method index_add of type object at 0x1144465b0> <class 'builtin_function_or_method'> index_add\n",
      "index_add ['isbuiltin', 'isroutine', 'callable']\n",
      "checking index_copy\n",
      "<built-in method index_copy of type object at 0x1144465b0> <class 'builtin_function_or_method'> index_copy\n",
      "index_copy ['isbuiltin', 'isroutine', 'callable']\n",
      "checking index_fill\n",
      "<built-in method index_fill of type object at 0x1144465b0> <class 'builtin_function_or_method'> index_fill\n",
      "index_fill ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find index_fill in native_func_dict\n",
      "Cant find index_fill in native_func_dict\n",
      "Cant find index_fill in native_func_dict\n",
      "checking index_put\n",
      "<built-in method index_put of type object at 0x1144465b0> <class 'builtin_function_or_method'> index_put\n",
      "index_put ['isbuiltin', 'isroutine', 'callable']\n",
      "checking index_put_\n",
      "<built-in method index_put_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> index_put_\n",
      "index_put_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking index_select\n",
      "<built-in method index_select of type object at 0x1144465b0> <class 'builtin_function_or_method'> index_select\n",
      "index_select ['isbuiltin', 'isroutine', 'callable']\n",
      "checking init_num_threads\n",
      "<built-in method init_num_threads of PyCapsule object at 0x114ccf390> <class 'builtin_function_or_method'> init_num_threads\n",
      "init_num_threads ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find init_num_threads in native_func_dict\n",
      "Cant find init_num_threads in native_func_dict\n",
      "Cant find init_num_threads in native_func_dict\n",
      "checking initial_seed\n",
      "<function initial_seed at 0x115a3ff28> <class 'function'> initial_seed\n",
      "initial_seed ['isfunction', 'isroutine', 'callable']\n",
      "checking instance_norm\n",
      "<built-in method instance_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> instance_norm\n",
      "instance_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking int\n",
      "torch.int32 <class 'torch.dtype'> \n",
      "int []\n",
      "checking int16\n",
      "torch.int16 <class 'torch.dtype'> \n",
      "int16 []\n",
      "checking int32\n",
      "torch.int32 <class 'torch.dtype'> \n",
      "int32 []\n",
      "checking int64\n",
      "torch.int64 <class 'torch.dtype'> \n",
      "int64 []\n",
      "checking int8\n",
      "torch.int8 <class 'torch.dtype'> \n",
      "int8 []\n",
      "checking int_repr\n",
      "<built-in method int_repr of type object at 0x1144465b0> <class 'builtin_function_or_method'> int_repr\n",
      "int_repr ['isbuiltin', 'isroutine', 'callable']\n",
      "checking inverse\n",
      "<built-in method inverse of type object at 0x1144465b0> <class 'builtin_function_or_method'> inverse\n",
      "inverse ['isbuiltin', 'isroutine', 'callable']\n",
      "checking irfft\n",
      "<built-in method irfft of type object at 0x1144465b0> <class 'builtin_function_or_method'> irfft\n",
      "irfft ['isbuiltin', 'isroutine', 'callable']\n",
      "checking is_anomaly_enabled\n",
      "<built-in function is_anomaly_enabled> <class 'builtin_function_or_method'> is_anomaly_enabled\n",
      "is_anomaly_enabled ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find is_anomaly_enabled in native_func_dict\n",
      "Cant find is_anomaly_enabled in native_func_dict\n",
      "Cant find is_anomaly_enabled in native_func_dict\n",
      "checking is_autocast_enabled\n",
      "<built-in function is_autocast_enabled> <class 'builtin_function_or_method'> is_autocast_enabled\n",
      "is_autocast_enabled ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find is_autocast_enabled in native_func_dict\n",
      "Cant find is_autocast_enabled in native_func_dict\n",
      "Cant find is_autocast_enabled in native_func_dict\n",
      "checking is_complex\n",
      "<built-in method is_complex of type object at 0x1144465b0> <class 'builtin_function_or_method'> is_complex\n",
      "is_complex ['isbuiltin', 'isroutine', 'callable']\n",
      "checking is_distributed\n",
      "<built-in method is_distributed of type object at 0x1144465b0> <class 'builtin_function_or_method'> is_distributed\n",
      "is_distributed ['isbuiltin', 'isroutine', 'callable']\n",
      "checking is_floating_point\n",
      "<built-in method is_floating_point of type object at 0x1144465b0> <class 'builtin_function_or_method'> is_floating_point\n",
      "is_floating_point ['isbuiltin', 'isroutine', 'callable']\n",
      "checking is_grad_enabled\n",
      "<built-in function is_grad_enabled> <class 'builtin_function_or_method'> is_grad_enabled\n",
      "is_grad_enabled ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find is_grad_enabled in native_func_dict\n",
      "Cant find is_grad_enabled in native_func_dict\n",
      "Cant find is_grad_enabled in native_func_dict\n",
      "checking is_nonzero\n",
      "<built-in method is_nonzero of type object at 0x1144465b0> <class 'builtin_function_or_method'> is_nonzero\n",
      "is_nonzero ['isbuiltin', 'isroutine', 'callable']\n",
      "checking is_same_size\n",
      "<built-in method is_same_size of type object at 0x1144465b0> <class 'builtin_function_or_method'> is_same_size\n",
      "is_same_size ['isbuiltin', 'isroutine', 'callable']\n",
      "checking is_signed\n",
      "<built-in method is_signed of type object at 0x1144465b0> <class 'builtin_function_or_method'> is_signed\n",
      "is_signed ['isbuiltin', 'isroutine', 'callable']\n",
      "checking is_storage\n",
      "<function is_storage at 0x114cdd048> <class 'function'> is_storage\n",
      "is_storage ['isfunction', 'isroutine', 'callable']\n",
      "checking is_tensor\n",
      "<function is_tensor at 0x10f19ae18> <class 'function'> is_tensor\n",
      "is_tensor ['isfunction', 'isroutine', 'callable']\n",
      "checking is_vulkan_available\n",
      "<built-in method is_vulkan_available of type object at 0x1144465b0> <class 'builtin_function_or_method'> is_vulkan_available\n",
      "is_vulkan_available ['isbuiltin', 'isroutine', 'callable']\n",
      "checking isclose\n",
      "<built-in method isclose of type object at 0x1144465b0> <class 'builtin_function_or_method'> isclose\n",
      "isclose ['isbuiltin', 'isroutine', 'callable']\n",
      "checking isfinite\n",
      "<built-in method isfinite of type object at 0x1144465b0> <class 'builtin_function_or_method'> isfinite\n",
      "isfinite ['isbuiltin', 'isroutine', 'callable']\n",
      "checking isinf\n",
      "<built-in method isinf of type object at 0x1144465b0> <class 'builtin_function_or_method'> isinf\n",
      "isinf ['isbuiltin', 'isroutine', 'callable']\n",
      "checking isnan\n",
      "<built-in method isnan of type object at 0x1144465b0> <class 'builtin_function_or_method'> isnan\n",
      "isnan ['isbuiltin', 'isroutine', 'callable']\n",
      "checking istft\n",
      "<function istft at 0x13770f8c8> <class 'function'> istft\n",
      "istft ['isfunction', 'isroutine', 'callable']\n",
      "checking jit\n",
      "<module 'torch.jit' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/jit/__init__.py'> <class 'module'> torch.jit\n",
      "jit ['ismodule']\n",
      "checking kl_div\n",
      "<built-in method kl_div of type object at 0x1144465b0> <class 'builtin_function_or_method'> kl_div\n",
      "kl_div ['isbuiltin', 'isroutine', 'callable']\n",
      "checking kthvalue\n",
      "<built-in method kthvalue of type object at 0x1144465b0> <class 'builtin_function_or_method'> kthvalue\n",
      "kthvalue ['isbuiltin', 'isroutine', 'callable']\n",
      "checking layer_norm\n",
      "<built-in method layer_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> layer_norm\n",
      "layer_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking layout\n",
      "<class 'torch.layout'> <class 'type'> layout\n",
      "Hierarchy: [<class 'torch.layout'>, <class 'object'>]\n",
      "layout ['isclass', 'callable']\n",
      "checking le\n",
      "<built-in method le of type object at 0x1144465b0> <class 'builtin_function_or_method'> le\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find le in native_func_dict\n",
      "Cant find le in native_func_dict\n",
      "Cant find le in native_func_dict\n",
      "checking legacy_contiguous_format\n",
      "torch.contiguous_format <class 'torch.memory_format'> \n",
      "legacy_contiguous_format []\n",
      "checking lerp\n",
      "<built-in method lerp of type object at 0x1144465b0> <class 'builtin_function_or_method'> lerp\n",
      "lerp ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find lerp in native_func_dict\n",
      "Cant find lerp in native_func_dict\n",
      "Cant find lerp in native_func_dict\n",
      "checking lgamma\n",
      "<built-in method lgamma of type object at 0x1144465b0> <class 'builtin_function_or_method'> lgamma\n",
      "lgamma ['isbuiltin', 'isroutine', 'callable']\n",
      "checking linspace\n",
      "<built-in method linspace of type object at 0x1144465b0> <class 'builtin_function_or_method'> linspace\n",
      "linspace ['isbuiltin', 'isroutine', 'callable']\n",
      "checking load\n",
      "<function load at 0x115a861e0> <class 'function'> load\n",
      "load ['isfunction', 'isroutine', 'callable']\n",
      "checking lobpcg\n",
      "<function lobpcg at 0x13798ac80> <class 'function'> lobpcg\n",
      "lobpcg ['isfunction', 'isroutine', 'callable']\n",
      "checking log\n",
      "<built-in method log of type object at 0x1144465b0> <class 'builtin_function_or_method'> log\n",
      "log ['isbuiltin', 'isroutine', 'callable']\n",
      "checking log10\n",
      "<built-in method log10 of type object at 0x1144465b0> <class 'builtin_function_or_method'> log10\n",
      "log10 ['isbuiltin', 'isroutine', 'callable']\n",
      "checking log10_\n",
      "<built-in method log10_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> log10_\n",
      "log10_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking log1p\n",
      "<built-in method log1p of type object at 0x1144465b0> <class 'builtin_function_or_method'> log1p\n",
      "log1p ['isbuiltin', 'isroutine', 'callable']\n",
      "checking log1p_\n",
      "<built-in method log1p_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> log1p_\n",
      "log1p_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking log2\n",
      "<built-in method log2 of type object at 0x1144465b0> <class 'builtin_function_or_method'> log2\n",
      "log2 ['isbuiltin', 'isroutine', 'callable']\n",
      "checking log2_\n",
      "<built-in method log2_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> log2_\n",
      "log2_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking log_\n",
      "<built-in method log_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> log_\n",
      "log_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking log_softmax\n",
      "<built-in method log_softmax of type object at 0x1144465b0> <class 'builtin_function_or_method'> log_softmax\n",
      "log_softmax ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find log_softmax in native_func_dict\n",
      "Cant find log_softmax in native_func_dict\n",
      "Cant find log_softmax in native_func_dict\n",
      "checking logaddexp\n",
      "<built-in method logaddexp of type object at 0x1144465b0> <class 'builtin_function_or_method'> logaddexp\n",
      "logaddexp ['isbuiltin', 'isroutine', 'callable']\n",
      "checking logaddexp2\n",
      "<built-in method logaddexp2 of type object at 0x1144465b0> <class 'builtin_function_or_method'> logaddexp2\n",
      "logaddexp2 ['isbuiltin', 'isroutine', 'callable']\n",
      "checking logcumsumexp\n",
      "<built-in method logcumsumexp of type object at 0x1144465b0> <class 'builtin_function_or_method'> logcumsumexp\n",
      "logcumsumexp ['isbuiltin', 'isroutine', 'callable']\n",
      "checking logdet\n",
      "<built-in method logdet of type object at 0x1144465b0> <class 'builtin_function_or_method'> logdet\n",
      "logdet ['isbuiltin', 'isroutine', 'callable']\n",
      "checking logical_and\n",
      "<built-in method logical_and of type object at 0x1144465b0> <class 'builtin_function_or_method'> logical_and\n",
      "logical_and ['isbuiltin', 'isroutine', 'callable']\n",
      "checking logical_not\n",
      "<built-in method logical_not of type object at 0x1144465b0> <class 'builtin_function_or_method'> logical_not\n",
      "logical_not ['isbuiltin', 'isroutine', 'callable']\n",
      "checking logical_or\n",
      "<built-in method logical_or of type object at 0x1144465b0> <class 'builtin_function_or_method'> logical_or\n",
      "logical_or ['isbuiltin', 'isroutine', 'callable']\n",
      "checking logical_xor\n",
      "<built-in method logical_xor of type object at 0x1144465b0> <class 'builtin_function_or_method'> logical_xor\n",
      "logical_xor ['isbuiltin', 'isroutine', 'callable']\n",
      "checking logspace\n",
      "<built-in method logspace of type object at 0x1144465b0> <class 'builtin_function_or_method'> logspace\n",
      "logspace ['isbuiltin', 'isroutine', 'callable']\n",
      "checking logsumexp\n",
      "<built-in method logsumexp of type object at 0x1144465b0> <class 'builtin_function_or_method'> logsumexp\n",
      "logsumexp ['isbuiltin', 'isroutine', 'callable']\n",
      "checking long\n",
      "torch.int64 <class 'torch.dtype'> \n",
      "long []\n",
      "checking lstm\n",
      "<built-in method lstm of type object at 0x1144465b0> <class 'builtin_function_or_method'> lstm\n",
      "lstm ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find lstm in native_func_dict\n",
      "Cant find lstm in native_func_dict\n",
      "Cant find lstm in native_func_dict\n",
      "checking lstm_cell\n",
      "<built-in method lstm_cell of type object at 0x1144465b0> <class 'builtin_function_or_method'> lstm_cell\n",
      "lstm_cell ['isbuiltin', 'isroutine', 'callable']\n",
      "checking lstsq\n",
      "<built-in method lstsq of type object at 0x1144465b0> <class 'builtin_function_or_method'> lstsq\n",
      "lstsq ['isbuiltin', 'isroutine', 'callable']\n",
      "checking lt\n",
      "<built-in method lt of type object at 0x1144465b0> <class 'builtin_function_or_method'> lt\n",
      "lt ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find lt in native_func_dict\n",
      "Cant find lt in native_func_dict\n",
      "Cant find lt in native_func_dict\n",
      "checking lu\n",
      "<function boolean_dispatch.<locals>.fn at 0x1377128c8> <class 'function'> lu\n",
      "lu ['isfunction', 'isroutine', 'callable']\n",
      "checking lu_solve\n",
      "<built-in method lu_solve of type object at 0x1144465b0> <class 'builtin_function_or_method'> lu_solve\n",
      "lu_solve ['isbuiltin', 'isroutine', 'callable']\n",
      "checking lu_unpack\n",
      "<function lu_unpack at 0x13770f6a8> <class 'function'> lu_unpack\n",
      "lu_unpack ['isfunction', 'isroutine', 'callable']\n",
      "checking manual_seed\n",
      "<function manual_seed at 0x115a3fe18> <class 'function'> manual_seed\n",
      "manual_seed ['isfunction', 'isroutine', 'callable']\n",
      "checking margin_ranking_loss\n",
      "<built-in method margin_ranking_loss of type object at 0x1144465b0> <class 'builtin_function_or_method'> margin_ranking_loss\n",
      "margin_ranking_loss ['isbuiltin', 'isroutine', 'callable']\n",
      "checking masked_fill\n",
      "<built-in method masked_fill of type object at 0x1144465b0> <class 'builtin_function_or_method'> masked_fill\n",
      "masked_fill ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find masked_fill in native_func_dict\n",
      "Cant find masked_fill in native_func_dict\n",
      "Cant find masked_fill in native_func_dict\n",
      "checking masked_scatter\n",
      "<built-in method masked_scatter of type object at 0x1144465b0> <class 'builtin_function_or_method'> masked_scatter\n",
      "masked_scatter ['isbuiltin', 'isroutine', 'callable']\n",
      "checking masked_select\n",
      "<built-in method masked_select of type object at 0x1144465b0> <class 'builtin_function_or_method'> masked_select\n",
      "masked_select ['isbuiltin', 'isroutine', 'callable']\n",
      "checking matmul\n",
      "<built-in method matmul of type object at 0x1144465b0> <class 'builtin_function_or_method'> matmul\n",
      "matmul ['isbuiltin', 'isroutine', 'callable']\n",
      "checking matrix_power\n",
      "<built-in method matrix_power of type object at 0x1144465b0> <class 'builtin_function_or_method'> matrix_power\n",
      "matrix_power ['isbuiltin', 'isroutine', 'callable']\n",
      "checking matrix_rank\n",
      "<built-in method matrix_rank of type object at 0x1144465b0> <class 'builtin_function_or_method'> matrix_rank\n",
      "matrix_rank ['isbuiltin', 'isroutine', 'callable']\n",
      "checking max\n",
      "<built-in method max of type object at 0x1144465b0> <class 'builtin_function_or_method'> max\n",
      "max ['isbuiltin', 'isroutine', 'callable']\n",
      "checking max_pool1d\n",
      "<built-in method max_pool1d of type object at 0x1144465b0> <class 'builtin_function_or_method'> max_pool1d\n",
      "max_pool1d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking max_pool1d_with_indices\n",
      "<built-in method max_pool1d_with_indices of type object at 0x1144465b0> <class 'builtin_function_or_method'> max_pool1d_with_indices\n",
      "max_pool1d_with_indices ['isbuiltin', 'isroutine', 'callable']\n",
      "checking max_pool2d\n",
      "<built-in method max_pool2d of type object at 0x1144465b0> <class 'builtin_function_or_method'> max_pool2d\n",
      "max_pool2d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking max_pool3d\n",
      "<built-in method max_pool3d of type object at 0x1144465b0> <class 'builtin_function_or_method'> max_pool3d\n",
      "max_pool3d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking mean\n",
      "<built-in method mean of type object at 0x1144465b0> <class 'builtin_function_or_method'> mean\n",
      "mean ['isbuiltin', 'isroutine', 'callable']\n",
      "checking median\n",
      "<built-in method median of type object at 0x1144465b0> <class 'builtin_function_or_method'> median\n",
      "median ['isbuiltin', 'isroutine', 'callable']\n",
      "checking memory_format\n",
      "<class 'torch.memory_format'> <class 'type'> memory_format\n",
      "Hierarchy: [<class 'torch.memory_format'>, <class 'object'>]\n",
      "memory_format ['isclass', 'callable']\n",
      "checking merge_type_from_type_comment\n",
      "<built-in method merge_type_from_type_comment of PyCapsule object at 0x114cc1600> <class 'builtin_function_or_method'> merge_type_from_type_comment\n",
      "merge_type_from_type_comment ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find merge_type_from_type_comment in native_func_dict\n",
      "Cant find merge_type_from_type_comment in native_func_dict\n",
      "Cant find merge_type_from_type_comment in native_func_dict\n",
      "checking meshgrid\n",
      "<function meshgrid at 0x13770f7b8> <class 'function'> meshgrid\n",
      "meshgrid ['isfunction', 'isroutine', 'callable']\n",
      "checking min\n",
      "<built-in method min of type object at 0x1144465b0> <class 'builtin_function_or_method'> min\n",
      "min ['isbuiltin', 'isroutine', 'callable']\n",
      "checking miopen_batch_norm\n",
      "<built-in method miopen_batch_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> miopen_batch_norm\n",
      "miopen_batch_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking miopen_convolution\n",
      "<built-in method miopen_convolution of type object at 0x1144465b0> <class 'builtin_function_or_method'> miopen_convolution\n",
      "miopen_convolution ['isbuiltin', 'isroutine', 'callable']\n",
      "checking miopen_convolution_transpose\n",
      "<built-in method miopen_convolution_transpose of type object at 0x1144465b0> <class 'builtin_function_or_method'> miopen_convolution_transpose\n",
      "miopen_convolution_transpose ['isbuiltin', 'isroutine', 'callable']\n",
      "checking miopen_depthwise_convolution\n",
      "<built-in method miopen_depthwise_convolution of type object at 0x1144465b0> <class 'builtin_function_or_method'> miopen_depthwise_convolution\n",
      "miopen_depthwise_convolution ['isbuiltin', 'isroutine', 'callable']\n",
      "checking miopen_rnn\n",
      "<built-in method miopen_rnn of type object at 0x1144465b0> <class 'builtin_function_or_method'> miopen_rnn\n",
      "miopen_rnn ['isbuiltin', 'isroutine', 'callable']\n",
      "checking mkldnn_adaptive_avg_pool2d\n",
      "<built-in method mkldnn_adaptive_avg_pool2d of type object at 0x1144465b0> <class 'builtin_function_or_method'> mkldnn_adaptive_avg_pool2d\n",
      "mkldnn_adaptive_avg_pool2d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking mkldnn_convolution\n",
      "<built-in method mkldnn_convolution of type object at 0x1144465b0> <class 'builtin_function_or_method'> mkldnn_convolution\n",
      "mkldnn_convolution ['isbuiltin', 'isroutine', 'callable']\n",
      "checking mkldnn_convolution_backward_weights\n",
      "<built-in method mkldnn_convolution_backward_weights of type object at 0x1144465b0> <class 'builtin_function_or_method'> mkldnn_convolution_backward_weights\n",
      "mkldnn_convolution_backward_weights ['isbuiltin', 'isroutine', 'callable']\n",
      "checking mkldnn_max_pool2d\n",
      "<built-in method mkldnn_max_pool2d of type object at 0x1144465b0> <class 'builtin_function_or_method'> mkldnn_max_pool2d\n",
      "mkldnn_max_pool2d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking mm\n",
      "<built-in method mm of type object at 0x1144465b0> <class 'builtin_function_or_method'> mm\n",
      "mm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking mode\n",
      "<built-in method mode of type object at 0x1144465b0> <class 'builtin_function_or_method'> mode\n",
      "mode ['isbuiltin', 'isroutine', 'callable']\n",
      "checking mul\n",
      "<built-in method mul of type object at 0x1144465b0> <class 'builtin_function_or_method'> mul\n",
      "mul ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find mul in native_func_dict\n",
      "Cant find mul in native_func_dict\n",
      "Cant find mul in native_func_dict\n",
      "checking multinomial\n",
      "<built-in method multinomial of type object at 0x1144465b0> <class 'builtin_function_or_method'> multinomial\n",
      "multinomial ['isbuiltin', 'isroutine', 'callable']\n",
      "checking multiprocessing\n",
      "<module 'torch.multiprocessing' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/multiprocessing/__init__.py'> <class 'module'> torch.multiprocessing\n",
      "multiprocessing ['ismodule']\n",
      "checking mv\n",
      "<built-in method mv of type object at 0x1144465b0> <class 'builtin_function_or_method'> mv\n",
      "mv ['isbuiltin', 'isroutine', 'callable']\n",
      "checking mvlgamma\n",
      "<built-in method mvlgamma of type object at 0x1144465b0> <class 'builtin_function_or_method'> mvlgamma\n",
      "mvlgamma ['isbuiltin', 'isroutine', 'callable']\n",
      "checking name\n",
      "zeros_like <class 'str'> \n",
      "name []\n",
      "checking narrow\n",
      "<built-in method narrow of type object at 0x1144465b0> <class 'builtin_function_or_method'> narrow\n",
      "narrow ['isbuiltin', 'isroutine', 'callable']\n",
      "checking native_batch_norm\n",
      "<built-in method native_batch_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> native_batch_norm\n",
      "native_batch_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking native_group_norm\n",
      "<built-in method native_group_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> native_group_norm\n",
      "native_group_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking native_layer_norm\n",
      "<built-in method native_layer_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> native_layer_norm\n",
      "native_layer_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking native_norm\n",
      "<built-in method native_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> native_norm\n",
      "native_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking ne\n",
      "<built-in method ne of type object at 0x1144465b0> <class 'builtin_function_or_method'> ne\n",
      "ne ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find ne in native_func_dict\n",
      "Cant find ne in native_func_dict\n",
      "Cant find ne in native_func_dict\n",
      "checking neg\n",
      "<built-in method neg of type object at 0x1144465b0> <class 'builtin_function_or_method'> neg\n",
      "neg ['isbuiltin', 'isroutine', 'callable']\n",
      "checking neg_\n",
      "<built-in method neg_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> neg_\n",
      "neg_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking nn\n",
      "<module 'torch.nn' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/nn/__init__.py'> <class 'module'> torch.nn\n",
      "nn ['ismodule']\n",
      "checking no_grad\n",
      "<class 'torch.autograd.grad_mode.no_grad'> <class 'type'> no_grad\n",
      "Hierarchy: [<class 'torch.autograd.grad_mode.no_grad'>, <class 'torch.autograd.grad_mode._DecoratorContextManager'>, <class 'object'>]\n",
      "no_grad ['isclass', 'callable']\n",
      "checking nonzero\n",
      "<built-in method nonzero of type object at 0x1144465b0> <class 'builtin_function_or_method'> nonzero\n",
      "nonzero ['isbuiltin', 'isroutine', 'callable']\n",
      "checking norm\n",
      "<function norm at 0x137712598> <class 'function'> norm\n",
      "norm ['isfunction', 'isroutine', 'callable']\n",
      "checking norm_except_dim\n",
      "<built-in method norm_except_dim of type object at 0x1144465b0> <class 'builtin_function_or_method'> norm_except_dim\n",
      "norm_except_dim ['isbuiltin', 'isroutine', 'callable']\n",
      "checking normal\n",
      "<built-in method normal of type object at 0x1144465b0> <class 'builtin_function_or_method'> normal\n",
      "normal ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find normal in native_func_dict\n",
      "Cant find normal in native_func_dict\n",
      "Cant find normal in native_func_dict\n",
      "checking nuclear_norm\n",
      "<built-in method nuclear_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> nuclear_norm\n",
      "nuclear_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking numel\n",
      "<built-in method numel of type object at 0x1144465b0> <class 'builtin_function_or_method'> numel\n",
      "numel ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find numel in native_func_dict\n",
      "Cant find numel in native_func_dict\n",
      "Cant find numel in native_func_dict\n",
      "checking ones\n",
      "<built-in method ones of type object at 0x1144465b0> <class 'builtin_function_or_method'> ones\n",
      "ones ['isbuiltin', 'isroutine', 'callable']\n",
      "checking ones_like\n",
      "<built-in method ones_like of type object at 0x1144465b0> <class 'builtin_function_or_method'> ones_like\n",
      "ones_like ['isbuiltin', 'isroutine', 'callable']\n",
      "checking onnx\n",
      "<module 'torch.onnx' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/onnx/__init__.py'> <class 'module'> torch.onnx\n",
      "onnx ['ismodule']\n",
      "checking ops\n",
      "<module 'torch.ops' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/_ops.py'> <class 'torch._ops._Ops'> torch.ops\n",
      "Exception with ops. '_OpNamespace' object is not callable\n",
      "checking optim\n",
      "<module 'torch.optim' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/optim/__init__.py'> <class 'module'> torch.optim\n",
      "optim ['ismodule']\n",
      "checking orgqr\n",
      "<built-in method orgqr of type object at 0x1144465b0> <class 'builtin_function_or_method'> orgqr\n",
      "orgqr ['isbuiltin', 'isroutine', 'callable']\n",
      "checking ormqr\n",
      "<built-in method ormqr of type object at 0x1144465b0> <class 'builtin_function_or_method'> ormqr\n",
      "ormqr ['isbuiltin', 'isroutine', 'callable']\n",
      "checking os\n",
      "<module 'os' from '/Users/madhavajay/.pyenv/versions/3.6.11/lib/python3.6/os.py'> <class 'module'> os\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os ['ismodule']\n",
      "checking pairwise_distance\n",
      "<built-in method pairwise_distance of type object at 0x1144465b0> <class 'builtin_function_or_method'> pairwise_distance\n",
      "pairwise_distance ['isbuiltin', 'isroutine', 'callable']\n",
      "checking parse_ir\n",
      "<built-in method parse_ir of PyCapsule object at 0x114ca3990> <class 'builtin_function_or_method'> parse_ir\n",
      "parse_ir ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find parse_ir in native_func_dict\n",
      "Cant find parse_ir in native_func_dict\n",
      "Cant find parse_ir in native_func_dict\n",
      "checking parse_schema\n",
      "<built-in method parse_schema of PyCapsule object at 0x114ca39c0> <class 'builtin_function_or_method'> parse_schema\n",
      "parse_schema ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find parse_schema in native_func_dict\n",
      "Cant find parse_schema in native_func_dict\n",
      "Cant find parse_schema in native_func_dict\n",
      "checking parse_type_comment\n",
      "<built-in method parse_type_comment of PyCapsule object at 0x114cc15d0> <class 'builtin_function_or_method'> parse_type_comment\n",
      "parse_type_comment ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find parse_type_comment in native_func_dict\n",
      "Cant find parse_type_comment in native_func_dict\n",
      "Cant find parse_type_comment in native_func_dict\n",
      "checking pca_lowrank\n",
      "<function pca_lowrank at 0x13770f488> <class 'function'> pca_lowrank\n",
      "pca_lowrank ['isfunction', 'isroutine', 'callable']\n",
      "checking pdist\n",
      "<built-in method pdist of type object at 0x1144465b0> <class 'builtin_function_or_method'> pdist\n",
      "pdist ['isbuiltin', 'isroutine', 'callable']\n",
      "checking per_channel_affine\n",
      "torch.per_channel_affine <class 'torch.qscheme'> \n",
      "per_channel_affine []\n",
      "checking per_channel_symmetric\n",
      "torch.per_channel_symmetric <class 'torch.qscheme'> \n",
      "per_channel_symmetric []\n",
      "checking per_tensor_affine\n",
      "torch.per_tensor_affine <class 'torch.qscheme'> \n",
      "per_tensor_affine []\n",
      "checking per_tensor_symmetric\n",
      "torch.per_tensor_symmetric <class 'torch.qscheme'> \n",
      "per_tensor_symmetric []\n",
      "checking pinverse\n",
      "<built-in method pinverse of type object at 0x1144465b0> <class 'builtin_function_or_method'> pinverse\n",
      "pinverse ['isbuiltin', 'isroutine', 'callable']\n",
      "checking pixel_shuffle\n",
      "<built-in method pixel_shuffle of type object at 0x1144465b0> <class 'builtin_function_or_method'> pixel_shuffle\n",
      "pixel_shuffle ['isbuiltin', 'isroutine', 'callable']\n",
      "checking platform\n",
      "<module 'platform' from '/Users/madhavajay/.pyenv/versions/3.6.11/lib/python3.6/platform.py'> <class 'module'> platform\n",
      "platform ['ismodule']\n",
      "checking poisson\n",
      "<built-in method poisson of type object at 0x1144465b0> <class 'builtin_function_or_method'> poisson\n",
      "poisson ['isbuiltin', 'isroutine', 'callable']\n",
      "checking poisson_nll_loss\n",
      "<built-in method poisson_nll_loss of type object at 0x1144465b0> <class 'builtin_function_or_method'> poisson_nll_loss\n",
      "poisson_nll_loss ['isbuiltin', 'isroutine', 'callable']\n",
      "checking polygamma\n",
      "<built-in method polygamma of type object at 0x1144465b0> <class 'builtin_function_or_method'> polygamma\n",
      "polygamma ['isbuiltin', 'isroutine', 'callable']\n",
      "checking pow\n",
      "<built-in method pow of type object at 0x1144465b0> <class 'builtin_function_or_method'> pow\n",
      "pow ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find pow in native_func_dict\n",
      "Cant find pow in native_func_dict\n",
      "Cant find pow in native_func_dict\n",
      "checking prelu\n",
      "<built-in method prelu of type object at 0x1144465b0> <class 'builtin_function_or_method'> prelu\n",
      "prelu ['isbuiltin', 'isroutine', 'callable']\n",
      "checking prepare_multiprocessing_environment\n",
      "<function prepare_multiprocessing_environment at 0x1111a2ae8> <class 'function'> prepare_multiprocessing_environment\n",
      "prepare_multiprocessing_environment ['isfunction', 'isroutine', 'callable']\n",
      "checking preserve_format\n",
      "torch.preserve_format <class 'torch.memory_format'> \n",
      "preserve_format []\n",
      "checking prod\n",
      "<built-in method prod of type object at 0x1144465b0> <class 'builtin_function_or_method'> prod\n",
      "prod ['isbuiltin', 'isroutine', 'callable']\n",
      "checking promote_types\n",
      "<built-in method promote_types of type object at 0x1144465b0> <class 'builtin_function_or_method'> promote_types\n",
      "promote_types ['isbuiltin', 'isroutine', 'callable']\n",
      "checking q_per_channel_axis\n",
      "<built-in method q_per_channel_axis of type object at 0x1144465b0> <class 'builtin_function_or_method'> q_per_channel_axis\n",
      "q_per_channel_axis ['isbuiltin', 'isroutine', 'callable']\n",
      "checking q_per_channel_scales\n",
      "<built-in method q_per_channel_scales of type object at 0x1144465b0> <class 'builtin_function_or_method'> q_per_channel_scales\n",
      "q_per_channel_scales ['isbuiltin', 'isroutine', 'callable']\n",
      "checking q_per_channel_zero_points\n",
      "<built-in method q_per_channel_zero_points of type object at 0x1144465b0> <class 'builtin_function_or_method'> q_per_channel_zero_points\n",
      "q_per_channel_zero_points ['isbuiltin', 'isroutine', 'callable']\n",
      "checking q_scale\n",
      "<built-in method q_scale of type object at 0x1144465b0> <class 'builtin_function_or_method'> q_scale\n",
      "q_scale ['isbuiltin', 'isroutine', 'callable']\n",
      "checking q_zero_point\n",
      "<built-in method q_zero_point of type object at 0x1144465b0> <class 'builtin_function_or_method'> q_zero_point\n",
      "q_zero_point ['isbuiltin', 'isroutine', 'callable']\n",
      "checking qint32\n",
      "torch.qint32 <class 'torch.dtype'> \n",
      "qint32 []\n",
      "checking qint8\n",
      "torch.qint8 <class 'torch.dtype'> \n",
      "qint8 []\n",
      "checking qr\n",
      "<built-in method qr of type object at 0x1144465b0> <class 'builtin_function_or_method'> qr\n",
      "qr ['isbuiltin', 'isroutine', 'callable']\n",
      "checking qscheme\n",
      "<class 'torch.qscheme'> <class 'type'> qscheme\n",
      "Hierarchy: [<class 'torch.qscheme'>, <class 'object'>]\n",
      "qscheme ['isclass', 'callable']\n",
      "checking quantization\n",
      "<module 'torch.quantization' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/quantization/__init__.py'> <class 'module'> torch.quantization\n",
      "quantization ['ismodule']\n",
      "checking quantize_per_channel\n",
      "<built-in method quantize_per_channel of type object at 0x1144465b0> <class 'builtin_function_or_method'> quantize_per_channel\n",
      "quantize_per_channel ['isbuiltin', 'isroutine', 'callable']\n",
      "checking quantize_per_tensor\n",
      "<built-in method quantize_per_tensor of type object at 0x1144465b0> <class 'builtin_function_or_method'> quantize_per_tensor\n",
      "quantize_per_tensor ['isbuiltin', 'isroutine', 'callable']\n",
      "checking quantized_batch_norm\n",
      "<built-in method quantized_batch_norm of type object at 0x1144465b0> <class 'builtin_function_or_method'> quantized_batch_norm\n",
      "quantized_batch_norm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking quantized_gru\n",
      "<built-in method quantized_gru of PyCapsule object at 0x137958d20> <class 'builtin_function_or_method'> quantized_gru\n",
      "quantized_gru ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find quantized_gru in native_func_dict\n",
      "Cant find quantized_gru in native_func_dict\n",
      "Cant find quantized_gru in native_func_dict\n",
      "checking quantized_gru_cell\n",
      "<built-in method quantized_gru_cell of type object at 0x1144465b0> <class 'builtin_function_or_method'> quantized_gru_cell\n",
      "quantized_gru_cell ['isbuiltin', 'isroutine', 'callable']\n",
      "checking quantized_lstm\n",
      "<built-in method quantized_lstm of PyCapsule object at 0x137958cf0> <class 'builtin_function_or_method'> quantized_lstm\n",
      "quantized_lstm ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find quantized_lstm in native_func_dict\n",
      "Cant find quantized_lstm in native_func_dict\n",
      "Cant find quantized_lstm in native_func_dict\n",
      "checking quantized_lstm_cell\n",
      "<built-in method quantized_lstm_cell of type object at 0x1144465b0> <class 'builtin_function_or_method'> quantized_lstm_cell\n",
      "quantized_lstm_cell ['isbuiltin', 'isroutine', 'callable']\n",
      "checking quantized_max_pool2d\n",
      "<built-in method quantized_max_pool2d of type object at 0x1144465b0> <class 'builtin_function_or_method'> quantized_max_pool2d\n",
      "quantized_max_pool2d ['isbuiltin', 'isroutine', 'callable']\n",
      "checking quantized_rnn_relu_cell\n",
      "<built-in method quantized_rnn_relu_cell of type object at 0x1144465b0> <class 'builtin_function_or_method'> quantized_rnn_relu_cell\n",
      "quantized_rnn_relu_cell ['isbuiltin', 'isroutine', 'callable']\n",
      "checking quantized_rnn_tanh_cell\n",
      "<built-in method quantized_rnn_tanh_cell of type object at 0x1144465b0> <class 'builtin_function_or_method'> quantized_rnn_tanh_cell\n",
      "quantized_rnn_tanh_cell ['isbuiltin', 'isroutine', 'callable']\n",
      "checking quasirandom\n",
      "<module 'torch.quasirandom' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/quasirandom.py'> <class 'module'> torch.quasirandom\n",
      "quasirandom ['ismodule']\n",
      "checking quint8\n",
      "torch.quint8 <class 'torch.dtype'> \n",
      "quint8 []\n",
      "checking rad2deg\n",
      "<built-in method rad2deg of type object at 0x1144465b0> <class 'builtin_function_or_method'> rad2deg\n",
      "rad2deg ['isbuiltin', 'isroutine', 'callable']\n",
      "checking rad2deg_\n",
      "<built-in method rad2deg_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> rad2deg_\n",
      "rad2deg_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking rand\n",
      "<built-in method rand of type object at 0x1144465b0> <class 'builtin_function_or_method'> rand\n",
      "rand ['isbuiltin', 'isroutine', 'callable']\n",
      "checking rand_like\n",
      "<built-in method rand_like of type object at 0x1144465b0> <class 'builtin_function_or_method'> rand_like\n",
      "rand_like ['isbuiltin', 'isroutine', 'callable']\n",
      "checking randint\n",
      "<built-in method randint of type object at 0x1144465b0> <class 'builtin_function_or_method'> randint\n",
      "randint ['isbuiltin', 'isroutine', 'callable']\n",
      "checking randint_like\n",
      "<built-in method randint_like of type object at 0x1144465b0> <class 'builtin_function_or_method'> randint_like\n",
      "randint_like ['isbuiltin', 'isroutine', 'callable']\n",
      "checking randn\n",
      "<built-in method randn of type object at 0x1144465b0> <class 'builtin_function_or_method'> randn\n",
      "randn ['isbuiltin', 'isroutine', 'callable']\n",
      "checking randn_like\n",
      "<built-in method randn_like of type object at 0x1144465b0> <class 'builtin_function_or_method'> randn_like\n",
      "randn_like ['isbuiltin', 'isroutine', 'callable']\n",
      "checking random\n",
      "<module 'torch.random' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/random.py'> <class 'module'> torch.random\n",
      "random ['ismodule']\n",
      "checking randperm\n",
      "<built-in method randperm of type object at 0x1144465b0> <class 'builtin_function_or_method'> randperm\n",
      "randperm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking range\n",
      "<built-in method range of type object at 0x1144465b0> <class 'builtin_function_or_method'> range\n",
      "range ['isbuiltin', 'isroutine', 'callable']\n",
      "checking real\n",
      "<built-in method real of type object at 0x1144465b0> <class 'builtin_function_or_method'> real\n",
      "real ['isbuiltin', 'isroutine', 'callable']\n",
      "checking reciprocal\n",
      "<built-in method reciprocal of type object at 0x1144465b0> <class 'builtin_function_or_method'> reciprocal\n",
      "reciprocal ['isbuiltin', 'isroutine', 'callable']\n",
      "checking reciprocal_\n",
      "<built-in method reciprocal_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> reciprocal_\n",
      "reciprocal_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking relu\n",
      "<built-in method relu of type object at 0x1144465b0> <class 'builtin_function_or_method'> relu\n",
      "relu ['isbuiltin', 'isroutine', 'callable']\n",
      "checking relu_\n",
      "<built-in method relu_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> relu_\n",
      "relu_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking remainder\n",
      "<built-in method remainder of type object at 0x1144465b0> <class 'builtin_function_or_method'> remainder\n",
      "remainder ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find remainder in native_func_dict\n",
      "Cant find remainder in native_func_dict\n",
      "Cant find remainder in native_func_dict\n",
      "checking renorm\n",
      "<built-in method renorm of type object at 0x1144465b0> <class 'builtin_function_or_method'> renorm\n",
      "renorm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking repeat_interleave\n",
      "<built-in method repeat_interleave of type object at 0x1144465b0> <class 'builtin_function_or_method'> repeat_interleave\n",
      "repeat_interleave ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find repeat_interleave in native_func_dict\n",
      "Cant find repeat_interleave in native_func_dict\n",
      "Cant find repeat_interleave in native_func_dict\n",
      "checking reshape\n",
      "<built-in method reshape of type object at 0x1144465b0> <class 'builtin_function_or_method'> reshape\n",
      "reshape ['isbuiltin', 'isroutine', 'callable']\n",
      "checking resize_as_\n",
      "<built-in method resize_as_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> resize_as_\n",
      "resize_as_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking result_type\n",
      "<built-in method result_type of type object at 0x1144465b0> <class 'builtin_function_or_method'> result_type\n",
      "result_type ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find result_type in native_func_dict\n",
      "Cant find result_type in native_func_dict\n",
      "Cant find result_type in native_func_dict\n",
      "checking rfft\n",
      "<built-in method rfft of type object at 0x1144465b0> <class 'builtin_function_or_method'> rfft\n",
      "rfft ['isbuiltin', 'isroutine', 'callable']\n",
      "checking rnn_relu\n",
      "<built-in method rnn_relu of type object at 0x1144465b0> <class 'builtin_function_or_method'> rnn_relu\n",
      "rnn_relu ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find rnn_relu in native_func_dict\n",
      "Cant find rnn_relu in native_func_dict\n",
      "Cant find rnn_relu in native_func_dict\n",
      "checking rnn_relu_cell\n",
      "<built-in method rnn_relu_cell of type object at 0x1144465b0> <class 'builtin_function_or_method'> rnn_relu_cell\n",
      "rnn_relu_cell ['isbuiltin', 'isroutine', 'callable']\n",
      "checking rnn_tanh\n",
      "<built-in method rnn_tanh of type object at 0x1144465b0> <class 'builtin_function_or_method'> rnn_tanh\n",
      "rnn_tanh ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find rnn_tanh in native_func_dict\n",
      "Cant find rnn_tanh in native_func_dict\n",
      "Cant find rnn_tanh in native_func_dict\n",
      "checking rnn_tanh_cell\n",
      "<built-in method rnn_tanh_cell of type object at 0x1144465b0> <class 'builtin_function_or_method'> rnn_tanh_cell\n",
      "rnn_tanh_cell ['isbuiltin', 'isroutine', 'callable']\n",
      "checking roll\n",
      "<built-in method roll of type object at 0x1144465b0> <class 'builtin_function_or_method'> roll\n",
      "roll ['isbuiltin', 'isroutine', 'callable']\n",
      "checking rot90\n",
      "<built-in method rot90 of type object at 0x1144465b0> <class 'builtin_function_or_method'> rot90\n",
      "rot90 ['isbuiltin', 'isroutine', 'callable']\n",
      "checking round\n",
      "<built-in method round of type object at 0x1144465b0> <class 'builtin_function_or_method'> round\n",
      "round ['isbuiltin', 'isroutine', 'callable']\n",
      "checking round_\n",
      "<built-in method round_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> round_\n",
      "round_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking rrelu\n",
      "<built-in method rrelu of type object at 0x1144465b0> <class 'builtin_function_or_method'> rrelu\n",
      "rrelu ['isbuiltin', 'isroutine', 'callable']\n",
      "checking rrelu_\n",
      "<built-in method rrelu_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> rrelu_\n",
      "rrelu_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking rsqrt\n",
      "<built-in method rsqrt of type object at 0x1144465b0> <class 'builtin_function_or_method'> rsqrt\n",
      "rsqrt ['isbuiltin', 'isroutine', 'callable']\n",
      "checking rsqrt_\n",
      "<built-in method rsqrt_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> rsqrt_\n",
      "rsqrt_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking rsub\n",
      "<built-in method rsub of type object at 0x1144465b0> <class 'builtin_function_or_method'> rsub\n",
      "rsub ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find rsub in native_func_dict\n",
      "Cant find rsub in native_func_dict\n",
      "Cant find rsub in native_func_dict\n",
      "checking saddmm\n",
      "<built-in method saddmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> saddmm\n",
      "saddmm ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find saddmm in native_func_dict\n",
      "Cant find saddmm in native_func_dict\n",
      "Cant find saddmm in native_func_dict\n",
      "checking save\n",
      "<function save at 0x115a86048> <class 'function'> save\n",
      "save ['isfunction', 'isroutine', 'callable']\n",
      "checking scalar_tensor\n",
      "<built-in method scalar_tensor of type object at 0x1144465b0> <class 'builtin_function_or_method'> scalar_tensor\n",
      "scalar_tensor ['isbuiltin', 'isroutine', 'callable']\n",
      "checking scatter\n",
      "<built-in method scatter of type object at 0x1144465b0> <class 'builtin_function_or_method'> scatter\n",
      "scatter ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find scatter in native_func_dict\n",
      "Cant find scatter in native_func_dict\n",
      "Cant find scatter in native_func_dict\n",
      "checking scatter_add\n",
      "<built-in method scatter_add of type object at 0x1144465b0> <class 'builtin_function_or_method'> scatter_add\n",
      "scatter_add ['isbuiltin', 'isroutine', 'callable']\n",
      "checking searchsorted\n",
      "<built-in method searchsorted of type object at 0x1144465b0> <class 'builtin_function_or_method'> searchsorted\n",
      "searchsorted ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find searchsorted in native_func_dict\n",
      "Cant find searchsorted in native_func_dict\n",
      "Cant find searchsorted in native_func_dict\n",
      "checking seed\n",
      "<function seed at 0x115a3fea0> <class 'function'> seed\n",
      "seed ['isfunction', 'isroutine', 'callable']\n",
      "checking select\n",
      "<built-in method select of type object at 0x1144465b0> <class 'builtin_function_or_method'> select\n",
      "select ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find select in native_func_dict\n",
      "Cant find select in native_func_dict\n",
      "Cant find select in native_func_dict\n",
      "checking selu\n",
      "<built-in method selu of type object at 0x1144465b0> <class 'builtin_function_or_method'> selu\n",
      "selu ['isbuiltin', 'isroutine', 'callable']\n",
      "checking selu_\n",
      "<built-in method selu_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> selu_\n",
      "selu_ ['isbuiltin', 'isroutine', 'callable']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking serialization\n",
      "<module 'torch.serialization' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/serialization.py'> <class 'module'> torch.serialization\n",
      "serialization ['ismodule']\n",
      "checking set_anomaly_enabled\n",
      "<built-in function set_anomaly_enabled> <class 'builtin_function_or_method'> set_anomaly_enabled\n",
      "set_anomaly_enabled ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find set_anomaly_enabled in native_func_dict\n",
      "Cant find set_anomaly_enabled in native_func_dict\n",
      "Cant find set_anomaly_enabled in native_func_dict\n",
      "checking set_autocast_enabled\n",
      "<built-in function set_autocast_enabled> <class 'builtin_function_or_method'> set_autocast_enabled\n",
      "set_autocast_enabled ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find set_autocast_enabled in native_func_dict\n",
      "Cant find set_autocast_enabled in native_func_dict\n",
      "Cant find set_autocast_enabled in native_func_dict\n",
      "checking set_default_dtype\n",
      "<function set_default_dtype at 0x115667e18> <class 'function'> set_default_dtype\n",
      "set_default_dtype ['isfunction', 'isroutine', 'callable']\n",
      "checking set_default_tensor_type\n",
      "<function set_default_tensor_type at 0x114cdd0d0> <class 'function'> set_default_tensor_type\n",
      "set_default_tensor_type ['isfunction', 'isroutine', 'callable']\n",
      "checking set_flush_denormal\n",
      "<built-in function set_flush_denormal> <class 'builtin_function_or_method'> set_flush_denormal\n",
      "set_flush_denormal ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find set_flush_denormal in native_func_dict\n",
      "Cant find set_flush_denormal in native_func_dict\n",
      "Cant find set_flush_denormal in native_func_dict\n",
      "checking set_grad_enabled\n",
      "<class 'torch.autograd.grad_mode.set_grad_enabled'> <class 'type'> set_grad_enabled\n",
      "Hierarchy: [<class 'torch.autograd.grad_mode.set_grad_enabled'>, <class 'object'>]\n",
      "set_grad_enabled ['isclass', 'callable']\n",
      "checking set_num_interop_threads\n",
      "<built-in function set_num_interop_threads> <class 'builtin_function_or_method'> set_num_interop_threads\n",
      "set_num_interop_threads ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find set_num_interop_threads in native_func_dict\n",
      "Cant find set_num_interop_threads in native_func_dict\n",
      "Cant find set_num_interop_threads in native_func_dict\n",
      "checking set_num_threads\n",
      "<built-in function set_num_threads> <class 'builtin_function_or_method'> set_num_threads\n",
      "set_num_threads ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find set_num_threads in native_func_dict\n",
      "Cant find set_num_threads in native_func_dict\n",
      "Cant find set_num_threads in native_func_dict\n",
      "checking set_printoptions\n",
      "<function set_printoptions at 0x115a86620> <class 'function'> set_printoptions\n",
      "set_printoptions ['isfunction', 'isroutine', 'callable']\n",
      "checking set_rng_state\n",
      "<function set_rng_state at 0x115a3fd08> <class 'function'> set_rng_state\n",
      "set_rng_state ['isfunction', 'isroutine', 'callable']\n",
      "checking short\n",
      "torch.int16 <class 'torch.dtype'> \n",
      "short []\n",
      "checking sigmoid\n",
      "<built-in method sigmoid of type object at 0x1144465b0> <class 'builtin_function_or_method'> sigmoid\n",
      "sigmoid ['isbuiltin', 'isroutine', 'callable']\n",
      "checking sigmoid_\n",
      "<built-in method sigmoid_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> sigmoid_\n",
      "sigmoid_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking sign\n",
      "<built-in method sign of type object at 0x1144465b0> <class 'builtin_function_or_method'> sign\n",
      "sign ['isbuiltin', 'isroutine', 'callable']\n",
      "checking sin\n",
      "<built-in method sin of type object at 0x1144465b0> <class 'builtin_function_or_method'> sin\n",
      "sin ['isbuiltin', 'isroutine', 'callable']\n",
      "checking sin_\n",
      "<built-in method sin_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> sin_\n",
      "sin_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking sinh\n",
      "<built-in method sinh of type object at 0x1144465b0> <class 'builtin_function_or_method'> sinh\n",
      "sinh ['isbuiltin', 'isroutine', 'callable']\n",
      "checking sinh_\n",
      "<built-in method sinh_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> sinh_\n",
      "sinh_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking slogdet\n",
      "<built-in method slogdet of type object at 0x1144465b0> <class 'builtin_function_or_method'> slogdet\n",
      "slogdet ['isbuiltin', 'isroutine', 'callable']\n",
      "checking smm\n",
      "<built-in method smm of type object at 0x1144465b0> <class 'builtin_function_or_method'> smm\n",
      "smm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking softmax\n",
      "<built-in method softmax of type object at 0x1144465b0> <class 'builtin_function_or_method'> softmax\n",
      "softmax ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find softmax in native_func_dict\n",
      "Cant find softmax in native_func_dict\n",
      "Cant find softmax in native_func_dict\n",
      "checking solve\n",
      "<built-in method solve of type object at 0x1144465b0> <class 'builtin_function_or_method'> solve\n",
      "solve ['isbuiltin', 'isroutine', 'callable']\n",
      "checking sort\n",
      "<built-in method sort of type object at 0x1144465b0> <class 'builtin_function_or_method'> sort\n",
      "sort ['isbuiltin', 'isroutine', 'callable']\n",
      "checking sparse\n",
      "<module 'torch.sparse' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/sparse/__init__.py'> <class 'module'> torch.sparse\n",
      "sparse ['ismodule']\n",
      "checking sparse_coo\n",
      "torch.sparse_coo <class 'torch.layout'> \n",
      "sparse_coo []\n",
      "checking sparse_coo_tensor\n",
      "<built-in method sparse_coo_tensor of type object at 0x1144465b0> <class 'builtin_function_or_method'> sparse_coo_tensor\n",
      "sparse_coo_tensor ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find sparse_coo_tensor in native_func_dict\n",
      "Cant find sparse_coo_tensor in native_func_dict\n",
      "Cant find sparse_coo_tensor in native_func_dict\n",
      "checking split\n",
      "<function split at 0x13770f510> <class 'function'> split\n",
      "split ['isfunction', 'isroutine', 'callable']\n",
      "checking split_with_sizes\n",
      "<built-in method split_with_sizes of type object at 0x1144465b0> <class 'builtin_function_or_method'> split_with_sizes\n",
      "split_with_sizes ['isbuiltin', 'isroutine', 'callable']\n",
      "checking spmm\n",
      "<built-in method spmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> spmm\n",
      "spmm ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find spmm in native_func_dict\n",
      "Cant find spmm in native_func_dict\n",
      "Cant find spmm in native_func_dict\n",
      "checking sqrt\n",
      "<built-in method sqrt of type object at 0x1144465b0> <class 'builtin_function_or_method'> sqrt\n",
      "sqrt ['isbuiltin', 'isroutine', 'callable']\n",
      "checking sqrt_\n",
      "<built-in method sqrt_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> sqrt_\n",
      "sqrt_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking square\n",
      "<built-in method square of type object at 0x1144465b0> <class 'builtin_function_or_method'> square\n",
      "square ['isbuiltin', 'isroutine', 'callable']\n",
      "checking square_\n",
      "<built-in method square_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> square_\n",
      "square_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking squeeze\n",
      "<built-in method squeeze of type object at 0x1144465b0> <class 'builtin_function_or_method'> squeeze\n",
      "squeeze ['isbuiltin', 'isroutine', 'callable']\n",
      "checking sspaddmm\n",
      "<built-in method sspaddmm of type object at 0x1144465b0> <class 'builtin_function_or_method'> sspaddmm\n",
      "sspaddmm ['isbuiltin', 'isroutine', 'callable']\n",
      "checking stack\n",
      "<built-in method stack of type object at 0x1144465b0> <class 'builtin_function_or_method'> stack\n",
      "stack ['isbuiltin', 'isroutine', 'callable']\n",
      "checking std\n",
      "<built-in method std of type object at 0x1144465b0> <class 'builtin_function_or_method'> std\n",
      "std ['isbuiltin', 'isroutine', 'callable']\n",
      "checking std_mean\n",
      "<built-in method std_mean of type object at 0x1144465b0> <class 'builtin_function_or_method'> std_mean\n",
      "std_mean ['isbuiltin', 'isroutine', 'callable']\n",
      "checking stft\n",
      "<function stft at 0x13770f840> <class 'function'> stft\n",
      "stft ['isfunction', 'isroutine', 'callable']\n",
      "checking storage\n",
      "<module 'torch.storage' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/storage.py'> <class 'module'> torch.storage\n",
      "storage ['ismodule']\n",
      "checking strided\n",
      "torch.strided <class 'torch.layout'> \n",
      "strided []\n",
      "checking sub\n",
      "<built-in method sub of type object at 0x1144465b0> <class 'builtin_function_or_method'> sub\n",
      "sub ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find sub in native_func_dict\n",
      "Cant find sub in native_func_dict\n",
      "Cant find sub in native_func_dict\n",
      "checking sum\n",
      "<built-in method sum of type object at 0x1144465b0> <class 'builtin_function_or_method'> sum\n",
      "sum ['isbuiltin', 'isroutine', 'callable']\n",
      "checking svd\n",
      "<built-in method svd of type object at 0x1144465b0> <class 'builtin_function_or_method'> svd\n",
      "svd ['isbuiltin', 'isroutine', 'callable']\n",
      "checking svd_lowrank\n",
      "<function svd_lowrank at 0x13770f378> <class 'function'> svd_lowrank\n",
      "svd_lowrank ['isfunction', 'isroutine', 'callable']\n",
      "checking symeig\n",
      "<built-in method symeig of type object at 0x1144465b0> <class 'builtin_function_or_method'> symeig\n",
      "symeig ['isbuiltin', 'isroutine', 'callable']\n",
      "checking sys\n",
      "<module 'sys' (built-in)> <class 'module'> sys\n",
      "sys ['ismodule']\n",
      "checking t\n",
      "<built-in method t of type object at 0x1144465b0> <class 'builtin_function_or_method'> t\n",
      "t ['isbuiltin', 'isroutine', 'callable']\n",
      "checking take\n",
      "<built-in method take of type object at 0x1144465b0> <class 'builtin_function_or_method'> take\n",
      "take ['isbuiltin', 'isroutine', 'callable']\n",
      "checking tan\n",
      "<built-in method tan of type object at 0x1144465b0> <class 'builtin_function_or_method'> tan\n",
      "tan ['isbuiltin', 'isroutine', 'callable']\n",
      "checking tan_\n",
      "<built-in method tan_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> tan_\n",
      "tan_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking tanh\n",
      "<built-in method tanh of type object at 0x1144465b0> <class 'builtin_function_or_method'> tanh\n",
      "tanh ['isbuiltin', 'isroutine', 'callable']\n",
      "checking tanh_\n",
      "<built-in method tanh_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> tanh_\n",
      "tanh_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking tensor\n",
      "<built-in method tensor of type object at 0x1144465b0> <class 'builtin_function_or_method'> tensor\n",
      "tensor ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find tensor in native_func_dict\n",
      "Cant find tensor in native_func_dict\n",
      "Cant find tensor in native_func_dict\n",
      "checking tensordot\n",
      "<function tensordot at 0x137712158> <class 'function'> tensordot\n",
      "tensordot ['isfunction', 'isroutine', 'callable']\n",
      "checking testing\n",
      "<module 'torch.testing' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/testing/__init__.py'> <class 'module'> torch.testing\n",
      "testing ['ismodule']\n",
      "checking threshold\n",
      "<built-in method threshold of type object at 0x1144465b0> <class 'builtin_function_or_method'> threshold\n",
      "threshold ['isbuiltin', 'isroutine', 'callable']\n",
      "checking threshold_\n",
      "<built-in method threshold_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> threshold_\n",
      "threshold_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking topk\n",
      "<built-in method topk of type object at 0x1144465b0> <class 'builtin_function_or_method'> topk\n",
      "topk ['isbuiltin', 'isroutine', 'callable']\n",
      "checking torch\n",
      "<module 'torch' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/__init__.py'> <class 'module'> torch\n",
      "torch ['ismodule']\n",
      "checking trace\n",
      "<built-in method trace of type object at 0x1144465b0> <class 'builtin_function_or_method'> trace\n",
      "trace ['isbuiltin', 'isroutine', 'callable']\n",
      "checking transpose\n",
      "<built-in method transpose of type object at 0x1144465b0> <class 'builtin_function_or_method'> transpose\n",
      "transpose ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find transpose in native_func_dict\n",
      "Cant find transpose in native_func_dict\n",
      "Cant find transpose in native_func_dict\n",
      "checking trapz\n",
      "<built-in method trapz of type object at 0x1144465b0> <class 'builtin_function_or_method'> trapz\n",
      "trapz ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find trapz in native_func_dict\n",
      "Cant find trapz in native_func_dict\n",
      "Cant find trapz in native_func_dict\n",
      "checking triangular_solve\n",
      "<built-in method triangular_solve of type object at 0x1144465b0> <class 'builtin_function_or_method'> triangular_solve\n",
      "triangular_solve ['isbuiltin', 'isroutine', 'callable']\n",
      "checking tril\n",
      "<built-in method tril of type object at 0x1144465b0> <class 'builtin_function_or_method'> tril\n",
      "tril ['isbuiltin', 'isroutine', 'callable']\n",
      "checking tril_indices\n",
      "<built-in method tril_indices of type object at 0x1144465b0> <class 'builtin_function_or_method'> tril_indices\n",
      "tril_indices ['isbuiltin', 'isroutine', 'callable']\n",
      "checking triplet_margin_loss\n",
      "<built-in method triplet_margin_loss of type object at 0x1144465b0> <class 'builtin_function_or_method'> triplet_margin_loss\n",
      "triplet_margin_loss ['isbuiltin', 'isroutine', 'callable']\n",
      "checking triu\n",
      "<built-in method triu of type object at 0x1144465b0> <class 'builtin_function_or_method'> triu\n",
      "triu ['isbuiltin', 'isroutine', 'callable']\n",
      "checking triu_indices\n",
      "<built-in method triu_indices of type object at 0x1144465b0> <class 'builtin_function_or_method'> triu_indices\n",
      "triu_indices ['isbuiltin', 'isroutine', 'callable']\n",
      "checking true_divide\n",
      "<built-in method true_divide of type object at 0x1144465b0> <class 'builtin_function_or_method'> true_divide\n",
      "true_divide ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find true_divide in native_func_dict\n",
      "Cant find true_divide in native_func_dict\n",
      "Cant find true_divide in native_func_dict\n",
      "checking trunc\n",
      "<built-in method trunc of type object at 0x1144465b0> <class 'builtin_function_or_method'> trunc\n",
      "trunc ['isbuiltin', 'isroutine', 'callable']\n",
      "checking trunc_\n",
      "<built-in method trunc_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> trunc_\n",
      "trunc_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking typename\n",
      "<function typename at 0x1111a2f28> <class 'function'> typename\n",
      "typename ['isfunction', 'isroutine', 'callable']\n",
      "checking types\n",
      "<module 'torch.types' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/types.py'> <class 'module'> torch.types\n",
      "types ['ismodule']\n",
      "checking uint8\n",
      "torch.uint8 <class 'torch.dtype'> \n",
      "uint8 []\n",
      "checking unbind\n",
      "<built-in method unbind of type object at 0x1144465b0> <class 'builtin_function_or_method'> unbind\n",
      "unbind ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find unbind in native_func_dict\n",
      "Cant find unbind in native_func_dict\n",
      "Cant find unbind in native_func_dict\n",
      "checking unique\n",
      "<function boolean_dispatch.<locals>.fn at 0x13770fd08> <class 'function'> unique\n",
      "unique ['isfunction', 'isroutine', 'callable']\n",
      "checking unique_consecutive\n",
      "<function boolean_dispatch.<locals>.fn at 0x1377120d0> <class 'function'> unique_consecutive\n",
      "unique_consecutive ['isfunction', 'isroutine', 'callable']\n",
      "checking unsqueeze\n",
      "<built-in method unsqueeze of type object at 0x1144465b0> <class 'builtin_function_or_method'> unsqueeze\n",
      "unsqueeze ['isbuiltin', 'isroutine', 'callable']\n",
      "checking utils\n",
      "<module 'torch.utils' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/utils/__init__.py'> <class 'module'> torch.utils\n",
      "utils ['ismodule']\n",
      "checking vander\n",
      "<built-in method vander of type object at 0x1144465b0> <class 'builtin_function_or_method'> vander\n",
      "vander ['isbuiltin', 'isroutine', 'callable']\n",
      "checking var\n",
      "<built-in method var of type object at 0x1144465b0> <class 'builtin_function_or_method'> var\n",
      "var ['isbuiltin', 'isroutine', 'callable']\n",
      "checking var_mean\n",
      "<built-in method var_mean of type object at 0x1144465b0> <class 'builtin_function_or_method'> var_mean\n",
      "var_mean ['isbuiltin', 'isroutine', 'callable']\n",
      "checking version\n",
      "<module 'torch.version' from '/Users/madhavajay/.local/share/virtualenvs/PySyft-lHlz_cKe/lib/python3.6/site-packages/torch/version.py'> <class 'module'> torch.version\n",
      "version ['ismodule']\n",
      "checking view_as_complex\n",
      "<built-in method view_as_complex of type object at 0x1144465b0> <class 'builtin_function_or_method'> view_as_complex\n",
      "view_as_complex ['isbuiltin', 'isroutine', 'callable']\n",
      "checking view_as_real\n",
      "<built-in method view_as_real of type object at 0x1144465b0> <class 'builtin_function_or_method'> view_as_real\n",
      "view_as_real ['isbuiltin', 'isroutine', 'callable']\n",
      "checking wait\n",
      "<built-in method wait of PyCapsule object at 0x114ca6060> <class 'builtin_function_or_method'> wait\n",
      "wait ['isbuiltin', 'isroutine', 'callable']\n",
      "Cant find wait in native_func_dict\n",
      "Cant find wait in native_func_dict\n",
      "Cant find wait in native_func_dict\n",
      "checking where\n",
      "<built-in method where of type object at 0x1144465b0> <class 'builtin_function_or_method'> where\n",
      "where ['isbuiltin', 'isroutine', 'callable']\n",
      "checking zero_\n",
      "<built-in method zero_ of type object at 0x1144465b0> <class 'builtin_function_or_method'> zero_\n",
      "zero_ ['isbuiltin', 'isroutine', 'callable']\n",
      "checking zeros\n",
      "<built-in method zeros of type object at 0x1144465b0> <class 'builtin_function_or_method'> zeros\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeros ['isbuiltin', 'isroutine', 'callable']\n",
      "checking zeros_like\n",
      "<built-in method zeros_like of type object at 0x1144465b0> <class 'builtin_function_or_method'> zeros_like\n",
      "zeros_like ['isbuiltin', 'isroutine', 'callable']\n"
     ]
    }
   ],
   "source": [
    "missing_attr_matches = []\n",
    "def detect_attrs(obj) -> dict:\n",
    "    path_dict = {}\n",
    "    attr_dict = {\"module\":{}, \"enum\": {}, \"class\": {}, \"method\":{}, \"function\":{}, \"property\":{}, \"builtin\": {}}\n",
    "    \n",
    "    count = 0\n",
    "    for key in dir(obj):\n",
    "        try:\n",
    "            if key.startswith(\"__\"):\n",
    "                continue\n",
    "            print(\"checking\", key)\n",
    "            count += 1\n",
    "    #         if count > 20:\n",
    "    #             break\n",
    "            attr = getattr(obj, key)\n",
    "            prop_types = whatis(attr)\n",
    "            print(key, prop_types)\n",
    "            if \"iscenum\" in prop_types and \"ispybind11\" not in prop_types:\n",
    "                attr_dict[\"enum\"][type(attr)] = {\"members\":type(attr).__members__}\n",
    "                path_dict[key] = {\"type\": \"enum\", \"members\":type(attr).__members__}\n",
    "            if \"isclass\" in prop_types and \"ispybind11\" not in prop_types:\n",
    "                attr_dict[\"class\"][key] = get_signature(attr)\n",
    "                path_dict[key] = {\"type\": \"class\", \"init\":get_signature(attr)}\n",
    "            if \"isclass\" in prop_types and \"ispybind11\" in prop_types:\n",
    "                attr_dict[\"class\"][key] = get_c_class_signature(attr)\n",
    "                path_dict[key] = {\"type\": \"class\", \"init\":get_c_class_signature(attr)}\n",
    "\n",
    "            if \"isbuiltin\" in prop_types and \"isroutine\" in prop_types:\n",
    "                if get_native_signature(key) is None or get_native_return_type(key) is None:\n",
    "                    missing_attr_matches.append(key)\n",
    "                path_dict[key] = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"signature\": get_native_signature(key),\n",
    "                    \"return_type\": get_native_return_type(key)\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Exception with {key}. {e}\")\n",
    "    return path_dict\n",
    "\n",
    "attrs = detect_attrs(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add',\n",
       " 'as_tensor',\n",
       " 'autocast_decrement_nesting',\n",
       " 'autocast_increment_nesting',\n",
       " 'bitwise_and',\n",
       " 'bitwise_or',\n",
       " 'bitwise_xor',\n",
       " 'bucketize',\n",
       " 'clear_autocast_cache',\n",
       " 'conv_transpose2d',\n",
       " 'conv_transpose3d',\n",
       " 'ctc_loss',\n",
       " 'dequantize',\n",
       " 'div',\n",
       " 'dsmm',\n",
       " 'empty',\n",
       " 'eq',\n",
       " 'fill_',\n",
       " 'flatten',\n",
       " 'fmod',\n",
       " 'fork',\n",
       " 'from_numpy',\n",
       " 'ge',\n",
       " 'get_default_dtype',\n",
       " 'get_device',\n",
       " 'get_num_interop_threads',\n",
       " 'get_num_threads',\n",
       " 'gru',\n",
       " 'gt',\n",
       " 'hsmm',\n",
       " 'import_ir_module',\n",
       " 'import_ir_module_from_buffer',\n",
       " 'index_fill',\n",
       " 'init_num_threads',\n",
       " 'is_anomaly_enabled',\n",
       " 'is_autocast_enabled',\n",
       " 'is_grad_enabled',\n",
       " 'le',\n",
       " 'lerp',\n",
       " 'log_softmax',\n",
       " 'lstm',\n",
       " 'lt',\n",
       " 'masked_fill',\n",
       " 'merge_type_from_type_comment',\n",
       " 'mul',\n",
       " 'ne',\n",
       " 'normal',\n",
       " 'numel',\n",
       " 'parse_ir',\n",
       " 'parse_schema',\n",
       " 'parse_type_comment',\n",
       " 'pow',\n",
       " 'quantized_gru',\n",
       " 'quantized_lstm',\n",
       " 'remainder',\n",
       " 'repeat_interleave',\n",
       " 'result_type',\n",
       " 'rnn_relu',\n",
       " 'rnn_tanh',\n",
       " 'rsub',\n",
       " 'saddmm',\n",
       " 'scatter',\n",
       " 'searchsorted',\n",
       " 'select',\n",
       " 'set_anomaly_enabled',\n",
       " 'set_autocast_enabled',\n",
       " 'set_flush_denormal',\n",
       " 'set_num_interop_threads',\n",
       " 'set_num_threads',\n",
       " 'softmax',\n",
       " 'sparse_coo_tensor',\n",
       " 'spmm',\n",
       " 'sub',\n",
       " 'tensor',\n",
       " 'transpose',\n",
       " 'trapz',\n",
       " 'true_divide',\n",
       " 'unbind',\n",
       " 'wait']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_attr_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.AVG - enum ({'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG})\n",
      "torch.AggregationType - class (1. torch._C.AggregationType(arg0: int))\n",
      "torch.AnyType - class (None)\n",
      "torch.Argument - class (None)\n",
      "torch.ArgumentSpec - class (None)\n",
      "torch.BFloat16Storage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.BFloat16Tensor - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.BenchmarkConfig - class (1. <class 'torch._C.BenchmarkConfig'>())\n",
      "torch.BenchmarkExecutionStats - class (None)\n",
      "torch.Block - class (None)\n",
      "torch.BoolStorage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.BoolTensor - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.BoolType - class (None)\n",
      "torch.BufferDict - class (1. torch._C.BufferDict(arg0: torch._C.ScriptModule))\n",
      "torch.ByteStorage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.ByteTensor - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.CONV_BN_FUSION - enum ({'CONV_BN_FUSION': MobileOptimizerType.CONV_BN_FUSION, 'INSERT_FOLD_PREPACK_OPS': MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, 'REMOVE_DROPOUT': MobileOptimizerType.REMOVE_DROPOUT})\n",
      "torch.CallStack - class (1. torch._C.CallStack(arg0: str, arg1: torch._C._jit_tree_views.SourceRange))\n",
      "torch.Capsule - class (None)\n",
      "torch.CharStorage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.CharTensor - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.ClassType - class (1. torch._C.ClassType(arg0: str))\n",
      "torch.Code - class (None)\n",
      "torch.CompilationUnit - class (1. <class 'torch._C.CompilationUnit'>())\n",
      "torch.CompleteArgumentSpec - class (None)\n",
      "torch.ComplexDoubleStorage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.ComplexFloatStorage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.ConcreteModuleType - class (None)\n",
      "torch.ConcreteModuleTypeBuilder - class (1. torch._C.ConcreteModuleTypeBuilder(arg0: object))\n",
      "torch.DeepCopyMemoTable - class (None)\n",
      "torch.DeviceObjType - class (None)\n",
      "torch.DictType - class (1. torch._C.DictType(arg0: torch._C.Type, arg1: torch._C.Type))\n",
      "torch.DoubleStorage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.DoubleTensor - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.ErrorReport - class (1. torch._C.ErrorReport(arg0: torch._C._jit_tree_views.SourceRange))\n",
      "torch.ExecutionPlan - class (None)\n",
      "torch.ExtraFilesMap - class (1. <class 'torch._C.ExtraFilesMap'>())\n",
      "torch.FatalError - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.FileCheck - class (1. <class 'torch._C.FileCheck'>())\n",
      "torch.FloatStorage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.FloatTensor - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.FloatType - class (None)\n",
      "torch.FunctionSchema - class (None)\n",
      "torch.Future - class (1. <class 'torch._C.Future'>())\n",
      "torch.FutureType - class (1. torch._C.FutureType(arg0: torch._C.Type))\n",
      "torch.Generator - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.Gradient - class (None)\n",
      "torch.Graph - class (1. <class 'torch._C.Graph'>())\n",
      "torch.GraphExecutorState - class (None)\n",
      "torch.HalfStorage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.HalfStorageBase - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.HalfTensor - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.INSERT_FOLD_PREPACK_OPS - enum ({'CONV_BN_FUSION': MobileOptimizerType.CONV_BN_FUSION, 'INSERT_FOLD_PREPACK_OPS': MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, 'REMOVE_DROPOUT': MobileOptimizerType.REMOVE_DROPOUT})\n",
      "torch.IODescriptor - class (None)\n",
      "torch.IntStorage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.IntTensor - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.IntType - class (None)\n",
      "torch.InterfaceType - class (1. torch._C.InterfaceType(arg0: str))\n",
      "torch.JITException - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.ListType - class (1. torch._C.ListType(arg0: torch._C.Type))\n",
      "torch.LiteScriptModule - class (1. torch._C.LiteScriptModule(arg0: c10::intrusive_ptr<c10::ivalue::Object, c10::detail::intrusive_target_default_null_type<c10::ivalue::Object> >, arg1: torch::jit::mobile::CompilationUnit))\n",
      "torch.LockingLogger - class (1. <class 'torch._C.LockingLogger'>())\n",
      "torch.LoggerBase - class (None)\n",
      "torch.LongStorage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.LongTensor - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.MobileOptimizerType - class (1. torch._C.MobileOptimizerType(arg0: int))\n",
      "torch.ModuleDict - class (1. torch._C.ModuleDict(arg0: torch._C.ScriptModule))\n",
      "torch.Node - class (None)\n",
      "torch.NoneType - class (None)\n",
      "torch.NoopLogger - class (1. <class 'torch._C.NoopLogger'>())\n",
      "torch.NumberType - class (None)\n",
      "torch.OptionalType - class (1. torch._C.OptionalType(arg0: torch._C.Type))\n",
      "torch.ParameterDict - class (1. torch._C.ParameterDict(arg0: torch._C.ScriptModule))\n",
      "torch.PyObjectType - class (None)\n",
      "torch.PyTorchFileReader - class (1. torch._C.PyTorchFileReader(arg0: str)\n",
      "    2. torch._C.PyTorchFileReader(arg0: object))\n",
      "torch.PyTorchFileWriter - class (1. torch._C.PyTorchFileWriter(arg0: str)\n",
      "    2. torch._C.PyTorchFileWriter(arg0: object)\n",
      "    3. torch._C.PyTorchFileWriter(arg0: Callable[[capsule, int], int]))\n",
      "torch.QInt32Storage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.QInt32StorageBase - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.QInt8Storage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.QInt8StorageBase - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.QUInt8Storage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.REMOVE_DROPOUT - enum ({'CONV_BN_FUSION': MobileOptimizerType.CONV_BN_FUSION, 'INSERT_FOLD_PREPACK_OPS': MobileOptimizerType.INSERT_FOLD_PREPACK_OPS, 'REMOVE_DROPOUT': MobileOptimizerType.REMOVE_DROPOUT})\n",
      "torch.RRefType - class (1. torch._C.RRefType(arg0: torch._C.Type))\n",
      "torch.SUM - enum ({'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG})\n",
      "torch.ScriptClass - class (None)\n",
      "torch.ScriptFunction - class (None)\n",
      "torch.ScriptMethod - class (None)\n",
      "torch.ScriptModule - class (1. torch._C.ScriptModule(arg0: str, arg1: torch::jit::CompilationUnit, arg2: bool))\n",
      "torch.ScriptObject - class (None)\n",
      "torch.Set - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.ShortStorage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.ShortTensor - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.Size - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.Storage - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.StringType - class (None)\n",
      "torch.Tensor - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.TensorType - class (None)\n",
      "torch.ThroughputBenchmark - class (1. torch._C.ThroughputBenchmark(arg0: torch._C.ScriptModule)\n",
      "    2. torch._C.ThroughputBenchmark(arg0: object))\n",
      "torch.TracingState - class (None)\n",
      "torch.TupleType - class (1. torch._C.TupleType(arg0: List[torch._C.Type]))\n",
      "torch.Type - class (None)\n",
      "torch.Use - class (None)\n",
      "torch.Value - class (None)\n",
      "torch._StorageBase - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch._adaptive_avg_pool2d - function (['self: Tensor', 'output_size: List[int]'] => ['Tensor'])\n",
      "torch._addmv_impl_ - function (['self: Tensor', 'self2: Tensor', 'mat: Tensor', 'vec: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch._addr - function (['self: Tensor', 'vec1: Tensor', 'vec2: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch._addr_ - function (['self: Tensor', 'vec1: Tensor', 'vec2: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch._amp_non_finite_check_and_unscale_ - function (['self: Tensor', 'found_inf: Tensor', 'inv_scale: Tensor'] => ['None'])\n",
      "torch._amp_update_scale - function (['growth_tracker: Tensor', 'current_scale: Tensor', 'found_inf: Tensor', 'scale_growth_factor: float', 'scale_backoff_factor: float', 'growth_interval: int'] => ['Tensor'])\n",
      "torch._baddbmm_mkl_ - function (['self: Tensor', 'batch1: Tensor', 'batch2: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch._batch_norm_impl_index - function (['input: Tensor', 'weight: Optional[Tensor]', 'bias: Optional[Tensor]', 'running_mean: Optional[Tensor]', 'running_var: Optional[Tensor]', 'training: bool', 'momentum: float', 'eps: float', 'cudnn_enabled: bool'] => ['Tensor', 'Tensor', 'Tensor', 'Tensor', 'int'])\n",
      "torch._bmm - function (['self: Tensor', 'mat2: Tensor', 'deterministic: bool = False'] => ['Tensor'])\n",
      "torch._cast_Byte - function (['self: Tensor', 'non_blocking: bool = False'] => ['Tensor'])\n",
      "torch._cast_Char - function (['self: Tensor', 'non_blocking: bool = False'] => ['Tensor'])\n",
      "torch._cast_Double - function (['self: Tensor', 'non_blocking: bool = False'] => ['Tensor'])\n",
      "torch._cast_Float - function (['self: Tensor', 'non_blocking: bool = False'] => ['Tensor'])\n",
      "torch._cast_Half - function (['self: Tensor', 'non_blocking: bool = False'] => ['Tensor'])\n",
      "torch._cast_Int - function (['self: Tensor', 'non_blocking: bool = False'] => ['Tensor'])\n",
      "torch._cast_Long - function (['self: Tensor', 'non_blocking: bool = False'] => ['Tensor'])\n",
      "torch._cast_Short - function (['self: Tensor', 'non_blocking: bool = False'] => ['Tensor'])\n",
      "torch._cat - function (['tensors: List[Tensor]', 'dim: int = 0'] => ['Tensor'])\n",
      "torch._choose_qparams_per_tensor - function (['self: Tensor', 'reduce_range: bool = False'] => ['float', 'int'])\n",
      "torch._convolution - function (['input: Tensor', 'weight: Tensor', 'bias: Optional[Tensor]', 'stride: List[int]', 'padding: List[int]', 'dilation: List[int]', 'transposed: bool', 'output_padding: List[int]', 'groups: int', 'benchmark: bool', 'deterministic: bool', 'cudnn_enabled: bool'] => ['Tensor'])\n",
      "torch._convolution_nogroup - function (['input: Tensor', 'weight: Tensor', 'bias: Optional[Tensor]', 'stride: List[int]', 'padding: List[int]', 'dilation: List[int]', 'transposed: bool', 'output_padding: List[int]'] => ['Tensor'])\n",
      "torch._copy_from - function (['self: Tensor', 'dst: Tensor', 'non_blocking: bool = False'] => ['Tensor'])\n",
      "torch._ctc_loss - function (['log_probs: Tensor', 'targets: Tensor', 'input_lengths: List[int]', 'target_lengths: List[int]', 'blank: int = 0', 'zero_infinity: bool = False'] => ['Tensor', 'Tensor'])\n",
      "torch._cudnn_ctc_loss - function (['log_probs: Tensor', 'targets: Tensor', 'input_lengths: List[int]', 'target_lengths: List[int]', 'blank: int', 'deterministic: bool', 'zero_infinity: bool'] => ['Tensor', 'Tensor'])\n",
      "torch._cudnn_init_dropout_state - function (['dropout: float', 'train: bool', 'dropout_seed: int', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = False'] => ['Tensor'])\n",
      "torch._cudnn_rnn - function (['input: Tensor', 'weight: List[Tensor]', 'weight_stride0: int', 'weight_buf: Optional[Tensor]', 'hx: Tensor', 'cx: Optional[Tensor]', 'mode: int', 'hidden_size: int', 'num_layers: int', 'batch_first: bool', 'dropout: float', 'train: bool', 'bidirectional: bool', 'batch_sizes: List[int]', 'dropout_state: Optional[Tensor]'] => ['Tensor', 'Tensor', 'Tensor', 'Tensor', 'Tensor'])\n",
      "torch._cudnn_rnn_flatten_weight - function (['weight_arr: List[Tensor]', 'weight_stride0: int', 'input_size: int', 'mode: int', 'hidden_size: int', 'num_layers: int', 'batch_first: bool', 'bidirectional: bool'] => ['Tensor'])\n",
      "torch._cufft_clear_plan_cache - function (['device_index: int'] => ['None'])\n",
      "torch._cufft_get_plan_cache_max_size - function (['device_index: int'] => ['int'])\n",
      "torch._cufft_get_plan_cache_size - function (['device_index: int'] => ['int'])\n",
      "torch._cufft_set_plan_cache_max_size - function (['device_index: int', 'max_size: int'] => ['None'])\n",
      "torch._cummax_helper - function (['self: Tensor', 'values: Tensor', 'indices: Tensor', 'dim: int'] => ['None'])\n",
      "torch._cummin_helper - function (['self: Tensor', 'values: Tensor', 'indices: Tensor', 'dim: int'] => ['None'])\n",
      "torch._debug_has_internal_overlap - function (['self: Tensor'] => ['int'])\n",
      "torch._dim_arange - function (['like: Tensor', 'dim: int'] => ['Tensor'])\n",
      "torch._dirichlet_grad - function (['x: Tensor', 'alpha: Tensor', 'total: Tensor'] => ['Tensor'])\n",
      "torch._embedding_bag - function (['weight: Tensor', 'indices: Tensor', 'offsets: Tensor', 'scale_grad_by_freq: bool = False', 'mode: int = 0', 'sparse: bool = False', 'per_sample_weights: Optional[Tensor] = None', 'include_last_offset: bool = False'] => ['Tensor', 'Tensor', 'Tensor', 'Tensor'])\n",
      "torch._empty_affine_quantized - function (['size: List[int]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None', 'scale: float = 1', 'zero_point: int = 0', 'memory_format: Optional[MemoryFormat] = contiguous_format'] => ['Tensor'])\n",
      "torch._empty_per_channel_affine_quantized - function (['size: List[int]', 'scales: Tensor', 'zero_points: Tensor', 'axis: int', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None', 'memory_format: Optional[MemoryFormat] = contiguous_format'] => ['Tensor'])\n",
      "torch._euclidean_dist - function (['x1: Tensor', 'x2: Tensor'] => ['Tensor'])\n",
      "torch._fft_with_size - function (['self: Tensor', 'signal_ndim: int', 'complex_input: bool', 'complex_output: bool', 'inverse: bool', 'checked_signal_sizes: List[int]', 'normalized: bool', 'onesided: bool', 'output_sizes: List[int]'] => ['Tensor'])\n",
      "torch._fused_dropout - function (['self: Tensor', 'p: float', 'generator: Optional[Generator] = None'] => ['Tensor', 'Tensor'])\n",
      "torch._has_compatible_shallow_copy_type - function (['self: Tensor', 'from: Tensor'] => ['bool'])\n",
      "torch._index_copy_ - function (['self: Tensor', 'dim: int', 'index: Tensor', 'source: Tensor'] => ['Tensor'])\n",
      "torch._index_put_impl_ - function (['self: Tensor', 'indices: Optional[List[Tensor]]', 'values: Tensor', 'accumulate: bool = False', 'unsafe: bool = False'] => ['Tensor'])\n",
      "torch._log_softmax - function (['self: Tensor', 'dim: int', 'half_to_float: bool'] => ['Tensor'])\n",
      "torch._log_softmax_backward_data - function (['grad_output: Tensor', 'output: Tensor', 'dim: int', 'self: Tensor'] => ['Tensor'])\n",
      "torch._logcumsumexp - function (['self: Tensor', 'dim: int'] => ['Tensor'])\n",
      "torch._lu_solve_helper - function (['self: Tensor', 'LU_data: Tensor', 'LU_pivots: Tensor'] => ['Tensor'])\n",
      "torch._lu_with_info - function (['self: Tensor', 'pivot: bool = True', 'check_errors: bool = True'] => ['Tensor', 'Tensor', 'Tensor'])\n",
      "torch._make_per_channel_quantized_tensor - function (['self: Tensor', 'scale: Tensor', 'zero_point: Tensor', 'axis: int'] => ['Tensor'])\n",
      "torch._make_per_tensor_quantized_tensor - function (['self: Tensor', 'scale: float', 'zero_point: int'] => ['Tensor'])\n",
      "torch._masked_scale - function (['self: Tensor', 'mask: Tensor', 'scale: float'] => ['Tensor'])\n",
      "torch._mkldnn_reshape - function (['self: Tensor', 'shape: List[int]'] => ['Tensor'])\n",
      "torch._mkldnn_transpose - function (['self: Tensor', 'dim0: int', 'dim1: int'] => ['Tensor'])\n",
      "torch._mkldnn_transpose_ - function (['self: Tensor', 'dim0: int', 'dim1: int'] => ['Tensor'])\n",
      "torch._mode - function (['self: Tensor', 'dim: int = -1', 'keepdim: bool = False'] => ['Tensor', 'Tensor'])\n",
      "torch._multinomial_alias_draw - function (['J: Tensor', 'q: Tensor', 'num_samples: int', 'generator: Optional[Generator] = None'] => ['Tensor'])\n",
      "torch._multinomial_alias_setup - function (['probs: Tensor'] => ['Tensor', 'Tensor'])\n",
      "torch._nnpack_available - function (['None'] => ['bool'])\n",
      "torch._nnpack_spatial_convolution - function (['input: Tensor', 'weight: Tensor', 'bias: Optional[Tensor]', 'padding: List[int]', 'stride: List[int] = 1'] => ['Tensor'])\n",
      "torch._pack_padded_sequence - function (['input: Tensor', 'lengths: Tensor', 'batch_first: bool'] => ['Tensor', 'Tensor'])\n",
      "torch._pad_packed_sequence - function (['data: Tensor', 'batch_sizes: Tensor', 'batch_first: bool', 'padding_value: Union[int, float, complex, Tensor]', 'total_length: int'] => ['Tensor', 'Tensor'])\n",
      "torch._reshape_from_tensor - function (['self: Tensor', 'shape: Tensor'] => ['Tensor'])\n",
      "torch._s_where - function (['condition: Tensor', 'self: Tensor', 'other: Tensor'] => ['Tensor'])\n",
      "torch._sample_dirichlet - function (['self: Tensor', 'generator: Optional[Generator] = None'] => ['Tensor'])\n",
      "torch._shape_as_tensor - function (['self: Tensor'] => ['Tensor'])\n",
      "torch._sobol_engine_draw - function (['quasi: Tensor', 'n: int', 'sobolstate: Tensor', 'dimension: int', 'num_generated: int', 'dtype: Optional[torch.dtype]'] => ['Tensor', 'Tensor'])\n",
      "torch._sobol_engine_ff_ - function (['self: Tensor', 'n: int', 'sobolstate: Tensor', 'dimension: int', 'num_generated: int'] => ['Tensor'])\n",
      "torch._sobol_engine_initialize_state_ - function (['self: Tensor', 'dimension: int'] => ['Tensor'])\n",
      "torch._sobol_engine_scramble_ - function (['self: Tensor', 'ltm: Tensor', 'dimension: int'] => ['Tensor'])\n",
      "torch._softmax - function (['self: Tensor', 'dim: int', 'half_to_float: bool'] => ['Tensor'])\n",
      "torch._softmax_backward_data - function (['grad_output: Tensor', 'output: Tensor', 'dim: int', 'self: Tensor'] => ['Tensor'])\n",
      "torch._sparse_addmm - function (['self: Tensor', 'sparse: Tensor', 'dense: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch._sparse_log_softmax - function (['self: Tensor', 'dim: int', 'half_to_float: bool'] => ['Tensor'])\n",
      "torch._sparse_log_softmax_backward_data - function (['grad_output: Tensor', 'output: Tensor', 'dim: int', 'self: Tensor'] => ['Tensor'])\n",
      "torch._sparse_mm - function (['sparse: Tensor', 'dense: Tensor'] => ['Tensor'])\n",
      "torch._sparse_softmax - function (['self: Tensor', 'dim: int', 'half_to_float: bool'] => ['Tensor'])\n",
      "torch._sparse_softmax_backward_data - function (['grad_output: Tensor', 'output: Tensor', 'dim: int', 'self: Tensor'] => ['Tensor'])\n",
      "torch._sparse_sum - function (['self: Tensor'] => ['Tensor'])\n",
      "torch._standard_gamma - function (['self: Tensor', 'generator: Optional[Generator] = None'] => ['Tensor'])\n",
      "torch._standard_gamma_grad - function (['self: Tensor', 'output: Tensor'] => ['Tensor'])\n",
      "torch._test_serialization_subcmul - function (['self: Tensor', 'other: Tensor', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch._trilinear - function (['i1: Tensor', 'i2: Tensor', 'i3: Tensor', 'expand1: List[int]', 'expand2: List[int]', 'expand3: List[int]', 'sumdim: List[int]', 'unroll_dim: int = 1'] => ['Tensor'])\n",
      "torch._unique - function (['self: Tensor', 'sorted: bool = True', 'return_inverse: bool = False'] => ['Tensor', 'Tensor'])\n",
      "torch._unique2 - function (['self: Tensor', 'sorted: bool = True', 'return_inverse: bool = False', 'return_counts: bool = False'] => ['Tensor', 'Tensor', 'Tensor'])\n",
      "torch._use_cudnn_ctc_loss - function (['log_probs: Tensor', 'targets: Tensor', 'input_lengths: List[int]', 'target_lengths: List[int]', 'blank: int'] => ['bool'])\n",
      "torch._use_cudnn_rnn_flatten_weight - function (['None'] => ['bool'])\n",
      "torch._weight_norm - function (['v: Tensor', 'g: Tensor', 'dim: int = 0'] => ['Tensor'])\n",
      "torch._weight_norm_cuda_interface - function (['v: Tensor', 'g: Tensor', 'dim: int = 0'] => ['Tensor', 'Tensor'])\n",
      "torch.abs - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.abs_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.absolute - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.absolute_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.acos - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.acos_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.acosh - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.acosh_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.adaptive_avg_pool1d - function (['self: Tensor', 'output_size: List[int]'] => ['Tensor'])\n",
      "torch.adaptive_max_pool1d - function (['self: Tensor', 'output_size: List[int]'] => ['Tensor', 'Tensor'])\n",
      "torch.add - function (None => None)\n",
      "torch.addbmm - function (['self: Tensor', 'batch1: Tensor', 'batch2: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch.addcdiv - function (['self: Tensor', 'tensor1: Tensor', 'tensor2: Tensor', 'value: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch.addcmul - function (['self: Tensor', 'tensor1: Tensor', 'tensor2: Tensor', 'value: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch.addmm - function (['self: Tensor', 'mat1: Tensor', 'mat2: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch.addmv - function (['self: Tensor', 'mat: Tensor', 'vec: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch.addmv_ - function (['self: Tensor', 'mat: Tensor', 'vec: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch.addr - function (['self: Tensor', 'vec1: Tensor', 'vec2: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch.affine_grid_generator - function (['theta: Tensor', 'size: List[int]', 'align_corners: bool'] => ['Tensor'])\n",
      "torch.all - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.allclose - function (['self: Tensor', 'other: Tensor', 'rtol: float = 1e-05', 'atol: float = 1e-08', 'equal_nan: bool = False'] => ['bool'])\n",
      "torch.alpha_dropout - function (['input: Tensor', 'p: float', 'train: bool'] => ['Tensor'])\n",
      "torch.alpha_dropout_ - function (['self: Tensor', 'p: float', 'train: bool'] => ['Tensor'])\n",
      "torch.angle - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.any - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.arange - function (['end: Union[int, float, complex, Tensor]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.argmax - function (['self: Tensor', 'dim: Optional[int] = None', 'keepdim: bool = False'] => ['Tensor'])\n",
      "torch.argmin - function (['self: Tensor', 'dim: Optional[int] = None', 'keepdim: bool = False'] => ['Tensor'])\n",
      "torch.argsort - function (['self: Tensor', 'dim: int = -1', 'descending: bool = False'] => ['Tensor'])\n",
      "torch.as_strided - function (['self: Tensor', 'size: List[int]', 'stride: List[int]', 'storage_offset: Optional[int] = None'] => ['Tensor'])\n",
      "torch.as_strided_ - function (['self: Tensor', 'size: List[int]', 'stride: List[int]', 'storage_offset: Optional[int] = None'] => ['Tensor'])\n",
      "torch.as_tensor - function (None => None)\n",
      "torch.asin - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.asin_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.asinh - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.asinh_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.atan - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.atan2 - function (['self: Tensor', 'other: Tensor'] => ['Tensor'])\n",
      "torch.atan_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.atanh - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.atanh_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.autocast_decrement_nesting - function (None => None)\n",
      "torch.autocast_increment_nesting - function (None => None)\n",
      "torch.avg_pool1d - function (['self: Tensor', 'kernel_size: List[int]', 'stride: List[int] = []', 'padding: List[int] = 0', 'ceil_mode: bool = False', 'count_include_pad: bool = True'] => ['Tensor'])\n",
      "torch.baddbmm - function (['self: Tensor', 'batch1: Tensor', 'batch2: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch.bartlett_window - function (['window_length: int', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.batch_norm - function (['input: Tensor', 'weight: Optional[Tensor]', 'bias: Optional[Tensor]', 'running_mean: Optional[Tensor]', 'running_var: Optional[Tensor]', 'training: bool', 'momentum: float', 'eps: float', 'cudnn_enabled: bool'] => ['Tensor'])\n",
      "torch.batch_norm_backward_elemt - function (['grad_out: Tensor', 'input: Tensor', 'mean: Tensor', 'invstd: Tensor', 'weight: Optional[Tensor]', 'mean_dy: Tensor', 'mean_dy_xmu: Tensor'] => ['Tensor'])\n",
      "torch.batch_norm_backward_reduce - function (['grad_out: Tensor', 'input: Tensor', 'mean: Tensor', 'invstd: Tensor', 'weight: Optional[Tensor]', 'input_g: bool', 'weight_g: bool', 'bias_g: bool'] => ['Tensor', 'Tensor', 'Tensor', 'Tensor'])\n",
      "torch.batch_norm_elemt - function (['input: Tensor', 'weight: Optional[Tensor]', 'bias: Optional[Tensor]', 'mean: Tensor', 'invstd: Tensor', 'eps: float'] => ['Tensor'])\n",
      "torch.batch_norm_gather_stats - function (['input: Tensor', 'mean: Tensor', 'invstd: Tensor', 'running_mean: Optional[Tensor]', 'running_var: Optional[Tensor]', 'momentum: float', 'eps: float', 'count: int'] => ['Tensor', 'Tensor'])\n",
      "torch.batch_norm_gather_stats_with_counts - function (['input: Tensor', 'mean: Tensor', 'invstd: Tensor', 'running_mean: Optional[Tensor]', 'running_var: Optional[Tensor]', 'momentum: float', 'eps: float', 'counts: Tensor'] => ['Tensor', 'Tensor'])\n",
      "torch.batch_norm_stats - function (['input: Tensor', 'eps: float'] => ['Tensor', 'Tensor'])\n",
      "torch.batch_norm_update_stats - function (['input: Tensor', 'running_mean: Optional[Tensor]', 'running_var: Optional[Tensor]', 'momentum: float'] => ['Tensor', 'Tensor'])\n",
      "torch.bernoulli - function (['self: Tensor', 'generator: Optional[Generator] = None'] => ['Tensor'])\n",
      "torch.bilinear - function (['input1: Tensor', 'input2: Tensor', 'weight: Tensor', 'bias: Optional[Tensor]'] => ['Tensor'])\n",
      "torch.binary_cross_entropy_with_logits - function (['self: Tensor', 'target: Tensor', 'weight: Optional[Tensor] = None', 'pos_weight: Optional[Tensor] = None', 'reduction: int = Mean'] => ['Tensor'])\n",
      "torch.bincount - function (['self: Tensor', 'weights: Optional[Tensor] = None', 'minlength: int = 0'] => ['Tensor'])\n",
      "torch.binomial - function (['count: Tensor', 'prob: Tensor', 'generator: Optional[Generator] = None'] => ['Tensor'])\n",
      "torch.bitwise_and - function (None => None)\n",
      "torch.bitwise_not - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.bitwise_or - function (None => None)\n",
      "torch.bitwise_xor - function (None => None)\n",
      "torch.blackman_window - function (['window_length: int', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.bmm - function (['self: Tensor', 'mat2: Tensor'] => ['Tensor'])\n",
      "torch.bucketize - function (None => None)\n",
      "torch.can_cast - function (['from: torch.dtype', 'to: torch.dtype'] => ['bool'])\n",
      "torch.cat - function (['tensors: List[Tensor]', 'dim: int = 0'] => ['Tensor'])\n",
      "torch.ceil - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.ceil_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.celu - function (['self: Tensor', 'alpha: Union[int, float, complex, Tensor] = 1.0'] => ['Tensor'])\n",
      "torch.celu_ - function (['self: Tensor', 'alpha: Union[int, float, complex, Tensor] = 1.0'] => ['Tensor'])\n",
      "torch.channel_shuffle - function (['self: Tensor', 'groups: int'] => ['Tensor'])\n",
      "torch.cholesky - function (['self: Tensor', 'upper: bool = False'] => ['Tensor'])\n",
      "torch.cholesky_inverse - function (['self: Tensor', 'upper: bool = False'] => ['Tensor'])\n",
      "torch.cholesky_solve - function (['self: Tensor', 'input2: Tensor', 'upper: bool = False'] => ['Tensor'])\n",
      "torch.chunk - function (['self: Tensor', 'chunks: int', 'dim: int = 0'] => ['List[Tensor]'])\n",
      "torch.clamp - function (['self: Tensor', 'min: Optional[Union[int, float, complex, Tensor]] = None', 'max: Optional[Union[int, float, complex, Tensor]] = None'] => ['Tensor'])\n",
      "torch.clamp_ - function (['self: Tensor', 'min: Optional[Union[int, float, complex, Tensor]] = None', 'max: Optional[Union[int, float, complex, Tensor]] = None'] => ['Tensor'])\n",
      "torch.clamp_max - function (['self: Tensor', 'max: Union[int, float, complex, Tensor]'] => ['Tensor'])\n",
      "torch.clamp_max_ - function (['self: Tensor', 'max: Union[int, float, complex, Tensor]'] => ['Tensor'])\n",
      "torch.clamp_min - function (['self: Tensor', 'min: Union[int, float, complex, Tensor]'] => ['Tensor'])\n",
      "torch.clamp_min_ - function (['self: Tensor', 'min: Union[int, float, complex, Tensor]'] => ['Tensor'])\n",
      "torch.clear_autocast_cache - function (None => None)\n",
      "torch.clone - function (['self: Tensor', 'memory_format: Optional[MemoryFormat] = None'] => ['Tensor'])\n",
      "torch.combinations - function (['self: Tensor', 'r: int = 2', 'with_replacement: bool = False'] => ['Tensor'])\n",
      "torch.conj - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.constant_pad_nd - function (['self: Tensor', 'pad: List[int]', 'value: Union[int, float, complex, Tensor] = 0'] => ['Tensor'])\n",
      "torch.conv1d - function (['input: Tensor', 'weight: Tensor', 'bias: Optional[Tensor] = None', 'stride: List[int] = 1', 'padding: List[int] = 0', 'dilation: List[int] = 1', 'groups: int = 1'] => ['Tensor'])\n",
      "torch.conv2d - function (['input: Tensor', 'weight: Tensor', 'bias: Optional[Tensor] = None', 'stride: List[int] = 1', 'padding: List[int] = 0', 'dilation: List[int] = 1', 'groups: int = 1'] => ['Tensor'])\n",
      "torch.conv3d - function (['input: Tensor', 'weight: Tensor', 'bias: Optional[Tensor] = None', 'stride: List[int] = 1', 'padding: List[int] = 0', 'dilation: List[int] = 1', 'groups: int = 1'] => ['Tensor'])\n",
      "torch.conv_tbc - function (['self: Tensor', 'weight: Tensor', 'bias: Tensor', 'pad: int = 0'] => ['Tensor'])\n",
      "torch.conv_transpose1d - function (['input: Tensor', 'weight: Tensor', 'bias: Optional[Tensor] = None', 'stride: List[int] = 1', 'padding: List[int] = 0', 'output_padding: List[int] = 0', 'groups: int = 1', 'dilation: List[int] = 1'] => ['Tensor'])\n",
      "torch.conv_transpose2d - function (None => None)\n",
      "torch.conv_transpose3d - function (None => None)\n",
      "torch.convolution - function (['input: Tensor', 'weight: Tensor', 'bias: Optional[Tensor]', 'stride: List[int]', 'padding: List[int]', 'dilation: List[int]', 'transposed: bool', 'output_padding: List[int]', 'groups: int'] => ['Tensor'])\n",
      "torch.cos - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.cos_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.cosh - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.cosh_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.cosine_embedding_loss - function (['input1: Tensor', 'input2: Tensor', 'target: Tensor', 'margin: float = 0.0', 'reduction: int = Mean'] => ['Tensor'])\n",
      "torch.cosine_similarity - function (['x1: Tensor', 'x2: Tensor', 'dim: int = 1', 'eps: float = 1e-08'] => ['Tensor'])\n",
      "torch.cross - function (['self: Tensor', 'other: Tensor', 'dim: Optional[int] = None'] => ['Tensor'])\n",
      "torch.ctc_loss - function (None => None)\n",
      "torch.cudnn_affine_grid_generator - function (['theta: Tensor', 'N: int', 'C: int', 'H: int', 'W: int'] => ['Tensor'])\n",
      "torch.cudnn_batch_norm - function (['input: Tensor', 'weight: Tensor', 'bias: Optional[Tensor]', 'running_mean: Optional[Tensor]', 'running_var: Optional[Tensor]', 'training: bool', 'exponential_average_factor: float', 'epsilon: float'] => ['Tensor', 'Tensor', 'Tensor', 'Tensor'])\n",
      "torch.cudnn_convolution - function (['self: Tensor', 'weight: Tensor', 'padding: List[int]', 'stride: List[int]', 'dilation: List[int]', 'groups: int', 'benchmark: bool', 'deterministic: bool'] => ['Tensor'])\n",
      "torch.cudnn_convolution_transpose - function (['self: Tensor', 'weight: Tensor', 'padding: List[int]', 'output_padding: List[int]', 'stride: List[int]', 'dilation: List[int]', 'groups: int', 'benchmark: bool', 'deterministic: bool'] => ['Tensor'])\n",
      "torch.cudnn_grid_sampler - function (['self: Tensor', 'grid: Tensor'] => ['Tensor'])\n",
      "torch.cudnn_is_acceptable - function (['self: Tensor'] => ['bool'])\n",
      "torch.cummax - function (['self: Tensor', 'dim: int'] => ['Tensor', 'Tensor'])\n",
      "torch.cummin - function (['self: Tensor', 'dim: int'] => ['Tensor', 'Tensor'])\n",
      "torch.cumprod - function (['self: Tensor', 'dim: int', 'dtype: Optional[torch.dtype] = None'] => ['Tensor'])\n",
      "torch.cumsum - function (['self: Tensor', 'dim: int', 'dtype: Optional[torch.dtype] = None'] => ['Tensor'])\n",
      "torch.deg2rad - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.deg2rad_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.dequantize - function (None => None)\n",
      "torch.det - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.detach - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.detach_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.device - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.diag - function (['self: Tensor', 'diagonal: int = 0'] => ['Tensor'])\n",
      "torch.diag_embed - function (['self: Tensor', 'offset: int = 0', 'dim1: int = -2', 'dim2: int = -1'] => ['Tensor'])\n",
      "torch.diagflat - function (['self: Tensor', 'offset: int = 0'] => ['Tensor'])\n",
      "torch.diagonal - function (['self: Tensor', 'offset: int = 0', 'dim1: int = 0', 'dim2: int = 1'] => ['Tensor'])\n",
      "torch.digamma - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.dist - function (['self: Tensor', 'other: Tensor', 'p: Union[int, float, complex, Tensor] = 2'] => ['Tensor'])\n",
      "torch.div - function (None => None)\n",
      "torch.dot - function (['self: Tensor', 'tensor: Tensor'] => ['Tensor'])\n",
      "torch.dropout - function (['input: Tensor', 'p: float', 'train: bool'] => ['Tensor'])\n",
      "torch.dropout_ - function (['self: Tensor', 'p: float', 'train: bool'] => ['Tensor'])\n",
      "torch.dsmm - function (None => None)\n",
      "torch.dtype - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.eig - function (['self: Tensor', 'eigenvectors: bool = False'] => ['Tensor', 'Tensor'])\n",
      "torch.embedding - function (['weight: Tensor', 'indices: Tensor', 'padding_idx: int = -1', 'scale_grad_by_freq: bool = False', 'sparse: bool = False'] => ['Tensor'])\n",
      "torch.embedding_bag - function (['weight: Tensor', 'indices: Tensor', 'offsets: Tensor', 'scale_grad_by_freq: bool = False', 'mode: int = 0', 'sparse: bool = False', 'per_sample_weights: Optional[Tensor] = None', 'include_last_offset: bool = False'] => ['Tensor', 'Tensor', 'Tensor', 'Tensor'])\n",
      "torch.embedding_renorm_ - function (['self: Tensor', 'indices: Tensor', 'max_norm: float', 'norm_type: float'] => ['Tensor'])\n",
      "torch.empty - function (None => None)\n",
      "torch.empty_like - function (['self: Tensor', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None', 'memory_format: Optional[MemoryFormat] = None'] => ['Tensor'])\n",
      "torch.empty_meta - function (['size: List[int]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None', 'memory_format: Optional[MemoryFormat] = None'] => ['Tensor'])\n",
      "torch.empty_quantized - function (['size: List[int]', 'qtensor: Tensor'] => ['Tensor'])\n",
      "torch.empty_strided - function (['size: List[int]', 'stride: List[int]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.enable_grad - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.eq - function (None => None)\n",
      "torch.equal - function (['self: Tensor', 'other: Tensor'] => ['bool'])\n",
      "torch.erf - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.erf_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.erfc - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.erfc_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.erfinv - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.exp - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.exp_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.expm1 - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.expm1_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.eye - function (['n: int', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.fake_quantize_per_channel_affine - function (['self: Tensor', 'scale: Tensor', 'zero_point: Tensor', 'axis: int', 'quant_min: int', 'quant_max: int'] => ['Tensor'])\n",
      "torch.fake_quantize_per_tensor_affine - function (['self: Tensor', 'scale: float', 'zero_point: int', 'quant_min: int', 'quant_max: int'] => ['Tensor'])\n",
      "torch.fbgemm_linear_fp16_weight - function (['input: Tensor', 'packed_weight: Tensor', 'bias: Tensor'] => ['Tensor'])\n",
      "torch.fbgemm_linear_fp16_weight_fp32_activation - function (['input: Tensor', 'packed_weight: Tensor', 'bias: Tensor'] => ['Tensor'])\n",
      "torch.fbgemm_linear_int8_weight - function (['input: Tensor', 'weight: Tensor', 'packed: Tensor', 'col_offsets: Tensor', 'weight_scale: Union[int, float, complex, Tensor]', 'weight_zero_point: Union[int, float, complex, Tensor]', 'bias: Tensor'] => ['Tensor'])\n",
      "torch.fbgemm_linear_int8_weight_fp32_activation - function (['input: Tensor', 'weight: Tensor', 'packed: Tensor', 'col_offsets: Tensor', 'weight_scale: Union[int, float, complex, Tensor]', 'weight_zero_point: Union[int, float, complex, Tensor]', 'bias: Tensor'] => ['Tensor'])\n",
      "torch.fbgemm_linear_quantize_weight - function (['input: Tensor'] => ['Tensor', 'Tensor', 'float', 'int'])\n",
      "torch.fbgemm_pack_gemm_matrix_fp16 - function (['input: Tensor'] => ['Tensor'])\n",
      "torch.fbgemm_pack_quantized_matrix - function (['input: Tensor'] => ['Tensor'])\n",
      "torch.feature_alpha_dropout - function (['input: Tensor', 'p: float', 'train: bool'] => ['Tensor'])\n",
      "torch.feature_alpha_dropout_ - function (['self: Tensor', 'p: float', 'train: bool'] => ['Tensor'])\n",
      "torch.feature_dropout - function (['input: Tensor', 'p: float', 'train: bool'] => ['Tensor'])\n",
      "torch.feature_dropout_ - function (['self: Tensor', 'p: float', 'train: bool'] => ['Tensor'])\n",
      "torch.fft - function (['self: Tensor', 'signal_ndim: int', 'normalized: bool = False'] => ['Tensor'])\n",
      "torch.fill_ - function (None => None)\n",
      "torch.finfo - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.flatten - function (None => None)\n",
      "torch.flip - function (['self: Tensor', 'dims: List[int]'] => ['Tensor'])\n",
      "torch.fliplr - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.flipud - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.floor - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.floor_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.floor_divide - function (['self: Tensor', 'other: Tensor'] => ['Tensor'])\n",
      "torch.fmod - function (None => None)\n",
      "torch.fork - function (None => None)\n",
      "torch.frac - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.frac_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.frobenius_norm - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.from_file - function (['filename: str', 'shared: Optional[bool] = None', 'size: Optional[int] = 0', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.from_numpy - function (None => None)\n",
      "torch.full - function (['size: List[int]', 'fill_value: Union[int, float, complex, Tensor]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.full_like - function (['self: Tensor', 'fill_value: Union[int, float, complex, Tensor]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None', 'memory_format: Optional[MemoryFormat] = None'] => ['Tensor'])\n",
      "torch.gather - function (['self: Tensor', 'dim: int', 'index: Tensor', 'sparse_grad: bool = False'] => ['Tensor'])\n",
      "torch.ge - function (None => None)\n",
      "torch.geqrf - function (['self: Tensor'] => ['Tensor', 'Tensor'])\n",
      "torch.ger - function (['self: Tensor', 'vec2: Tensor'] => ['Tensor'])\n",
      "torch.get_default_dtype - function (None => None)\n",
      "torch.get_device - function (None => None)\n",
      "torch.get_num_interop_threads - function (None => None)\n",
      "torch.get_num_threads - function (None => None)\n",
      "torch.grid_sampler - function (['input: Tensor', 'grid: Tensor', 'interpolation_mode: int', 'padding_mode: int', 'align_corners: bool'] => ['Tensor'])\n",
      "torch.grid_sampler_2d - function (['input: Tensor', 'grid: Tensor', 'interpolation_mode: int', 'padding_mode: int', 'align_corners: bool'] => ['Tensor'])\n",
      "torch.grid_sampler_3d - function (['input: Tensor', 'grid: Tensor', 'interpolation_mode: int', 'padding_mode: int', 'align_corners: bool'] => ['Tensor'])\n",
      "torch.group_norm - function (['input: Tensor', 'num_groups: int', 'weight: Optional[Tensor] = None', 'bias: Optional[Tensor] = None', 'eps: float = 1e-05', 'cudnn_enabled: bool = True'] => ['Tensor'])\n",
      "torch.gru - function (None => None)\n",
      "torch.gru_cell - function (['input: Tensor', 'hx: Tensor', 'w_ih: Tensor', 'w_hh: Tensor', 'b_ih: Optional[Tensor] = None', 'b_hh: Optional[Tensor] = None'] => ['Tensor'])\n",
      "torch.gt - function (None => None)\n",
      "torch.hamming_window - function (['window_length: int', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.hann_window - function (['window_length: int', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.hardshrink - function (['self: Tensor', 'lambd: Union[int, float, complex, Tensor] = 0.5'] => ['Tensor'])\n",
      "torch.hinge_embedding_loss - function (['self: Tensor', 'target: Tensor', 'margin: float = 1.0', 'reduction: int = Mean'] => ['Tensor'])\n",
      "torch.histc - function (['self: Tensor', 'bins: int = 100', 'min: Union[int, float, complex, Tensor] = 0', 'max: Union[int, float, complex, Tensor] = 0'] => ['Tensor'])\n",
      "torch.hsmm - function (None => None)\n",
      "torch.hspmm - function (['mat1: Tensor', 'mat2: Tensor'] => ['Tensor'])\n",
      "torch.ifft - function (['self: Tensor', 'signal_ndim: int', 'normalized: bool = False'] => ['Tensor'])\n",
      "torch.iinfo - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.imag - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.import_ir_module - function (None => None)\n",
      "torch.import_ir_module_from_buffer - function (None => None)\n",
      "torch.index_add - function (['self: Tensor', 'dim: int', 'index: Tensor', 'source: Tensor'] => ['Tensor'])\n",
      "torch.index_copy - function (['self: Tensor', 'dim: int', 'index: Tensor', 'source: Tensor'] => ['Tensor'])\n",
      "torch.index_fill - function (None => None)\n",
      "torch.index_put - function (['self: Tensor', 'indices: Optional[List[Tensor]]', 'values: Tensor', 'accumulate: bool = False'] => ['Tensor'])\n",
      "torch.index_put_ - function (['self: Tensor', 'indices: Optional[List[Tensor]]', 'values: Tensor', 'accumulate: bool = False'] => ['Tensor'])\n",
      "torch.index_select - function (['self: Tensor', 'dim: int', 'index: Tensor'] => ['Tensor'])\n",
      "torch.init_num_threads - function (None => None)\n",
      "torch.instance_norm - function (['input: Tensor', 'weight: Optional[Tensor]', 'bias: Optional[Tensor]', 'running_mean: Optional[Tensor]', 'running_var: Optional[Tensor]', 'use_input_stats: bool', 'momentum: float', 'eps: float', 'cudnn_enabled: bool'] => ['Tensor'])\n",
      "torch.int_repr - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.inverse - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.irfft - function (['self: Tensor', 'signal_ndim: int', 'normalized: bool = False', 'onesided: bool = True', 'signal_sizes: List[int] = []'] => ['Tensor'])\n",
      "torch.is_anomaly_enabled - function (None => None)\n",
      "torch.is_autocast_enabled - function (None => None)\n",
      "torch.is_complex - function (['self: Tensor'] => ['bool'])\n",
      "torch.is_distributed - function (['self: Tensor'] => ['bool'])\n",
      "torch.is_floating_point - function (['self: Tensor'] => ['bool'])\n",
      "torch.is_grad_enabled - function (None => None)\n",
      "torch.is_nonzero - function (['self: Tensor'] => ['bool'])\n",
      "torch.is_same_size - function (['self: Tensor', 'other: Tensor'] => ['bool'])\n",
      "torch.is_signed - function (['self: Tensor'] => ['bool'])\n",
      "torch.is_vulkan_available - function (['None'] => ['bool'])\n",
      "torch.isclose - function (['self: Tensor', 'other: Tensor', 'rtol: float = 1e-05', 'atol: float = 1e-08', 'equal_nan: bool = False'] => ['Tensor'])\n",
      "torch.isfinite - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.isinf - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.isnan - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.kl_div - function (['self: Tensor', 'target: Tensor', 'reduction: int = Mean', 'log_target: bool = False'] => ['Tensor'])\n",
      "torch.kthvalue - function (['self: Tensor', 'k: int', 'dim: int = -1', 'keepdim: bool = False'] => ['Tensor', 'Tensor'])\n",
      "torch.layer_norm - function (['input: Tensor', 'normalized_shape: List[int]', 'weight: Optional[Tensor] = None', 'bias: Optional[Tensor] = None', 'eps: float = 1e-05', 'cudnn_enable: bool = True'] => ['Tensor'])\n",
      "torch.layout - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.le - function (None => None)\n",
      "torch.lerp - function (None => None)\n",
      "torch.lgamma - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.linspace - function (['start: Union[int, float, complex, Tensor]', 'end: Union[int, float, complex, Tensor]', 'steps: int = 100', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.log - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.log10 - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.log10_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.log1p - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.log1p_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.log2 - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.log2_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.log_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.log_softmax - function (None => None)\n",
      "torch.logaddexp - function (['self: Tensor', 'other: Tensor'] => ['Tensor'])\n",
      "torch.logaddexp2 - function (['self: Tensor', 'other: Tensor'] => ['Tensor'])\n",
      "torch.logcumsumexp - function (['self: Tensor', 'dim: int'] => ['Tensor'])\n",
      "torch.logdet - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.logical_and - function (['self: Tensor', 'other: Tensor'] => ['Tensor'])\n",
      "torch.logical_not - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.logical_or - function (['self: Tensor', 'other: Tensor'] => ['Tensor'])\n",
      "torch.logical_xor - function (['self: Tensor', 'other: Tensor'] => ['Tensor'])\n",
      "torch.logspace - function (['start: Union[int, float, complex, Tensor]', 'end: Union[int, float, complex, Tensor]', 'steps: int = 100', 'base: float = 10.0', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.logsumexp - function (['self: Tensor', 'dim: List[int]', 'keepdim: bool = False'] => ['Tensor'])\n",
      "torch.lstm - function (None => None)\n",
      "torch.lstm_cell - function (['input: Tensor', 'hx: List[Tensor]', 'w_ih: Tensor', 'w_hh: Tensor', 'b_ih: Optional[Tensor] = None', 'b_hh: Optional[Tensor] = None'] => ['Tensor', 'Tensor'])\n",
      "torch.lstsq - function (['self: Tensor', 'A: Tensor'] => ['Tensor', 'Tensor'])\n",
      "torch.lt - function (None => None)\n",
      "torch.lu_solve - function (['self: Tensor', 'LU_data: Tensor', 'LU_pivots: Tensor'] => ['Tensor'])\n",
      "torch.margin_ranking_loss - function (['input1: Tensor', 'input2: Tensor', 'target: Tensor', 'margin: float = 0.0', 'reduction: int = Mean'] => ['Tensor'])\n",
      "torch.masked_fill - function (None => None)\n",
      "torch.masked_scatter - function (['self: Tensor', 'mask: Tensor', 'source: Tensor'] => ['Tensor'])\n",
      "torch.masked_select - function (['self: Tensor', 'mask: Tensor'] => ['Tensor'])\n",
      "torch.matmul - function (['self: Tensor', 'other: Tensor'] => ['Tensor'])\n",
      "torch.matrix_power - function (['self: Tensor', 'n: int'] => ['Tensor'])\n",
      "torch.matrix_rank - function (['self: Tensor', 'symmetric: bool = False'] => ['Tensor'])\n",
      "torch.max - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.max_pool1d - function (['self: Tensor', 'kernel_size: List[int]', 'stride: List[int] = []', 'padding: List[int] = 0', 'dilation: List[int] = 1', 'ceil_mode: bool = False'] => ['Tensor'])\n",
      "torch.max_pool1d_with_indices - function (['self: Tensor', 'kernel_size: List[int]', 'stride: List[int] = []', 'padding: List[int] = 0', 'dilation: List[int] = 1', 'ceil_mode: bool = False'] => ['Tensor', 'Tensor'])\n",
      "torch.max_pool2d - function (['self: Tensor', 'kernel_size: List[int]', 'stride: List[int] = []', 'padding: List[int] = 0', 'dilation: List[int] = 1', 'ceil_mode: bool = False'] => ['Tensor'])\n",
      "torch.max_pool3d - function (['self: Tensor', 'kernel_size: List[int]', 'stride: List[int] = []', 'padding: List[int] = 0', 'dilation: List[int] = 1', 'ceil_mode: bool = False'] => ['Tensor'])\n",
      "torch.mean - function (['self: Tensor', 'dtype: Optional[torch.dtype] = None'] => ['Tensor'])\n",
      "torch.median - function (['self: Tensor'] => ['Tensor'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.memory_format - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.merge_type_from_type_comment - function (None => None)\n",
      "torch.min - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.miopen_batch_norm - function (['input: Tensor', 'weight: Tensor', 'bias: Optional[Tensor]', 'running_mean: Optional[Tensor]', 'running_var: Optional[Tensor]', 'training: bool', 'exponential_average_factor: float', 'epsilon: float'] => ['Tensor', 'Tensor', 'Tensor'])\n",
      "torch.miopen_convolution - function (['self: Tensor', 'weight: Tensor', 'bias: Optional[Tensor]', 'padding: List[int]', 'stride: List[int]', 'dilation: List[int]', 'groups: int', 'benchmark: bool', 'deterministic: bool'] => ['Tensor'])\n",
      "torch.miopen_convolution_transpose - function (['self: Tensor', 'weight: Tensor', 'bias: Optional[Tensor]', 'padding: List[int]', 'output_padding: List[int]', 'stride: List[int]', 'dilation: List[int]', 'groups: int', 'benchmark: bool', 'deterministic: bool'] => ['Tensor'])\n",
      "torch.miopen_depthwise_convolution - function (['self: Tensor', 'weight: Tensor', 'bias: Optional[Tensor]', 'padding: List[int]', 'stride: List[int]', 'dilation: List[int]', 'groups: int', 'benchmark: bool', 'deterministic: bool'] => ['Tensor'])\n",
      "torch.miopen_rnn - function (['input: Tensor', 'weight: List[Tensor]', 'weight_stride0: int', 'hx: Tensor', 'cx: Optional[Tensor]', 'mode: int', 'hidden_size: int', 'num_layers: int', 'batch_first: bool', 'dropout: float', 'train: bool', 'bidirectional: bool', 'batch_sizes: List[int]', 'dropout_state: Optional[Tensor]'] => ['Tensor', 'Tensor', 'Tensor', 'Tensor', 'Tensor'])\n",
      "torch.mkldnn_adaptive_avg_pool2d - function (['self: Tensor', 'output_size: List[int]'] => ['Tensor'])\n",
      "torch.mkldnn_convolution - function (['self: Tensor', 'weight: Tensor', 'bias: Optional[Tensor]', 'padding: List[int]', 'stride: List[int]', 'dilation: List[int]', 'groups: int'] => ['Tensor'])\n",
      "torch.mkldnn_convolution_backward_weights - function (['weight_size: List[int]', 'grad_output: Tensor', 'self: Tensor', 'padding: List[int]', 'stride: List[int]', 'dilation: List[int]', 'groups: int', 'bias_defined: bool'] => ['Tensor', 'Tensor'])\n",
      "torch.mkldnn_max_pool2d - function (['self: Tensor', 'kernel_size: List[int]', 'stride: List[int] = []', 'padding: List[int] = 0', 'dilation: List[int] = 1', 'ceil_mode: bool = False'] => ['Tensor'])\n",
      "torch.mm - function (['self: Tensor', 'mat2: Tensor'] => ['Tensor'])\n",
      "torch.mode - function (['self: Tensor', 'dim: int = -1', 'keepdim: bool = False'] => ['Tensor', 'Tensor'])\n",
      "torch.mul - function (None => None)\n",
      "torch.multinomial - function (['self: Tensor', 'num_samples: int', 'replacement: bool = False', 'generator: Optional[Generator] = None'] => ['Tensor'])\n",
      "torch.mv - function (['self: Tensor', 'vec: Tensor'] => ['Tensor'])\n",
      "torch.mvlgamma - function (['self: Tensor', 'p: int'] => ['Tensor'])\n",
      "torch.narrow - function (['self: Tensor', 'dim: int', 'start: int', 'length: int'] => ['Tensor'])\n",
      "torch.native_batch_norm - function (['input: Tensor', 'weight: Optional[Tensor]', 'bias: Optional[Tensor]', 'running_mean: Optional[Tensor]', 'running_var: Optional[Tensor]', 'training: bool', 'momentum: float', 'eps: float'] => ['Tensor', 'Tensor', 'Tensor'])\n",
      "torch.native_group_norm - function (['input: Tensor', 'weight: Optional[Tensor]', 'bias: Optional[Tensor]', 'N: int', 'C: int', 'HxW: int', 'group: int', 'eps: float'] => ['Tensor', 'Tensor', 'Tensor'])\n",
      "torch.native_layer_norm - function (['input: Tensor', 'weight: Optional[Tensor]', 'bias: Optional[Tensor]', 'M: int', 'N: int', 'eps: float'] => ['Tensor', 'Tensor', 'Tensor'])\n",
      "torch.native_norm - function (['self: Tensor', 'p: Union[int, float, complex, Tensor] = 2'] => ['Tensor'])\n",
      "torch.ne - function (None => None)\n",
      "torch.neg - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.neg_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.no_grad - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.nonzero - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.norm_except_dim - function (['v: Tensor', 'pow: int = 2', 'dim: int = 0'] => ['Tensor'])\n",
      "torch.normal - function (None => None)\n",
      "torch.nuclear_norm - function (['self: Tensor', 'keepdim: bool = False'] => ['Tensor'])\n",
      "torch.numel - function (None => None)\n",
      "torch.ones - function (['size: List[int]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.ones_like - function (['self: Tensor', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None', 'memory_format: Optional[MemoryFormat] = None'] => ['Tensor'])\n",
      "torch.orgqr - function (['self: Tensor', 'input2: Tensor'] => ['Tensor'])\n",
      "torch.ormqr - function (['self: Tensor', 'input2: Tensor', 'input3: Tensor', 'left: bool = True', 'transpose: bool = False'] => ['Tensor'])\n",
      "torch.pairwise_distance - function (['x1: Tensor', 'x2: Tensor', 'p: float = 2', 'eps: float = 1e-06', 'keepdim: bool = False'] => ['Tensor'])\n",
      "torch.parse_ir - function (None => None)\n",
      "torch.parse_schema - function (None => None)\n",
      "torch.parse_type_comment - function (None => None)\n",
      "torch.pdist - function (['self: Tensor', 'p: float = 2'] => ['Tensor'])\n",
      "torch.pinverse - function (['self: Tensor', 'rcond: float = 1e-15'] => ['Tensor'])\n",
      "torch.pixel_shuffle - function (['self: Tensor', 'upscale_factor: int'] => ['Tensor'])\n",
      "torch.poisson - function (['self: Tensor', 'generator: Optional[Generator] = None'] => ['Tensor'])\n",
      "torch.poisson_nll_loss - function (['input: Tensor', 'target: Tensor', 'log_input: bool', 'full: bool', 'eps: float', 'reduction: int'] => ['Tensor'])\n",
      "torch.polygamma - function (['n: int', 'self: Tensor'] => ['Tensor'])\n",
      "torch.pow - function (None => None)\n",
      "torch.prelu - function (['self: Tensor', 'weight: Tensor'] => ['Tensor'])\n",
      "torch.prod - function (['self: Tensor', 'dtype: Optional[torch.dtype] = None'] => ['Tensor'])\n",
      "torch.promote_types - function (['type1: torch.dtype', 'type2: torch.dtype'] => ['torch.dtype'])\n",
      "torch.q_per_channel_axis - function (['self: Tensor'] => ['int'])\n",
      "torch.q_per_channel_scales - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.q_per_channel_zero_points - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.q_scale - function (['self: Tensor'] => ['float'])\n",
      "torch.q_zero_point - function (['self: Tensor'] => ['int'])\n",
      "torch.qr - function (['self: Tensor', 'some: bool = True'] => ['Tensor', 'Tensor'])\n",
      "torch.qscheme - class (FullArgSpec(args=['self'], varargs='args', varkw='kwargs', defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.quantize_per_channel - function (['self: Tensor', 'scales: Tensor', 'zero_points: Tensor', 'axis: int', 'dtype: torch.dtype'] => ['Tensor'])\n",
      "torch.quantize_per_tensor - function (['self: Tensor', 'scale: float', 'zero_point: int', 'dtype: torch.dtype'] => ['Tensor'])\n",
      "torch.quantized_batch_norm - function (['input: Tensor', 'weight: Optional[Tensor]', 'bias: Optional[Tensor]', 'mean: Tensor', 'var: Tensor', 'eps: float', 'output_scale: float', 'output_zero_point: int'] => ['Tensor'])\n",
      "torch.quantized_gru - function (None => None)\n",
      "torch.quantized_gru_cell - function (['input: Tensor', 'hx: Tensor', 'w_ih: Tensor', 'w_hh: Tensor', 'b_ih: Tensor', 'b_hh: Tensor', 'packed_ih: Tensor', 'packed_hh: Tensor', 'col_offsets_ih: Tensor', 'col_offsets_hh: Tensor', 'scale_ih: Union[int, float, complex, Tensor]', 'scale_hh: Union[int, float, complex, Tensor]', 'zero_point_ih: Union[int, float, complex, Tensor]', 'zero_point_hh: Union[int, float, complex, Tensor]'] => ['Tensor'])\n",
      "torch.quantized_lstm - function (None => None)\n",
      "torch.quantized_lstm_cell - function (['input: Tensor', 'hx: List[Tensor]', 'w_ih: Tensor', 'w_hh: Tensor', 'b_ih: Tensor', 'b_hh: Tensor', 'packed_ih: Tensor', 'packed_hh: Tensor', 'col_offsets_ih: Tensor', 'col_offsets_hh: Tensor', 'scale_ih: Union[int, float, complex, Tensor]', 'scale_hh: Union[int, float, complex, Tensor]', 'zero_point_ih: Union[int, float, complex, Tensor]', 'zero_point_hh: Union[int, float, complex, Tensor]'] => ['Tensor', 'Tensor'])\n",
      "torch.quantized_max_pool2d - function (['self: Tensor', 'kernel_size: List[int]', 'stride: List[int] = []', 'padding: List[int] = 0', 'dilation: List[int] = 1', 'ceil_mode: bool = False'] => ['Tensor'])\n",
      "torch.quantized_rnn_relu_cell - function (['input: Tensor', 'hx: Tensor', 'w_ih: Tensor', 'w_hh: Tensor', 'b_ih: Tensor', 'b_hh: Tensor', 'packed_ih: Tensor', 'packed_hh: Tensor', 'col_offsets_ih: Tensor', 'col_offsets_hh: Tensor', 'scale_ih: Union[int, float, complex, Tensor]', 'scale_hh: Union[int, float, complex, Tensor]', 'zero_point_ih: Union[int, float, complex, Tensor]', 'zero_point_hh: Union[int, float, complex, Tensor]'] => ['Tensor'])\n",
      "torch.quantized_rnn_tanh_cell - function (['input: Tensor', 'hx: Tensor', 'w_ih: Tensor', 'w_hh: Tensor', 'b_ih: Tensor', 'b_hh: Tensor', 'packed_ih: Tensor', 'packed_hh: Tensor', 'col_offsets_ih: Tensor', 'col_offsets_hh: Tensor', 'scale_ih: Union[int, float, complex, Tensor]', 'scale_hh: Union[int, float, complex, Tensor]', 'zero_point_ih: Union[int, float, complex, Tensor]', 'zero_point_hh: Union[int, float, complex, Tensor]'] => ['Tensor'])\n",
      "torch.rad2deg - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.rad2deg_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.rand - function (['size: List[int]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.rand_like - function (['self: Tensor', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None', 'memory_format: Optional[MemoryFormat] = None'] => ['Tensor'])\n",
      "torch.randint - function (['high: int', 'size: List[int]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.randint_like - function (['self: Tensor', 'high: int', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None', 'memory_format: Optional[MemoryFormat] = None'] => ['Tensor'])\n",
      "torch.randn - function (['size: List[int]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.randn_like - function (['self: Tensor', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None', 'memory_format: Optional[MemoryFormat] = None'] => ['Tensor'])\n",
      "torch.randperm - function (['n: int', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.range - function (['start: Union[int, float, complex, Tensor]', 'end: Union[int, float, complex, Tensor]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.real - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.reciprocal - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.reciprocal_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.relu - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.relu_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.remainder - function (None => None)\n",
      "torch.renorm - function (['self: Tensor', 'p: Union[int, float, complex, Tensor]', 'dim: int', 'maxnorm: Union[int, float, complex, Tensor]'] => ['Tensor'])\n",
      "torch.repeat_interleave - function (None => None)\n",
      "torch.reshape - function (['self: Tensor', 'shape: List[int]'] => ['Tensor'])\n",
      "torch.resize_as_ - function (['self: Tensor', 'the_template: Tensor', 'memory_format: Optional[MemoryFormat] = None'] => ['Tensor'])\n",
      "torch.result_type - function (None => None)\n",
      "torch.rfft - function (['self: Tensor', 'signal_ndim: int', 'normalized: bool = False', 'onesided: bool = True'] => ['Tensor'])\n",
      "torch.rnn_relu - function (None => None)\n",
      "torch.rnn_relu_cell - function (['input: Tensor', 'hx: Tensor', 'w_ih: Tensor', 'w_hh: Tensor', 'b_ih: Optional[Tensor] = None', 'b_hh: Optional[Tensor] = None'] => ['Tensor'])\n",
      "torch.rnn_tanh - function (None => None)\n",
      "torch.rnn_tanh_cell - function (['input: Tensor', 'hx: Tensor', 'w_ih: Tensor', 'w_hh: Tensor', 'b_ih: Optional[Tensor] = None', 'b_hh: Optional[Tensor] = None'] => ['Tensor'])\n",
      "torch.roll - function (['self: Tensor', 'shifts: List[int]', 'dims: List[int] = []'] => ['Tensor'])\n",
      "torch.rot90 - function (['self: Tensor', 'k: int = 1', 'dims: List[int] = [0,1]'] => ['Tensor'])\n",
      "torch.round - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.round_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.rrelu - function (['self: Tensor', 'lower: Union[int, float, complex, Tensor] = 0.125', 'upper: Union[int, float, complex, Tensor] = 0.3333333333333333', 'training: bool = False', 'generator: Optional[Generator] = None'] => ['Tensor'])\n",
      "torch.rrelu_ - function (['self: Tensor', 'lower: Union[int, float, complex, Tensor] = 0.125', 'upper: Union[int, float, complex, Tensor] = 0.3333333333333333', 'training: bool = False', 'generator: Optional[Generator] = None'] => ['Tensor'])\n",
      "torch.rsqrt - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.rsqrt_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.rsub - function (None => None)\n",
      "torch.saddmm - function (None => None)\n",
      "torch.scalar_tensor - function (['s: Union[int, float, complex, Tensor]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.scatter - function (None => None)\n",
      "torch.scatter_add - function (['self: Tensor', 'dim: int', 'index: Tensor', 'src: Tensor'] => ['Tensor'])\n",
      "torch.searchsorted - function (None => None)\n",
      "torch.select - function (None => None)\n",
      "torch.selu - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.selu_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.set_anomaly_enabled - function (None => None)\n",
      "torch.set_autocast_enabled - function (None => None)\n",
      "torch.set_flush_denormal - function (None => None)\n",
      "torch.set_grad_enabled - class (FullArgSpec(args=['self', 'mode'], varargs=None, varkw=None, defaults=None, kwonlyargs=[], kwonlydefaults=None, annotations={}))\n",
      "torch.set_num_interop_threads - function (None => None)\n",
      "torch.set_num_threads - function (None => None)\n",
      "torch.sigmoid - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.sigmoid_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.sign - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.sin - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.sin_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.sinh - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.sinh_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.slogdet - function (['self: Tensor'] => ['Tensor', 'Tensor'])\n",
      "torch.smm - function (['self: Tensor', 'mat2: Tensor'] => ['Tensor'])\n",
      "torch.softmax - function (None => None)\n",
      "torch.solve - function (['self: Tensor', 'A: Tensor'] => ['Tensor', 'Tensor'])\n",
      "torch.sort - function (['self: Tensor', 'dim: int = -1', 'descending: bool = False'] => ['Tensor', 'Tensor'])\n",
      "torch.sparse_coo_tensor - function (None => None)\n",
      "torch.split_with_sizes - function (['self: Tensor', 'split_sizes: List[int]', 'dim: int = 0'] => ['List[Tensor]'])\n",
      "torch.spmm - function (None => None)\n",
      "torch.sqrt - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.sqrt_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.square - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.square_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.squeeze - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.sspaddmm - function (['self: Tensor', 'mat1: Tensor', 'mat2: Tensor', 'beta: Union[int, float, complex, Tensor] = 1', 'alpha: Union[int, float, complex, Tensor] = 1'] => ['Tensor'])\n",
      "torch.stack - function (['tensors: List[Tensor]', 'dim: int = 0'] => ['Tensor'])\n",
      "torch.std - function (['self: Tensor', 'unbiased: bool = True'] => ['Tensor'])\n",
      "torch.std_mean - function (['self: Tensor', 'unbiased: bool = True'] => ['Tensor', 'Tensor'])\n",
      "torch.sub - function (None => None)\n",
      "torch.sum - function (['self: Tensor', 'dtype: Optional[torch.dtype] = None'] => ['Tensor'])\n",
      "torch.svd - function (['self: Tensor', 'some: bool = True', 'compute_uv: bool = True'] => ['Tensor', 'Tensor', 'Tensor'])\n",
      "torch.symeig - function (['self: Tensor', 'eigenvectors: bool = False', 'upper: bool = True'] => ['Tensor', 'Tensor'])\n",
      "torch.t - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.take - function (['self: Tensor', 'index: Tensor'] => ['Tensor'])\n",
      "torch.tan - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.tan_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.tanh - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.tanh_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.tensor - function (None => None)\n",
      "torch.threshold - function (['self: Tensor', 'threshold: Union[int, float, complex, Tensor]', 'value: Union[int, float, complex, Tensor]'] => ['Tensor'])\n",
      "torch.threshold_ - function (['self: Tensor', 'threshold: Union[int, float, complex, Tensor]', 'value: Union[int, float, complex, Tensor]'] => ['Tensor'])\n",
      "torch.topk - function (['self: Tensor', 'k: int', 'dim: int = -1', 'largest: bool = True', 'sorted: bool = True'] => ['Tensor', 'Tensor'])\n",
      "torch.trace - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.transpose - function (None => None)\n",
      "torch.trapz - function (None => None)\n",
      "torch.triangular_solve - function (['self: Tensor', 'A: Tensor', 'upper: bool = True', 'transpose: bool = False', 'unitriangular: bool = False'] => ['Tensor', 'Tensor'])\n",
      "torch.tril - function (['self: Tensor', 'diagonal: int = 0'] => ['Tensor'])\n",
      "torch.tril_indices - function (['row: int', 'col: int', 'offset: int = 0', 'dtype: Optional[torch.dtype] = long', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.triplet_margin_loss - function (['anchor: Tensor', 'positive: Tensor', 'negative: Tensor', 'margin: float = 1.0', 'p: float = 2', 'eps: float = 1e-06', 'swap: bool = False', 'reduction: int = Mean'] => ['Tensor'])\n",
      "torch.triu - function (['self: Tensor', 'diagonal: int = 0'] => ['Tensor'])\n",
      "torch.triu_indices - function (['row: int', 'col: int', 'offset: int = 0', 'dtype: Optional[torch.dtype] = long', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.true_divide - function (None => None)\n",
      "torch.trunc - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.trunc_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.unbind - function (None => None)\n",
      "torch.unsqueeze - function (['self: Tensor', 'dim: int'] => ['Tensor'])\n",
      "torch.vander - function (['x: Tensor', 'N: Optional[int] = None', 'increasing: bool = False'] => ['Tensor'])\n",
      "torch.var - function (['self: Tensor', 'unbiased: bool = True'] => ['Tensor'])\n",
      "torch.var_mean - function (['self: Tensor', 'unbiased: bool = True'] => ['Tensor', 'Tensor'])\n",
      "torch.view_as_complex - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.view_as_real - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.wait - function (None => None)\n",
      "torch.where - function (['condition: Tensor'] => ['List[Tensor]'])\n",
      "torch.zero_ - function (['self: Tensor'] => ['Tensor'])\n",
      "torch.zeros - function (['size: List[int]', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None'] => ['Tensor'])\n",
      "torch.zeros_like - function (['self: Tensor', 'dtype: Optional[torch.dtype] = None', 'layout: Optional[Layout] = None', 'device: Optional[Device] = None', 'pin_memory: Optional[bool] = None', 'memory_format: Optional[MemoryFormat] = None'] => ['Tensor'])\n"
     ]
    }
   ],
   "source": [
    "for attr, meta in attrs.items():\n",
    "    if meta[\"type\"] == \"enum\":\n",
    "        meta_info = meta[\"members\"]\n",
    "    if meta[\"type\"] == \"class\":\n",
    "        meta_info = meta[\"init\"]\n",
    "    if meta[\"type\"] == \"function\":\n",
    "        sig = meta[\"signature\"]\n",
    "        return_type = meta[\"return_type\"]\n",
    "        meta_info = f\"{sig} => {return_type}\"\n",
    "    t = meta[\"type\"]\n",
    "    print(f\"torch.{attr} - {t} ({meta_info})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch._C.AggregationType'>\n",
      "AggregationType.SUM\n"
     ]
    }
   ],
   "source": [
    "issubclass(type(torch.AggregationType), Enum)\n",
    "print(torch.AggregationType)\n",
    "print(torch.AggregationType.SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "set_(source=None, storage_offset=0, size=None, stride=None) -> Tensor\n",
      "\n",
      "Sets the underlying storage, size, and strides. If :attr:`source` is a tensor,\n",
      ":attr:`self` tensor will share the same storage and have the same size and\n",
      "strides as :attr:`source`. Changes to elements in one tensor will be reflected\n",
      "in the other.\n",
      "\n",
      "If :attr:`source` is a :class:`~torch.Storage`, the method sets the underlying\n",
      "storage, offset, size, and stride.\n",
      "\n",
      "Args:\n",
      "    source (Tensor or Storage): the tensor or storage to use\n",
      "    storage_offset (int, optional): the offset in the storage\n",
      "    size (torch.Size, optional): the desired size. Defaults to the size of the source.\n",
      "    stride (tuple, optional): the desired stride. Defaults to C-contiguous strides.\n",
      "\n",
      "['self: Tensor']\n",
      "['Tensor']\n"
     ]
    }
   ],
   "source": [
    "print(torch.Tensor.set_.__doc__)\n",
    "print(get_native_signature(\"set_\"))\n",
    "print(get_native_return_type(\"set_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AggregationType.SUM <class 'torch._C.AggregationType'> \n",
      "members:  {'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatis(torch.SUM)\n",
    "torch.SUM.__members__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AggregationType.SUM\n"
     ]
    }
   ],
   "source": [
    "a = torch._C.AggregationType(0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG}\n"
     ]
    }
   ],
   "source": [
    "print(torch.AVG.__members__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch._C.AggregationType'> <class 'pybind11_builtins.pybind11_type'> AggregationType\n",
      "Hierarchy: [<class 'torch._C.AggregationType'>, <class 'pybind11_builtins.pybind11_object'>, <class 'object'>]\n",
      "members:  {'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['isclass', 'callable', 'iscenum', 'ispybind11']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatis(torch._C.AggregationType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SUM': AggregationType.SUM, 'AVG': AggregationType.AVG}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4643865800"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(torch.AVG.__members__)\n",
    "# print(torch.AVG.mro())\n",
    "\n",
    "#callable(torch.AVG)\n",
    "#inspect.getmro(torch.AVG)\n",
    "id(torch.AVG)\n",
    "id(torch.SUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ismodule: False\n",
      "isclass: False\n",
      "ismethod: False\n",
      "ismethoddescriptor: False\n",
      "isfunction: False\n",
      "isgeneratorfunction: False\n",
      "isgenerator: False\n",
      "isbuiltin: False\n",
      "isroutine: False\n",
      "isdatadescriptor: False\n",
      "isgetsetdescriptor: False\n",
      "ismemberdescriptor: False\n"
     ]
    }
   ],
   "source": [
    "thing = torch.AVG\n",
    "print(f\"ismodule: {inspect.ismodule(thing)}\")\n",
    "print(f\"isclass: {inspect.isclass(thing)}\")\n",
    "print(f\"ismethod: {inspect.ismethod(thing)}\")\n",
    "print(f\"ismethoddescriptor: {inspect.ismethoddescriptor(thing)}\")\n",
    "print(f\"isfunction: {inspect.isfunction(thing)}\")\n",
    "print(f\"isgeneratorfunction: {inspect.isgeneratorfunction(thing)}\")\n",
    "print(f\"isgenerator: {inspect.isgenerator(thing)}\")\n",
    "print(f\"isbuiltin: {inspect.isbuiltin(thing)}\")\n",
    "print(f\"isroutine: {inspect.isroutine(thing)}\")\n",
    "print(f\"isdatadescriptor: {inspect.isroutine(thing)}\")\n",
    "print(f\"isgetsetdescriptor: {inspect.isroutine(thing)}\")\n",
    "print(f\"ismemberdescriptor: {inspect.isroutine(thing)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_class_w_instance(klass, inst):\n",
    "    a = set(list(dir(klass)))\n",
    "    b = set(list(dir(inst)))\n",
    "\n",
    "    print(a.difference(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'volatile', '__cuda_array_interface__'}\n"
     ]
    }
   ],
   "source": [
    "compare_class_w_instance(torch.Tensor, torch.Tensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
