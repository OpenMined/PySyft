{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Federated Learning Training Plan: Create Plan\n",
    "\n",
    "Let's try to make protobuf-serializable Training Plan and Model that work after deserializing :)\n",
    "\n",
    "Current list of problems:\n",
    " * No support for autograd in Plan tracing (.backward() doesn't work inside the Plan).\n",
    " * `tensor.shape` value seem to be recorded as constant during Plan tracing, so we need to pass `batch_size`, can't take it from tensor itself.\n",
    " * Plan needs a list of all Model params in the argument list, it would be nicer if this list is figured out automatically so you just pass the Model (not sure it's solvable jit might not accept the model as ScriptModule input?)\n",
    " * Plan doesn't return input args from Plan, e.g. if they were updated with inplace operation\n",
    " * others? \n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "C:\\Users\\Vova\\AppData\\Local\\conda\\conda\\envs\\pysyft\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Vova\\AppData\\Local\\conda\\conda\\envs\\pysyft\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Vova\\AppData\\Local\\conda\\conda\\envs\\pysyft\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Vova\\AppData\\Local\\conda\\conda\\envs\\pysyft\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Vova\\AppData\\Local\\conda\\conda\\envs\\pysyft\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Vova\\AppData\\Local\\conda\\conda\\envs\\pysyft\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'C:\\Users\\Vova\\AppData\\Local\\conda\\conda\\envs\\pysyft\\lib\\site-packages\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.13.1.so'\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "Setting up Sandbox...\n",
      "Done!\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1827270e950>"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 1
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import syft as sy\n",
    "import torch as th\n",
    "from torch import jit\n",
    "from syft.serde import protobuf\n",
    "import os\n",
    "from syft.execution.state import State\n",
    "from syft.execution.placeholder import PlaceHolder\n",
    "from syft.execution.placeholder_id import PlaceholderId\n",
    "from syft.execution.translation.torchscript import PlanTranslatorTorchscript \n",
    "\n",
    "sy.hook(globals())\n",
    "# force protobuf serialization for tensors\n",
    "hook.local_worker.framework = None\n",
    "th.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This utility function will serialize any object to protobuf binary and save to a file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def serializeToBinPb(worker, obj, filename):\n",
    "    pb = protobuf.serde._bufferize(worker, obj)\n",
    "    bin = pb.SerializeToString()\n",
    "    print(\"Writing %s to %s/%s\" % (obj.__class__.__name__, os.getcwd(), filename))\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(bin)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Define the model\n",
    "\n",
    "This model will train on MNIST data, it's very simple yet can demonstrate learning process.\n",
    "There're 2 linear layers: \n",
    "\n",
    "* Linear 784x392\n",
    "* ReLU\n",
    "* Linear 392x10 \n",
    "\n",
    "Not using nn.Module or nn.Linear for now, just vanilla class and tensors.\n",
    "No autograd, gradients are hand-coded. \n",
    "\n",
    "As no loops supported inside Plan, \n",
    "we can't iterate over parameters so everything that works with params\n",
    "(get/set, step) is moved into the model to make Plan code more generic.  "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Net():\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.W1 = th.randn(392, 784) / th.sqrt(th.tensor(784.))\n",
    "        self.b1 = th.zeros(392)\n",
    "        self.W2 = th.randn(10, 392) / th.sqrt(th.tensor(392.))\n",
    "        self.b2 = th.zeros(10)\n",
    "        self.update_fn = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z1 = X @ self.W1.t() + self.b1\n",
    "        self.A1 = th.nn.functional.relu(self.Z1)\n",
    "        self.Z2 = self.A1 @ self.W2.t() + self.b2\n",
    "        return self.Z2\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.W1, self.b1, self.W2, self.b2\n",
    "\n",
    "    def set_params(self, *model_params):\n",
    "        self.W1, self.b1, self.W2, self.b2 = model_params\n",
    "\n",
    "    def grad(self, X, error):\n",
    "        Z1_grad = (error @ self.W2) * (self.Z1 > 0).float()\n",
    "        W1_grad = Z1_grad.t() @ X\n",
    "        b1_grad = Z1_grad.sum(0)\n",
    "        W2_grad = error.t() @ self.A1 \n",
    "        b2_grad = error.sum(0)\n",
    "        return W1_grad, b1_grad, W2_grad, b2_grad\n",
    "\n",
    "model = Net()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Define Training Plan\n",
    "### Loss function \n",
    "Batch size needs to be passed because otherwise `target.shape[0]` will be saved as `1` constant during Plan trace with dummy data.\n",
    "Grad is also returned here. "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def cross_entropy_with_logits(output, target, batch_size):\n",
    "    probs = th.nn.functional.softmax(output, dim=1)\n",
    "    loss = -(target * th.log(probs)).mean()\n",
    "    loss_grad = (probs - target) / (batch_size * target.shape[1])\n",
    "    return probs, loss, loss_grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimization function\n",
    " \n",
    "Just updates weights with grad*lr.\n",
    "\n",
    "Note: can't do inplace update and return this value from the Plan, which is potentially bad for memory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def naive_sgd(param, grad, **kwargs):\n",
    "    return param - grad * kwargs['lr']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Plan procedure"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model_params = model.get_params()\n",
    "\n",
    "# define plan input dimensions\n",
    "X_size = (-1, 784)\n",
    "y_size = (-1, 10)\n",
    "scalar_size = (1,)\n",
    "model_params_shapes = [p.shape for p in model_params]\n",
    "\n",
    "args_shape = [\n",
    "    X_size,  # X\n",
    "    y_size,  # y\n",
    "    scalar_size,  # batch_size\n",
    "    scalar_size,  # lr\n",
    "    *model_params_shapes  # *model_params\n",
    "]\n",
    "\n",
    "@sy.func2plan(args_shape=args_shape)\n",
    "def training_plan(X, y, batch_size, lr, *model_params):\n",
    "    # inject params into model\n",
    "    model.set_params(*model_params)\n",
    "\n",
    "    # forward pass\n",
    "    output = model.forward(X)\n",
    "    \n",
    "    # loss\n",
    "    probs, loss, loss_grad = cross_entropy_with_logits(output, y, batch_size)\n",
    "\n",
    "    # backprop\n",
    "    grads = model.grad(X, loss_grad)\n",
    "\n",
    "    # step\n",
    "    updated_params = [naive_sgd(param, grads[i], lr=lr) \n",
    "                      for i, param in enumerate(model_params)]\n",
    "    \n",
    "    # accuracy\n",
    "    pred = th.argmax(probs, dim=1)\n",
    "    target = th.argmax(y, dim=1)\n",
    "    acc = pred.eq(target).float().sum() / batch_size\n",
    "\n",
    "    return (\n",
    "        loss,\n",
    "        acc,\n",
    "        *updated_params\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look inside the Plan and print out the list of operations recorded."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "<Plan training_plan id:48324959290 owner:me built>\n",
      "def training_plan(_arg1, _arg2, _arg3, _arg4, _arg5, _arg6, _arg7, _arg8):\n",
      "    _9 = _arg5.t()\n",
      "    _10 = _arg1.__matmul__(_9)\n",
      "    _11 = _10.__add__(_arg6)\n",
      "    _12 = torch.nn.functional.relu(_11)\n",
      "    _13 = _arg7.t()\n",
      "    _14 = _12.__matmul__(_13)\n",
      "    _15 = _14.__add__(_arg8)\n",
      "    _16 = torch.nn.functional.softmax(_15, dim=1)\n",
      "    _17 = torch.log(_16)\n",
      "    _18 = _arg2.__mul__(_17)\n",
      "    _19 = _18.mean()\n",
      "    _out1 = _19.__neg__()\n",
      "    _21 = _16.__sub__(_arg2)\n",
      "    _22 = _arg3.__mul__(10)\n",
      "    _23 = _21.__truediv__(_22)\n",
      "    _24 = _23.__matmul__(_arg7)\n",
      "    _25 = _11.__gt__(0)\n",
      "    _26 = _25.float()\n",
      "    _27 = _24.__mul__(_26)\n",
      "    _28 = _27.t()\n",
      "    _29 = _28.__matmul__(_arg1)\n",
      "    _30 = _27.sum(0)\n",
      "    _31 = _23.t()\n",
      "    _32 = _31.__matmul__(_12)\n",
      "    _33 = _23.sum(0)\n",
      "    _34 = _29.__mul__(_arg4)\n",
      "    _out2 = _arg5.__sub__(_34)\n",
      "    _36 = _30.__mul__(_arg4)\n",
      "    _out3 = _arg6.__sub__(_36)\n",
      "    _38 = _32.__mul__(_arg4)\n",
      "    _out4 = _arg7.__sub__(_38)\n",
      "    _40 = _33.__mul__(_arg4)\n",
      "    _out5 = _arg8.__sub__(_40)\n",
      "    _42 = torch.argmax(_16, dim=1)\n",
      "    _43 = torch.argmax(_arg2, dim=1)\n",
      "    _44 = _42.eq(_43)\n",
      "    _45 = _44.float()\n",
      "    _46 = _45.sum()\n",
      "    _out6 = _46.__truediv__(_arg3)\n",
      "    return _out1, _out2, _out3, _out4, _out5, _out6\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(training_plan)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: JIT Trace Training Plan\n",
    "\n",
    "Note: Plan expects everything to be a tensor.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "def forward(self,\n",
      "    argument_1: Tensor,\n",
      "    argument_2: Tensor,\n",
      "    argument_3: Tensor,\n",
      "    argument_4: Tensor,\n",
      "    argument_5: Tensor,\n",
      "    argument_6: Tensor,\n",
      "    argument_7: Tensor,\n",
      "    argument_8: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
      "  _0 = torch.matmul(argument_1, torch.t(argument_5))\n",
      "  _1 = torch.add(_0, argument_6, alpha=1)\n",
      "  _2 = torch.relu(_1)\n",
      "  _3 = torch.add(torch.matmul(_2, torch.t(argument_7)), argument_8, alpha=1)\n",
      "  _4 = torch.softmax(_3, 1, None)\n",
      "  _5 = torch.mean(torch.mul(argument_2, torch.log(_4)), dtype=None)\n",
      "  _6 = torch.neg(_5)\n",
      "  _7 = torch.div(torch.sub(_4, argument_2, alpha=1), torch.mul(argument_3, CONSTANTS.c0))\n",
      "  _8 = torch.matmul(_7, argument_7)\n",
      "  _9 = torch.to(torch.gt(_1, 0), 6, False, False, None)\n",
      "  _10 = torch.mul(_8, _9)\n",
      "  _11 = torch.matmul(torch.t(_10), argument_1)\n",
      "  _12 = torch.sum(_10, [0], False, dtype=None)\n",
      "  _13 = torch.matmul(torch.t(_7), _2)\n",
      "  _14 = torch.sum(_7, [0], False, dtype=None)\n",
      "  _15 = torch.sub(argument_5, torch.mul(_11, argument_4), alpha=1)\n",
      "  _16 = torch.sub(argument_6, torch.mul(_12, argument_4), alpha=1)\n",
      "  _17 = torch.sub(argument_7, torch.mul(_13, argument_4), alpha=1)\n",
      "  _18 = torch.sub(argument_8, torch.mul(_14, argument_4), alpha=1)\n",
      "  _19 = torch.eq(torch.argmax(_4, 1, False), torch.argmax(argument_2, 1, False))\n",
      "  _20 = torch.sum(torch.to(_19, 6, False, False, None), dtype=None)\n",
      "  _21 = (_6, torch.div(_20, argument_3), _15, _16, _17, _18)\n",
      "  return _21\n",
      "\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "X_trace = th.randn(2, 784)\n",
    "y_trace = th.randn(2, 10)\n",
    "lr = th.tensor(0.001)\n",
    "batch_size = th.tensor(32)\n",
    "\n",
    "# Make torchscript plan\n",
    "training_plan_torchscript = training_plan.translate_with(PlanTranslatorTorchscript)\n",
    "\n",
    "# Let's see torchscript code\n",
    "print(training_plan_torchscript.torchscript.code)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Serialize!\n",
    "\n",
    "Now it's time to serialize model params and plans to protobuf and save them for further usage:\n",
    " * In \"Execute Plan\" notebook, we load and execute these plans & model, from Python.\n",
    " * In \"Host Plan\" notebook, we send these plans & model to PyGrid, so it can be executed from other worker (e.g. syft.js).\n",
    "\n",
    "**NOTE:**\n",
    " * We don't serialize full Model, only weights. How the Model is serialized is TBD.\n",
    "   State is suitable protobuf class to wrap list of Model params tensors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Writing Plan to e:\\ml/tp_ops.pb\n",
      "Writing Plan to e:\\ml/tp_ts.pb\n",
      "Writing State to e:\\ml/model_params.pb\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "serializeToBinPb(hook.local_worker, training_plan, \"tp_ops.pb\")\n",
    "serializeToBinPb(hook.local_worker, training_plan_torchscript, \"tp_ts.pb\")\n",
    "\n",
    "# wrap weights in State to serialize\n",
    "model_params_state = State(\n",
    "    owner=hook.local_worker,\n",
    "    state_placeholders=[PlaceHolder().instantiate(param) for param in model_params]\n",
    ")\n",
    "\n",
    "serializeToBinPb(hook.local_worker, model_params_state, \"model_params.pb\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}