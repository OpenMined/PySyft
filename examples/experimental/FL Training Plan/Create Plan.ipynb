{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Federated Learning Training Plan: Create Plan\n",
    "\n",
    "Let's try to make protobuf-serializable Training Plan and Model that work after deserializing :)\n",
    "\n",
    "Current list of problems:\n",
    " * `tensor.shape` is not traceable inside the Plan (issue [#3554](https://github.com/OpenMined/PySyft/issues/3554)).\n",
    " * Autograd/Plan tracing doesn't work with native torch's loss functions and optimizers.\n",
    " * others?\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0527 14:55:18.152521 23204 secure_random.py:26] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was 'D:\\Anaconda3\\envs\\syft\\lib\\site-packages\\tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n",
      "W0527 14:55:18.230508 23204 deprecation_wrapper.py:119] From D:\\Anaconda3\\envs\\syft\\lib\\site-packages\\tf_encrypted\\session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Sandbox...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x1d7b4f15e30>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import syft as sy\n",
    "import torch as th\n",
    "from torch import jit\n",
    "from torch import nn\n",
    "from syft.serde import protobuf\n",
    "import os\n",
    "from syft.execution.state import State\n",
    "from syft.execution.placeholder import PlaceHolder\n",
    "\n",
    "\n",
    "\n",
    "sy.make_hook(globals())\n",
    "# force protobuf serialization for tensors\n",
    "hook.local_worker.framework = None\n",
    "th.random.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This utility function will serialize any object to protobuf binary and save to a file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def serialize_to_bin_pb(worker, obj, filename):\n",
    "    pb = protobuf.serde._bufferize(worker, obj)\n",
    "    bin = pb.SerializeToString()\n",
    "    print(\"Writing %s to %s/%s\" % (obj.__class__.__name__, os.getcwd(), filename))\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(bin)\n",
    "\n",
    "\n",
    "def set_model_params(module, params_list, start_param_idx=0):\n",
    "    \"\"\" Set params list into model recursively\n",
    "    \"\"\"\n",
    "    param_idx = start_param_idx\n",
    "\n",
    "    for name, param in module._parameters.items():\n",
    "        module._parameters[name] = params_list[param_idx]\n",
    "        param_idx += 1\n",
    "\n",
    "    for name, child in module._modules.items():\n",
    "        if child is not None:\n",
    "            param_idx += set_model_params(child, params_list, param_idx)\n",
    "\n",
    "    return param_idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Define the model\n",
    "\n",
    "This model will train on MNIST data, it's very simple yet can demonstrate learning process.\n",
    "There're 2 linear layers: \n",
    "\n",
    "* Linear 784x392\n",
    "* ReLU\n",
    "* Linear 392x10 "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 392)\n",
    "        self.fc2 = nn.Linear(392, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = Net()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Define Training Plan\n",
    "### Loss function \n",
    "Batch size needs to be passed because otherwise `target.shape[0]` is not traced inside Plan yet (Issue [#3554](https://github.com/OpenMined/PySyft/issues/3554)).\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def softmax_cross_entropy_with_logits(logits, targets, batch_size):\n",
    "    \"\"\" Calculates softmax entropy\n",
    "        Args:\n",
    "            * logits: (NxC) outputs of dense layer\n",
    "            * targets: (NxC) one-hot encoded labels\n",
    "            * batch_size: value of N, temporarily required because Plan cannot trace .shape\n",
    "    \"\"\"\n",
    "    # numstable logsoftmax\n",
    "    norm_logits = logits - logits.max()\n",
    "    log_probs = norm_logits - norm_logits.exp().sum(dim=1, keepdim=True).log()\n",
    "    # NLL, reduction = mean\n",
    "    return -(targets * log_probs).sum() / batch_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimization function\n",
    " \n",
    "Just updates weights with grad*lr.\n",
    "\n",
    "Note: can't do inplace update because of Autograd/Plan tracing specifics."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def naive_sgd(param, **kwargs):\n",
    "    return param - kwargs['lr'] * param.grad"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Plan procedure\n",
    "\n",
    "We define a routine that will take one batch of training data, and model parameters,\n",
    "and will update model parameters to optimize them for given loss function using SGD."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "@sy.func2plan()\n",
    "def training_plan(X, y, batch_size, lr, model_params):\n",
    "    # inject params into model\n",
    "    set_model_params(model, model_params)\n",
    "\n",
    "    # forward pass\n",
    "    logits = model.forward(X)\n",
    "    \n",
    "    # loss\n",
    "    loss = softmax_cross_entropy_with_logits(logits, y, batch_size)\n",
    "\n",
    "    # backprop\n",
    "    loss.backward()\n",
    "\n",
    "    # step\n",
    "    updated_params = [\n",
    "        naive_sgd(param, lr=lr)\n",
    "        for param in model_params\n",
    "    ]\n",
    "    \n",
    "    # accuracy\n",
    "    pred = th.argmax(logits, dim=1)\n",
    "    target = th.argmax(y, dim=1)\n",
    "    acc = pred.eq(target).sum().float() / batch_size\n",
    "\n",
    "    return (\n",
    "        loss,\n",
    "        acc,\n",
    "        *updated_params\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's build this procedure into the Plan that we can serialize."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Dummy input parameters to make the trace\n",
    "model_params = list(model.parameters())\n",
    "X = th.randn(3, 28 * 28)\n",
    "y = nn.functional.one_hot(th.tensor([1, 2, 3]), 10)\n",
    "lr = th.tensor([0.01])\n",
    "batch_size = th.tensor([3.0])\n",
    "\n",
    "_ = training_plan.build(X, y, batch_size, lr, model_params, trace_autograd=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's look inside the Syft Plan and print out the list of operations recorded."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def training_plan(arg_1, arg_2, arg_3, arg_4, arg_5, arg_6, arg_7, arg_8):\n",
      "    2 = arg_1.dim()\n",
      "    var_0 = arg_5.t()\n",
      "    var_1 = arg_1.matmul(var_0)\n",
      "    var_2 = arg_6.add(var_1)\n",
      "    var_3 = var_2.relu()\n",
      "    2 = var_3.dim()\n",
      "    var_4 = arg_7.t()\n",
      "    var_5 = var_3.matmul(var_4)\n",
      "    var_6 = arg_8.add(var_5)\n",
      "    var_7 = var_6.max()\n",
      "    var_8 = var_6.sub(var_7)\n",
      "    var_9 = var_8.exp()\n",
      "    var_10 = var_9.sum(dim=1, keepdim=True)\n",
      "    var_11 = var_10.log()\n",
      "    var_12 = var_8.sub(var_11)\n",
      "    var_13 = arg_2.mul(var_12)\n",
      "    var_14 = var_13.sum()\n",
      "    var_15 = var_14.neg()\n",
      "    out_1 = var_15.div(arg_3)\n",
      "    var_16 = out_1.mul(0)\n",
      "    var_17 = var_16.add(1)\n",
      "    var_18 = var_17.div(arg_3)\n",
      "    var_19 = var_18.mul(-1)\n",
      "    var_20 = var_13.mul(0)\n",
      "    var_21 = var_20.add(1)\n",
      "    var_22 = var_21.mul(var_19)\n",
      "    var_23 = var_22.mul(var_12)\n",
      "    var_24 = var_22.mul(arg_2)\n",
      "    var_25 = var_23.copy()\n",
      "    var_26 = var_24.add(0)\n",
      "    var_27 = var_24.mul(-1)\n",
      "    var_28 = var_27.sum(dim=[1], keepdim=True)\n",
      "    var_29 = var_26.add(0)\n",
      "    var_30 = var_26.mul(-1)\n",
      "    var_31 = var_30.sum(dim=[1, 0])\n",
      "    var_32 = var_29.add(0)\n",
      "    var_33 = var_29.add(0)\n",
      "    var_34 = var_32.sum(dim=[0])\n",
      "    var_35 = var_34.copy()\n",
      "    var_36 = var_4.t()\n",
      "    var_37 = var_33.matmul(var_36)\n",
      "    var_38 = var_3.t()\n",
      "    var_39 = var_38.matmul(var_33)\n",
      "    var_40 = var_2.mul(0)\n",
      "    var_41 = var_2.__gt__(var_40)\n",
      "    var_42 = var_41.mul(var_37)\n",
      "    var_43 = var_42.add(0)\n",
      "    var_44 = var_42.add(0)\n",
      "    var_45 = var_43.sum(dim=[0])\n",
      "    var_46 = var_45.copy()\n",
      "    var_47 = var_0.t()\n",
      "    var_48 = var_44.matmul(var_47)\n",
      "    var_49 = arg_1.t()\n",
      "    var_50 = var_49.matmul(var_44)\n",
      "    var_51 = var_48.copy()\n",
      "    var_52 = var_50.t()\n",
      "    var_53 = var_52.copy()\n",
      "    var_54 = var_39.t()\n",
      "    var_55 = var_54.copy()\n",
      "    var_56 = var_31.copy()\n",
      "    var_57 = var_10.__rtruediv__(1)\n",
      "    var_58 = var_28.mul(var_57)\n",
      "    var_59 = var_9.mul(0)\n",
      "    var_60 = var_59.add(1)\n",
      "    var_61 = var_60.mul(var_58)\n",
      "    var_62 = var_8.exp()\n",
      "    var_63 = var_61.mul(var_62)\n",
      "    var_64 = var_63.add(0)\n",
      "    var_65 = var_63.mul(-1)\n",
      "    var_66 = var_65.sum(dim=[1, 0])\n",
      "    var_67 = var_64.add(0)\n",
      "    var_68 = var_64.add(0)\n",
      "    var_69 = var_67.sum(dim=[0])\n",
      "    var_70 = var_35.add_(var_69)\n",
      "    var_71 = var_4.t()\n",
      "    var_72 = var_68.matmul(var_71)\n",
      "    var_73 = var_3.t()\n",
      "    var_74 = var_73.matmul(var_68)\n",
      "    var_75 = var_2.mul(0)\n",
      "    var_76 = var_2.__gt__(var_75)\n",
      "    var_77 = var_76.mul(var_72)\n",
      "    var_78 = var_77.add(0)\n",
      "    var_79 = var_77.add(0)\n",
      "    var_80 = var_78.sum(dim=[0])\n",
      "    var_81 = var_46.add_(var_80)\n",
      "    var_82 = var_0.t()\n",
      "    var_83 = var_79.matmul(var_82)\n",
      "    var_84 = arg_1.t()\n",
      "    var_85 = var_84.matmul(var_79)\n",
      "    var_86 = var_51.add_(var_83)\n",
      "    var_87 = var_85.t()\n",
      "    var_88 = var_53.add_(var_87)\n",
      "    var_89 = var_74.t()\n",
      "    var_90 = var_55.add_(var_89)\n",
      "    var_91 = var_56.add_(var_66)\n",
      "    var_92 = arg_4.mul(var_53)\n",
      "    out_3 = arg_5.sub(var_92)\n",
      "    var_93 = arg_4.mul(var_46)\n",
      "    out_4 = arg_6.sub(var_93)\n",
      "    var_94 = arg_4.mul(var_55)\n",
      "    out_5 = arg_7.sub(var_94)\n",
      "    var_95 = arg_4.mul(var_35)\n",
      "    out_6 = arg_8.sub(var_95)\n",
      "    var_96 = torch.argmax(var_6, dim=1)\n",
      "    var_97 = torch.argmax(arg_2, dim=1)\n",
      "    var_98 = var_96.eq(var_97)\n",
      "    var_99 = var_98.sum()\n",
      "    var_100 = var_99.float()\n",
      "    out_2 = var_100.div(arg_3)\n",
      "    return out_1, out_2, out_3, out_4, out_5, out_6\n"
     ]
    }
   ],
   "source": [
    "print(training_plan.code)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plan should be automatically translated to torchscript, too.\n",
    "Let's examine torchscript code:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def <Plan training_plan id:27645662168 owner:me built>\n",
      "(argument_0: Tensor,\n",
      "    argument_1: Tensor,\n",
      "    argument_2: Tensor,\n",
      "    argument_3: Tensor,\n",
      "    argument_4: List[Tensor]) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:\n",
      "  _0, _1, _2, _3, = argument_4\n",
      "  _4 = torch.add(_1, torch.matmul(argument_0, torch.t(_0)), alpha=1)\n",
      "  _5 = torch.relu(_4)\n",
      "  _6 = torch.t(_2)\n",
      "  _7 = torch.add(_3, torch.matmul(_5, _6), alpha=1)\n",
      "  _8 = torch.sub(_7, torch.max(_7), alpha=1)\n",
      "  _9 = torch.exp(_8)\n",
      "  _10 = torch.sum(_9, [1], True, dtype=None)\n",
      "  _11 = torch.sub(_8, torch.log(_10), alpha=1)\n",
      "  _12 = torch.mul(argument_1, _11)\n",
      "  _13 = torch.div(torch.neg(torch.sum(_12, dtype=None)), argument_2)\n",
      "  _14 = torch.add(torch.mul(_13, CONSTANTS.c0), CONSTANTS.c1, alpha=1)\n",
      "  _15 = torch.mul(torch.div(_14, argument_2), CONSTANTS.c2)\n",
      "  _16 = torch.add(torch.mul(_12, CONSTANTS.c0), CONSTANTS.c1, alpha=1)\n",
      "  _17 = torch.mul(torch.mul(_16, _15), argument_1)\n",
      "  _18 = torch.add(_17, CONSTANTS.c0, alpha=1)\n",
      "  _19 = torch.sum(torch.mul(_17, CONSTANTS.c2), [1], True, dtype=None)\n",
      "  _20 = torch.add(_18, CONSTANTS.c0, alpha=1)\n",
      "  _21 = torch.add(_20, CONSTANTS.c0, alpha=1)\n",
      "  _22 = torch.add(_20, CONSTANTS.c0, alpha=1)\n",
      "  _23 = torch.sum(_21, [0], False, dtype=None)\n",
      "  _24 = torch.matmul(_22, torch.t(_6))\n",
      "  _25 = torch.matmul(torch.t(_5), _22)\n",
      "  _26 = torch.gt(_4, torch.mul(_4, CONSTANTS.c0))\n",
      "  _27 = torch.mul(_26, _24)\n",
      "  _28 = torch.add(_27, CONSTANTS.c0, alpha=1)\n",
      "  _29 = torch.add(_27, CONSTANTS.c0, alpha=1)\n",
      "  _30 = torch.sum(_28, [0], False, dtype=None)\n",
      "  _31 = torch.matmul(torch.t(argument_0), _29)\n",
      "  _32 = torch.t(_31)\n",
      "  _33 = torch.t(_25)\n",
      "  _34 = torch.mul(torch.reciprocal(_10), CONSTANTS.c1)\n",
      "  _35 = torch.mul(_19, _34)\n",
      "  _36 = torch.add(torch.mul(_9, CONSTANTS.c0), CONSTANTS.c1, alpha=1)\n",
      "  _37 = torch.mul(torch.mul(_36, _35), torch.exp(_8))\n",
      "  _38 = torch.add(_37, CONSTANTS.c0, alpha=1)\n",
      "  _39 = torch.add(_38, CONSTANTS.c0, alpha=1)\n",
      "  _40 = torch.add(_38, CONSTANTS.c0, alpha=1)\n",
      "  _41 = torch.sum(_39, [0], False, dtype=None)\n",
      "  _42 = torch.add_(_23, _41, alpha=1)\n",
      "  _43 = torch.matmul(_40, torch.t(_6))\n",
      "  _44 = torch.matmul(torch.t(_5), _40)\n",
      "  _45 = torch.gt(_4, torch.mul(_4, CONSTANTS.c0))\n",
      "  _46 = torch.mul(_45, _43)\n",
      "  _47 = torch.add(_46, CONSTANTS.c0, alpha=1)\n",
      "  _48 = torch.add(_46, CONSTANTS.c0, alpha=1)\n",
      "  _49 = torch.sum(_47, [0], False, dtype=None)\n",
      "  _50 = torch.add_(_30, _49, alpha=1)\n",
      "  _51 = torch.matmul(torch.t(argument_0), _48)\n",
      "  _52 = torch.add_(_32, torch.t(_51), alpha=1)\n",
      "  _53 = torch.add_(_33, torch.t(_44), alpha=1)\n",
      "  _54 = torch.sub(_0, torch.mul(argument_3, _52), alpha=1)\n",
      "  _55 = torch.sub(_1, torch.mul(argument_3, _50), alpha=1)\n",
      "  _56 = torch.sub(_2, torch.mul(argument_3, _53), alpha=1)\n",
      "  _57 = torch.sub(_3, torch.mul(argument_3, _42), alpha=1)\n",
      "  _58 = torch.eq(torch.argmax(_7, 1, False), torch.argmax(argument_1, 1, False))\n",
      "  _59 = torch.to(torch.sum(_58, dtype=None), 6, False, False, None)\n",
      "  _60 = (_13, torch.div(_59, argument_2), _54, _55, _56, _57)\n",
      "  return _60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(training_plan.torchscript.code)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Serialize!\n",
    "\n",
    "Now it's time to serialize model params and plans to protobuf and save them for further usage:\n",
    " * In \"Execute Plan\" notebook, we load and execute these plans & model, from Python.\n",
    " * In \"Host Plan\" notebook, we send these plans & model to PyGrid, so it can be executed from other worker (e.g. syft.js).\n",
    "\n",
    "**NOTE:**\n",
    " * We don't serialize full Model, only weights. How the Model is serialized is TBD.\n",
    "   State is suitable protobuf class to wrap list of Model params tensors."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Plan to D:\\projects\\openmined\\PySyft\\examples\\experimental\\FL Training Plan/tp_full.pb\n",
      "Writing State to D:\\projects\\openmined\\PySyft\\examples\\experimental\\FL Training Plan/model_params.pb\n"
     ]
    }
   ],
   "source": [
    "serialize_to_bin_pb(hook.local_worker, training_plan, \"tp_full.pb\")\n",
    "\n",
    "# wrap weights in State to serialize\n",
    "model_params_state = State(\n",
    "    state_placeholders=[\n",
    "        PlaceHolder().instantiate(param)\n",
    "        for param in model_params\n",
    "    ]\n",
    ")\n",
    "\n",
    "serialize_to_bin_pb(hook.local_worker, model_params_state, \"model_params.pb\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}