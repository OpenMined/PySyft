{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "grave-probability",
   "metadata": {},
   "source": [
    "# Attacking and Defending a Split Neural Network\n",
    "\n",
    "- Introducing the problem\n",
    "- Model inversion\n",
    "  - White box\n",
    "  - Black box\n",
    "- Defences\n",
    "  - NoPeekNN\n",
    "  - Noise defence\n",
    "  - Other defences\n",
    "- Lesson objective\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In Split neural networks (SplitNNs),\n",
    "there are at minimum two parties:\n",
    "one data owner\n",
    "and one computational server.\n",
    "Previously,\n",
    "we've assumed that the computational server is a trustworthy individual.\n",
    "But what if they're not?\n",
    "What if they will try to learn information about you\n",
    "in addition to completing training or inference,\n",
    "as you would expect them to;\n",
    "the technical term for this is honest-but-curious.\n",
    "\n",
    "What are the risks?\n",
    "While the data owner doesn't send raw data\n",
    "to the computational server,\n",
    "the data they do send still contains related information -\n",
    "it has to, otherwise the model wouldn't be able to learn!\n",
    "In this lesson,\n",
    "we'll explore how to perform an attack know as **model inversion**,\n",
    "in which the computational server will try to turn the intermediate data\n",
    "back into raw data.\n",
    "\n",
    "## Model Inversion\n",
    "\n",
    "### White box\n",
    "\n",
    "Model inversion in machine learning\n",
    "was introduced by [Fredrikson et al. (2015)](https://www.cs.cmu.edu/~mfredrik/papers/fjr2015ccs.pdf),\n",
    "who showed that you can exploit a model's overfitting\n",
    "to recreate training data examples.\n",
    "This is known as a \"white box\" attack\n",
    "because it we utilise model weights.\n",
    "In this lesson,\n",
    "we'll be trying a \"black box\" attack,\n",
    "in which we  only need query-access to the model\n",
    "(i.e. we can run inference on it).\n",
    "\n",
    "### Black box\n",
    "\n",
    "The attack we're trying was introduced\n",
    "by [He, Zhang and Lee (2019)](https://personal.ntu.edu.sg/tianwei.zhang/paper/acsac19.pdf).\n",
    "The process is:\n",
    "1. the attacker collects a dataset of (intermediate data, raw data)\n",
    "1. the attacker trains an attack model to map intermediate data -> raw data\n",
    "1. the attacker run new intermediate data on the attack model to recreate the raw data of a victim\n",
    "\n",
    "Who is the attacker and how do they get hold of the data?\n",
    "There are a few possibilities:\n",
    "- **Computational server colluding with a data owner**.\n",
    "The colluding data owner runs data through the model\n",
    "and sends the (raw, intermediate) data pairs to the computational server.\n",
    "The computational server already has access to victims' intermediate data,\n",
    "so can run the attack model with impunity.\n",
    "- **The computational server acts alone**.\n",
    "For this,\n",
    "the computational server must source their own attack training data.\n",
    "If they don't know exactly what the data is\n",
    "(e.g. they are not the controlling party in the process\n",
    "and have simply been paid to do computations),\n",
    "they could guess and make their own dataset.\n",
    "It turns out that you don't need to get the exact distribution\n",
    "of the victim dataset to train a useful attack model.\n",
    "- **A data owner works alone**.\n",
    "Any data own can train an attack model on their own data.\n",
    "The data owner must somehow intercept intermediate data from victims\n",
    "in order to run the attack.\n",
    "\n",
    "## Defences\n",
    "\n",
    "### NoPeek\n",
    "\n",
    "[Vepakomma et. al (2020)](https://dam-prod.media.mit.edu/x/2020/10/29/NoPeek_SplitLearning.pdf)\n",
    "introduced NoPeek as a possible defence.\n",
    "NoPeek is a SplitNN is trained with a two-part loss function:\n",
    "a weighted combination of the objective loss\n",
    "(e.g. cross-entropy for classification)\n",
    "and a _Distance correlation_ (DC) loss.\n",
    "The DC loss measures how similar the input data\n",
    "is to the intermediate data.\n",
    "As the loss function is minimised (the model is trained),\n",
    "the intermediate data looks less and less like the input data,\n",
    "while retaining information which is useful for performing the task.\n",
    "The less correlated the input and intermediate data are,\n",
    "the harder it is for an attack model to learn\n",
    "how to map one back into the other.\n",
    "\n",
    "### Noise defence\n",
    "\n",
    "Titcombe et al. (2021) (under review)\n",
    "introduced a simple noise defence,\n",
    "where Laplacian noise is added to the intermediate data\n",
    "_before_ being transmitted to the computational server.\n",
    "The idea of this is similar to NoPeek: make the intermediate data dissimlar to the input data.\n",
    "Crucially,\n",
    "the noise is not introduced during training (although it can be, if needed).\n",
    "Instead, the noise is only introduced on inference-time data.\n",
    "This means the noise defence can be applied to **any** model\n",
    "and it can be applied unilaterally by the data owner,\n",
    "which is useful if they don't fully trust the computational server.\n",
    "\n",
    "### Other methods\n",
    "\n",
    "[Mireshghallah et al. (2019)](https://arxiv.org/pdf/1905.11814v2.pdf)\n",
    "introduced _Shredder_,\n",
    "a defensive method similar to the noise defence.\n",
    "However, the noise learned by Shredder is adaptive\n",
    "to the input data and must be learned.\n",
    "While this can produce better performance-privacy trade-offs,\n",
    "it requires Shredder to be applied during training-time.\n",
    "\n",
    "## Lesson Objective\n",
    "\n",
    "By the end of this lesson,\n",
    "you will:\n",
    "* Be able to run black box model inversion on a SplitNN (part 2)\n",
    "* Be able to implement NoPeek defence (part 3)\n",
    "* Be able to implement the noise defence (part 4)\n",
    "* Be able to experiment with hyperparameters to improve a defensive measure (parts 3 and 4)\n",
    "\n",
    "Parts 1 and 2 must be completed to move on to the later parts.\n",
    "Parts 3 and 4 are independent of one another,\n",
    "so can be completed in either order.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.transforms import Normalize\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-index",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Developing a SplitNN\n",
    "\n",
    "First,\n",
    "we need to make something to attack.\n",
    "We will train a simple model on MNIST.\n",
    "Importantly,\n",
    "we won't use all the available training data,\n",
    "so there will be some data left over for us to use in the attack.\n",
    "We will develop a single PyTorch model here\n",
    "just to make things simpler,\n",
    "but you can think of it as two separate models\n",
    "with data communicated in between.\n",
    "\n",
    "In this first step,\n",
    "all of the code you need has been\n",
    "provided.\n",
    "Simply run all the cells until\n",
    "you get to part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # The data owner's model\n",
    "        self.part1 = nn.Sequential(\n",
    "            nn.Linear(784, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 500),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # The computational server's model\n",
    "        self.part2 = nn.Sequential(\n",
    "            nn.Linear(500, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def data_owner(self, data):\n",
    "        x = self.part1(data)\n",
    "        return x\n",
    "\n",
    "    def computational_server(self, intermediate):\n",
    "        x = self.part2(intermediate)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.data_owner(x)\n",
    "        x = self.computational_server(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([ToTensor(), Normalize((0.5), (0.5))])\n",
    "\n",
    "train_data = Subset(MNIST(\".\", download=True, train=True, transform=transform), list(range(30_000)))  # train on first 30'000 images\n",
    "train_loader = DataLoader(train_data, batch_size=32)\n",
    "\n",
    "val_data = Subset(MNIST(\".\", download=True, train=False, transform=transform), list(range(5_000)))\n",
    "val_loader = DataLoader(val_data, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier()\n",
    "opt = torch.optim.Adam(classifier.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, targets in train_loader:\n",
    "        opt.zero_grad()\n",
    "\n",
    "        out = classifier(data)\n",
    "        loss = loss_fn(out, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        correct += out.max(1)[1].eq(targets).sum()\n",
    "        total += out.size(0)\n",
    "\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    for data_val, targets_val in val_loader:\n",
    "        with torch.no_grad():\n",
    "            out_val = classifier(data_val)\n",
    "\n",
    "            val_correct += out_val.max(1)[1].eq(targets_val).sum()\n",
    "            val_total += out_val.size(0)\n",
    "\n",
    "    print(f\"Train acc: {100*correct/total:.3f}\")\n",
    "    print(f\"Val acc: {100*val_correct/val_total:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interested-momentum",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Attacking the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-poetry",
   "metadata": {},
   "source": [
    "### Step 1 - Get the data\n",
    "\n",
    "Create an \"attack dataset\" -\n",
    "a dataset on which to train an attack model.\n",
    "We'll do this by collecting a dataset of (input, intermediate)\n",
    "data from the trained model.\n",
    "We'll give you the dataset class,\n",
    "you have to implement the code to collect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-finland",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.intermediate_data = torch.empty((0, 500))  # Inputs\n",
    "        self.actual_data = torch.empty((0, 784))  # Targets\n",
    "\n",
    "    def push(self, intermediate, actual):\n",
    "        \"\"\"\n",
    "        Add data to the dataset\n",
    "\n",
    "        Args:\n",
    "            intermediate (torch.tensor): A tensor of data trasmitted between the parts of a SplitNN. the data to attack\n",
    "            actual (torch.tensor): A tensor of input data to the SplitNN under attack\n",
    "        \"\"\"\n",
    "        assert intermediate.size(0) == actual.size(0)\n",
    "\n",
    "        self.intermediate_data = torch.cat([self.intermediate_data, intermediate])\n",
    "        self.actual_data = torch.cat([self.actual_data, actual])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.intermediate_data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.intermediate_data[idx], self.actual_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train = AttackDataset()\n",
    "\n",
    "# Input data to use in attack dataset\n",
    "temp_attack_train = Subset(MNIST(\".\", train=True, transform=transform), list(range(30_000, 40_000)))  # use 10'000 unseen images to train the attacker\n",
    "temp_attack_train_loader = DataLoader(temp_attack_train, batch_size=128)\n",
    "\n",
    "raise NotImplementedError(\"Complete the code to make `attack_train` a dataset of (intermediate, input) tensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Follow the same process to get an attacker validation dataset using images 5'000-10'000 in the MNIST 'test' dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-palace",
   "metadata": {},
   "source": [
    "### Step 2 - Get the model\n",
    "\n",
    "We need to define an attack model to train.\n",
    "The attack model ideally would mirror the architecture\n",
    "of the model under attack (the target model).\n",
    "In this lesson we know what the target model looks like,\n",
    "and in practice the attackers would too,\n",
    "as the colluding data owner has a copy of the target model.\n",
    "Give the attack model a similar number and size of layers\n",
    "to the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-influence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        raise NotImplementedError(\"You need to finish __init__\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"You need to finish forward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cutting-amino",
   "metadata": {},
   "source": [
    "### Step 3 - Train the attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-jonathan",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_train_loader = DataLoader(attack_train, batch_size=128)\n",
    "\n",
    "raise NotImplementedError(\"Define and run a training loop for the attack model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-hypothetical",
   "metadata": {},
   "source": [
    "### Step 4 - Attack new data\n",
    "\n",
    "Earlier,\n",
    "we only used the firs 5'000 \"test\" MNIST images\n",
    "to validate the classifier.\n",
    "We will use the remaining 5'000\n",
    "to use to validate our attacker.\n",
    "In practice,\n",
    "this would be new data coming into the computational server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-sugar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(tensors, rows):\n",
    "    \"\"\"\n",
    "    Plot images in a grid\n",
    "\n",
    "    Args:\n",
    "        tensors (list of tensors): list of tensors to plot\n",
    "        rows (int): number of rows to plot\n",
    "    \"\"\"\n",
    "    Unnormalise = Normalize([-0.1], [2.0])\n",
    "\n",
    "    images = []\n",
    "    for tensor in tensors:\n",
    "        tensor = Unnormalise(tensor)\n",
    "\n",
    "        # Clip image values so we can plot\n",
    "        tensor[tensor < 0] = 0\n",
    "        tensor[tensor > 1] = 1\n",
    "\n",
    "        tensor = tensor.unsqueeze(0)  # add batch dim\n",
    "        images.append(tensor)\n",
    "\n",
    "    images = torch.cat(images)\n",
    "    grid_image = torchvision.utils.make_grid(images, nrow=rows).permute(1, 2, 0)\n",
    "\n",
    "    plt.imshow(grid_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Attack the model to recreate input data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-hydrogen",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: NoPeek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-difficulty",
   "metadata": {},
   "source": [
    "### Step 1 - Create the NoPeek loss\n",
    "\n",
    "NoPeekNN is a SplitNN which is optimised on task loss (cross entropy, for classification tasks) and _distance correlation loss_.\n",
    "Distance correlation is a measure of the correlation between two matrices (in PyTorch, that's two tensors).\n",
    "NoPeek loss is a weighted sum of the two terms,\n",
    "governed by a hyperparameter $\\alpha$:\n",
    "\n",
    "\n",
    "#### L = L<sub>ce</sub> + $\\alpha$ L<sub>dc</sub>\n",
    "\n",
    "so a higher alpha means more emphasis on minimizing distance correlation.\\\n",
    "**Bonus question**: what are the bounds (minimum and maximum values) of $\\alpha$?\n",
    "\n",
    "We'll provide the Distance correlation loss term.\n",
    "You must create the full NoPeek loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceCorrelationLoss(nn.modules.loss._Loss):\n",
    "    def forward(self, input_data, intermediate_data):\n",
    "        input_data = input_data.view(input_data.size(0), -1)\n",
    "        intermediate_data = intermediate_data.view(intermediate_data.size(0), -1)\n",
    "\n",
    "        A_input = self._A_matrix(input_data)\n",
    "        A_intermediate = self._A_matrix(intermediate_data)\n",
    "\n",
    "        # Get distance variances\n",
    "        input_dvar = self._distance_variance(A_input)\n",
    "        intermediate_dvar = self._distance_variance(A_intermediate)\n",
    "\n",
    "        # Get distance covariance\n",
    "        dcov = self._distance_covariance(A_input, A_intermediate)\n",
    "\n",
    "        dcorr = dcov / (input_dvar * intermediate_dvar).sqrt()\n",
    "\n",
    "        return dcorr\n",
    "\n",
    "    def _distance_covariance(self, a_matrix, b_matrix):\n",
    "        return (a_matrix * b_matrix).sum().sqrt() / a_matrix.size(0)\n",
    "\n",
    "    def _distance_variance(self, a_matrix):\n",
    "        return (a_matrix ** 2).sum().sqrt() / a_matrix.size(0)\n",
    "\n",
    "    def _A_matrix(self, data):\n",
    "        distance_matrix = self._distance_matrix(data)\n",
    "\n",
    "        row_mean = distance_matrix.mean(dim=0, keepdim=True)\n",
    "        col_mean = distance_matrix.mean(dim=1, keepdim=True)\n",
    "        data_mean = distance_matrix.mean()\n",
    "\n",
    "        return distance_matrix - row_mean - col_mean + data_mean\n",
    "\n",
    "    def _distance_matrix(self, data):\n",
    "        n = data.size(0)\n",
    "        distance_matrix = torch.zeros((n, n))\n",
    "\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                row_diff = data[i] - data[j]\n",
    "                distance_matrix[i, j] = (row_diff ** 2).sum()\n",
    "\n",
    "        return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaging-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoPeekLoss(nn.modules.loss._Loss):\n",
    "    raise NotImplementedError(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-indianapolis",
   "metadata": {},
   "source": [
    "### Bonus step\n",
    "\n",
    "The implementation of distance correlation loss above\n",
    "is quite inefficient.\n",
    "Can you improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-testament",
   "metadata": {},
   "source": [
    "### Step 2 - Train a NoPeekNN\n",
    "\n",
    "Train a classifier using the NoPeek loss.\n",
    "Because distance correlation loss is quite inefficient,\n",
    "we have to keep to quite low batch sizes (e.g. 32).\n",
    "Of course, this depends on the power and memory of your computer.\n",
    "You can re-use most of the training code above\n",
    "to train a NoPeekNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "nopeeknn = Classifier()\n",
    "raise NotImplementedError(\"Train NoPeekNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-things",
   "metadata": {},
   "source": [
    "### Step 3 - Train an attacker\n",
    "\n",
    "Use the code from earlier to train an attack model on the NoPeek classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-palmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Train an attack model on NoPeekNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-vintage",
   "metadata": {},
   "source": [
    "### Step 4- Validate attacker / NoPeek\n",
    "\n",
    "Visualize reconstructions from the NoPeek attacker.\n",
    "Has the defence worked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-rehabilitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Visualize reconstructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-worse",
   "metadata": {},
   "source": [
    "### Step 5 - Experiment\n",
    "\n",
    "Vary the weight of distance correlation loss term\n",
    "in the NoPeek loss function.\n",
    "What happens as you increase $\\alpha$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Experiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-round",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Noise defence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-arbitration",
   "metadata": {},
   "source": [
    "### Step 1 - Make a noisy classifier\n",
    "\n",
    "In the noise defence,\n",
    "we add noise to the intermediate data of an already-trained model.\n",
    "To make the noisy classifier,\n",
    "let's first define a classifier class with noise.\n",
    "You can get noise using `torch.distributions`.\n",
    "The noisy classifier should take a noise scale parameter,\n",
    "which defines how much noise to add.\n",
    "\n",
    "Next,\n",
    "we must copy over the parameters\n",
    "from our trained classifier\n",
    "to the noisy classifier\n",
    "so we have access to the new noise functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyClassifier(Classifier):\n",
    "    def __init__(self, noise_scale):\n",
    "        super().__init__()\n",
    "        self.set_noise(noise_scale)\n",
    "\n",
    "    def set_noise(self, noise):\n",
    "        raise NotImplementedError(\"Set a noise distribution using PyTorch\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"Define a forward pass which adds noise to the intermediate data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reverse-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_classifier = NoisyClassifier(0.1)\n",
    "raise NotImplementedError(\"Copy parameters from trained classifier to `noisy_classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boxed-consequence",
   "metadata": {},
   "source": [
    "### Step 2 - Train attacker\n",
    "\n",
    "Train an attack model on the noisy classifier.\n",
    "You can re-use the code from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Train attack model on the noisy classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-president",
   "metadata": {},
   "source": [
    "### Step 3 - Validate attacker / noise defence\n",
    "\n",
    "Plot reconstructions from the attacker to see if the noise defence works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adopted-purpose",
   "metadata": {},
   "source": [
    "### Step 4 - Experiment\n",
    "\n",
    "Change aspects of the noisy classifier and attacker\n",
    "to find out whether the defence is useful or not.\n",
    "Some experiments to try:\n",
    "- What happens when you increase noise scale?\n",
    "- How does noise scale differ from NoPeek?\n",
    "- Is the accuracy/privacy trade-off useful?\n",
    "- What happens when you use a gaussian noise distribution instead?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Experiment - have fun with it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-canal",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Can you:\n",
    "- [ ] Run model inversion of SplitNN\n",
    "- [ ] Implement NoPeek\n",
    "- [ ] Implement the noise defence\n",
    "- [ ] Experiment with hyperparameters to improve a defensive measure\n",
    "\n",
    "Model inversion is not the only attack\n",
    "which can be performed on neural networks - \n",
    "far from it,\n",
    "in fact.\n",
    "There are attacks which can steal model hyperparameters,\n",
    "extract training data memorized by the model,\n",
    "detect whether some data was part of the training data,\n",
    "or even destroy your model's ability to make predictions.\n",
    "Hopefully this lesson has given\n",
    "you an insight into their vulnerabilities.\n",
    "If you take one thing from this lesson,\n",
    "it should be that models are computational systems,\n",
    "and are therefore susceptible to attack.\n",
    "You should incorporate InfoSec processes\n",
    "and reviews into model development\n",
    "if you want to stay ahead of embarrassing\n",
    "and potentially costly attacks.\n",
    "If you found yourself enjoying\n",
    "developing the attack model,\n",
    "remember to use this new-found power for good!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
