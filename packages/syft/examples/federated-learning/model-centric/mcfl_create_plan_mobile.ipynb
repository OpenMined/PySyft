{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model-Centric Federated Learning for Mobile - MNIST Example\n",
    "\n",
    "This notebook will walk you through creating a simple model and a training plan, and hosting both as a federated learning process\n",
    "for further training using OpenMined mobile FL workers.\n",
    "\n",
    "This notebook is similar to \"[MCFL - Create Plan](mcfl_create_plan.ipynb)\"\n",
    "however due to mobile limitations, the training plan is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import base64\n",
    "import json\n",
    "\n",
    "# third party\n",
    "import torch as th\n",
    "\n",
    "# syft absolute\n",
    "import syft as sy\n",
    "from syft.core.plan.plan_builder import ROOT_CLIENT\n",
    "from syft.core.plan.plan_builder import PLAN_BUILDER_VM\n",
    "from syft.core.plan.plan_builder import make_plan\n",
    "from syft.core.plan.translation.torchscript.plan_translate import (\n",
    "    translate as translate_to_ts,\n",
    ")\n",
    "from syft.federated.model_centric_fl_client import ModelCentricFLClient\n",
    "from syft.lib.python.int import Int\n",
    "from syft.lib.python.list import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb63143d4f0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th.random.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the model\n",
    "\n",
    "This model will train on MNIST data, it's very simple yet can demonstrate learning process.\n",
    "There're 2 linear layers: \n",
    "\n",
    "* Linear 784x100\n",
    "* ReLU\n",
    "* Linear 100x10 \n",
    "\n",
    "Note that the model contains additional methods for convenience of torch reference usage:\n",
    "\n",
    "* `backward` - calculates backward pass gradients because autograd doesn't work on mobile (yet).\n",
    "* `softmax_cross_entropy_with_logits` - loss function\n",
    "* `accuracy` - calculates accuracy of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(sy.Module):\n",
    "    \"\"\"\n",
    "    Simple model with method for loss and hand-written backprop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, torch_ref) -> None:\n",
    "        super(MLP, self).__init__(torch_ref=torch_ref)\n",
    "        self.fc1 = torch_ref.nn.Linear(784, 100)\n",
    "        self.relu = torch_ref.nn.ReLU()\n",
    "        self.fc2 = torch_ref.nn.Linear(100, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z1 = self.fc1(x)\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        return self.fc2(self.a1)\n",
    "\n",
    "    def backward(self, X, error):\n",
    "        z1_grad = (error @ self.fc2.state_dict()[\"weight\"]) * (self.a1 > 0).float()\n",
    "        fc1_weight_grad = z1_grad.t() @ X\n",
    "        fc1_bias_grad = z1_grad.sum(0)\n",
    "        fc2_weight_grad = error.t() @ self.a1\n",
    "        fc2_bias_grad = error.sum(0)\n",
    "        return fc1_weight_grad, fc1_bias_grad, fc2_weight_grad, fc2_bias_grad\n",
    "\n",
    "    def softmax_cross_entropy_with_logits(self, logits, target, batch_size):\n",
    "        probs = self.torch_ref.softmax(logits, dim=1)\n",
    "        loss = -(target * self.torch_ref.log(probs)).sum(dim=1).mean()\n",
    "        loss_grad = (probs - target) / batch_size\n",
    "        return loss, loss_grad\n",
    "\n",
    "    def accuracy(self, logits, targets, batch_size):\n",
    "        pred = self.torch_ref.argmax(logits, dim=1)\n",
    "        targets_idx = self.torch_ref.argmax(targets, dim=1)\n",
    "        acc = pred.eq(targets_idx).sum().float() / batch_size\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define Training Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def set_remote_model_params(module_ptrs, params_list_ptr):\n",
    "    \"\"\"Sets the model parameters into traced model\"\"\"\n",
    "    param_idx = 0\n",
    "    for module_name, module_ptr in module_ptrs.items():\n",
    "        for param_name, _ in PLAN_BUILDER_VM.store[\n",
    "            module_ptr.id_at_location\n",
    "        ].data.named_parameters():\n",
    "            module_ptr.register_parameter(param_name, params_list_ptr[param_idx])\n",
    "            param_idx += 1\n",
    "\n",
    "# Create the model\n",
    "local_model = MLP(th)\n",
    "\n",
    "# Dummy inputs\n",
    "bs = 3\n",
    "classes_num = 10\n",
    "model_params_zeros = sy.lib.python.List(\n",
    "    [th.nn.Parameter(th.zeros_like(param)) for param in local_model.parameters()]\n",
    ")\n",
    "\n",
    "@make_plan\n",
    "def training_plan(\n",
    "    xs=th.randn(bs, 28 * 28),\n",
    "    ys=th.nn.functional.one_hot(th.randint(0, classes_num, [bs]), classes_num),\n",
    "    batch_size=th.tensor([bs]),\n",
    "    lr=th.tensor([0.1]),\n",
    "    params=model_params_zeros,\n",
    "):\n",
    "    # send the model to plan builder (but not its default params)\n",
    "    # this is required to build the model inside the Plan\n",
    "    model = local_model.send(ROOT_CLIENT, send_parameters=False)\n",
    "\n",
    "    # set model params from input\n",
    "    set_remote_model_params(model.modules, params)\n",
    "\n",
    "    # forward\n",
    "    logits = model(xs)\n",
    "\n",
    "    # loss\n",
    "    loss, loss_grad = model.softmax_cross_entropy_with_logits(\n",
    "        logits, ys, batch_size\n",
    "    )\n",
    "\n",
    "    # backward\n",
    "    grads = model.backward(xs, loss_grad)\n",
    "\n",
    "    # SGD step\n",
    "    updated_params = tuple(\n",
    "        param - lr * grad for param, grad in zip(model.parameters(), grads)\n",
    "    )\n",
    "\n",
    "    # accuracy\n",
    "    acc = model.accuracy(logits, ys, batch_size)\n",
    "\n",
    "    # return things\n",
    "    return (loss, acc, *updated_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the training plan to torchscript so it can be used with mobile workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    input: Tensor,\n",
      "    tensor: Tensor,\n",
      "    tensor0: Tensor,\n",
      "    tensor1: Tensor,\n",
      "    argument_5: List[Tensor]) -> List[Tensor]:\n",
      "  weight, tensor2, tensor3, tensor4, = argument_5\n",
      "  input0 = torch.addmm(tensor2, input, torch.t(weight), beta=1, alpha=1)\n",
      "  input1 = torch.relu(input0)\n",
      "  _0 = torch.addmm(tensor4, input1, torch.t(tensor3), beta=1, alpha=1)\n",
      "  tensor5 = torch.softmax(_0, 1, None)\n",
      "  tensor6 = torch.mul(tensor, torch.log(tensor5))\n",
      "  tensor7 = torch.sum(tensor6, [1], False, dtype=None)\n",
      "  _1 = torch.neg(torch.mean(tensor7, dtype=None))\n",
      "  tensor8 = torch.sub(tensor5, tensor, alpha=1)\n",
      "  tensor9 = torch.div(tensor8, tensor0)\n",
      "  tensor10 = torch.detach(tensor3)\n",
      "  tensor11 = torch.matmul(tensor9, tensor10)\n",
      "  tensor12 = torch.gt(input1, 0)\n",
      "  _2 = torch.to(tensor12, 6, False, False, None)\n",
      "  tensor13 = torch.mul(tensor11, _2)\n",
      "  tensor14 = torch.t(tensor13)\n",
      "  _3 = torch.matmul(tensor14, input)\n",
      "  _4 = torch.sum(tensor13, [0], False, dtype=None)\n",
      "  tensor15 = torch.t(tensor9)\n",
      "  _5 = torch.matmul(tensor15, input1)\n",
      "  _6 = torch.sum(tensor9, [0], False, dtype=None)\n",
      "  _7 = torch.sub(weight, torch.mul(tensor1, _3), alpha=1)\n",
      "  _8 = torch.sub(tensor2, torch.mul(tensor1, _4), alpha=1)\n",
      "  _9 = torch.sub(tensor3, torch.mul(tensor1, _5), alpha=1)\n",
      "  _10 = torch.sub(tensor4, torch.mul(tensor1, _6), alpha=1)\n",
      "  tensor16 = torch.argmax(_0, 1, False)\n",
      "  tensor17 = torch.eq(tensor16, torch.argmax(tensor, 1, False))\n",
      "  _11 = torch.to(torch.sum(tensor17, dtype=None), 6, False, False, None)\n",
      "  _12 = [_1, torch.div(_11, tensor0), _7, _8, _9, _10]\n",
      "  return _12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Translate to torchscript\n",
    "ts_plan = translate_to_ts(training_plan)\n",
    "\n",
    "# Let's examine its contents\n",
    "print(ts_plan.torchscript.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Averaging Plan\n",
    "\n",
    "Averaging Plan is executed by PyGrid at the end of the cycle,\n",
    "to average _diffs_ submitted by workers and update the model\n",
    "and create new checkpoint for the next cycle.\n",
    "\n",
    "_Diff_ is the difference between client-trained\n",
    "model params and original model params,\n",
    "so it has same number of tensors and tensor's shapes\n",
    "as the model parameters.\n",
    "\n",
    "We define Plan that processes one diff at a time.\n",
    "Such Plans require `iterative_plan` flag set to `True`\n",
    "in `server_config` when hosting FL model to PyGrid.\n",
    "\n",
    "Plan below will calculate simple mean of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@make_plan\n",
    "def avg_plan(\n",
    "    avg=List(local_model.parameters()), item=List(local_model.parameters()), num=Int(0)\n",
    "):\n",
    "    new_avg = []\n",
    "    for i, param in enumerate(avg):\n",
    "        new_avg.append((avg[i] * num + item[i]) / (num + 1))\n",
    "    return new_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Federated Learning Process Configuration\n",
    "\n",
    "Before hosting the model and training plan to PyGrid,\n",
    "we need to define some configuration parameters, such as\n",
    "FL process name, version, workers configuration,\n",
    "authentication method, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "name = \"mnist\"\n",
    "version = \"1.0\"\n",
    "\n",
    "client_config = {\n",
    "    \"name\": name,\n",
    "    \"version\": version,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 0.01,\n",
    "    \"max_updates\": 100,  # number of updates to execute on workers\n",
    "}\n",
    "\n",
    "server_config = {\n",
    "    \"num_cycles\": 30,  # total number of cycles (how many times global model is updated)\n",
    "    \"cycle_length\": 60*60*24,  # max duration of the training cycle in seconds\n",
    "    \"max_diffs\": 1,  # number of diffs to collect before updating global model\n",
    "    \"minimum_upload_speed\": 0,\n",
    "    \"minimum_download_speed\": 0,\n",
    "    \"iterative_plan\": True,  # tells PyGrid that avg plan is executed per diff\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This FL process will require workers to authenticate with signed JWT token.\n",
    "Providing the `pub_key` in FL configuration allows PyGrid to verify JWT tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(fname):\n",
    "    with open(fname, \"r\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "public_key = read_file(\"example_rsa.pub\").strip()\n",
    "\n",
    "server_config[\"authentication\"] = {\n",
    "    \"type\": \"jwt\",\n",
    "    \"pub_key\": public_key,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Host in PyGrid\n",
    "\n",
    "Let's now host everything in PyGrid so that it can be accessed by worker libraries.\n",
    "\n",
    "Note: assuming the PyGrid Domain is running locally on port 7000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.1: Start a PyGrid Domain\n",
    "- Clone PyGrid Github repository from https://github.com/OpenMined/PyGrid\n",
    "\n",
    "- Install poetry using pip:\n",
    "```\n",
    "$ pip install poetry\n",
    "```\n",
    "\n",
    "- Go to apps/domain and install requirements:\n",
    "```\n",
    "$ poetry install\n",
    "```\n",
    "\n",
    "- run a Grid domain using the command:\n",
    "```\n",
    "$ ./run.sh --name bob --port 7000 --start_local_db\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "grid_address = \"localhost:7000\"\n",
    "grid = ModelCentricFLClient(address=grid_address, secure=False)\n",
    "grid.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code sends FL model, training plans, and configuration to the PyGrid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "response = grid.host_federated_training(\n",
    "    model=local_model,\n",
    "    client_plans={\n",
    "        # Grid can store both types of plans (regular for python worker, torchscript for mobile):\n",
    "        \"training_plan\": training_plan,\n",
    "        \"training_plan:ts\": ts_plan,\n",
    "    },\n",
    "    client_protocols={},\n",
    "    server_averaging_plan=avg_plan,\n",
    "    client_config=client_config,\n",
    "    server_config=server_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'model-centric/host-training', 'data': {'status': 'success'}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see successful response, you've just hosted your first FL process into PyGrid!\n",
    "\n",
    "If you see error that FL process already exists,\n",
    "this means FL process with such name and version is already hosted.\n",
    "You might want to update name/version in configuration above, or cleanup PyGrid database.\n",
    "\n",
    "To cleanup database, set path below correctly and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !rm PyGrid/apps/domain/src/nodedatabase.db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To train hosted model, use one of the existing mobile FL workers:\n",
    " * [SwiftSyft](https://github.com/OpenMined/SwiftSyft) (see included worker example)\n",
    " * [KotlinSyft](https://github.com/OpenMined/KotlinSyft) (see included worker example)\n",
    "\n",
    "Support for javascript worker is coming soon:\n",
    " * [syft.js](https://github.com/OpenMined/syft.js)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
