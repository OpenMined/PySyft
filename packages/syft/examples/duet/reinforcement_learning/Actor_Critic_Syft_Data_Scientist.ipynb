{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic - Syft Duet - Data Scientist ü•Å\n",
    "\n",
    "Contributed by [@Koukyosyumei](https://github.com/Koukyosyumei)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: Connect to a Remote Duet Server\n",
    "\n",
    "As the Data Scientist, you want to perform data science on data that is sitting in the Data Owner's Duet server in their Notebook.\n",
    "\n",
    "In order to do this, we must run the code that the Data Owner sends us, which importantly includes their Duet Session ID. The code will look like this, importantly with their real Server ID.\n",
    "\n",
    "```\n",
    "import syft as sy\n",
    "duet = sy.duet('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "```\n",
    "\n",
    "This will create a direct connection from my notebook to the remote Duet server. Once the connection is established all traffic is sent directly between the two nodes.\n",
    "\n",
    "Paste the code or Server ID that the Data Owner gives you and run it in the cell below. It will return your Client ID which you must send to the Data Owner to enter into Duet so it can pair your notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import syft as sy\n",
    "duet = sy.join_duet(loopback=True)\n",
    "sy.logger.add(sink=\"./syft_ds.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 0 : Now STOP and run the Data Owner notebook until Checkpoint 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sy.load(\"gym\")\n",
    "sy.load(\"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"gamma\": 0.99,\n",
    "    \"seed\": 543,\n",
    "    \"render\": False,\n",
    "    \"log_interval\": 10,\n",
    "    \"no_cuda\": False,\n",
    "    \"log_interval\": 1,\n",
    "    \"wait_interval\": 1,\n",
    "    \"dry_run\":True,\n",
    "}\n",
    "\n",
    "remote_torch = duet.torch\n",
    "remote_torch.manual_seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cuda = False\n",
    "has_cuda_ptr = remote_torch.cuda.is_available()\n",
    "\n",
    "# lets ask to see if our Data Owner has CUDA\n",
    "has_cuda = bool(\n",
    "    has_cuda_ptr.get(\n",
    "        request_block=True,\n",
    "        reason=\"To run test and inference locally\",\n",
    "        timeout_secs=3,  # change to something slower\n",
    "    )\n",
    ")\n",
    "print(\"Is cuda available ? : \", has_cuda)\n",
    "\n",
    "\n",
    "use_cuda = not config[\"no_cuda\"] and has_cuda\n",
    "# now we can set the seed\n",
    "remote_torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "device = remote_torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# print(f\"Data Owner device is {device.type.get()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SavedAction = namedtuple(\"SavedAction\", [\"log_prob\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_saved_actions = []\n",
    "buffer_rewards =[]\n",
    "\n",
    "class Policy(sy.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self, torch_ref):\n",
    "        super(Policy, self).__init__(torch_ref=torch_ref)\n",
    "        self.affine1 = self.torch_ref.nn.Linear(4, 128)\n",
    "        # actor's layer\n",
    "        self.action_head = self.torch_ref.nn.Linear(128, 2)\n",
    "        # critic's layer\n",
    "        self.value_head = self.torch_ref.nn.Linear(128, 1)\n",
    "        # action & reward buffer\n",
    "        # self.saved_actions = []\n",
    "        # self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward of both actor and critic\n",
    "        \"\"\"\n",
    "        x = remote_torch.relu(self.affine1(x))\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob = remote_torch.softmax(self.action_head(x), dim=-1)\n",
    "        # critic: evaluates being in the state s_t\n",
    "        state_values = self.value_head(x)\n",
    "        # return values for both actor and critic as a tuple of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t\n",
    "        return action_prob, state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send our model to remote\n",
    "policy = Policy(torch)\n",
    "remote_policy = policy.send(duet)\n",
    "\n",
    "optimizer = remote_torch.optim.Adam(remote_policy.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have CUDA lets send our model to the GPU\n",
    "if has_cuda:\n",
    "    remote_policy.cuda(device)\n",
    "else:\n",
    "    remote_policy.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You cannot see the state\n",
    "def select_action(state):\n",
    "    global buffer_saved_actions\n",
    "    global buffer_rewards\n",
    "    \n",
    "    state = remote_torch.from_numpy(state).float()\n",
    "    probs_ptr, state_value_ptr = remote_policy(state)\n",
    "    \n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    m = remote_torch.distributions.Categorical(probs_ptr)\n",
    "    \n",
    "    # and sample an action using the distribution\n",
    "    action = m.sample()\n",
    "    \n",
    "    # save to action buffer\n",
    "    buffer_saved_actions.append(SavedAction(m.log_prob(action),\n",
    "                                                   state_value_ptr))\n",
    "    \n",
    "    # the action to take (left or right)\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    \"\"\"\n",
    "    Training code. Calculates actor and critic loss and performs backpropagation.\n",
    "    \"\"\"\n",
    "    global buffer_saved_actions\n",
    "    global buffer_rewards\n",
    "    \n",
    "    gamma = duet.python.Float(config[\"gamma\"])\n",
    "    \n",
    "    R = duet.python.Float(0)\n",
    "    policy_losses = duet.python.List([])\n",
    "    value_losses = duet.python.List([])\n",
    "    returns = duet.python.List([])\n",
    "    \n",
    "    for r in buffer_rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "        \n",
    "    returns = remote_torch.Tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    \n",
    "    for (log_prob, value), R in zip(buffer_saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "        \n",
    "        # calculate actor (policy) loss\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        \n",
    "        # calculate critic (value) loss using L1 smooth loss\n",
    "        value_losses.append(remote_torch.nn.functional.smooth_l1_loss(value,\n",
    "                                                                R.reshape(1)))\n",
    "    # reset gradients    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = remote_torch.stack(policy_losses).sum() + remote_torch.stack(value_losses).sum()\n",
    "    \n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # reset rewards and action buffer\n",
    "    del buffer_saved_actions[:]\n",
    "    del buffer_rewards[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_threshold_ptr = duet.store[\"reward_threshold\"]\n",
    "reward_threshold = reward_threshold_ptr.get(request_block=True, delete_obj=False)\n",
    "print(f\"reward_threshold is {reward_threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_gym = duet.gym\n",
    "remote_env = remote_gym.make(\"CartPole-v0\")\n",
    "remote_env.seed(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_reward = 10\n",
    "\n",
    "# run inifinitely many episodes\n",
    "for i_episode in count(1):\n",
    "\n",
    "    # reset environment and episode reward\n",
    "    state = remote_env.reset()\n",
    "    ep_reward = duet.python.Float(0)\n",
    "\n",
    "    # for each episode, only run 9999 steps so that we don't\n",
    "    # infinite loop while learning\n",
    "    for t in range(1, 10000):\n",
    "        # select action from policy\n",
    "        action = select_action(state)\n",
    "\n",
    "        # take the action\n",
    "        state, reward, done, _ = remote_env.step(action)\n",
    "\n",
    "        buffer_rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        if done.get(request_block=True):\n",
    "            break\n",
    "\n",
    "    # update cumulative reward\n",
    "    running_reward = 0.05 * ep_reward.get(request_block=True, delete_obj=False) + (1 - 0.05) * running_reward\n",
    "\n",
    "    # perform backprop\n",
    "    finish_episode()\n",
    "\n",
    "    # log results\n",
    "    if i_episode % config[\"log_interval\"] == 0:\n",
    "        print(\n",
    "                \"Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}\".format(\n",
    "                    i_episode,\n",
    "                    ep_reward.get(request_block=True, delete_obj=False),\n",
    "                    running_reward\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # check if we have \"solved\" the cart pole problem\n",
    "    if running_reward > reward_threshold:\n",
    "        print(\n",
    "                \"Solved! Running reward is now {} and \"\n",
    "                \"the last episode runs to {} time steps!\".format(running_reward, t)\n",
    "            )\n",
    "        break\n",
    "        \n",
    "    if config[\"dry_run\"]:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 1 : Now STOP and run the Data Owner notebook until Checkpoint 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
