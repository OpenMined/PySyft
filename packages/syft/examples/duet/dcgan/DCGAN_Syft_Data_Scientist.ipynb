{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "appointed-times",
   "metadata": {},
   "source": [
    "# DCGAN - Syft Duet - Data Scientist ü•Å\n",
    "\n",
    "Contributed by [@Koukyosyumei](https://github.com/Koukyosyumei)\n",
    "\n",
    "This example trains a DCGAN network on the BSD300 dataset with Syft.\n",
    "This notebook is mainly based on the original pytorch [example](https://github.com/OpenMined/PySyft/tree/dev/packages/syft/examples/duet/dcgan/original)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-february",
   "metadata": {
    "id": "following-accessory"
   },
   "source": [
    "## PART 1: Connect to a Remote Duet Server\n",
    "\n",
    "As the Data Scientist, you want to perform data science on data that is sitting in the Data Owner's Duet server in their Notebook.\n",
    "\n",
    "In order to do this, we must run the code that the Data Owner sends us, which importantly includes their Duet Session ID. The code will look like this, importantly with their real Server ID.\n",
    "\n",
    "```\n",
    "import syft as sy\n",
    "duet = sy.duet('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "```\n",
    "\n",
    "This will create a direct connection from my notebook to the remote Duet server. Once the connection is established all traffic is sent directly between the two nodes.\n",
    "\n",
    "Paste the code or Server ID that the Data Owner gives you and run it in the cell below. It will return your Client ID which you must send to the Data Owner to enter into Duet so it can pair your notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-sampling",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "executionInfo": {
     "elapsed": 177901,
     "status": "ok",
     "timestamp": 1613196714554,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "durable-birmingham",
    "outputId": "38050e45-0e0c-4d25-acd1-1d612228535c"
   },
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "duet = sy.join_duet(loopback=True)\n",
    "sy.logger.add(sink=\"./syft_ds.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-nothing",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 0 : Now STOP and run the Data Owner notebook until Checkpoint 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serial-pencil",
   "metadata": {
    "id": "waiting-chapel"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "try:\n",
    "    # make notebook progress bars nicer\n",
    "    from tqdm.notebook import tqdm\n",
    "except ImportError:\n",
    "    print(f\"Unable to import tqdm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-calendar",
   "metadata": {
    "id": "proud-bench"
   },
   "outputs": [],
   "source": [
    "remote_torch = duet.torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-conversion",
   "metadata": {
    "id": "intensive-cycle"
   },
   "source": [
    "# Set params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-harvest",
   "metadata": {
    "id": "blessed-convertible"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"image_size\":28,\n",
    "    \"batch_size\":64,\n",
    "    \"no_cuda\":True,\n",
    "    \"seed\":42,\n",
    "    \"nz\":100,\n",
    "    \"ngf\":28,\n",
    "    \"ndf\":28,\n",
    "    \"lr\":0.0002,\n",
    "    \"beta1\":0.5,\n",
    "    \"ngpu\":0,\n",
    "    \"num_iter\":100,\n",
    "    \"dry_run\":True,\n",
    "    \"log_interval\":2,\n",
    "    \"save_model\":False,\n",
    "    \"save_model_interval\":20,\n",
    "    \"cuda\":False,\n",
    "    \"outf\":\"result\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-width",
   "metadata": {
    "id": "appreciated-engineering"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(config[\"outf\"]):\n",
    "    os.mkdir(config[\"outf\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-index",
   "metadata": {
    "id": "funded-springfield"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-photography",
   "metadata": {
    "id": "abandoned-springer"
   },
   "outputs": [],
   "source": [
    "from syft.util import get_root_data_path\n",
    "\n",
    "# we need some transforms for the MNIST data set\n",
    "remote_torchvision = duet.torchvision\n",
    "\n",
    "transform_1 = remote_torchvision.transforms.ToTensor()  # this converts PIL images to Tensors\n",
    "transform_2 = remote_torchvision.transforms.Normalize((0.5), (0.5))  # this normalizes the dataset\n",
    "\n",
    "remote_list = duet.python.List()  # create a remote list to add the transforms to\n",
    "remote_list.append(transform_1)\n",
    "remote_list.append(transform_2)\n",
    "\n",
    "# compose our transforms\n",
    "transforms = remote_torchvision.transforms.Compose(remote_list)\n",
    "\n",
    "# The DO has kindly let us initialise a DataLoader for their training set\n",
    "train_kwargs = {\n",
    "    \"batch_size\": config[\"batch_size\"],\n",
    "    \"shuffle\":True\n",
    "}\n",
    "train_data_ptr = remote_torchvision.datasets.MNIST(str(get_root_data_path()), train=True, download=True, transform=transforms)\n",
    "train_loader_ptr = remote_torch.utils.data.DataLoader(train_data_ptr,**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-gnome",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3837,
     "status": "ok",
     "timestamp": 1613196734020,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "superb-statistics",
    "outputId": "8e2b057c-344d-4f6c-d7ce-a613adb99628"
   },
   "outputs": [],
   "source": [
    "# normally we would not necessarily know the length of a remote dataset so lets ask for it\n",
    "# so we can pass that to our training loop and know when to stop\n",
    "def get_train_length(train_data_ptr):\n",
    "    train_length = train_data_ptr.__len__()\n",
    "    return train_length\n",
    "\n",
    "try:\n",
    "    if train_data_length is None:\n",
    "        train_data_length = get_train_length(train_data_ptr)\n",
    "except NameError:\n",
    "        train_data_length = get_train_length(train_data_ptr)\n",
    "\n",
    "print(f\"Training Dataset size is: {train_data_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-newton",
   "metadata": {
    "id": "knowing-clerk"
   },
   "source": [
    "# Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-facing",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2239,
     "status": "ok",
     "timestamp": 1613196734451,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "scheduled-sleeping",
    "outputId": "17706d00-c129-4a84-8728-fdae1edb6e64"
   },
   "outputs": [],
   "source": [
    "has_cuda = False\n",
    "has_cuda_ptr = remote_torch.cuda.is_available().get(request_block=True)\n",
    "\n",
    "# lets ask to see if our Data Owner has CUDA\n",
    "print(\"Is cuda available ? : \", has_cuda)\n",
    "\n",
    "use_cuda = not config[\"no_cuda\"] and has_cuda\n",
    "# now we can set the seed\n",
    "remote_torch.manual_seed(config[\"seed\"])\n",
    "\n",
    "device = remote_torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "#print(f\"Data Owner device is {device.type.get()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-honey",
   "metadata": {
    "id": "interested-column"
   },
   "source": [
    "# Define and Create models\n",
    "\n",
    "In this notebook, we'll use two models based on the following link. \n",
    "\n",
    "https://github.com/pytorch/packages/syft/examples/tree/master/dcgan\n",
    "\n",
    "The structure of the models is a bit different from the original ones because pysyft doesn't support transform.Resize and we have to match the dimension of input without Resize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-privilege",
   "metadata": {
    "id": "subsequent-apple"
   },
   "outputs": [],
   "source": [
    "nz = config[\"nz\"]\n",
    "ngf = config[\"ngf\"]\n",
    "ndf = config[\"ndf\"]\n",
    "nc = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-mouse",
   "metadata": {
    "id": "taken-costume"
   },
   "outputs": [],
   "source": [
    "class Generator(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(Generator, self).__init__(torch_ref=torch_ref)\n",
    "\n",
    "        \"\"\"\n",
    "        input is Z, going into a convolution\n",
    "        ----------------------------------------------------------------\n",
    "                Layer (type)               Output Shape         Param #\n",
    "        ================================================================\n",
    "           ConvTranspose2d-1            [-1, 112, 4, 4]         179,200\n",
    "               BatchNorm2d-2            [-1, 112, 4, 4]             224\n",
    "                      ReLU-3            [-1, 112, 4, 4]               0\n",
    "           ConvTranspose2d-4             [-1, 56, 7, 7]          56,448\n",
    "               BatchNorm2d-5             [-1, 56, 7, 7]             112\n",
    "                      ReLU-6             [-1, 56, 7, 7]               0\n",
    "           ConvTranspose2d-7           [-1, 28, 14, 14]          25,088\n",
    "               BatchNorm2d-8           [-1, 28, 14, 14]              56\n",
    "                      ReLU-9           [-1, 28, 14, 14]               0\n",
    "          ConvTranspose2d-10            [-1, 1, 28, 28]             448\n",
    "                     Tanh-11            [-1, 1, 28, 28]               0\n",
    "        ================================================================\n",
    "        Total params: 261,576\n",
    "        Trainable params: 261,576\n",
    "        Non-trainable params: 0      \n",
    "        \"\"\"\n",
    "        \n",
    "        self.conv1 = self.torch_ref.nn.ConvTranspose2d(nz, ngf*4, 4, 1, 0, bias=False)\n",
    "        self.norm1 = self.torch_ref.nn.BatchNorm2d(ngf*4)\n",
    "        self.relu1 = self.torch_ref.nn.ReLU(True)\n",
    "\n",
    "        self.conv2 = self.torch_ref.nn.ConvTranspose2d(ngf*4, ngf*2, 3, 2, 1, bias=False)\n",
    "        self.norm2 = self.torch_ref.nn.BatchNorm2d(ngf*2)\n",
    "        self.relu2 = self.torch_ref.nn.ReLU(True)\n",
    "\n",
    "        self.conv3 = self.torch_ref.nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False)\n",
    "        self.norm3 = self.torch_ref.nn.BatchNorm2d(ngf)\n",
    "        self.relu3 = self.torch_ref.nn.ReLU(True)\n",
    "\n",
    "        self.conv4 = self.torch_ref.nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv4(x)\n",
    "        output = self.torch_ref.nn.Tanh()(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-redhead",
   "metadata": {
    "id": "satisfactory-footage"
   },
   "outputs": [],
   "source": [
    "class Discriminator(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(Discriminator, self).__init__(torch_ref=torch_ref)\n",
    "        \n",
    "        \"\"\"\n",
    "        input is (nc) x 28 x 28\n",
    "        ----------------------------------------------------------------\n",
    "                Layer (type)               Output Shape         Param #\n",
    "        ================================================================\n",
    "                    Conv2d-1           [-1, 28, 14, 14]             448\n",
    "                 LeakyReLU-2           [-1, 28, 14, 14]               0\n",
    "                    Conv2d-3             [-1, 56, 7, 7]          25,088\n",
    "               BatchNorm2d-4             [-1, 56, 7, 7]             112\n",
    "                 LeakyReLU-5             [-1, 56, 7, 7]               0\n",
    "                    Conv2d-6            [-1, 112, 4, 4]          56,448\n",
    "               BatchNorm2d-7            [-1, 112, 4, 4]             224\n",
    "                 LeakyReLU-8            [-1, 112, 4, 4]               0\n",
    "                    Conv2d-9              [-1, 1, 1, 1]           1,792\n",
    "                  Sigmoid-10              [-1, 1, 1, 1]               0\n",
    "        ================================================================\n",
    "        Total params: 84,112\n",
    "        Trainable params: 84,112\n",
    "        Non-trainable params: 0        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.conv1 = self.torch_ref.nn.Conv2d(nc, ndf, 4, 2, 1, bias=False)\n",
    "        self.lerelu1 = self.torch_ref.nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv2 = self.torch_ref.nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False)\n",
    "        self.norm2 = self.torch_ref.nn.BatchNorm2d(ndf * 2)\n",
    "        self.lerelu2 = self.torch_ref.nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv3 = self.torch_ref.nn.Conv2d(ndf * 2, ndf * 4, 3, 2, 1, bias=False)\n",
    "        self.norm3 = self.torch_ref.nn.BatchNorm2d(ndf * 4)\n",
    "        self.lerelu3 = self.torch_ref.nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv4 = self.torch_ref.nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.lerelu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.lerelu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.lerelu3(x)\n",
    "        x = self.conv4(x)\n",
    "        output = self.torch_ref.nn.Sigmoid()(x)\n",
    "\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-burning",
   "metadata": {
    "id": "underlying-classification"
   },
   "outputs": [],
   "source": [
    "local_netG =  Generator(torch)\n",
    "local_netD =  Discriminator(torch)\n",
    "netG = local_netG.send(duet)\n",
    "netD = local_netD.send(duet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lonely-functionality",
   "metadata": {
    "id": "regulation-vampire"
   },
   "outputs": [],
   "source": [
    "# if we have CUDA lets send our model to the GPU\n",
    "if has_cuda:\n",
    "    netD.cuda(device)\n",
    "    netG.cuda(device)\n",
    "else:\n",
    "    netD.cpu()\n",
    "    netG.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-encyclopedia",
   "metadata": {
    "id": "legitimate-reader"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interesting-baseline",
   "metadata": {
    "id": "shaped-soviet"
   },
   "source": [
    "make sure that models locate in remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-cache",
   "metadata": {
    "id": "explicit-differential"
   },
   "outputs": [],
   "source": [
    "assert not netD.is_local, \"Training requires remote model\"\n",
    "assert not netG.is_local, \"Training requires remote model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-immune",
   "metadata": {
    "id": "verbal-slide"
   },
   "source": [
    "fixed noise for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-transport",
   "metadata": {
    "id": "crazy-experience"
   },
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(config[\"batch_size\"], nz, 1, 1)\n",
    "fixed_noise.tag(\"noise\")\n",
    "fixed_noise_ptr = fixed_noise.send(duet, pointable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-spine",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1613196746033,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "occupied-startup",
    "outputId": "126a383b-42fc-4398-d533-31401cfc38a8"
   },
   "outputs": [],
   "source": [
    "train_batches = round((train_data_length / config[\"batch_size\"]) + 0.5)\n",
    "print(f\"> Running train in {train_batches} batches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-instruction",
   "metadata": {
    "id": "tested-launch"
   },
   "source": [
    "set optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-resident",
   "metadata": {
    "id": "knowing-letters"
   },
   "outputs": [],
   "source": [
    "optimizerD = remote_torch.optim.Adam(netD.parameters(), lr=config[\"lr\"], betas=(config[\"beta1\"], 0.999))\n",
    "optimizerG = remote_torch.optim.Adam(netG.parameters(), lr=config[\"lr\"], betas=(config[\"beta1\"], 0.999))\n",
    "criterion = remote_torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funded-aspect",
   "metadata": {
    "id": "seventh-vaccine"
   },
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-parcel",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67122,
     "status": "ok",
     "timestamp": 1613196817482,
     "user": {
      "displayName": "hideaki takahashi",
      "photoUrl": "",
      "userId": "16154026581542772178"
     },
     "user_tz": -540
    },
    "id": "quantitative-share",
    "outputId": "793055a6-9c00-41f8-a69b-251989f16eb9"
   },
   "outputs": [],
   "source": [
    "if config[\"dry_run\"]:\n",
    "    num_iter = 1\n",
    "else:\n",
    "    num_iter = config[\"num_iter\"]\n",
    "\n",
    "for epoch in range(num_iter):\n",
    "    \n",
    "    netD.train()\n",
    "    netG.train()\n",
    "    \n",
    "    for batch_idx, data in enumerate(train_loader_ptr):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        \n",
    "        optimizerD.zero_grad()\n",
    "        data_ptr, target_ptr = data[0], data[1]\n",
    "        batch_size = config[\"batch_size\"]\n",
    "       \n",
    "        output = netD(data_ptr)\n",
    "        label = remote_torch.zeros_like(output)\n",
    "        label = label.fill_(real_label)\n",
    "        \n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1)\n",
    "        fake = netG(noise)\n",
    "        output = netD(fake.detach())\n",
    "        label = remote_torch.zeros_like(output)\n",
    "        label.fill_(fake_label)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        \n",
    "        #netG.zero_grad()\n",
    "        optimizerG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        if batch_idx % config[\"log_interval\"] == 0:\n",
    "            # get loss\n",
    "            local_errD = None\n",
    "            local_errD = errD.item().get(request_block=True)\n",
    "            local_errG = None\n",
    "            local_errG = errG.item().get(request_block=True)\n",
    "            \n",
    "            if (local_errD is not None) and (local_errG is not None):\n",
    "                print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f'\n",
    "                  % (epoch, config[\"num_iter\"], batch_idx, train_batches,\n",
    "                    local_errD, local_errG))  \n",
    "            else:\n",
    "                print('[%d/%d][%d/%d] Loss_D: - Loss_G: - '\n",
    "                  % (epoch, config[\"num_iter\"], batch_idx, train_batches))\n",
    "            \n",
    "            # create fake image\n",
    "            created_img = netG(fixed_noise).get(request_block=True)\n",
    "            vutils.save_image(\n",
    "                            created_img.detach(),\n",
    "                            f\"{config['outf']}/fake_samples_{epoch}_{batch_idx}.png\",\n",
    "                            normalize=True,\n",
    "                            )\n",
    "            \n",
    "        if batch_idx >= train_batches - 1:\n",
    "            break\n",
    "\n",
    "        if config[\"dry_run\"]:\n",
    "            break\n",
    "         \n",
    "    # save snapshots\n",
    "    if (config[\"save_model\"]) and (epoch % config[\"save_model_interval\"] == 0):\n",
    "        netG.get(\n",
    "            request_block=True,\n",
    "            timeout_secs=5,\n",
    "            delete_obj=False\n",
    "        ).save(f\"{config['outf']}/netG_mnist_{epoch}.pt\")\n",
    "\n",
    "        netD.get(\n",
    "            request_block=True,\n",
    "            timeout_secs=5,\n",
    "            delete_obj=False\n",
    "        ).save(f\"{config['outf']}/netD_mnist_{epoch}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-prisoner",
   "metadata": {
    "id": "interstate-service"
   },
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-panama",
   "metadata": {
    "id": "nervous-ministry"
   },
   "outputs": [],
   "source": [
    "local_netG = netG.get(\n",
    "    request_block=True,\n",
    "    timeout_secs=5,\n",
    "    delete_obj=False\n",
    ")\n",
    "\n",
    "local_netG.save(f\"{config['outf']}/final_netG_mnist.pt\")\n",
    "\n",
    "netD.get(\n",
    "    request_block=True,\n",
    "    timeout_secs=5,\n",
    "    delete_obj=False\n",
    ").save(f\"{config['outf']}/final_netD_mnist.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-diving",
   "metadata": {
    "id": "amazing-hammer"
   },
   "source": [
    "# Inference locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-reminder",
   "metadata": {
    "id": "ready-empty"
   },
   "outputs": [],
   "source": [
    "assert local_netG.is_local, \"model is remote try .get()\"\n",
    "\n",
    "created_img = local_netG(fixed_noise)\n",
    "\n",
    "vutils.save_image(\n",
    "    created_img.detach(),\n",
    "    \"fake_samples_local.png\",\n",
    "    normalize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-infrared",
   "metadata": {
    "id": "Bn0r-zcJBWHl"
   },
   "outputs": [],
   "source": [
    "fake_image = Image.open(\"fake_samples_local.png\")\n",
    "#display(fake_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-cruise",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 1 : Now STOP and run the Data Owner notebook until the next checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "re_DCGAN_Syft_Data_Scientist_re.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
