{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legal-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from opacus import PrivacyEngine\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import tenseal as ts\n",
    "import syft as sy\n",
    "sy.load(\"tenseal\")\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root=\".\", train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-collapse",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_mean = 0.1307\n",
    "mnist_std = 0.3081\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "lr = 0.1\n",
    "sigma = 1.0\n",
    "max_per_sample_grad_norm = 1.0\n",
    "delta = 1e-5\n",
    "root = \".\"\n",
    "weights_filename = \"mnist_cnn_weights.pt\"\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 4, 2)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))  # -> [B, 16, 14, 14]\n",
    "        x = F.max_pool2d(x, 2, 1)  # -> [B, 16, 13, 13]\n",
    "        x = F.relu(self.conv2(x))  # -> [B, 32, 5, 5]\n",
    "        x = F.max_pool2d(x, 2, 1)  # -> [B, 32, 4, 4]\n",
    "        x = x.view(-1, 32 * 4 * 4)  # -> [B, 512]\n",
    "        x = self.fc1(x)  # -> [B, 32]\n",
    "        x = x * x  # -> [B, 32] square activation\n",
    "        x = self.fc2(x)  # -> [B, 10]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-aaron",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    for _batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "    return correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def train_model():  \n",
    "    model = Model().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            root,\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((mnist_mean,), (mnist_std,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers = 1,\n",
    "        pin_memory = True\n",
    "    )\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            root,\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((mnist_mean,), (mnist_std,)),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=1000,\n",
    "        shuffle=True,\n",
    "        num_workers = 1,\n",
    "        pin_memory = True\n",
    "    )\n",
    "    \n",
    "    privacy_engine = PrivacyEngine(\n",
    "        model,\n",
    "        batch_size=batch_size,\n",
    "        sample_size=len(train_loader.dataset),\n",
    "        alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64)),\n",
    "        noise_multiplier=sigma,\n",
    "        max_grad_norm=max_per_sample_grad_norm,\n",
    "    )\n",
    "\n",
    "    privacy_engine.attach(optimizer)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)\n",
    "    torch.save(model.state_dict(), weights_filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(root + \"/\" + weights_filename):\n",
    "    model = Model()\n",
    "    model.load_state_dict(torch.load(weights_filename))\n",
    "    model.eval()\n",
    "else:\n",
    "    model = train_model()\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-mentor",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base_params = sy.lib.python.List([\"conv1.weight\", \"conv1.bias\", \"conv2.weight\", \"conv2.bias\"])\n",
    "fc_head_params = [\"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\"]\n",
    "\n",
    "state_dict = model.state_dict()\n",
    "conv_base_weights = sy.lib.python.List()\n",
    "fully_connected_weights = OrderedDict()\n",
    "\n",
    "for param_name in conv_base_params:\n",
    "    conv_base_weights.append(state_dict[param_name])\n",
    "\n",
    "for param_name in fc_head_params:\n",
    "    fully_connected_weights[param_name] = state_dict[param_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedHead():\n",
    "    def __init__(self, parameters):\n",
    "        self.fc1_weight = parameters[\"fc1.weight\"]\n",
    "        self.fc1_bias = parameters[\"fc1.bias\"]\n",
    "        self.fc2_weight = parameters[\"fc2.weight\"]\n",
    "        self.fc2_bias = parameters[\"fc2.bias\"]\n",
    "\n",
    "    def forward(self, enc_x: ts.CKKSTensor, batch_size: int) -> ts.CKKSTensor:\n",
    "        if batch_size == 1:\n",
    "            fc1_bias = self.fc1_bias.unsqueeze(0)\n",
    "            fc2_bias = self.fc2_bias.unsqueeze(0)\n",
    "        else:\n",
    "            fc1_bias = torch.stack([self.fc1_bias for elem in range(batch_size)])\n",
    "            fc2_bias = torch.stack([self.fc2_bias for elem in range(batch_size)])\n",
    "        print(fc1_bias.shape)\n",
    "        out = enc_x.mm(self.fc1_weight.T) + fc1_bias\n",
    "        out.square_()\n",
    "        out = out.mm(self.fc2_weight.T) + fc2_bias\n",
    "        return out\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "fully_connected_head = FullyConnectedHead(fully_connected_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-language",
   "metadata": {},
   "outputs": [],
   "source": [
    "duet = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base_params_ptr_ = conv_base_params.send(duet, searchable=True, tags=[\"conv_base_names\"])\n",
    "conv_base_weights_ptr = conv_base_weights.send(duet, searchable=True, tags=[\"conv_base_weights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "above-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duet.store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-stability",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 1 : Now STOP and run the Data Owner notebook until the next checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = duet.store[\"batch_size\"].get(request_block=True)\n",
    "print(\"Got the batch_size\")\n",
    "context = duet.store[\"context\"].get(request_block=True)\n",
    "print(\"Got the context\")\n",
    "encrypted_activation = duet.store[\"encrypted_activation\"].get(request_block=True)\n",
    "print(\"Got the encrypted_activation\")\n",
    "encrypted_activation.link_context(context)\n",
    "print(\"Linked the context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-complaint",
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_result = fully_connected_head(encrypted_activation, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-potter",
   "metadata": {},
   "outputs": [],
   "source": [
    "encrypted_result_ptr = encrypted_result.send(duet, searchable=True, tags=[\"result\"])\n",
    "# comment this to hang the store\n",
    "# print(duet.store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-commissioner",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 2 : Now STOP and run the Data Owner notebook until the next checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-third",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:om_work] *",
   "language": "python",
   "name": "conda-env-om_work-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
