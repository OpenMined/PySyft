{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TimeSequence - Syft Duet - Data Scientist ü•Å\n",
    "\n",
    "Contributed by [@Koukyosyumei](https://github.com/Koukyosyumei)\n",
    "\n",
    "## PART 1: Connect to a Remote Duet Server\n",
    "\n",
    "As the Data Scientist, you want to perform data science on data that is sitting in the Data Owner's Duet server in their Notebook.\n",
    "\n",
    "In order to do this, we must run the code that the Data Owner sends us, which importantly includes their Duet Session ID. The code will look like this, importantly with their real Server ID.\n",
    "\n",
    "```\n",
    "import syft as sy\n",
    "duet = sy.duet('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "```\n",
    "\n",
    "This will create a direct connection from my notebook to the remote Duet server. Once the connection is established all traffic is sent directly between the two nodes.\n",
    "\n",
    "Paste the code or Server ID that the Data Owner gives you and run it in the cell below. It will return your Client ID which you must send to the Data Owner to enter into Duet so it can pair your notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from typing import List as TypeList\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from syft import SyModule\n",
    "from syft.core.plan.plan_builder import ROOT_CLIENT, make_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "\n",
    "duet = sy.join_duet(loopback=True)\n",
    "sy.logger.add(sink=\"./syft_ds.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 0 : Now STOP and run the Data Owner notebook until Checkpoint 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"steps\": 15, \"dry_run\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duet.store.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = duet.store[\"L\"].get(request_block=True, delete_obj=False)\n",
    "input = duet.store[\"input\"]\n",
    "input_size_0 = duet.store[\"input_size_0\"].get(request_block=True, delete_obj=False)\n",
    "target = duet.store[\"target\"]\n",
    "test_input = duet.store[\"test_input\"]\n",
    "test_target = duet.store[\"test_target\"]\n",
    "train_ptr = duet.store[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_torch = ROOT_CLIENT.torch\n",
    "remote_python = ROOT_CLIENT.python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior of the model changes in inference time because it has to predict future values. Although the `forward` method in original implementation receives two arguments, input and futures, which indicates the length of future prediction, `SyModule` can accept only one argument. Thus, this notebook defines two separate models for training and prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for training\n",
    "\n",
    "\n",
    "class Sequence(SyModule):\n",
    "    def __init__(self, batch_size=97, **kwargs: Any) -> None:\n",
    "        super().__init__(**kwargs)\n",
    "        self.lstm1 = torch.nn.LSTMCell(1, 51)\n",
    "        self.lstm2 = torch.nn.LSTMCell(51, 51)\n",
    "        self.linear = torch.nn.Linear(51, 1)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, input: Any) -> Any:\n",
    "        outputs = remote_python.List([])\n",
    "\n",
    "        h_t = torch.zeros(self.batch_size, 51)\n",
    "        c_t = torch.zeros(self.batch_size, 51)\n",
    "        h_t2 = torch.zeros(self.batch_size, 51)\n",
    "        c_t2 = torch.zeros(self.batch_size, 51)\n",
    "\n",
    "        state_1 = remote_python.Tuple((h_t, c_t))\n",
    "        state_2 = remote_python.Tuple((h_t2, c_t2))\n",
    "\n",
    "        for input_t in input.split(1, dim=1):\n",
    "            state_1 = self.lstm1(input_t, state_1)\n",
    "            state_2 = self.lstm2(h_t, state_2)\n",
    "            output = self.linear(state_2[0])\n",
    "            outputs.append(output)\n",
    "\n",
    "        \"\"\" eval mode is currently not supported in ModulePointer\n",
    "        if not self.training:\n",
    "            for i in range(10):# if we should predict the future\n",
    "                state_1 = self.lstm1(output, state_1)\n",
    "                state_2 = self.lstm2(h_t, state_2)\n",
    "                output = self.linear(state_2[0])\n",
    "                outputs.append(output)\n",
    "        \"\"\"\n",
    "\n",
    "        outputs = remote_torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model for prediction\n",
    "\n",
    "\n",
    "class Sequence_Pred(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Sequence_Pred, self).__init__()\n",
    "        self.lstm1 = nn.LSTMCell(1, 51)\n",
    "        self.lstm2 = nn.LSTMCell(51, 51)\n",
    "        self.linear = nn.Linear(51, 1)\n",
    "\n",
    "    def forward(self, input, future=0):\n",
    "        outputs = []\n",
    "        h_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.double)\n",
    "\n",
    "        for input_t in input.split(1, dim=1):\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        for i in range(future):  # if we should predict the future\n",
    "            h_t, c_t = self.lstm1(output, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            output = self.linear(h_t2)\n",
    "            outputs += [output]\n",
    "        outputs = torch.cat(outputs, dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_test_input = test_input.get(request_block=True, delete_obj=False)\n",
    "local_test_target = test_target.get(request_block=True, delete_obj=False)\n",
    "\n",
    "input_size_1 = local_test_input.shape[1]\n",
    "future = L\n",
    "\n",
    "criterion_pred = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequence(input_size=(input_size_0, L - 1))\n",
    "model_pred = Sequence_Pred()\n",
    "model_pred.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(config[\"steps\"]):\n",
    "    # training with Plan\n",
    "    res_ptr = train_ptr(input=input, target=target, model=model)\n",
    "    # download the trained model to local\n",
    "    model = res_ptr[0].get(request_block=True, delete_obj=False)\n",
    "    # update the model for prediction\n",
    "    model_pred.load_state_dict(model.state_dict())\n",
    "\n",
    "    # begin to predict, no need to track gradient here\n",
    "    with torch.no_grad():\n",
    "        pred = model_pred(local_test_input.double(), future=future)\n",
    "        loss = criterion_pred(pred[:, :-L], local_test_target)\n",
    "        print(\"test loss:\", loss.item())\n",
    "        y = pred.detach().numpy()\n",
    "\n",
    "    # draw the result\n",
    "    plt.figure(figsize=(30, 10))\n",
    "    plt.title(\n",
    "        \"Predict future values for time sequences\\n(Dashlines are predicted values)\",\n",
    "        fontsize=30,\n",
    "    )\n",
    "    plt.xlabel(\"x\", fontsize=20)\n",
    "    plt.ylabel(\"y\", fontsize=20)\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    def draw(yi, color):\n",
    "        plt.plot(np.arange(input_size_1), yi[:input_size_1], color, linewidth=2.0)\n",
    "        plt.plot(\n",
    "            np.arange(input_size_1, input_size_1 + future),\n",
    "            yi[input_size_1:],\n",
    "            color + \":\",\n",
    "            linewidth=2.0,\n",
    "        )\n",
    "\n",
    "    draw(y[0], \"r\")\n",
    "    draw(y[1], \"g\")\n",
    "    draw(y[2], \"b\")\n",
    "\n",
    "    plt.savefig(\"predict%d.pdf\" % i)\n",
    "    plt.close()\n",
    "\n",
    "    if config[\"dry_run\"]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <img src=\"https://github.com/OpenMined/design-assets/raw/master/logos/OM/mark-primary-light.png\" alt=\"he-black-box\" width=\"100\"/> Checkpoint 1 : Now STOP and run the Data Owner notebook until Checkpoint 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-showcode": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
