{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import names\n",
    "from syft.core.common import UID\n",
    "from sympy import symbols\n",
    "from scipy import optimize\n",
    "import sympy as sym\n",
    "import numpy as np\n",
    "import random\n",
    "from sympy.solvers import solve\n",
    "\n",
    "from functools import lru_cache\n",
    "\n",
    "# ordered_symbols = list()\n",
    "# for i in range(100):\n",
    "#     ordered_symbols.append(symbols(\"s\"+str(i)))\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def maximize_flattened_poly(flattened_poly, *rranges, force_all_searches=False, **s2i):\n",
    "\n",
    "    search_fun = create_searchable_function(flattened_poly, s2i)\n",
    "    \n",
    "    return minimize_function(f=search_fun, rranges=rranges, force_all_searches=False)\n",
    "\n",
    "def flatten_and_maximize_poly(poly, force_all_searches=False):\n",
    "\n",
    "    i2s = list(poly.free_symbols)\n",
    "    s2i = {s:i for i,s in enumerate(i2s)}\n",
    "    \n",
    "    # this code seems to make things slower - although there might be a memory improvement (i haven't checked)\n",
    "#     flattened_poly = poly.copy().subs({k:v for k,v in zip(i2s, ordered_symbols[0:len(i2s)])})\n",
    "#     flattened_s2i = {str(ordered_symbols[i]):i for s,i in s2i.items()}\n",
    "\n",
    "    flattened_poly = poly\n",
    "    flattened_s2i = {str(s):i for s,i in s2i.items()}    \n",
    "\n",
    "    rranges = [(ssid2obj[i2s[i].name].min_val, ssid2obj[i2s[i].name].max_val) for i in range(len(s2i))]\n",
    "\n",
    "    return maximize_flattened_poly(flattened_poly, *rranges, force_all_searches=force_all_searches, **flattened_s2i)\n",
    "\n",
    "def create_lookup_tables_for_symbol(polynomial):\n",
    "\n",
    "    index2symbol = [str(x) for x in polynomial.free_symbols]\n",
    "    symbol2index = {sym: i for i, sym in enumerate(index2symbol)}\n",
    "\n",
    "    return index2symbol, symbol2index\n",
    "\n",
    "def create_searchable_function(f, symbol2index):\n",
    "\n",
    "        # Tudor: Here you weren't using *params\n",
    "        # Tudor: If I understand correctly, .subs returns\n",
    "        def _run_specific_args(tuple_of_args: tuple):\n",
    "            kwargs = {sym: tuple_of_args[i] for sym, i in symbol2index.items()}\n",
    "            output =  f.subs(kwargs)\n",
    "            return output\n",
    "\n",
    "        return _run_specific_args\n",
    "\n",
    "def minimize_function(f, rranges, constraints=[], force_all_searches=False):\n",
    "    \n",
    "    results = list()\n",
    "    \n",
    "    # Step 1: try simplicial\n",
    "    shgo_results = optimize.shgo(f, rranges, sampling_method='simplicial', constraints=constraints)\n",
    "    results.append(shgo_results)\n",
    "    \n",
    "    if not shgo_results.success or force_all_searches:\n",
    "        # sometimes simplicial has trouble as a result of initialization\n",
    "        # see: https://github.com/scipy/scipy/issues/10429 for details\n",
    "#         if not force_all_searches:\n",
    "#             print(\"Simplicial search didn't work... trying sobol\")\n",
    "        shgo_results = optimize.shgo(f, rranges, sampling_method='sobol', constraints=constraints)\n",
    "        results.append(shgo_results)\n",
    "        \n",
    "    if not shgo_results.success:\n",
    "        raise Exception(\"Search algorithm wasn't solvable... abort\")\n",
    "        \n",
    "    return results\n",
    "\n",
    "def max_lipschitz_wrt_entity(scalars, entity):\n",
    "    result = max_lipschitz_via_jacobian(scalars, input_entity=entity)[0][-1]\n",
    "    if isinstance(result, float):\n",
    "        return -result\n",
    "    else:\n",
    "        return -float(result.fun)\n",
    "\n",
    "def max_lipschitz_via_jacobian(scalars, input_entity=None, data_dependent=True, force_all_searches=False, try_hessian_shortcut=False):\n",
    "        \n",
    "        polys = [x.poly for x in scalars]\n",
    "        input_scalars = set()\n",
    "        for s in scalars:\n",
    "            for i_s in s.input_scalars:\n",
    "                input_scalars.add(i_s)\n",
    "        \n",
    "        # the numberator of the partial derivative\n",
    "        out = sym.Matrix([x.poly for x in scalars])\n",
    "        \n",
    "\n",
    "        if input_entity is None:\n",
    "            j = out.jacobian([x.poly for x in input_scalars])\n",
    "        else:\n",
    "            \n",
    "            # In general it doesn't make sense to consider the max partial derivative over all inputs because we dont' want the \n",
    "            # Lipschiptz bound of the entire jacobian, we want the lipschitz bound with respect to entity \"i\" (see https://arxiv.org/abs/2008.11193).\n",
    "            # For example, if I had a polynomial y = a + b**2 + c**3 + d**4 where each a,b,c,d variable was from a different entity, \n",
    "            # the fact taht d has a big derivative should change the max lipschitz bound of y with respect to \"a\". Thus, we're only interested\n",
    "            # in searching for the maximum partial derivative with respect to the variables from the focus entity \"i\".\n",
    "            \n",
    "            # And if we're looking to compute the max parital derivative with respect to input scalars from only one entity, then\n",
    "            # we select only the variables corresponding to that entity here. \n",
    "            relevant_scalars = list(filter(lambda s:s.entity == input_entity, input_scalars))\n",
    "            relevant_inputs = [x.poly for x in relevant_scalars]\n",
    "            j = out.jacobian(relevant_inputs)\n",
    "            \n",
    "            # for higher order functions - it's possible that some of the partial derivatives are conditioned\n",
    "            # on data from the input entity. The philosophy of input DP is that when producing an epsilon\n",
    "            # guarantee for entity[i] that we don't need to search over the possible range of data for that entity\n",
    "            # but can instead use the data itself - this results in an epsilon for each entity which is private\n",
    "            # but it also means the bound is tighter. So we could skip this step but it would in some cases\n",
    "            # make the bound looser than it needs to be.\n",
    "            if data_dependent:\n",
    "                j = j.subs({x.poly:x.value for x in relevant_scalars})\n",
    "\n",
    "        neg_l2_j = -(np.sum(np.square(j)))**0.5\n",
    "        \n",
    "        if(len(np.sum(j).free_symbols) == 0):\n",
    "            result = -float(np.max(j))\n",
    "            return [result], neg_l2_j\n",
    "        \n",
    "        if(try_hessian_shortcut):\n",
    "            h = j.jacobian([x.poly for x in input_scalars])\n",
    "            if(len(solve(np.sum(h**2), *[x.poly for x in input_scalars], dict=True)) == 0):\n",
    "                print(\"The gradient is linear - solve through brute force search over edges of domain\")\n",
    "\n",
    "                i2s,s2i = create_lookup_tables_for_symbol(neg_l2_j)\n",
    "                search_fun = create_searchable_function(f=neg_l2_j, symbol2index=s2i)        \n",
    "\n",
    "                constant = 0.000001\n",
    "                rranges = [(x.min_val, x.max_val, x.max_val - x.min_val) for x in input_scalars]\n",
    "                skewed_results = optimize.brute(search_fun, rranges, finish=None, full_output=False)\n",
    "                result_inputs = skewed_results + constant\n",
    "                result_output = search_fun(result_inputs)\n",
    "                return [float(result_output)], neg_l2_j\n",
    "        \n",
    "        return flatten_and_maximize_poly(neg_l2_j), neg_l2_j    \n",
    "    \n",
    "\n",
    "def get_mechanism_for_entity(scalars, entity, sigma=1.5):\n",
    "    \n",
    "    m_id = \"ms_\"\n",
    "    for s in scalars:\n",
    "        m_id += str(s.id).split(\" \")[1][:-1]+\"_\"\n",
    "    \n",
    "    return iDPGaussianMechanism(sigma=sigma,\n",
    "                                value=np.sqrt(np.sum(np.square(np.array([float(s.value) for s in scalars])))),\n",
    "                                L=float(max_lipschitz_wrt_entity(scalars, entity=entity)),\n",
    "                                entity=entity.unique_name,\n",
    "                                name=m_id)\n",
    "\n",
    "def get_all_entity_mechanisms(scalars, sigma:float=1.5):\n",
    "    entities = set()\n",
    "    for s in scalars:\n",
    "        for i_s in s.input_scalars:\n",
    "            entities.add(i_s.entity)\n",
    "    return {e:[get_mechanism_for_entity(scalars=scalars,entity=e,sigma=sigma)] for e in entities}\n",
    "\n",
    "\n",
    "\n",
    "def publish(scalars, acc, sigma: float = 1.5) -> float:\n",
    "\n",
    "    acc_original = acc\n",
    "\n",
    "    acc_temp = deepcopy(acc_original)\n",
    "\n",
    "    ms = get_all_entity_mechanisms(scalars=scalars, sigma=sigma)\n",
    "    acc_temp.append(ms)\n",
    "\n",
    "    overbudgeted_entities = acc_temp.overbudgeted_entities\n",
    "\n",
    "    # so that we don't modify the original polynomial\n",
    "    # it might be fine to do so but just playing it safe\n",
    "    if len(overbudgeted_entities) > 0:\n",
    "        scalars = deepcopy(scalars)\n",
    "\n",
    "    while len(overbudgeted_entities) > 0:\n",
    "\n",
    "        input_scalars = set()\n",
    "        for s in scalars:\n",
    "            for i_s in s.input_scalars:\n",
    "                input_scalars.add(i_s)\n",
    "\n",
    "        for symbol in input_scalars:\n",
    "            if symbol.entity in overbudgeted_entities:\n",
    "                self.poly = self.poly.subs(symbol.poly, 0)\n",
    "                break\n",
    "\n",
    "        acc_temp = deepcopy(acc_original)\n",
    "\n",
    "        # get mechanisms for new publish event\n",
    "        ms = self.get_all_entity_mechanisms(sigma=sigma)\n",
    "        acc_temp.append(ms)\n",
    "\n",
    "        overbudgeted_entities = acc_temp.overbudgeted_entities\n",
    "\n",
    "    output = [s.value + random.gauss(0, sigma) for s in scalars]\n",
    "\n",
    "    acc_original.entity2ledger = deepcopy(acc_temp.entity2ledger)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class Scalar():\n",
    "        \n",
    "    def publish(self, acc, sigma: float = 1.5) -> float:\n",
    "        return publish([self], acc=acc, sigma=sigma)\n",
    "    \n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return \"<\"+str(type(self).__name__) + \": (\" + str(self.min_val)+\" < \"+str(self.value)+\" < \" + str(self.max_val) + \")>\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "\n",
    "class IntermediateScalar(Scalar):\n",
    "    \n",
    "    def __init__(self, poly, id=None):\n",
    "        self.poly = poly\n",
    "        self._gamma = None\n",
    "        self.id = id if id else UID()        \n",
    "    \n",
    "    def __rmul__(self, other: \"Scalar\") -> \"Scalar\":\n",
    "        return self * other\n",
    "\n",
    "    def __radd__(self, other: \"Scalar\") -> \"Scalar\":\n",
    "        return self + other\n",
    "    \n",
    "    @property\n",
    "    def input_scalars(self):\n",
    "        phi_scalars = list()\n",
    "        for ssid in self.input_polys:\n",
    "            phi_scalars.append(ssid2obj[str(ssid)])\n",
    "        return phi_scalars\n",
    "    \n",
    "    @property\n",
    "    def input_entities(self):\n",
    "        return list(set([x.entity for x in self.input_scalars]))\n",
    "    \n",
    "    @property\n",
    "    def input_polys(self):\n",
    "        return self.poly.free_symbols\n",
    "    \n",
    "    @property\n",
    "    def max_val(self):\n",
    "        return -flatten_and_maximize_poly(-self.poly)[-1].fun\n",
    "    @property\n",
    "    def min_val(self):\n",
    "        return flatten_and_maximize_poly(self.poly)[-1].fun\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.poly.subs({obj.poly:obj.value for obj in self.input_scalars})\n",
    "    \n",
    "class IntermediatePhiScalar(IntermediateScalar):\n",
    "\n",
    "    def __init__(self, poly, entity):\n",
    "        super().__init__(poly=poly)\n",
    "        self.entity = entity\n",
    "    \n",
    "    def max_lipschitz_wrt_entity(self, *args, **kwargs):\n",
    "        return self.gamma.max_lipschitz_wrt_entity(*args, **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def max_lipschitz(self):\n",
    "        return self.gamma.max_lipschitz\n",
    "    \n",
    "    def __mul__(self, other: \"Scalar\") -> \"Scalar\":\n",
    "\n",
    "        if isinstance(other, IntermediateGammaScalar):          \n",
    "            return self.gamma * other\n",
    "        \n",
    "        if not isinstance(other, IntermediatePhiScalar):\n",
    "            return IntermediatePhiScalar(poly=self.poly * other, entity=self.entity)\n",
    "            \n",
    "        # if other is referencing the same individual\n",
    "        if self.entity == other.entity:\n",
    "            return IntermediatePhiScalar(poly=self.poly * other.poly, entity=self.entity)\n",
    "\n",
    "        return self.gamma * other.gamma\n",
    "\n",
    "    def __add__(self, other: \"Scalar\") -> \"Scalar\":\n",
    "\n",
    "        if isinstance(other, IntermediateGammaScalar):\n",
    "            return self.gamma + other\n",
    "        \n",
    "        # if other is a public value\n",
    "        if not isinstance(other, Scalar):\n",
    "            return IntermediatePhiScalar(poly=self.poly + other, entity=self.entity)\n",
    "        \n",
    "        # if other is referencing the same individual\n",
    "        if self.entity == other.entity:\n",
    "            return IntermediatePhiScalar(poly=self.poly + other.poly, entity=self.entity)\n",
    "        \n",
    "        return self.gamma + other.gamma\n",
    "    \n",
    "    \n",
    "    def __sub__(self, other: \"Scalar\") -> \"Scalar\":\n",
    "\n",
    "        if isinstance(other, IntermediateGammaScalar):\n",
    "            return self.gamma - other\n",
    "        \n",
    "        # if other is a public value\n",
    "        if not isinstance(other, IntermediatePhiScalar):\n",
    "            return IntermediatePhiScalar(poly=self.poly - other, entity=self.entity)\n",
    "\n",
    "        # if other is referencing the same individual\n",
    "        if self.entity == other.entity:\n",
    "            return IntermediatePhiScalar(poly=self.poly - other.poly, entity=self.entity)\n",
    "\n",
    "        return self.gamma - other.gamma\n",
    "    \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        \n",
    "        if self._gamma is None:\n",
    "            self._gamma = GammaScalar(min_val=self.min_val,\n",
    "                               value=self.value,\n",
    "                               max_val=self.max_val,\n",
    "                               entity=self.entity)\n",
    "        return self._gamma\n",
    "    \n",
    "class OriginScalar(Scalar):\n",
    "    \"\"\"A scalar which stores the root polynomial values. When this is a superclass of\n",
    "    PhiScalar it represents data that was loaded in by a data owner. When this is a superclass\n",
    "    of GammaScalar this represents the node at which point data from mulitple entities was combined.\"\"\"\n",
    "    \n",
    "    def __init__(self, min_val, value, max_val, entity=None, id=None):\n",
    "\n",
    "        self.id = id if id else UID()\n",
    "        self._value = value\n",
    "        self._min_val = min_val\n",
    "        self._max_val = max_val\n",
    "        self.entity = entity if entity is not None else Entity()\n",
    "        \n",
    "    @property\n",
    "    def value(self):\n",
    "        return self._value\n",
    "        \n",
    "    @property\n",
    "    def max_val(self):\n",
    "        return self._max_val\n",
    "    \n",
    "    @property\n",
    "    def min_val(self):\n",
    "        return self._min_val\n",
    "    \n",
    "class PhiScalar(OriginScalar, IntermediatePhiScalar):\n",
    "    \"\"\"A scalar over data from a single entity\"\"\"\n",
    "    \n",
    "    def __init__(self, min_val, value, max_val, entity=None, id=None, ssid=None):\n",
    "        super().__init__(min_val=min_val, value=value, max_val=max_val, entity=entity,id=id)\n",
    "        \n",
    "        # the scalar string identifier (SSID) - because we're using polynomial libraries\n",
    "        # we need to be able to reference this object in string form. the library doesn't\n",
    "        # know how to process things that aren't strings\n",
    "        if ssid is None:\n",
    "            ssid = str(self.id).split(\" \")[1][:-1]# + \"_\" + str(self.entity.id).split(\" \")[1][:-1]\n",
    "            \n",
    "        self.ssid = ssid\n",
    "        \n",
    "        IntermediatePhiScalar.__init__(self, poly=symbols(self.ssid), entity=self.entity)\n",
    "        \n",
    "        ssid2obj[self.ssid] = self\n",
    "    \n",
    "    \n",
    "class IntermediateGammaScalar(IntermediateScalar):\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, Scalar):\n",
    "            if isinstance(other, IntermediatePhiScalar):\n",
    "                other = other.gamma\n",
    "            return IntermediateGammaScalar(poly=self.poly + other.poly)\n",
    "        return IntermediateGammaScalar(poly=self.poly + other)\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Scalar):\n",
    "            if isinstance(other, IntermediatePhiScalar):\n",
    "                other = other.gamma\n",
    "            return IntermediateGammaScalar(poly=self.poly - other.poly)\n",
    "        return IntermediateGammaScalar(poly=self.poly - other)    \n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Scalar):\n",
    "            if isinstance(other, IntermediatePhiScalar):\n",
    "                other = other.gamma\n",
    "            return IntermediateGammaScalar(poly=self.poly * other.poly)\n",
    "        return IntermediateGammaScalar(poly=self.poly * other)    \n",
    "    \n",
    "    def max_lipschitz_via_explicit_search(self, force_all_searches=False):\n",
    "\n",
    "        r1 = np.array([x.poly for x in self.input_scalars])\n",
    "\n",
    "        r2_diffs = np.array([GammaScalar(x.min_val,x.value,x.max_val, entity=x.entity).poly for x in self.input_scalars])\n",
    "        r2 = r1 + r2_diffs\n",
    "\n",
    "        fr1 = self.poly\n",
    "        fr2 = self.poly.copy().subs({x[0]:x[1] for x in list(zip(r1, r2))})\n",
    "\n",
    "        left = np.sum(np.square(fr1 - fr2)) ** 0.5\n",
    "        right = np.sum(np.square(r1 - r2)) ** 0.5\n",
    "\n",
    "        C = -left / right\n",
    "\n",
    "        i2s, s2i = create_lookup_tables_for_symbol(C)\n",
    "        search_fun = create_searchable_function(C, s2i)\n",
    "\n",
    "        r1r2diff_zip = list(zip(r1, r2_diffs))\n",
    "\n",
    "        s2range = {}\n",
    "        for _input_scalar, _additive_counterpart in r1r2diff_zip:\n",
    "\n",
    "            input_scalar = ssid2obj[_input_scalar.name]\n",
    "            additive_counterpart = ssid2obj[_additive_counterpart.name]\n",
    "\n",
    "            s2range[input_scalar.ssid] = (input_scalar.min_val, input_scalar.max_val)\n",
    "            s2range[additive_counterpart.ssid] = (input_scalar.min_val, input_scalar.max_val)\n",
    "\n",
    "        rranges = list()\n",
    "        for index,symbol in enumerate(i2s):\n",
    "            rranges.append(s2range[symbol])\n",
    "\n",
    "        r2_indices_list = list()\n",
    "        min_max_list = list()\n",
    "        for r2_val in r2:\n",
    "            r2_syms = [ssid2obj[x.name] for x in r2_val.free_symbols]\n",
    "            r2_indices = [s2i[x.ssid] for x in r2_syms]\n",
    "\n",
    "            r2_indices_list.append(r2_indices)\n",
    "            min_max_list.append((r2_syms[0].min_val, r2_syms[0].max_val))\n",
    "\n",
    "        functions = list()\n",
    "        for i in range(2):\n",
    "            f1 = lambda x,i=i: x[r2_indices_list[i][0]]+x[r2_indices_list[i][1]] + min_max_list[i][0]\n",
    "            f2 = lambda x,i=i: -(x[r2_indices_list[i][0]]+x[r2_indices_list[i][1]]) + min_max_list[i][1]\n",
    "\n",
    "            functions.append(f1)\n",
    "            functions.append(f2)\n",
    "\n",
    "        constraints = [{'type':'ineq', 'fun':f} for f in functions]\n",
    "\n",
    "        def non_negative_additive_terms(symbol_vector):\n",
    "            out = 0\n",
    "            for index in [s2i[x.name] for x in r2_diffs]:\n",
    "                out += (symbol_vector[index]**2)\n",
    "            # theres a small bit of rounding error from this constraint - this should\n",
    "            # only be used as a double check or as a backup!!!\n",
    "            return out**0.5 - 1/2**16 \n",
    "\n",
    "        constraints.append({'type':'ineq', 'fun':non_negative_additive_terms})\n",
    "        results = minimize_function(f=search_fun, rranges=rranges, constraints=constraints, force_all_searches=force_all_searches)\n",
    "        \n",
    "        return results, C\n",
    "\n",
    "    def max_lipschitz_via_jacobian(self, input_entity=None, data_dependent=True, force_all_searches=False, try_hessian_shortcut=False):\n",
    "        return max_lipschitz_via_jacobian(scalars=[self], input_entity=input_entity, data_dependent=data_dependent, force_all_searches=force_all_searches, try_hessian_shortcut=try_hessian_shortcut)  \n",
    "    \n",
    "    @property\n",
    "    def max_lipschitz(self):\n",
    "        result = self.max_lipschitz_via_jacobian()[0][-1]\n",
    "        if isinstance(result, float):\n",
    "            return -result\n",
    "        else:\n",
    "            return -float(result.fun)\n",
    "    \n",
    "    def max_lipschitz_wrt_entity(self, entity):\n",
    "        result = self.max_lipschitz_via_jacobian(input_entity=entity)[0][-1]\n",
    "        if isinstance(result, float):\n",
    "            return -result\n",
    "        else:\n",
    "            return -float(result.fun)\n",
    "    \n",
    "class GammaScalar(OriginScalar, IntermediateGammaScalar):\n",
    "    \"\"\"A scalar over data from multiple entities\"\"\"\n",
    "    \n",
    "    def __init__(self, min_val, value, max_val, entity=None, id=None, ssid=None):\n",
    "        super().__init__(min_val=min_val, value=value, max_val=max_val, entity=entity, id=id)\n",
    "        \n",
    "        # the scalar string identifier (SSID) - because we're using polynomial libraries\n",
    "        # we need to be able to reference this object in string form. the library doesn't\n",
    "        # know how to process things that aren't strings\n",
    "        if ssid is None:\n",
    "            ssid = str(self.id).split(\" \")[1][:-1] + \"_\" + str(self.entity.id).split(\" \")[1][:-1]\n",
    "            \n",
    "        self.ssid = ssid\n",
    "        \n",
    "        IntermediateGammaScalar.__init__(self, poly=symbols(self.ssid))\n",
    "        \n",
    "        ssid2obj[self.ssid] = self\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.core.adp.adversarial_accountant import AdversarialAccountant\n",
    "from syft.core.adp.entity import Entity\n",
    "from copy import deepcopy\n",
    "from syft.core.adp.idp_gaussian_mechanism import iDPGaussianMechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from typing import Dict as TypeDict\n",
    "from typing import KeysView as TypeKeysView\n",
    "from typing import List as TypeList\n",
    "from typing import Set as TypeSet\n",
    "\n",
    "# third party\n",
    "from autodp.autodp_core import Mechanism\n",
    "from autodp.transformer_zoo import Composition\n",
    "\n",
    "\n",
    "class AdversarialAccountant:\n",
    "    def __init__(self, max_budget: float = 10, delta: float = 1e-6) -> None:\n",
    "        self.entity2ledger: TypeDict[Entity, Mechanism] = {}\n",
    "        self.max_budget = max_budget\n",
    "        self.delta = delta\n",
    "\n",
    "    def append(self, entity2mechanisms: TypeDict[str, TypeList[Mechanism]]) -> None:\n",
    "        for key, ms in entity2mechanisms.items():\n",
    "            if key not in self.entity2ledger.keys():\n",
    "                self.entity2ledger[key] = list()\n",
    "            for m in ms:\n",
    "                self.entity2ledger[key].append(m)\n",
    "\n",
    "    def get_eps_for_entity(self, entity: Entity) -> Scalar:\n",
    "        # compose them with the transformation: compose.\n",
    "        compose = Composition()\n",
    "        mechanisms = self.entity2ledger[entity]\n",
    "        composed_mech = compose(mechanisms, [1] * len(mechanisms))\n",
    "\n",
    "        # Query for eps given delta\n",
    "        return PhiScalar(\n",
    "            value=composed_mech.get_approxDP(self.delta),\n",
    "            min_val=0,\n",
    "            max_val=self.max_budget,\n",
    "            entity=entity,\n",
    "        )\n",
    "\n",
    "    def has_budget(self, entity_name: str) -> bool:\n",
    "        eps = self.get_eps_for_entity(entity_name)\n",
    "        if eps.value is not None:\n",
    "            return eps.value < self.max_budget\n",
    "\n",
    "    @property\n",
    "    def entities(self) -> TypeKeysView[str]:\n",
    "        return self.entity2ledger.keys()\n",
    "\n",
    "    @property\n",
    "    def overbudgeted_entities(self) -> TypeSet[str]:\n",
    "        entities = set()\n",
    "\n",
    "        for ent in self.entities:\n",
    "            if not self.has_budget(ent):\n",
    "                entities.add(ent)\n",
    "\n",
    "        return entities\n",
    "\n",
    "    def print_ledger(self, delta: float = 1e-6) -> None:\n",
    "        for entity, mechanisms in self.entity2ledger.items():\n",
    "            print(str(entity) + \"\\t\" + str(self.get_eps_for_entity(entity)._value))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = PhiScalar(0,0.01,1)\n",
    "y = PhiScalar(0,0.02,1)\n",
    "z = PhiScalar(0,0.02,1)\n",
    "\n",
    "o = x*x + y*y + z\n",
    "z = o * o * o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sympy.core.basic.Basic'>\n",
      "<class 'sympy.core.expr.Expr'>\n",
      "<class 'sympy.core.power.Pow'>\n"
     ]
    }
   ],
   "source": [
    "for k in sym.class_registry.all_classes:\n",
    "    if (isinstance(z.poly, k)):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-250-d4ad1d48d340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'function' is not defined"
     ]
    }
   ],
   "source": [
    "from p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(z.poly, sym.Symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sympy.core.power.Pow"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(z.poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([     fun: -46.47853095774777\n",
       "      funl: array([-46.47853096, -46.14593642, -46.14593642, -45.72380853])\n",
       "   message: 'Optimization terminated successfully.'\n",
       "      nfev: 395\n",
       "       nit: 2\n",
       "     nlfev: 366\n",
       "     nlhev: 0\n",
       "     nljev: 35\n",
       "   success: True\n",
       "         x: array([0.62855774, 0.62855775, 1.        , 1.        , 0.37144225,\n",
       "         0.37144226])\n",
       "        xl: array([[0.62855774, 0.62855775, 1.        , 1.        , 0.37144225,\n",
       "          0.37144226],\n",
       "         [0.64833262, 0.5       , 1.        , 1.        , 0.5       ,\n",
       "          0.35166738],\n",
       "         [0.5       , 0.64833265, 1.        , 1.        , 0.35166735,\n",
       "          0.5       ],\n",
       "         [0.5       , 0.5       , 1.        , 1.        , 0.5       ,\n",
       "          0.5       ]])],\n",
       " -(4ae41313c3a941f4bc818d040bf5d1c6_7b7ecdd5de5d4f7da6215a8a97db5ab8**2 + 79584fa604334b038341109712bb3f32_f196c0224a6f4cc389c7f122ac2c9e8e**2 + dcad1295b5b345ab88856e2b91cc5146_16cf56ce29ef4408993ee71a0e51b9dd**2)**(-0.5)*(((69f93b3cedbf4087a607b592c7a0b711_f196c0224a6f4cc389c7f122ac2c9e8e + 7f150aa4dc774de59837714580f3656e_7b7ecdd5de5d4f7da6215a8a97db5ab8 + c73d8bc28f6b4c5881f976b8a657e61f_16cf56ce29ef4408993ee71a0e51b9dd)**3 - (4ae41313c3a941f4bc818d040bf5d1c6_7b7ecdd5de5d4f7da6215a8a97db5ab8 + 69f93b3cedbf4087a607b592c7a0b711_f196c0224a6f4cc389c7f122ac2c9e8e + 79584fa604334b038341109712bb3f32_f196c0224a6f4cc389c7f122ac2c9e8e + 7f150aa4dc774de59837714580f3656e_7b7ecdd5de5d4f7da6215a8a97db5ab8 + c73d8bc28f6b4c5881f976b8a657e61f_16cf56ce29ef4408993ee71a0e51b9dd + dcad1295b5b345ab88856e2b91cc5146_16cf56ce29ef4408993ee71a0e51b9dd)**3)**2)**0.5)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.max_lipschitz_via_explicit_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([     fun: -46.76537180435969\n",
       "      funl: array([-46.7653718])\n",
       "   message: 'Optimization terminated successfully.'\n",
       "      nfev: 14\n",
       "       nit: 2\n",
       "     nlfev: 5\n",
       "     nlhev: 0\n",
       "     nljev: 1\n",
       "   success: True\n",
       "         x: array([1., 1., 1.])\n",
       "        xl: array([[1., 1., 1.]])],\n",
       " -5.19615242270663*((69f93b3cedbf4087a607b592c7a0b711_f196c0224a6f4cc389c7f122ac2c9e8e + 7f150aa4dc774de59837714580f3656e_7b7ecdd5de5d4f7da6215a8a97db5ab8 + c73d8bc28f6b4c5881f976b8a657e61f_16cf56ce29ef4408993ee71a0e51b9dd)**4)**0.5)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.max_lipschitz_via_jacobian()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.9 ms ± 81.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "acc = AdversarialAccountant(max_budget=10)\n",
    "z.publish(acc=acc, sigma=0.2)\n",
    "z2.publish(acc=acc, sigma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 ms ± 124 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "acc = AdversarialAccountant(max_budget=10)\n",
    "\n",
    "publish([z,z2], acc=acc, sigma=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.lib.autograd.value import to_values\n",
    "from syft.lib.autograd.value import grad\n",
    "\n",
    "def make_entities(n=100):\n",
    "    ents = list()\n",
    "    for i in range(n):\n",
    "        ents.append(Entity(name=names.get_full_name().replace(\" \", \"_\")))\n",
    "    return ents\n",
    "\n",
    "\n",
    "def private(input_data, min_val, max_val, entities=None, is_discrete=False):\n",
    "\n",
    "    self = input_data\n",
    "    if entities is None:\n",
    "        flat_data = self.flatten()\n",
    "        entities = make_entities(n=len(flat_data))\n",
    "\n",
    "        scalars = list()\n",
    "        for i in flat_data:\n",
    "            value = max(min(float(i), max_val), min_val)\n",
    "            s = Scalar(\n",
    "                PhiScalar=value,\n",
    "                min_val=min_val,\n",
    "                max_val=max_val,\n",
    "                entity=entities[len(scalars)],\n",
    "#                 is_discrete=is_discrete\n",
    "            )\n",
    "            scalars.append(s)\n",
    "\n",
    "        return to_values(np.array(scalars)).reshape(input_data.shape)\n",
    "\n",
    "    elif isinstance(entities, list):\n",
    "        if len(entities) == len(self):\n",
    "            output_rows = list()\n",
    "            for row_i, row in enumerate(self):\n",
    "                row_of_entries = list()\n",
    "                for item in row.flatten():\n",
    "                    s = PhiScalar(\n",
    "                        value=item,\n",
    "                        min_val=min_val,\n",
    "                        max_val=max_val,\n",
    "                        entity=entities[row_i],\n",
    "#                         is_discrete=is_discrete\n",
    "                    )\n",
    "                    row_of_entries.append(s)\n",
    "                output_rows.append(np.array(row_of_entries).reshape(row.shape))\n",
    "            return to_values(np.array(output_rows)).reshape(self.shape)\n",
    "        else:\n",
    "            print(len(entities))\n",
    "            print(len(self))\n",
    "            raise Exception(\"len(entities) must equal len(self)\")\n",
    "\n",
    "\n",
    "class Tensor(np.ndarray):\n",
    "    def __new__(cls, input_array, min_val=None, max_val=None, entities=None, info=None, is_discrete=False):\n",
    "\n",
    "        is_private = False\n",
    "\n",
    "        if min_val is not None and max_val is not None:\n",
    "            input_array = private(\n",
    "                input_array, min_val=min_val, max_val=max_val, entities=entities, is_discrete=is_discrete\n",
    "            )\n",
    "            is_private = True\n",
    "        else:\n",
    "            input_array = to_values(input_array)\n",
    "\n",
    "        obj = np.asarray(input_array).view(cls)\n",
    "        obj.info = info\n",
    "        obj.is_private = is_private\n",
    "\n",
    "        return obj\n",
    "\n",
    "    def __array_finalize__(self, obj):\n",
    "        if obj is None:\n",
    "            return\n",
    "        self.info = getattr(obj, \"info\", None)\n",
    "        self.is_private = getattr(obj, \"is_private\", None)\n",
    "\n",
    "    def __array_wrap__(self, out_arr, context=None):\n",
    "        output = out_arr.view(Tensor)\n",
    "\n",
    "        is_private = False\n",
    "        if context is not None:\n",
    "            for arg in context[1]:\n",
    "                if hasattr(arg, \"is_private\") and arg.is_private:\n",
    "                    is_private = True\n",
    "        output.is_private = is_private\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        if self.shape == ():\n",
    "            return grad(self.flatten()[0])\n",
    "        else:\n",
    "            raise Exception(\"Can only call .backward() on single-value tensor.\")\n",
    "\n",
    "    @property\n",
    "    def grad(self):\n",
    "        grads = list()\n",
    "        for val in self.flatten().tolist():\n",
    "            grads.append(val._grad)\n",
    "        return Tensor(grads).reshape(self.shape)\n",
    "\n",
    "    def slow_publish(self, **kwargs):\n",
    "        grads = list()\n",
    "        for val in self.flatten().tolist():\n",
    "            grads.append(val.value.publish(**kwargs))\n",
    "        return np.array(grads).reshape(self.shape)\n",
    "    \n",
    "    def publish(self, **kwargs):\n",
    "        grads = list()\n",
    "        for val in self.flatten().tolist():\n",
    "            grads.append(val.value)\n",
    "        grads = publish(scalars=grads, **kwargs)\n",
    "        return np.array(grads).reshape(self.shape)    \n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        values = list()\n",
    "        for val in self.flatten().tolist():\n",
    "            if hasattr(val.value, \"value\"):\n",
    "                values.append(val.value.value)\n",
    "            else:\n",
    "                values.append(val.value)\n",
    "        return np.array(values).reshape(self.shape)\n",
    "\n",
    "    def private(self, min_val, max_val, entities=None, is_discrete=False):\n",
    "        if self.is_private:\n",
    "            raise Exception(\"Cannot call .private() on tensor which is already private\")\n",
    "\n",
    "        return Tensor(self.value, min_val=min_val, max_val=max_val, entities=entities, is_discrete=is_discrete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = AdversarialAccountant(max_budget=3000000)\n",
    "\n",
    "entities = [Entity(unique_name=\"Tudor\"), Entity(unique_name=\"Madhava\"), Entity(unique_name=\"Kritika\"), Entity(unique_name=\"George\")]\n",
    "\n",
    "x = Tensor(np.array([[1,1],[1,0],[0,1],[0,0]])).private(min_val=0, max_val=1, entities=entities, is_discrete=True)\n",
    "y = Tensor(np.array([[1],[1],[0],[0]])).private(min_val=0, max_val=1, entities=entities, is_discrete=False)\n",
    "\n",
    "_weights = Tensor(np.random.uniform(size=(2,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Entity:George>\t7.316032539740162\n",
      "<Entity:Tudor>\t7.316032539740162\n",
      "<Entity:Madhava>\t7.316032539740162\n",
      "<Entity:Kritika>\t7.316032539740162\n",
      "<Entity:George>\t9.520798620725053\n",
      "<Entity:Tudor>\t9.520798620725053\n",
      "<Entity:Madhava>\t9.520798620725053\n",
      "<Entity:Kritika>\t9.520798620725053\n",
      "<Entity:George>\t9.726133952597301\n",
      "<Entity:Tudor>\t9.726133952597301\n",
      "<Entity:Madhava>\t9.726133952597301\n",
      "<Entity:Kritika>\t9.726133952597301\n",
      "<Entity:George>\t10.043728493829402\n",
      "<Entity:Tudor>\t10.043728493829402\n",
      "<Entity:Madhava>\t10.043728493829402\n",
      "<Entity:Kritika>\t10.043728493829402\n",
      "<Entity:George>\t10.39935836365433\n",
      "<Entity:Tudor>\t10.39935836365433\n",
      "<Entity:Madhava>\t10.39935836365433\n",
      "<Entity:Kritika>\t10.39935836365433\n",
      "<Entity:George>\t10.413935084796428\n",
      "<Entity:Tudor>\t10.413935084796428\n",
      "<Entity:Madhava>\t10.413935084796428\n",
      "<Entity:Kritika>\t10.413935084796428\n",
      "<Entity:George>\t10.464816287127624\n",
      "<Entity:Tudor>\t10.464816287127624\n",
      "<Entity:Madhava>\t10.464816287127624\n",
      "<Entity:Kritika>\t10.464816287127624\n",
      "<Entity:George>\t11.409293745747654\n",
      "<Entity:Tudor>\t11.409293745747654\n",
      "<Entity:Madhava>\t11.409293745747654\n",
      "<Entity:Kritika>\t11.409293745747654\n",
      "<Entity:George>\t12.630258946985862\n",
      "<Entity:Tudor>\t12.630258946985862\n",
      "<Entity:Madhava>\t12.630258946985862\n",
      "<Entity:Kritika>\t12.630258946985862\n",
      "<Entity:George>\t12.728530248476105\n",
      "<Entity:Tudor>\t12.728530248476105\n",
      "<Entity:Madhava>\t12.728530248476105\n",
      "<Entity:Kritika>\t12.728530248476105\n"
     ]
    }
   ],
   "source": [
    "weights = _weights + 0\n",
    "acc = AdversarialAccountant(max_budget=3000000)\n",
    "\n",
    "for i in range(10):\n",
    "    batch_loss = 0\n",
    "\n",
    "    pred = x.dot(weights)\n",
    "    loss = np.mean(np.square(y-pred))\n",
    "    loss.backward()\n",
    "\n",
    "    weight_grad = (weights.grad * 0.5)\n",
    "    weight_grad = weight_grad.publish(acc=acc, sigma=0.1)\n",
    "\n",
    "    weights = weights - weight_grad\n",
    "    batch_loss += loss.value\n",
    "\n",
    "    acc.print_ledger()\n",
    "#     print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "564 ms ± 4.86 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
