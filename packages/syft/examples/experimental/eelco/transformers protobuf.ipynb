{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda3d784",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a292aab7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft import make_plan\n",
    "from syft.lib.python import String, List\n",
    "from syft import logger\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "sy.load('transformers')\n",
    "\n",
    "# Create client and test string\n",
    "alice = sy.VirtualMachine(name=\"alice\")\n",
    "alice_client = alice.get_client()\n",
    "\n",
    "serde = lambda x: x.send(alice_client).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650a253d",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = String(\"test\")\n",
    "string_ptr = string.send(alice_client)\n",
    "\n",
    "batch = List([1])\n",
    "batch_ptr = batch.send(alice_client)\n",
    "\n",
    "batch = List([\"test\"])\n",
    "batch_ptr = batch.send(alice_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed765790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "print(hf_tokenizer)\n",
    "\n",
    "tok_serde = serde(hf_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91c68482",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test batchencoding serde\n",
    "\n",
    "be_list = hf_tokenizer([\"test\", \"test2\"], padding=True, truncation=True, max_length=100)\n",
    "be_serde = serde(be_list)\n",
    "assert be_serde == be_list\n",
    "\n",
    "be_tensor = hf_tokenizer([\"test\", \"test2\"], padding=True, return_tensors='pt', truncation=True, max_length=100)\n",
    "be_serde = serde(be_tensor)\n",
    "for k, v in be_tensor.items():\n",
    "    assert torch.eq(v, be_serde[k]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d515d087",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# syft relative\n",
    "from syft.proto.lib.transformers.tokenizerfast_pb2 import TokenizerFast as TokenizerFast_PB\n",
    "from syft.generate_wrapper import GenerateWrapper\n",
    "from syft.lib.python.primitive_factory import PrimitiveFactory\n",
    "from syft import serialize, deserialize\n",
    "from syft.lib.python.util import upcast\n",
    "\n",
    "# Third party\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "def object2proto(obj: PreTrainedTokenizerFast) -> TokenizerFast_PB:\n",
    "    # TODO Tokenizer.to_str serializes to a json string.\n",
    "    # Protobuf has protobuf.json_format, which might be better than sending a raw string.\n",
    "\n",
    "    tokenizer_str = obj._tokenizer.to_str()\n",
    "    tokenizer_str = PrimitiveFactory.generate_primitive(value=tokenizer_str)\n",
    "    \n",
    "    kwargs = obj.special_tokens_map\n",
    "    kwargs[\"name_or_path\"] = obj.name_or_path\n",
    "    kwargs[\"padding_side\"] = obj.padding_side\n",
    "    kwargs[\"model_max_length\"] = obj.model_max_length\n",
    "    kwargs = PrimitiveFactory.generate_primitive(value=kwargs)\n",
    "\n",
    "    protobuf_tokenizer = TokenizerFast_PB(\n",
    "        id=kwargs.id._object2proto(),\n",
    "        tokenizer=tokenizer_str._object2proto(),\n",
    "        kwargs=kwargs._object2proto()\n",
    "    )\n",
    "    return protobuf_tokenizer\n",
    "\n",
    "def proto2object(proto: TokenizerFast_PB) -> PreTrainedTokenizerFast:\n",
    "    _tokenizer = Tokenizer.from_str(proto.tokenizer.data)\n",
    "    kwargs = deserialize(proto.kwargs)\n",
    "    kwargs = upcast(kwargs)\n",
    "    \n",
    "    tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=_tokenizer,\n",
    "        **kwargs\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d21411f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='left', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proto = object2proto(hf_tokenizer)\n",
    "proto2object(proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea1f490b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unk_token': '[UNK]',\n",
       " 'sep_token': '[SEP]',\n",
       " 'pad_token': '[PAD]',\n",
       " 'cls_token': '[CLS]',\n",
       " 'mask_token': '[MASK]'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c4f1ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_from_pretrained',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_push_to_hub',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'clean_up_tokenization',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'describe',\n",
       " 'description',\n",
       " 'do_lower_case',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'id',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'max_model_input_sizes',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_init_configuration',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'send',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'tag',\n",
       " 'tags',\n",
       " 'tokenize',\n",
       " 'truncate_sequences',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(hf_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "724014ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from syft.lib.python.util import upcast\n",
    "string = String(\"test\")\n",
    "type(upcast(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5239023",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
