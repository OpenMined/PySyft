{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a430f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "import time\n",
    "\n",
    "import syft as sy\n",
    "from syft import VirtualMachine\n",
    "from syft.core.plan.plan_builder import PLAN_BUILDER_VM, make_plan, build_plan_inputs, ROOT_CLIENT\n",
    "from syft.lib.python.collections.ordered_dict import OrderedDict\n",
    "from syft.lib.python.list import List\n",
    "from syft import logger\n",
    "from syft import SyModule, SySequential\n",
    "\n",
    "# transformers imports, not needed in AST\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertConfig, create_sinusoidal_embeddings\n",
    "\n",
    "# Add in AST\n",
    "from transformers.activations import gelu\n",
    "\n",
    "logger.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf054b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client\n",
    "alice = sy.VirtualMachine(name=\"alice\")\n",
    "alice_client = alice.get_client()\n",
    "remote_torch = ROOT_CLIENT.torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361500ac",
   "metadata": {},
   "source": [
    "# DistilBERT\n",
    "\n",
    "- [x] PretrainedTokenizerFast\n",
    "- [ ] DistilBertModel\n",
    "    - [x] Embedding\n",
    "    - [x] MultiHeadSelfAttention\n",
    "    - [x] FFN\n",
    "        - [ ] Chunking\n",
    "    - [x] TransformerBlock\n",
    "    - [X] Transformer\n",
    "        - [ ] SyModuleList\n",
    "- [ ] Load pretrained weights\n",
    "\n",
    "### TODO\n",
    "- Defining inputs isn't a great interface. \n",
    "    - Get inspiration from other libraries that pre-define inputs.\n",
    "      Keras: https://keras.io/api/layers/core_layers/input/\n",
    "    - Defining tensor: full control\n",
    "    - defining dummy Input(shape, dtype, ..), easier use and closer to other APIs.\n",
    "- support for output_shape or make_dummy_output in SyModule would be nice for nesting modules:\n",
    "```\n",
    "class Net(SyModule):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # Simple case\n",
    "        self.layer1 = SyLayer(input_shape=kwargs[\"input_shape\"])\n",
    "        self.layer2 = SyLayer(input_shape=self.layer1.output_shape)\n",
    "        \n",
    "        # Advanced case\n",
    "        self.layer1 = SyLayer(inputs=kwargs['inputs])\n",
    "        self.layer2 = SyLayer(inputs=self.layer1.make_dummy_output())\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6b2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_x = Input(\n",
    "    shape=(-1, -1, 10),\n",
    "    dtype=torch.long,\n",
    "    sparse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ed42fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.sparse.Embedding"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Small config for testing\n",
    "vocab_size = 10\n",
    "dim = 256\n",
    "max_length = 100\n",
    "n_heads = 2\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    dim=dim,\n",
    "    max_position_embeddings=max_length,\n",
    "    n_heads=n_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    ")\n",
    "\n",
    "model = DistilBertModel(config)\n",
    "type(model.get_input_embeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1fe6d",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "https://github.com/huggingface/transformers/blob/master/src/transformers/models/distilbert/modeling_distilbert.py#L82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012ab5d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Embeddings(SyModule):\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.dim, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.dim)\n",
    "        if config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                n_pos=config.max_position_embeddings, dim=config.dim, out=self.position_embeddings.weight\n",
    "            )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        seq_length = input_ids.size(1)\n",
    "        # TODO setting device from input_ids from remotely created tensor throws KeyError: UID <...>.\n",
    "        position_ids = remote_torch.arange(seq_length)  # (max_seq_length)\n",
    "        position_ids = remote_torch.unsqueeze(position_ids, 0).expand_as(input_ids)  # (bs, max_seq_length)\n",
    "        \n",
    "        word_embeddings = self.word_embeddings(input_ids)  # (bs, max_seq_length, dim)\n",
    "        position_embeddings = self.position_embeddings(input_ids)  # (bs, max_seq_length, dim)\n",
    "\n",
    "        embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
    "        embeddings = self.LayerNorm(embeddings)  # (bs, max_seq_length, dim)\n",
    "        embeddings = self.dropout(embeddings)  # (bs, max_seq_length, dim)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "# dummy_x = torch.ones(10, 100).long()\n",
    "# sy_embedding = Embeddings(config, inputs={'input_ids': dummy_x})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "431b09ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# @make_plan\n",
    "# def train(model=sy_embedding, x=dummy_x):\n",
    "#     opt = remote_torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0)\n",
    "#     out = model(input_ids=x)\n",
    "#     loss = remote_torch.mean(out[0])\n",
    "#     loss.backward()\n",
    "#     opt.step()\n",
    "#     return [model]\n",
    "\n",
    "# dummy_x = torch.LongTensor(5, 80).random_(10)\n",
    "\n",
    "\n",
    "# train_ptr = train.send(alice_client)\n",
    "# model_ptr = train_ptr(model=sy_embedding, x=torch.LongTensor(100, 90).random_(10))\n",
    "# updated_model = model_ptr.get()[0]\n",
    "# print(updated_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a9410",
   "metadata": {},
   "source": [
    "## MultiHeadSelfAttention\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/master/src/transformers/models/distilbert/modeling_distilbert.py#L116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85872ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from syft.lib.python.int import Int\n",
    "\n",
    "class MultiHeadSelfAttention(SyModule):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.dim = config.dim\n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "\n",
    "        assert self.dim % self.n_heads == 0\n",
    "\n",
    "        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def shape(self, x, bs, dim_per_head):\n",
    "            \"\"\"separate heads for linear layers\"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "        \n",
    "    def unshape(self, x, bs, dim_per_head):\n",
    "        \"\"\"group heads\"\"\"\n",
    "        return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            query: torch.tensor(bs, seq_length, dim)\n",
    "            key: torch.tensor(bs, seq_length, dim)\n",
    "            value: torch.tensor(bs, seq_length, dim)\n",
    "            mask: torch.tensor(bs, seq_length)\n",
    "        Returns:\n",
    "            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,\n",
    "        \"\"\"\n",
    "        bs = query.size(0)\n",
    "        q_length = query.size(1)\n",
    "        dim = query.size(2)\n",
    "        k_length = key.size(1)\n",
    "        \n",
    "        dim_per_head = self.dim // self.n_heads\n",
    "        mask_reshp = (bs, 1, 1, k_length)\n",
    "        \n",
    "\n",
    "        q = self.shape(self.q_lin(query), bs, dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        k = self.shape(self.k_lin(key), bs, dim_per_head)  # (bs, n_heads, k_length, dim_per_head)\n",
    "        v = self.shape(self.v_lin(value), bs, dim_per_head)  # (bs, n_heads, k_length, dim_per_head)\n",
    "\n",
    "        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = remote_torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\n",
    "        mask = (mask == 0).view(*mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)\n",
    "                \n",
    "        scores.masked_fill_(mask, -float(\"inf\"))  # (bs, n_heads, q_length, k_length)\n",
    "        weights = remote_torch.softmax(scores, dim=-1) # (bs, n_heads, q_length, k_length)\n",
    "        weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n",
    "        \n",
    "        context = remote_torch.matmul(weights, v)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        context = self.unshape(context, bs, dim_per_head)  # (bs, q_length, dim)\n",
    "        context = self.out_lin(context)  # (bs, q_length, dim)\n",
    "\n",
    "        return context\n",
    "\n",
    "# dummy inputs\n",
    "bs = 10\n",
    "seq_length = 15\n",
    "dim = config.dim\n",
    "\n",
    "dummy_q = torch.randn(bs, seq_length, dim)\n",
    "dummy_k = torch.randn(bs, seq_length, dim)\n",
    "dummy_v = torch.randn(bs, seq_length, dim)\n",
    "dummy_mask = torch.ones([bs, seq_length], dtype=torch.long)\n",
    "dummy_inputs = {\n",
    "    \"query\": dummy_q,\n",
    "    \"key\": dummy_k,\n",
    "    \"value\": dummy_v,\n",
    "    \"mask\": dummy_mask\n",
    "}\n",
    "\n",
    "# sy_mhsa = MultiHeadSelfAttention(config, inputs=dummy_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abea5a05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# @make_plan\n",
    "# def train(model=sy_mhsa, query=dummy_q, key=dummy_k, value=dummy_v, mask=dummy_mask):\n",
    "#     opt = remote_torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0)\n",
    "#     out = model(query=query,\n",
    "#                 key=key,\n",
    "#                 value=value,\n",
    "#                 mask=mask)\n",
    "#     loss = remote_torch.mean(out[0])\n",
    "#     loss.backward()\n",
    "#     opt.step()\n",
    "#     return [model]\n",
    "\n",
    "# q = torch.randn(bs, seq_length, dim)\n",
    "# k = torch.randn(bs, seq_length, dim)\n",
    "# v = torch.randn(bs, seq_length, dim)\n",
    "# mask = torch.ones([bs, seq_length], dtype=torch.long)\n",
    "# batch = {\n",
    "#     \"query\": q,\n",
    "#     \"key\": k,\n",
    "#     \"value\": v,\n",
    "#     \"mask\": mask\n",
    "# }\n",
    "\n",
    "# train_ptr = train.send(alice_client)\n",
    "# model_ptr = train_ptr(model=sy_mhsa, **batch)\n",
    "# updated_model = model_ptr.get()[0]\n",
    "# print(updated_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ccf67",
   "metadata": {},
   "source": [
    "## FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "593cbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(SyModule):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        # self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.lin1 = nn.Linear(in_features=config.dim, out_features=config.hidden_dim)\n",
    "        self.lin2 = nn.Linear(in_features=config.hidden_dim, out_features=config.dim)\n",
    "        # TODO transformers.activations.gelu in AST\n",
    "        # NOTE torch F.gelu has a slightly different implementation, can't use that one.\n",
    "        # assert config.activation in [\"relu\", \"gelu\"], f\"activation ({config.activation}) must be in ['relu', 'gelu']\"\n",
    "        # self.activation = gelu if config.activation == \"gelu\" else nn.ReLU()\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO: Chunking\n",
    "        # return apply_chunking_to_forward(self.ff_chunk, self.chunk_size_feed_forward, self.seq_len_dim, input)\n",
    "        x = self.lin1(input)\n",
    "        x = self.activation(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# bs = 10\n",
    "# seq_length = 15\n",
    "# dim = config.dim\n",
    "\n",
    "# dummy_input = torch.randn(bs, seq_length, dim)\n",
    "# ffn_inputs = {\n",
    "#     \"input\": dummy_input\n",
    "# }\n",
    "\n",
    "# sy_FFN = FFN(config, inputs=ffn_inputs)\n",
    "# print(sy_FFN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2043b26b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# @make_plan\n",
    "# def train(model=sy_FFN, input=dummy_input):\n",
    "#     opt = remote_torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0)\n",
    "#     out = model(input=input)\n",
    "#     loss = remote_torch.mean(out[0])\n",
    "#     loss.backward()\n",
    "#     opt.step()\n",
    "#     return [model]\n",
    "\n",
    "# bx = torch.randn(bs, seq_length, dim)\n",
    "\n",
    "# train_ptr = train.send(alice_client)\n",
    "# model_ptr = train_ptr(model=sy_FFN, input=bx)\n",
    "# updated_model = model_ptr.get()[0]\n",
    "# print(updated_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26507ad8",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ff606d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(SyModule):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert config.dim % config.n_heads == 0\n",
    "        \n",
    "        attn_dummy_inputs = {\n",
    "            \"query\": kwargs[\"inputs\"][\"x\"],\n",
    "            \"key\": kwargs[\"inputs\"][\"x\"],\n",
    "            \"value\": kwargs[\"inputs\"][\"x\"],\n",
    "            \"mask\": kwargs[\"inputs\"][\"attn_mask\"],\n",
    "        }\n",
    "        self.attention = MultiHeadSelfAttention(config, inputs=attn_dummy_inputs)\n",
    "        \n",
    "        ffn_dummy_inputs = {\n",
    "            \"input\": kwargs[\"inputs\"][\"x\"]\n",
    "        }\n",
    "        self.ffn = FFN(config, inputs=ffn_dummy_inputs)\n",
    "        \n",
    "        self.sa_layer_norm = nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)\n",
    "        self.output_layer_norm = nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)\n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        # Self-Attention\n",
    "        sa_output = self.attention(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            mask=attn_mask\n",
    "        )[0]\n",
    "\n",
    "        sa_output = self.sa_layer_norm(sa_output + x)  # (bs, seq_length, dim)\n",
    "\n",
    "        # Feed Forward Network\n",
    "        ffn_output = self.ffn(input=sa_output)[0]  # (bs, seq_length, dim)\n",
    "        ffn_output = self.output_layer_norm(ffn_output + sa_output)  # (bs, seq_length, dim)\n",
    "\n",
    "        return ffn_output\n",
    "\n",
    "class Transformer(SyModule):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        \n",
    "        t_init_0 = time.time()\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.n_layers = config.n_layers\n",
    "        \n",
    "        # TODO replace this with SyModuleList\n",
    "        for i in range(config.n_layers):\n",
    "            setattr(self, f\"layer_{i}\", TransformerBlock(config, inputs=kwargs[\"inputs\"]))\n",
    "            \n",
    "        print(\"transformer init:\", time.time() - t_init_0)\n",
    "            \n",
    "            \n",
    "    \n",
    "    def forward(self, x, attn_mask):\n",
    "        \"\"\"\n",
    "        - SyModule does not work; multiple inputs\n",
    "        - getattr does not work; \n",
    "        \"\"\"\n",
    "        f_time = time.time()\n",
    "        hidden_state = x\n",
    "        \n",
    "        # TODO SyModuleList\n",
    "        for i in range(self.n_layers):\n",
    "            layer = getattr(self, f\"layer_{i}\")\n",
    "            hidden_state = layer(\n",
    "                x=hidden_state,\n",
    "                attn_mask=attn_mask\n",
    "            )[0]\n",
    "            \n",
    "        print(\"forward plan time\", time.time() - f_time)\n",
    "        return hidden_state\n",
    "\n",
    "bs = 10\n",
    "seq_length = 15\n",
    "dim = config.dim\n",
    "\n",
    "dummy_x = torch.randn(bs, seq_length, dim)\n",
    "dummy_mask = torch.ones([bs, seq_length], dtype=torch.long)\n",
    "transformer_dummy_inputs = {\n",
    "    \"x\": dummy_x,\n",
    "    \"attn_mask\": dummy_mask\n",
    "}\n",
    "# sy_transformer = Transformer(config, inputs=transformer_dummy_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f545f3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# @make_plan\n",
    "# def train(model=sy_transformer, x=dummy_x, attn_mask=dummy_mask):\n",
    "#     opt = remote_torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0)\n",
    "#     out = model(x=x,\n",
    "#                 attn_mask=attn_mask)\n",
    "#     loss = remote_torch.mean(out[0][0])\n",
    "#     loss.backward()\n",
    "#     opt.step()\n",
    "#     return [model]\n",
    "\n",
    "# x = torch.randn(bs, seq_length, dim)\n",
    "# mask = torch.ones([bs, seq_length], dtype=torch.long)\n",
    "# batch = {\n",
    "#     \"x\": x,\n",
    "#     \"attn_mask\": mask\n",
    "# }\n",
    "\n",
    "# train_ptr = train.send(alice_client)\n",
    "# model_ptr = train_ptr(model=sy_transformer, **batch)\n",
    "# updated_model = model_ptr.get()[0]\n",
    "# print(updated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8952e63b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SyDistilBert(SyModule):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings\n",
    "        embedding_inputs = {\n",
    "            'input_ids': kwargs['inputs']['input_ids']\n",
    "        }\n",
    "        self.embeddings = Embeddings(config=config, inputs=embedding_inputs)\n",
    "        \n",
    "        # Transformer\n",
    "        transformer_x = torch.rand(*kwargs['inputs']['input_ids'].size(), config.dim)\n",
    "        transformer_mask = kwargs['inputs']['attention_mask']\n",
    "        transformer_inputs = {\n",
    "            \"x\": transformer_x,\n",
    "            \"attn_mask\": transformer_mask\n",
    "        }\n",
    "        \n",
    "        self.transformer = Transformer(config=config,\n",
    "                                       inputs=transformer_inputs)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        input_embeds = self.embeddings(input_ids=input_ids)[0]\n",
    "        out = self.transformer(x=input_embeds,\n",
    "                               attn_mask=attention_mask)[0]\n",
    "        return out\n",
    "\n",
    "# dummy_x = torch.ones(1, 1, dtype=torch.long)\n",
    "# dummy_mask = torch.ones(1, 1, dtype=torch.long)\n",
    "# sydistilbert = SyDistilBert(config, inputs={'input_ids': dummy_x, 'attention_mask': dummy_mask})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41436a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# @make_plan\n",
    "# def train(model=sydistilbert, x=dummy_x, attn_mask=dummy_mask):\n",
    "#     opt = remote_torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0)\n",
    "#     out = model(input_ids=x,\n",
    "#                 attention_mask=dummy_mask)\n",
    "#     loss = remote_torch.mean(out[0][0])\n",
    "#     loss.backward()\n",
    "#     opt.step()\n",
    "#     return [model]\n",
    "\n",
    "# x = torch.ones([bs, seq_length], dtype=torch.long)\n",
    "# mask = torch.ones([bs, seq_length], dtype=torch.long)\n",
    "# batch = {\n",
    "#     \"x\": x,\n",
    "#     \"attn_mask\": mask\n",
    "# }\n",
    "\n",
    "# train_ptr = train.send(alice_client)\n",
    "# model_ptr = train_ptr(model=sydistilbert, **batch)\n",
    "# updated_model = model_ptr.get()[0]\n",
    "# print(updated_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f16a7ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertModel(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        )\n",
      "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "distilbert_model = AutoModel.from_pretrained(model_name)\n",
    "print(distilbert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49fa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "transformer init: 427.7230200767517\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "forward plan time 571.7766692638397\n"
     ]
    }
   ],
   "source": [
    "dummy_x = torch.ones(1, 2, dtype=torch.long)\n",
    "dummy_mask = torch.ones(1, 2, dtype=torch.long)\n",
    "sydistilbert_model = SyDistilBert(distilbert_model.config, inputs={'input_ids': dummy_x, 'attention_mask': dummy_mask})\n",
    "sydistilbert_model.load_state_dict(distilbert_model.load_state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4714762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert.modeling_distilbert import Embeddings, Transformer, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7986bae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "239px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
