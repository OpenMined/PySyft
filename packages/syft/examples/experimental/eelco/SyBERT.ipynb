{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a430f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import syft as sy\n",
    "from syft import VirtualMachine\n",
    "from syft.core.plan.plan_builder import PLAN_BUILDER_VM, make_plan, build_plan_inputs, ROOT_CLIENT\n",
    "from syft.lib.python.collections.ordered_dict import OrderedDict\n",
    "from syft.lib.python.list import List\n",
    "from syft import logger\n",
    "from syft import SyModule, SySequential\n",
    "\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertConfig, create_sinusoidal_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf054b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client\n",
    "alice = sy.VirtualMachine(name=\"alice\")\n",
    "alice_client = alice.get_client()\n",
    "remote_torch = ROOT_CLIENT.torch\n",
    "\n",
    "vocab_size = 10\n",
    "dim = 256\n",
    "max_length = 100\n",
    "n_heads = 2\n",
    "config = DistilBertConfig(vocab_size=vocab_size, dim=dim, max_position_embeddings=max_length, n_heads=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361500ac",
   "metadata": {},
   "source": [
    "# DistilSyBERT\n",
    "\n",
    "- [x] PretrainedTokenizerFast\n",
    "- [ ] DistilBertModel\n",
    "    - [x] Embedding\n",
    "    - [ ] MultiHeadSelfAttention\n",
    "    - [ ] FFN\n",
    "    - [ ] TransformerBlock\n",
    "    - [ ] Transformer\n",
    "* Load pretrained weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1fe6d",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "https://github.com/huggingface/transformers/blob/master/src/transformers/models/distilbert/modeling_distilbert.py#L82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "012ab5d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Implementation\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertConfig, create_sinusoidal_embeddings\n",
    "\n",
    "class Embedding(SyModule):\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.dim, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.dim)\n",
    "        if config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                n_pos=config.max_position_embeddings, dim=config.dim, out=self.position_embeddings.weight\n",
    "            )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        seq_length = input_ids.size(1)\n",
    "        # TODO setting device from input_ids from remotely created tensor throws KeyError: UID <...>.\n",
    "        position_ids = remote_torch.arange(seq_length)  # (max_seq_length)\n",
    "        position_ids = remote_torch.unsqueeze(position_ids, 0).expand_as(input_ids)  # (bs, max_seq_length)\n",
    "        \n",
    "        word_embeddings = self.word_embeddings(input_ids)  # (bs, max_seq_length, dim)\n",
    "        position_embeddings = self.position_embeddings(input_ids)  # (bs, max_seq_length, dim)\n",
    "\n",
    "        embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
    "        embeddings = self.LayerNorm(embeddings)  # (bs, max_seq_length, dim)\n",
    "        embeddings = self.dropout(embeddings)  # (bs, max_seq_length, dim)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "431b09ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-05-28T15:03:38.154265+0200][CRITICAL][logger]][12992] <class 'syft.core.store.store_memory.MemoryStore'> __delitem__ error <UID: 665bc3a9eb894f24999fb4a7a0954284>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "RECOMPILING\n",
      "Embedding(\n",
      "  (word_embeddings): Embedding(10, 256, padding_idx=0)\n",
      "  (position_embeddings): Embedding(100, 256)\n",
      "  (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "sy_embedding = Embedding(config, input_size=(10, 100), input_dtype=torch.long)\n",
    "\n",
    "dummy_x = torch.LongTensor(5, 80).random_(10)\n",
    "\n",
    "@make_plan\n",
    "def train(model=sy_embedding, x=dummy_x):\n",
    "    opt = remote_torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0)\n",
    "    out = model(input_ids=x)\n",
    "    loss = remote_torch.mean(out[0])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return [model]\n",
    "\n",
    "train_ptr = train.send(alice_client)\n",
    "model_ptr = train_ptr(model=sy_embedding, x=torch.LongTensor(100, 90).random_(10))\n",
    "updated_model = model_ptr.get()[0]\n",
    "print(updated_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a9410",
   "metadata": {},
   "source": [
    "## MultiHeadSelfAttention\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/master/src/transformers/models/distilbert/modeling_distilbert.py#L116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85872ef3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "SyModule needs `input_size`: Tuple(Int) as kwarg to trace the forward plan.Also, make sure to call **super().__init__(**kwargs)** in ALL your SyModules",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5f793afc9278>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mMultiHeadSelfAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\eelco\\documents\\projects\\pysyft\\packages\\syft\\src\\syft\\lib\\torch\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    537\u001b[0m         \u001b[1;31m# TODO: check if contains input_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 539\u001b[1;33m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_forward_plan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    540\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eelco\\documents\\projects\\pysyft\\packages\\syft\\src\\syft\\lib\\torch\\module.py\u001b[0m in \u001b[0;36m_make_forward_plan\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    576\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Missing .forward() method for Module\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_forward_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilding_forward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eelco\\documents\\projects\\pysyft\\packages\\syft\\src\\syft\\lib\\torch\\module.py\u001b[0m in \u001b[0;36m_get_forward_inputs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m         \"\"\"\n\u001b[1;32m--> 684\u001b[1;33m         \u001b[0minput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_inp_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    685\u001b[0m         \u001b[0minput_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_inp_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[0minp_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_inp_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\eelco\\documents\\projects\\pysyft\\packages\\syft\\src\\syft\\lib\\torch\\module.py\u001b[0m in \u001b[0;36m_get_inp_size\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    654\u001b[0m         ):\n\u001b[1;32m--> 655\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    656\u001b[0m                 \u001b[1;34m\"SyModule needs `input_size`: Tuple(Int) as kwarg to trace the forward plan.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m                 \u001b[1;34m\"Also, make sure to call **super().__init__(**kwargs)** in ALL your SyModules\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: SyModule needs `input_size`: Tuple(Int) as kwarg to trace the forward plan.Also, make sure to call **super().__init__(**kwargs)** in ALL your SyModules"
     ]
    }
   ],
   "source": [
    "class MultiHeadSelfAttention(SyModule):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.dim = config.dim\n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "\n",
    "        assert self.dim % self.n_heads == 0\n",
    "\n",
    "        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            query: torch.tensor(bs, seq_length, dim)\n",
    "            key: torch.tensor(bs, seq_length, dim)\n",
    "            value: torch.tensor(bs, seq_length, dim)\n",
    "            mask: torch.tensor(bs, seq_length)\n",
    "        Returns:\n",
    "            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,\n",
    "        \"\"\"\n",
    "        \n",
    "        bs, q_length, dim = query.size()\n",
    "        k_length = key.size(1)\n",
    "\n",
    "\n",
    "        dim_per_head = self.dim // self.n_heads\n",
    "\n",
    "        mask_reshp = (bs, 1, 1, k_length)\n",
    "\n",
    "        def shape(x):\n",
    "            \"\"\"separate heads\"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "\n",
    "        def unshape(x):\n",
    "            \"\"\"group heads\"\"\"\n",
    "            return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "\n",
    "        q = shape(self.q_lin(query))  # (bs, n_heads, q_length, dim_per_head)\n",
    "        k = shape(self.k_lin(key))  # (bs, n_heads, k_length, dim_per_head)\n",
    "        v = shape(self.v_lin(value))  # (bs, n_heads, k_length, dim_per_head)\n",
    "\n",
    "        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\n",
    "        mask = (mask == 0).view(mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)\n",
    "        scores.masked_fill_(mask, -float(\"inf\"))  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "        weights = nn.Softmax(dim=-1)(scores)  # (bs, n_heads, q_length, k_length)\n",
    "        weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n",
    "\n",
    "        context = torch.matmul(weights, v)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        context = unshape(context)  # (bs, q_length, dim)\n",
    "        context = self.out_lin(context)  # (bs, q_length, dim)\n",
    "\n",
    "        return (context,)\n",
    "    \n",
    "MultiHeadSelfAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abea5a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"activation\": \"gelu\",\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 256,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"max_position_embeddings\": 100,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"transformers_version\": \"4.6.1\",\n",
       "  \"vocab_size\": 10\n",
       "}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f96703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
