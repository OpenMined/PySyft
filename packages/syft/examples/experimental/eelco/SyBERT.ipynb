{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a430f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "import syft as sy\n",
    "from syft import VirtualMachine\n",
    "from syft.core.plan.plan_builder import PLAN_BUILDER_VM, make_plan, build_plan_inputs, ROOT_CLIENT\n",
    "from syft.lib.python.collections.ordered_dict import OrderedDict\n",
    "from syft.lib.python.list import List\n",
    "from syft import logger\n",
    "from syft import SyModule\n",
    "\n",
    "# transformers imports, not needed in AST\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertConfig, create_sinusoidal_embeddings\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Add in AST\n",
    "from transformers.activations import gelu\n",
    "\n",
    "logger.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf054b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client\n",
    "alice = sy.VirtualMachine(name=\"alice\")\n",
    "alice_client = alice.get_client()\n",
    "remote_torch = ROOT_CLIENT.torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361500ac",
   "metadata": {},
   "source": [
    "# DistilBERT\n",
    "\n",
    "- [x] PretrainedTokenizerFast\n",
    "- [ ] DistilBertModel\n",
    "    - [x] Embedding\n",
    "    - [x] MultiHeadSelfAttention\n",
    "        - [ ] gelu in AST (torch._C issue)\n",
    "    - [x] FFN\n",
    "        - [ ] Chunking\n",
    "    - [x] TransformerBlock\n",
    "    - [X] Transformer\n",
    "        - [X] SyModuleList\n",
    "    - [ ] Shape issue in SyDistilBert when batch size changes\n",
    "- [ ] Fix `nn.ModuleList.__iter__` in AST: incorrect Pointer type\n",
    "- [X] Load pretrained weights\n",
    "- [ ] Training example on real dataset\n",
    "- [ ] Classifier (DistilBertForSequenceClassification)\n",
    "- [ ] Schedulers\n",
    "- [ ] Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35ed42fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Small config for testing\n",
    "vocab_size = 10\n",
    "dim = 256\n",
    "max_length = 100\n",
    "n_heads = 2\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    dim=dim,\n",
    "    max_position_embeddings=max_length,\n",
    "    n_heads=n_heads,\n",
    "    hidden_dim=hidden_dim,\n",
    "    n_layers=n_layers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1fe6d",
   "metadata": {},
   "source": [
    "## Embedding\n",
    "https://github.com/huggingface/transformers/blob/master/src/transformers/models/distilbert/modeling_distilbert.py#L82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "012ab5d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Embeddings(SyModule):\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.dim, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.dim)\n",
    "        if config.sinusoidal_pos_embds:\n",
    "            create_sinusoidal_embeddings(\n",
    "                n_pos=config.max_position_embeddings, dim=config.dim, out=self.position_embeddings.weight\n",
    "            )\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, input_ids: torch.LongTensor):\n",
    "        seq_length = input_ids.size(1)\n",
    "        # TODO setting device from input_ids from remotely created tensor throws KeyError: UID <...>.\n",
    "        position_ids = remote_torch.arange(seq_length)  # (max_seq_length)\n",
    "        position_ids = remote_torch.unsqueeze(position_ids, 0).expand_as(input_ids)  # (bs, max_seq_length)\n",
    "        \n",
    "        word_embeddings = self.word_embeddings(input_ids)  # (bs, max_seq_length, dim)\n",
    "        position_embeddings = self.position_embeddings(input_ids)  # (bs, max_seq_length, dim)\n",
    "\n",
    "        embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
    "        embeddings = self.LayerNorm(embeddings)  # (bs, max_seq_length, dim)\n",
    "        embeddings = self.dropout(embeddings)  # (bs, max_seq_length, dim)\n",
    "        return embeddings\n",
    "\n",
    "# dummy_x = torch.ones(10, 100).long()\n",
    "# sy_embedding = Embeddings(config, inputs={'input_ids': dummy_x})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852a9410",
   "metadata": {},
   "source": [
    "## MultiHeadSelfAttention\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/master/src/transformers/models/distilbert/modeling_distilbert.py#L116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85872ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from syft.lib.python.int import Int\n",
    "\n",
    "class MultiHeadSelfAttention(SyModule):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.n_heads = config.n_heads\n",
    "        self.dim = config.dim\n",
    "        self.dropout = nn.Dropout(p=config.attention_dropout)\n",
    "\n",
    "        assert self.dim % self.n_heads == 0\n",
    "\n",
    "        self.q_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.k_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.v_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "        self.out_lin = nn.Linear(in_features=config.dim, out_features=config.dim)\n",
    "\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def shape(self, x, bs, dim_per_head):\n",
    "            \"\"\"separate heads for linear layers\"\"\"\n",
    "            return x.view(bs, -1, self.n_heads, dim_per_head).transpose(1, 2)\n",
    "        \n",
    "    def unshape(self, x, bs, dim_per_head):\n",
    "        \"\"\"group heads\"\"\"\n",
    "        return x.transpose(1, 2).contiguous().view(bs, -1, self.n_heads * dim_per_head)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            query: torch.tensor(bs, seq_length, dim)\n",
    "            key: torch.tensor(bs, seq_length, dim)\n",
    "            value: torch.tensor(bs, seq_length, dim)\n",
    "            mask: torch.tensor(bs, seq_length)\n",
    "        Returns:\n",
    "            weights: torch.tensor(bs, n_heads, seq_length, seq_length) Attention weights context: torch.tensor(bs,\n",
    "        \"\"\"\n",
    "        bs = query.size(0)\n",
    "        q_length = query.size(1)\n",
    "        dim = query.size(2)\n",
    "        k_length = key.size(1)\n",
    "        \n",
    "        dim_per_head = self.dim // self.n_heads\n",
    "        mask_reshp = (bs, 1, 1, k_length)\n",
    "        \n",
    "\n",
    "        q = self.shape(self.q_lin(query), bs, dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        k = self.shape(self.k_lin(key), bs, dim_per_head)  # (bs, n_heads, k_length, dim_per_head)\n",
    "        v = self.shape(self.v_lin(value), bs, dim_per_head)  # (bs, n_heads, k_length, dim_per_head)\n",
    "\n",
    "        q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        scores = remote_torch.matmul(q, k.transpose(2, 3))  # (bs, n_heads, q_length, k_length)\n",
    "        mask = (mask == 0).view(*mask_reshp).expand_as(scores)  # (bs, n_heads, q_length, k_length)\n",
    "                \n",
    "        scores.masked_fill_(mask, -float(\"inf\"))  # (bs, n_heads, q_length, k_length)\n",
    "        weights = remote_torch.softmax(scores, dim=-1) # (bs, n_heads, q_length, k_length)\n",
    "        weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)\n",
    "        \n",
    "        context = remote_torch.matmul(weights, v)  # (bs, n_heads, q_length, dim_per_head)\n",
    "        context = self.unshape(context, bs, dim_per_head)  # (bs, q_length, dim)\n",
    "        context = self.out_lin(context)  # (bs, q_length, dim)\n",
    "\n",
    "        return context\n",
    "\n",
    "# dummy inputs\n",
    "# bs = 10\n",
    "# seq_length = 15\n",
    "# dim = config.dim\n",
    "\n",
    "# dummy_q = torch.randn(bs, seq_length, dim)\n",
    "# dummy_k = torch.randn(bs, seq_length, dim)\n",
    "# dummy_v = torch.randn(bs, seq_length, dim)\n",
    "# dummy_mask = torch.ones([bs, seq_length], dtype=torch.long)\n",
    "# dummy_inputs = {\n",
    "#     \"query\": dummy_q,\n",
    "#     \"key\": dummy_k,\n",
    "#     \"value\": dummy_v,\n",
    "#     \"mask\": dummy_mask\n",
    "# }\n",
    "\n",
    "# sy_mhsa = MultiHeadSelfAttention(config, inputs=dummy_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8ccf67",
   "metadata": {},
   "source": [
    "## FFN\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/master/src/transformers/models/distilbert/modeling_distilbert.py#L203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593cbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(SyModule):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(p=config.dropout)\n",
    "        self.seq_len_dim = 1\n",
    "        self.lin1 = nn.Linear(in_features=config.dim, out_features=config.hidden_dim)\n",
    "        self.lin2 = nn.Linear(in_features=config.hidden_dim, out_features=config.dim)\n",
    "        \n",
    "        self.activation = nn.ReLU()\n",
    "        # TODO GeLU torch._C issue, not in AST\n",
    "#         if config.activation == \"gelu\":\n",
    "#             self.activation = remote_torch.nn.functional.gelu\n",
    "#         elif config.activation == \"relu\":\n",
    "#             self.activation = remote_torch.nn.functional.relu\n",
    "#         else:\n",
    "#             raise ValueError(\n",
    "#                 f\"activation ({config.activation}) must be in ['relu', 'gelu']\"\n",
    "#             )\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO Chunking\n",
    "        x = self.lin1(input)\n",
    "        x = self.activation(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# bs = 10\n",
    "# seq_length = 15\n",
    "# dim = config.dim\n",
    "\n",
    "# dummy_input = torch.randn(bs, seq_length, dim)\n",
    "# ffn_inputs = {\n",
    "#     \"input\": dummy_input\n",
    "# }\n",
    "\n",
    "# sy_FFN = FFN(config, inputs=ffn_inputs)\n",
    "# print(sy_FFN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26507ad8",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/master/src/transformers/models/distilbert/modeling_distilbert.py#L273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff606d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(SyModule):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert config.dim % config.n_heads == 0\n",
    "        \n",
    "        attn_dummy_inputs = {\n",
    "            \"query\": kwargs[\"inputs\"][\"x\"],\n",
    "            \"key\": kwargs[\"inputs\"][\"x\"],\n",
    "            \"value\": kwargs[\"inputs\"][\"x\"],\n",
    "            \"mask\": kwargs[\"inputs\"][\"attn_mask\"],\n",
    "        }\n",
    "        self.attention = MultiHeadSelfAttention(config, inputs=attn_dummy_inputs)\n",
    "        \n",
    "        ffn_dummy_inputs = {\n",
    "            \"input\": kwargs[\"inputs\"][\"x\"]\n",
    "        }\n",
    "        self.ffn = FFN(config, inputs=ffn_dummy_inputs)\n",
    "        \n",
    "        self.sa_layer_norm = nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)\n",
    "        self.output_layer_norm = nn.LayerNorm(normalized_shape=config.dim, eps=1e-12)\n",
    "        \n",
    "    def forward(self, x, attn_mask):\n",
    "        # Self-Attention\n",
    "        sa_output = self.attention(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            mask=attn_mask\n",
    "        )[0]\n",
    "\n",
    "        sa_output = self.sa_layer_norm(sa_output + x)  # (bs, seq_length, dim)\n",
    "\n",
    "        # Feed Forward Network\n",
    "        ffn_output = self.ffn(input=sa_output)[0]  # (bs, seq_length, dim)\n",
    "        ffn_output = self.output_layer_norm(ffn_output + sa_output)  # (bs, seq_length, dim)\n",
    "\n",
    "        return ffn_output\n",
    "\n",
    "class Transformer(SyModule):\n",
    "    def __init__(self, config, **kwargs):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "        self.n_layers = config.n_layers\n",
    "        \n",
    "        self.layer = nn.ModuleList([\n",
    "            TransformerBlock(config, inputs=kwargs[\"inputs\"]) for _ in range(self.n_layers)\n",
    "        ])\n",
    "            \n",
    "        print(\"transformer init: {:.2f} s\".format(\n",
    "            time.time() - t0))\n",
    "  \n",
    "\n",
    "    def forward(self, x, attn_mask):\n",
    "        \"\"\"\n",
    "        - SyModule does not work; multiple inputs\n",
    "        - getattr does not work; \n",
    "        \"\"\"\n",
    "        t0 = time.time()\n",
    "        hidden_state = x\n",
    "        \n",
    "        \n",
    "        \n",
    "        # TODO fix ModuleList.__iter__; items in iter need to be ModulePointer\n",
    "        for i in range(self.n_layers):\n",
    "            layer = self.layer[i]\n",
    "            hidden_state = layer(\n",
    "                x=hidden_state,\n",
    "                attn_mask=attn_mask\n",
    "            )[0]\n",
    "            \n",
    "        print(\"transformer forward: {:.2f} s\".format(\n",
    "            time.time() - t0))\n",
    "        return hidden_state\n",
    "\n",
    "# bs = 10\n",
    "# seq_length = 15\n",
    "# dim = config.dim\n",
    "\n",
    "# dummy_x = torch.randn(bs, seq_length, dim)\n",
    "# dummy_mask = torch.ones([bs, seq_length], dtype=torch.long)\n",
    "# transformer_dummy_inputs = {\n",
    "#     \"x\": dummy_x,\n",
    "#     \"attn_mask\": dummy_mask\n",
    "# }\n",
    "# sy_transformer = Transformer(config, inputs=transformer_dummy_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4912a2d3",
   "metadata": {},
   "source": [
    "## SyDistilBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8952e63b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SyDistilBert(SyModule):\n",
    "    def __init__(self, config: DistilBertConfig, **kwargs):\n",
    "        \"\"\"\n",
    "        SyDistilBert is a re-implementation of huggingface DistilBert in pysyft, \n",
    "        with all non-torch-native submodules rewritten as SyModules.\n",
    "        \n",
    "        Use the `from_pretrained` and `from_config` classmethods to instantiate this\n",
    "        model from an existing HuggingFace pretrained model.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings\n",
    "        embedding_inputs = {\n",
    "            'input_ids': kwargs['inputs']['input_ids']\n",
    "        }\n",
    "        self.embeddings = Embeddings(config=config, inputs=embedding_inputs)\n",
    "        \n",
    "        # Transformer\n",
    "        transformer_x = torch.rand(*kwargs['inputs']['input_ids'].size(), config.dim)\n",
    "        transformer_mask = kwargs['inputs']['attention_mask']\n",
    "        transformer_inputs = {\n",
    "            \"x\": transformer_x,\n",
    "            \"attn_mask\": transformer_mask\n",
    "        }\n",
    "        \n",
    "        self.transformer = Transformer(config=config,\n",
    "                                       inputs=transformer_inputs)\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        input_embeds = self.embeddings(input_ids=input_ids)[0]\n",
    "\n",
    "        out = self.transformer(x=input_embeds,\n",
    "                               attn_mask=attention_mask)[0]\n",
    "        return input_embeds\n",
    "    \n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_name: str) -> \"SyDistilBert\":\n",
    "        # Make dummy inputs\n",
    "        dummy_x = torch.ones(1, 1, dtype=torch.long)\n",
    "        dummy_mask = torch.ones(1, 1, dtype=torch.long)\n",
    "\n",
    "        dummy_inputs = {\n",
    "            \"input_ids\": dummy_x,\n",
    "            \"attention_mask\": dummy_mask\n",
    "        }\n",
    "        \n",
    "        # Load huggingface model\n",
    "        hf_model = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Construct model\n",
    "        model = cls(hf_model.config, inputs=dummy_inputs)\n",
    "        \n",
    "        # Load weights\n",
    "        model.load_state_dict(hf_model.state_dict())\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config: DistilBertConfig) -> \"SyDistilBert\":\n",
    "        # Make dummy inputs\n",
    "        dummy_x = torch.ones(1, 1, dtype=torch.long)\n",
    "        dummy_mask = torch.ones(1, 1, dtype=torch.long)\n",
    "\n",
    "        dummy_inputs = {\n",
    "            \"input_ids\": dummy_x,\n",
    "            \"attention_mask\": dummy_mask\n",
    "        }\n",
    "        \n",
    "        # Construct model\n",
    "        model = cls(config, inputs=dummy_inputs)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179a1d2",
   "metadata": {},
   "source": [
    "## Test with small config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41436a19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECOMPILING Embeddings\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING Embeddings\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING SyDistilBert\n",
      "RECOMPILING SyDistilBert\n",
      "RECOMPILING Embeddings\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING Embeddings\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING Embeddings\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING SyDistilBert\n",
      "RECOMPILING SyDistilBert\n",
      "RECOMPILING Embeddings\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING Embeddings\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING Embeddings\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING SyDistilBert\n",
      "SyDistilBert(\n",
      "  (embeddings): Embeddings(\n",
      "    (word_embeddings): Embedding(10, 256, padding_idx=0)\n",
      "    (position_embeddings): Embedding(100, 256)\n",
      "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (layer): ModuleList(\n",
      "      (0): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (lin2): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "        (output_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerBlock(\n",
      "        (attention): MultiHeadSelfAttention(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (q_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (k_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (v_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (out_lin): Linear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (ffn): FFN(\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (lin1): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (lin2): Linear(in_features=128, out_features=256, bias=True)\n",
      "          (activation): ReLU()\n",
      "        )\n",
      "        (sa_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "        (output_layer_norm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "sydistilbert = SyDistilBert.from_config(config)\n",
    "\n",
    "dummy_x = torch.ones(2, 2, dtype=torch.long)\n",
    "dummy_mask = torch.ones(2, 2, dtype=torch.long)\n",
    "\n",
    "@make_plan\n",
    "def train(model=sydistilbert, x=dummy_x, attn_mask=dummy_mask):\n",
    "    \"\"\"Single training iteration\"\"\"\n",
    "    opt = remote_torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0)\n",
    "    out = model(input_ids=x,\n",
    "                attention_mask=dummy_mask)[0]\n",
    "    loss = remote_torch.mean(out[0])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return [model]\n",
    "\n",
    "dummy_x = torch.ones(2, 2, dtype=torch.long)\n",
    "dummy_mask = torch.ones(2, 2, dtype=torch.long)\n",
    "\n",
    "train_ptr = train.send(alice_client)\n",
    "model_ptr = train_ptr(model=sydistilbert, x=dummy_x, attn_mask=dummy_mask)\n",
    "updated_model = model_ptr.get()[0]\n",
    "print(updated_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea50f178",
   "metadata": {},
   "source": [
    "## Load full DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f16a7ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "transformer init: 44.28 s\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "transformer forward: 58.40 s\n",
      "RECOMPILING Embeddings\n",
      "RECOMPILING Embeddings\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n"
     ]
    }
   ],
   "source": [
    "# This takes a while, loading state_dict sometimes crashes kernel\n",
    "sydistilbert = SyDistilBert.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e4401a",
   "metadata": {},
   "source": [
    "## Memory issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41352e85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING FFN\n",
      "transformer init: 46.83 s\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "transformer forward: 60.13 s\n",
      "RECOMPILING Embeddings\n",
      "RECOMPILING Embeddings\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING Transformer\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n",
      "RECOMPILING TransformerBlock\n",
      "RECOMPILING MultiHeadSelfAttention\n",
      "RECOMPILING FFN\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Message syft.lib.torch.Module exceeds maximum protobuf size of 2GB: 3104940090",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-93eaba184bed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mmake_plan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msydistilbert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremote_torch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     out = model(input_ids=x,\n",
      "\u001b[0;32m~/projects/PySyft/packages/syft/src/syft/core/plan/plan_builder.py\u001b[0m in \u001b[0;36mmake_plan\u001b[0;34m(func, inputs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_plan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPlan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_plan_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPLAN_BUILDER_VM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/PySyft/packages/syft/src/syft/core/plan/plan_builder.py\u001b[0m in \u001b[0;36mbuild_plan_inputs\u001b[0;34m(forward_func)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mROOT_CLIENT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m~/projects/PySyft/packages/syft/src/syft/ast/klass.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, client, pointable, description, tags, searchable)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;31m# Step 3: send message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m             \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_immediate_msg_without_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# Step 4: return pointer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/PySyft/packages/syft/src/syft/core/node/common/client.py\u001b[0m in \u001b[0;36msend_immediate_msg_without_reply\u001b[0;34m(self, msg, route_index)\u001b[0m\n\u001b[1;32m    259\u001b[0m             )\n\u001b[1;32m    260\u001b[0m             \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigning_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigning_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"> Sending {msg.pprint} {self.pprint} ➡️  {msg.address.pprint}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroutes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroute_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_immediate_msg_without_reply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/PySyft/packages/syft/src/syft/core/common/message.py\u001b[0m in \u001b[0;36msign\u001b[0;34m(self, signing_key)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[1;32m     90\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"> Signing with {self.address.key_emoji(key=signing_key.verify_key)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0msigned_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigning_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_bytes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# signed_type will be the final subclass callee's closest parent signed_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/PySyft/packages/syft/src/syft/core/common/serde/serialize.py\u001b[0m in \u001b[0;36m_serialize\u001b[0;34m(obj, to_proto, to_bytes)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# indent=None means no white space or \\n in the serialized version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# this is compatible with json.dumps(x, indent=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mserialized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_serializable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object2proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         blob: Message = DataMessage(\n\u001b[1;32m     67\u001b[0m             \u001b[0mobj_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_fully_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_serializable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/PySyft/packages/syft/src/syft/core/node/common/action/save_object_action.py\u001b[0m in \u001b[0;36m_object2proto\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_object2proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSaveObjectAction_PB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object2proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSaveObjectAction_PB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maddr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/PySyft/packages/syft/src/syft/core/store/storeable_object.py\u001b[0m in \u001b[0;36m_object2proto\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object2proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"description\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pysyft/lib/python3.9/site-packages/google/protobuf/internal/well_known_types.py\u001b[0m in \u001b[0;36mPack\u001b[0;34m(self, msg, type_url_prefix, deterministic)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype_url_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDESCRIPTOR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mUnpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Message syft.lib.torch.Module exceeds maximum protobuf size of 2GB: 3104940090"
     ]
    }
   ],
   "source": [
    "# Memory issue when sending plan with distilbert-sized model\n",
    "from transformers import AutoConfig\n",
    "\n",
    "# Fully sized distilbert without pretrained weights (takes too long)\n",
    "distilbert_config = AutoConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "sydistilbert = SyDistilBert.from_config(distilbert_config)\n",
    "\n",
    "dummy_x = torch.ones(2, 2, dtype=torch.long)\n",
    "dummy_mask = torch.ones(2, 2, dtype=torch.long)\n",
    "\n",
    "@make_plan\n",
    "def train(model=sydistilbert, x=dummy_x, attn_mask=dummy_mask):\n",
    "    opt = remote_torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0)\n",
    "    out = model(input_ids=x,\n",
    "                attention_mask=dummy_mask)[0]\n",
    "    loss = remote_torch.mean(out[0])\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return [model]\n",
    "\n",
    "dummy_x = torch.ones(2, 2, dtype=torch.long)\n",
    "dummy_mask = torch.ones(2, 2, dtype=torch.long)\n",
    "\n",
    "train_ptr = train.send(alice_client)\n",
    "model_ptr = train_ptr(model=sydistilbert, x=dummy_x, attn_mask=dummy_mask)\n",
    "updated_model = model_ptr.get()[0]\n",
    "print(updated_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "239px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
