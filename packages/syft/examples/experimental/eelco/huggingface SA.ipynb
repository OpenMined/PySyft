{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e065d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43c1e7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import datasets\n",
    "import tokenizers\n",
    "\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab57ea1",
   "metadata": {},
   "source": [
    "# Minimal transformers pipeline for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbcbf34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Reusing dataset imdb (/home/eelco/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Model\n",
    "model_name = 'distilbert-base-uncased'\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "hf_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = datasets.load_dataset('imdb', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90f60aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "\n",
      "Unique labels: [0 1]\n",
      "\n",
      "{'label': 1,\n",
      " 'text': 'Bromwell High is a cartoon comedy. It ran at the same time as some '\n",
      "         'other programs about school life, such as \"Teachers\". My 35 years in '\n",
      "         \"the teaching profession lead me to believe that Bromwell High's \"\n",
      "         'satire is much closer to reality than is \"Teachers\". The scramble to '\n",
      "         'survive financially, the insightful students who can see right '\n",
      "         \"through their pathetic teachers' pomp, the pettiness of the whole \"\n",
      "         'situation, all remind me of the schools I knew and their students. '\n",
      "         'When I saw the episode in which a student repeatedly tried to burn '\n",
      "         'down the school, I immediately recalled ......... at .......... '\n",
      "         \"High. A classic line: INSPECTOR: I'm here to sack one of your \"\n",
      "         'teachers. STUDENT: Welcome to Bromwell High. I expect that many '\n",
      "         'adults of my age think that Bromwell High is far fetched. What a '\n",
      "         \"pity that it isn't!\"}\n"
     ]
    }
   ],
   "source": [
    "# Check out dataset\n",
    "pprint(train_dataset)\n",
    "print()\n",
    "print(\"Unique labels:\", np.unique(train_dataset['label']))\n",
    "print()\n",
    "pprint(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3b6110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadSelfAttention(\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_model.distilbert.transformer.layer[0].attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e48a1",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9675963c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train setup\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# lr from BERT paper\n",
    "opt = torch.optim.AdamW([\n",
    "        {'params': hf_model.distilbert.parameters(), 'lr': 2e-5},\n",
    "        {'params': hf_model.pre_classifier.parameters(), 'lr': 1e-3},\n",
    "        {'params': hf_model.classifier.parameters(), 'lr': 1e-3}\n",
    "    ], lr=1e-2)\n",
    "\n",
    "batch_size = 32\n",
    "max_length = 100 # Keep text short for CPU. Should be 512\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a054899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss tensor(0.6874, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def train_iter(model, tokenizer, opt, batch, max_length):\n",
    "    # Single train iter\n",
    "    model.train()\n",
    "    opt.zero_grad()\n",
    "    bx = tokenizer(batch['text'], padding=True, return_tensors='pt', truncation=True, max_length=100)\n",
    "    by = torch.LongTensor(batch['label'])\n",
    "    out = model(**bx).logits\n",
    "    loss = F.cross_entropy(out, by)\n",
    "\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    return loss\n",
    "    \n",
    "batch = next(iter(train_loader))\n",
    "loss = train_iter(hf_model, hf_tokenizer, opt, batch, max_length)\n",
    "print('loss', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe8228",
   "metadata": {},
   "source": [
    "# Syft\n",
    "\n",
    "TODO\n",
    "- tokenizer, `tokenizer.__call__`\n",
    "- BatchEncoding\n",
    "- model\n",
    "    - serializable DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e984ec42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft import make_plan\n",
    "from syft.lib.python.string import String\n",
    "from syft import logger\n",
    "\n",
    "# sy.load('tokenizers')\n",
    "# sy.load('transformers')\n",
    "\n",
    "# Create client and test string\n",
    "alice = sy.VirtualMachine(name=\"alice\")\n",
    "alice_client = alice.get_client()\n",
    "\n",
    "text = \"This is a test.\"\n",
    "text_ptr = String(text).send(alice_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5debb0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8402d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tokenizer ptr\n",
    "tokenizer = hf_tokenizer._tokenizer\n",
    "encoding = tokenizer.encode(text)\n",
    "\n",
    "tokenizer_ptr = tokenizer.send(alice_client)\n",
    "encoding_ptr = tokenizer_ptr.encode(text_ptr)\n",
    "encoding_2 = encoding_ptr.get()\n",
    "assert encoding.__getstate__() == encoding_2.__getstate__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "46eadf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct tokenizer from ptr\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "hf_tokenizer_2 = PreTrainedTokenizerFast(tokenizer_object=tokenizer_ptr.get())\n",
    "hf_tokenizer_2.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Test if same\n",
    "enc = hf_tokenizer(batch['text'], padding=True, return_tensors='pt', truncation=True, max_length=100)\n",
    "enc_2 = hf_tokenizer_2(batch['text'], padding=True, return_tensors='pt', truncation=True, max_length=100)\n",
    "\n",
    "# Issue: It forgets some params from wrapper (pad tokens, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ed0d3425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "for k in enc.keys():\n",
    "    print(torch.eq(enc[k], enc_2[k]).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d4f3f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method AutoTokenizer.from_pretrained of <class 'transformers.models.auto.tokenization_auto.AutoTokenizer'>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.AutoTokenizer.from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0bfb0fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_tokenizer.save_pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "086c248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sy.load('transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fb655f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.auto.tokenization_auto.AutoTokenizer"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377480bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
