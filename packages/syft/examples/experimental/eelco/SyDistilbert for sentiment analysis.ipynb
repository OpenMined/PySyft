{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "743acb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If needed, install huggingface datasets\n",
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b74d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from pprint import pprint\n",
    "from itertools import islice\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import syft as sy\n",
    "from syft import VirtualMachine\n",
    "from syft.core.plan.plan_builder import make_plan, ROOT_CLIENT\n",
    "from syft.lib.python.collections.ordered_dict import OrderedDict\n",
    "from syft import logger\n",
    "from syft import SyModule\n",
    "from syft.lib.transformers.models.distilbert import SyDistilBert\n",
    "\n",
    "from transformers.models.distilbert.modeling_distilbert import DistilBertConfig\n",
    "from transformers import AutoConfig, AutoTokenizer, PreTrainedTokenizerFast\n",
    "import datasets\n",
    "\n",
    "logger.remove()\n",
    "sy.load('transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebcfcde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create client\n",
    "alice = sy.VirtualMachine(name=\"alice\")\n",
    "alice_client = alice.get_client()\n",
    "remote_torch = ROOT_CLIENT.torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4c603ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# Use distilbert weights distilled from `bert-base-uncased`, \n",
    "# other pretrained configurations can be found here:\n",
    "# https://huggingface.co/transformers/pretrained_models.html\n",
    "\n",
    "batch_size = 64\n",
    "model_name = 'distilbert-base-uncased'\n",
    "config = AutoConfig.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8572a4",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will use the IMDB dataset for binary sentence classification:\n",
    "\n",
    "https://huggingface.co/datasets/imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5353cde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/eelco/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 25000\n",
      "})\n",
      "\n",
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "train_set = datasets.load_dataset('imdb', split='train')\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "num_labels = len(set(train_set['label']))\n",
    "config.num_labels = num_labels\n",
    "\n",
    "print(train_set)\n",
    "print()\n",
    "print(train_set[0]['text'])\n",
    "print(\"label:\", train_set[0]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d5a72",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "The `PretrainedTokenizerFast` tokenizer used by most transformer models can now be serialized by Syft, so we can just load one here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d21ee635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=128, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Change tokenizer to a PreTrainedTokenizerFast,\n",
    "# instead of the type returned by AutoTokenizer.\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer._tokenizer,\n",
    "    name_or_path = tokenizer.name_or_path,\n",
    "    padding_side = tokenizer.padding_side,\n",
    "    model_max_length = tokenizer.model_max_length,\n",
    "    **tokenizer.special_tokens_map\n",
    ")\n",
    "\n",
    "# Set small model_max_length for faster testing\n",
    "tokenizer.model_max_length = 128\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d85ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]]),\n",
      " 'input_ids': tensor([[ 101, 1045, 1010,  ..., 3286, 1010,  102],\n",
      "        [ 101, 2049, 4234,  ..., 4569, 2007,  102],\n",
      "        [ 101, 1999, 1996,  ..., 1007, 1010,  102],\n",
      "        ...,\n",
      "        [ 101, 2073, 2000,  ..., 2431, 2126,  102],\n",
      "        [ 101, 3462, 1997,  ..., 2036, 3625,  102],\n",
      "        [ 101, 2054, 2019,  ..., 1012, 1026,  102]]),\n",
      " 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])}\n",
      "\n",
      "sizes: {'input_ids': torch.Size([64, 128]), 'token_type_ids': torch.Size([64, 128]), 'attention_mask': torch.Size([64, 128])}\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer on a single batch\n",
    "\n",
    "bx = next(iter(train_loader))\n",
    "tokenized = tokenizer(bx['text'], padding=True, return_tensors='pt', truncation=True)\n",
    "pprint(tokenized)\n",
    "print()\n",
    "print(\"sizes:\", {k: v.size() for k, v in tokenized.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e011aa4",
   "metadata": {},
   "source": [
    "# DistilBERT for sentiment analysis\n",
    "\n",
    "A simple classifier using distilbert, copied from [ DistilBertForSequenceClassification](https://github.com/huggingface/transformers/blob/61c506349134db0a0a2fd6fb2eff8e29a2f84e79/src/transformers/models/distilbert/modeling_distilbert.py#L578)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02ac97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBertClassifier(SyModule):\n",
    "    def __init__(self, base_model: SyModule, config: DistilBertConfig, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.distilbert = base_model\n",
    "        self.pre_classifier = nn.Linear(config.dim, config.dim)\n",
    "        self.classifier = nn.Linear(config.dim, config.num_labels)\n",
    "        self.dropout = nn.Dropout(config.seq_classif_dropout)\n",
    "        self.activation = remote_torch.nn.functional.relu\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        distilbert_output = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )[0]\n",
    "        \n",
    "        pooled_output = distilbert_output[:, 0]  # (bs, dim)\n",
    "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.activation(pooled_output)  # (bs, dim)\n",
    "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
    "        logits = self.classifier(pooled_output)  # (bs, num_labels)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c48240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST CONFIG\n",
    "\n",
    "# Until Issue #5627 gets resolved, we use a tiny bert config to make the model serializable.\n",
    "# When fixed, remove this cell to use a full model with pretrained weights.\n",
    "\n",
    "config = DistilBertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    dim=10,\n",
    "    max_position_embeddings=129,\n",
    "    n_heads=2,\n",
    "    hidden_dim=10,\n",
    "    n_layers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da47ff9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer init: 0.21 s\n",
      "transformer forward: 0.10 s\n",
      "DistilBertClassifier(\n",
      "  (distilbert): SyDistilBert(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 10, padding_idx=0)\n",
      "      (position_embeddings): Embedding(129, 10)\n",
      "      (LayerNorm): LayerNorm((10,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=10, out_features=10, bias=True)\n",
      "            (k_lin): Linear(in_features=10, out_features=10, bias=True)\n",
      "            (v_lin): Linear(in_features=10, out_features=10, bias=True)\n",
      "            (out_lin): Linear(in_features=10, out_features=10, bias=True)\n",
      "          )\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=10, out_features=10, bias=True)\n",
      "            (lin2): Linear(in_features=10, out_features=10, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((10,), eps=1e-12, elementwise_affine=True)\n",
      "          (output_layer_norm): LayerNorm((10,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (classifier): Linear(in_features=10, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Small model\n",
    "base_model = SyDistilBert.from_config(config)\n",
    "\n",
    "# Large model with pretrained weights\n",
    "# base_model = SyDistilBert.from_pretrained(model_name)\n",
    "\n",
    "classifier = DistilBertClassifier(base_model, config, inputs=base_model.inputs)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9b7b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape: torch.Size([64, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eelco/projects/PySyft/packages/syft/src/syft/lib/torch/uppercase_tensor.py:30: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  grad = getattr(obj, \"grad\", None)\n"
     ]
    }
   ],
   "source": [
    "# Test a local forward pass\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "batch_x = tokenizer(batch['text'], padding=True, \n",
    "                    return_tensors='pt', truncation=True)\n",
    "batch_y = batch['label']\n",
    "logits = classifier(**batch_x)[0]\n",
    "print(\"logits shape:\", logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed57dc3",
   "metadata": {},
   "source": [
    "# Train Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb896f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_batches = sy.lib.python.List([next(iter(train_loader))])\n",
    "\n",
    "@make_plan\n",
    "def train(classifier=classifier, tokenizer=tokenizer, batches=dummy_batches):\n",
    "    \"\"\"\n",
    "    Train classifier on batches, and return updated classifier\n",
    "    \"\"\"\n",
    "    opt = remote_torch.optim.AdamW(classifier.parameters(), lr=1e-3)\n",
    "    \n",
    "    for batch in batches:\n",
    "        classifier.train()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        # Prepare data\n",
    "        batch_x = tokenizer(batch['text'], padding=True, \n",
    "                            return_tensors='pt', truncation=True)\n",
    "        batch_y = batch['label']\n",
    "        \n",
    "        # Forward, loss, backward\n",
    "        out = classifier(input_ids=batch_x['input_ids'],\n",
    "                         attention_mask=batch_x['attention_mask'])[0]\n",
    "        loss = remote_torch.nn.functional.cross_entropy(out, batch_y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return [classifier]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43958756",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddf6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on 10 batches and return the updated model\n",
    "\n",
    "train_batches = sy.lib.python.List(islice(iter(train_loader), 10))\n",
    "train_ptr = train.send(alice_client)\n",
    "classifier_ptr = train_ptr(classifier=classifier, tokenizer=tokenizer, batches=train_batches)\n",
    "\n",
    "classifier_updated = classifier_ptr.get()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c8e1da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models have same parameters: False\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: check if the parameters have updated\n",
    "\n",
    "def check_same_parameters(model1, model2) -> bool:\n",
    "    for p1, p2 in zip(model1.parameters(), model2.parameters()):\n",
    "        if p1.data.ne(p2.data).any():\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "print(\"models have same parameters:\", check_same_parameters(classifier, classifier_updated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "282px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
