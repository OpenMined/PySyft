{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Pull in dependencies and initiate Duet session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First lets pull in our dependencies and initiate our duet session\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import syft as sy\n",
    "\n",
    "duet = sy.join_duet(loopback=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Define and send our remote assets\n",
    "\n",
    "Here we'll define the remote model which will have the remote input data fed into it. This includes;\n",
    "\n",
    "- Our first model segment\n",
    "- Our dummy input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "# we need some transforms for the MNIST data set\n",
    "local_transform_1 = torchvision.transforms.ToTensor()  # this converts PIL images to Tensors\n",
    "local_transform_2 = torchvision.transforms.Normalize(0.1307, 0.3081)  # this normalizes the dataset\n",
    "\n",
    "# compose our transforms\n",
    "local_transforms = torchvision.transforms.Compose([local_transform_1, local_transform_2])\n",
    "\n",
    "args = {\n",
    "    \"batch_size\": 64,\n",
    "    \"test_batch_size\": 1000,\n",
    "    \"epochs\": 8,\n",
    "    \"lr\": 1.0,\n",
    "    \"gamma\": 0.7,\n",
    "    \"no_cuda\": False,\n",
    "    \"dry_run\": False,\n",
    "    \"seed\": 42, # the meaning of life\n",
    "    \"log_interval\": 10,\n",
    "    \"save_model\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from syft.util import get_root_data_path\n",
    "\n",
    "train_kwargs = {\n",
    "    \"batch_size\": args[\"batch_size\"],\n",
    "}\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(str(get_root_data_path()), train=True, download=True, transform=local_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_data,**train_kwargs)\n",
    "\n",
    "data_pointer = []\n",
    "labels = []\n",
    "\n",
    "for image, label in train_loader:\n",
    "    data_pointer.append(image.send(duet))\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remote_torch = duet.torch\n",
    "\n",
    "# In order to serialise our model we need to define it as below\n",
    "class SyNet1(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(SyNet1, self).__init__(torch_ref=torch_ref)\n",
    "        self.conv1 = self.torch_ref.nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = self.torch_ref.nn.Conv2d(32, 64, 3, 1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.torch_ref.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.torch_ref.nn.functional.relu(x)\n",
    "        x = self.torch_ref.nn.functional.max_pool2d(x, 2)\n",
    "        output = x\n",
    "        return output\n",
    "    \n",
    "model1 = SyNet1(torch)\n",
    "model1_ptr = model1.send(duet)\n",
    "opt1 = duet.torch.optim.SGD(params=model1_ptr.parameters(),lr=[\"lr\"])\n",
    "sch1 = duet.torch.optim.lr_scheduler.StepLR(opt1, step_size=1, gamma=args[\"gamma\"])\n",
    "\n",
    "#Define and send our dummy input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class SyNet2(sy.Module):\n",
    "    def __init__(self, torch_ref):\n",
    "        super(SyNet2, self).__init__(torch_ref=torch_ref)\n",
    "        self.dropout1 = self.torch_ref.nn.Dropout2d(0.25)\n",
    "        self.dropout2 = self.torch_ref.nn.Dropout2d(0.5)\n",
    "        self.fc1 = self.torch_ref.nn.Linear(9216, 128)\n",
    "        self.fc2 = self.torch_ref.nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(x)\n",
    "        x = self.torch_ref.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.torch_ref.nn.functional.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = self.torch_ref.nn.functional.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "model2 = SyNet2(torch)\n",
    "opt2 = torch.optim.Adadelta(model2.parameters(), lr=args[\"lr\"])\n",
    "sch2 = torch.optim.lr_scheduler.StepLR(opt2, step_size=1, gamma=args[\"gamma\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Define our training logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dry_run = True\n",
    "\n",
    "for x in range(args[\"epochs\"]):\n",
    "    for y in range (len(labels)):\n",
    "        if y > 10 and dry_run:\n",
    "            break\n",
    "        opt1.zero_grad()\n",
    "        opt2.zero_grad()\n",
    "\n",
    "        activation_ptr = model1_ptr(data_pointer[y])\n",
    "        activation = activation_ptr.clone().get(request_block=True, reason=\"process the model\")\n",
    "        activation.retain_grad()\n",
    "        \n",
    "#         print(activation[0])\n",
    "\n",
    "        pred = model2(activation)\n",
    "        loss = torch.nn.functional.nll_loss(pred, labels[y])\n",
    "        loss.backward()\n",
    "        \n",
    "#         print(activation.grad)\n",
    "\n",
    "        grad_ptr = activation.grad.clone().send(duet)\n",
    "        activation_ptr.backward(grad_ptr)\n",
    "\n",
    "        opt1.step()\n",
    "        opt2.step()\n",
    "    print(f\"Epoch {x} Loss: {loss.item()}\")\n",
    "    if dry_run:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pull in our Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_kwargs = {\n",
    "    \"batch_size\": args[\"test_batch_size\"],\n",
    "}\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(str(get_root_data_path()), train=False, download=True, transform=local_transforms)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,**train_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model1 = model1_ptr.get(request_block=True, reason=\"run testing ont the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Test our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_data_length = len(test_loader.dataset)\n",
    "# test_batches = round((test_data_length / args[\"test_batch_size\"]) + 0.5)\n",
    "# test_loss = 0.0\n",
    "# correct = 0.0\n",
    "    \n",
    "# for batch_idx, (data, target) in enumerate(test_loader):\n",
    "#     output = model2(model1(data))\n",
    "#     iter_loss = torch.nn.functional.nll_loss(output, target, reduction=\"sum\").item()\n",
    "#     test_loss = test_loss + iter_loss\n",
    "#     pred = output.argmax(dim=1)\n",
    "#     total = pred.eq(target).sum().item()\n",
    "#     correct += total\n",
    "            \n",
    "#     if batch_idx >= test_batches - 1:\n",
    "#                 print(\"batch_idx >= test_batches, breaking\")\n",
    "#                 break\n",
    "#     accuracy = correct / test_data_length\n",
    "#     print(f\"Test Set Accuracy: {test_loss}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
