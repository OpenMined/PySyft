Okay so I fixed the multi domain stack issue that @trask was having.
the TL;DR is:
DOMAIN_PORT=8081 TRAEFIK_TAG=ca docker compose -p ca up
DOMAIN_PORT=8082 TRAEFIK_TAG=au docker compose -p au up





8:11
The longer explanation is this:
proxy:
    ports:
      - "${DOMAIN_PORT?80}:80"
8:11
I have placed an environment variable in the host bind side of the proxy
8:12
But the issue of the proxy doing a round robin is because traefik is designed to auto map your containers as services and proxy them including things like round robin etc which makes a lot of sense and is super cool.
8:12
- --providers.docker.constraints=Label(`traefik.constraint-label-stack`, `${TRAEFIK_TAG?Variable not set}`)
8:13
this part says, only map containers that have the Label $TRAEFIK_TAG
8:13
so by changing that for each compose project invocation we get a seperate traefik instance doing all the same magic mapping for only its related stack of containers
8:14
so then the next question is what about inter domain / stack networking
8:14
well there are many ways to handle it but for now iv made things nice and simple
8:14
networks:
  traefik-public:
    # For local dev, don't expect an external Traefik network
    external: false
8:14
this says make a network called traefix-public but due to the --project isolation of docker compose this will be a unique traefik-public for each domain / stack
8:15
docker network ls
TWORK ID     NAME                DRIVER    SCOPE
f1c7fa13e872   bridge              bridge    local
4db5284326df   ca_default          bridge    local
0aa17b4f17af   ca_traefik-public   bridge    local
25ea30e686af   host                host      local
4b2fec4fa9c2   none                null      local
(edited)
8:15
interestingly the published ports that are bound to localhost on main host are actually available on the same iface inside the docker container
8:16
to mess around with a containers networking you want to run an exec shell install some network tools and then do some ifconfig, cat, ping etc
8:16
first....
8:16
I highly suggest using ctop (edited)
8:16
brew install ctop
8:17
https://github.com/bcicen/ctop
8:17
Screen Shot 2021-07-01 at 5.17.07 pm.png
Screen Shot 2021-07-01 at 5.17.07 pm.png


8:17
tap right and you will see lots of great info on the container
8:17
Screen Shot 2021-07-01 at 5.17.27 pm.png
Screen Shot 2021-07-01 at 5.17.27 pm.png


8:17
tap enter and you can run an exec easily
8:18
Screen Shot 2021-07-01 at 5.18.18 pm.png
Screen Shot 2021-07-01 at 5.18.18 pm.png


8:18
okay so many containers are ubuntu and some are alpine
8:18
run bash to make your life sane (edited)
8:18
bash
8:19
then if its ubuntu install some stuff
8:19
apt-get update && apt-get install -y iputils-ping net-tools
if it says apt-get is missing the you might be in alpine just change to:
apk add iputils net-tools
unfortunately some of the containers have a locked down user so you cant do this stuff without some other workarounds (edited)
8:19
Screen Shot 2021-07-01 at 5.19.22 pm.png
Screen Shot 2021-07-01 at 5.19.22 pm.png


8:19
okay so first thing is who are we?
8:19
hostname
8:19
that will output the container id which is the hostname
8:19
then you can cat /etc/hosts
8:19
root@e80c8c512be4:/app# cat /etc/hosts
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
192.168.96.8	e80c8c512be4
root@e80c8c512be4:/app#
8:20
netstat -tulpn
8:20
root@e80c8c512be4:/app# netstat -tulpn
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 127.0.0.11:36655        0.0.0.0:*               LISTEN      -
tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      1/python3
udp        0      0 127.0.0.11:43039        0.0.0.0:*                           -
8:20
that helps us see what ports are being used on what interfaces etc
8:20
this is the backend container and its running on port 80
8:21
ifconfig
8:21
eth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
        inet 192.168.96.8  netmask 255.255.240.0  broadcast 192.168.111.255
        ether 02:42:c0:a8:60:08  txqueuelen 0  (Ethernet)
        RX packets 6523  bytes 9142650 (8.7 MiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 2674  bytes 182233 (177.9 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536
1        inet 127.0.0.1  netmask 255.0.0.0
        loop  txqueuelen 1000  (Local Loopback)
        RX packets 50  bytes 3546 (3.4 KiB)
        RX errors 0  dropped 0  overruns 0  frame 0
        TX packets 50  bytes 3546 (3.4 KiB)
        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
8:22
so we can see that the container has an ip, lets just assume the gateway is .1
8:22
root@e80c8c512be4:/app# ping 192.168.96.1
PING 192.168.96.1 (192.168.96.1) 56(84) bytes of data.
64 bytes from 192.168.96.1: icmp_seq=1 ttl=64 time=0.060 ms
64 bytes from 192.168.96.1: icmp_seq=2 ttl=64 time=0.067 ms
(edited)
8:22
nice
8:23
Now inside another backend for a different stack
8:23
cat /etc/hosts
8:23
# cat /etc/hosts
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
192.168.128.8	3f9dec9a0520
8:23
okay cool so different subnets
8:24
now heres where things get a little bit surprising but also easy
8:24
# curl 192.168.128.1:8081
<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial
8:24
# curl 192.168.128.1:8082
<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-wid
8:25
same goes for curl 192.168.96.1:8081 and curl 192.168.96.1:8082
8:26
so all these gateways are actually the same docker host iface so we can easily just call these internally... only that would be a little annoying since we need to find out what one of them is
8:26
it turns out there are several better ways
8:26
on mac we get this for free
8:26
host.docker.internal
8:26
root@3f9dec9a0520:/app# curl host.docker.internal:8081
<!DOCTYPE html><html lang="en"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><!
8:27
https://github.com/docker/for-linux/issues/264
8:27
but... possibly not available on linux
8:27
so instead we have this
8:27
docker-host:
    image: qoomon/docker-host
    cap_add: ["NET_ADMIN", "NET_RAW"]
    mem_limit: 8M
    restart: on-failure
8:27
https://github.com/qoomon/docker-host
8:27
an container which basically provides a docker-host hostname in all containers
8:28
Screen Shot 2021-07-01 at 5.28.18 pm.png
Screen Shot 2021-07-01 at 5.28.18 pm.png


8:28
there will be n times of them running which is highly likely redundant but we can fix all this stuff up later
8:29
the other thing we can do is add a network which is external: true and create it independently of the compose file but this might actually be less user friendly
8:30
traefik is actually designed to do a lot of this stuff so i think we can use it once i get time to go in more detail through the docs and also have some nice local domain name configuration
8:30
what i would be shooting for would be something like
8:31
tox -e grid.stack -- -name mydomain
8:31
which would create a whole docker stack with that as the name and provide a locally accessible hostname like:
8:31
curl mydomain.opengrid.local:80 or something
8:32
another note on ports
8:32
proxy: 80
backend: 80
rabbitmq: 5672
pgadmin: 5050
frontend: 80
postgresql: 5432
flower (monitor celery?) 5555
8:32
those are the internal ports for most of the containers
8:32
if you publish a port without specifying both the host and container side
8:32
  pgadmin:
    ports:
      - "5050"
  db:
    ports:
      - "5432"
  flower:
    ports:
      - "5555"
8:32
you get a random port on the host side
8:34
you can check them manually
8:34
  ~/dev  docker port ca_backend_1                                  ✔
8888/tcp -> 0.0.0.0:54351
8:34
or with ctop
8:35
Screen Shot 2021-07-01 at 5.34.58 pm.png
Screen Shot 2021-07-01 at 5.34.58 pm.png


8:35
this shows that ca_pgadmin_1 is using port 54691 for the 5050 pgadmin interface on my local interface
8:36
so i think for now if we have manually defineable $DOMAIN_PORTs for the proxy which handles all the incoming traffic then we just have random ports for all the other containers so that we can still do stuff like connect with a database client or view their own web admin interfaces if we so desire
8:36
also
8:36
shout out to TablePlus
8:37
FREE all the databases in one GUI basically a navicat killer https://tableplus.com/ (edited)

tableplus.comtableplus.com
TablePlus | Modern, Native Tool for Database Management.
Modern, native client with intuitive GUI tools to create, access, query & edit multiple relational databases: MySQL, PostgreSQL, SQLite, Microsoft SQL Server, Amazon Redshift, MariaDB, CockroachDB, Vertica, Cassandra, and Redis. (130 kB)
https://tableplus.com/

8:38
Screen Shot 2021-07-01 at 5.38.10 pm.png
Screen Shot 2021-07-01 at 5.38.10 pm.png


8:38
Screen Shot 2021-07-01 at 5.38.34 pm.png
Screen Shot 2021-07-01 at 5.38.34 pm.png


8:39
So I hope that unblocks everyone for now and gives some good insight into how to debug the inside of containers and check network stuff from the inside.
8:39
Oh also, make sure you have Docker Engine 20+ and Docker Compose 2+ (its still in beta i think)
8:40
I had a lot of weird issues once i started tweaking the docker compose files due to some changes in the yaml format which was annoying and now it seems that its refusing to let me down the stacks without manually injecting two ENV variables which is really weird
8:40
STACK_NAME=grid-openmined-org TRAEFIK_PUBLIC_NETWORK=traefik-public docker compose -p au down
8:41
if anyone has an idea on why that is happening let me know but im sure we can figure it out and wrap away all this stuff
8:41
it makes no sense since these are defined in the .env file which is referenced from the compose file
8:45
image.png
image.png


8:45
Also make sure you use the internal host name for the hosts if you are using the JUPYTER notebook inside the containers because localhost will be wrong in that context
8:46
alternatively you can run the notebook outside all of the containers and use localhost but the ports will be the same which is helpful
8:49
So i guess my next step will be to take a look at the blocking incoming message which can be resolved async internally which considering we have a builtin queue and worker container should 100% be achieveable

