[tox]
envlist =
    dev.k8s.launch.datasite
    dev.k8s.launch.gateway
    dev.k8s.launch.datasite.highlow
    dev.k8s.destroy.datasite.highlow
    dev.k8s.registry
    dev.k8s.start
    dev.k8s.deploy
    dev.k8s.hotreload
    dev.k8s.info
    dev.k8s.cleanup
    dev.k8s.destroy
    dev.k8s.destroyall
    dev.k8s.install.signoz
    lint
    stack.test.integration
    syft.docs
    syft.jupyter
    syft.publish
    syft.test.security
    syft.test.unit
    syft.test.scenario
    syft.test.notebook
    syft.test.notebook.scenario
    syft.test.notebook.scenario.sync
    single_container.launch
    single_container.destroy
    stack.test.notebook
    stack.test.integration.k8s
    stack.test.notebook.scenario.k8s
    stack.test.notebook.scenario.k8s.sync
    frontend.test.unit
    frontend.test.e2e
    frontend.generate.types
    syft.build.helm
    syft.package.helm
    syft.test.helm
    syft.test.helm.upgrade
    syft.protocol.check
    syftcli.test.unit
    syftcli.publish
    syftcli.build
    seaweedfs.test.unit
    backend.test.basecpu
    e2e.test.notebook
    migration.prepare
    migration.test
    syft.api.snapshot
skipsdist = True


[testenv]
basepython = {env:TOX_PYTHON:python3}
commands =
    python --version
setenv =
    UV_HTTP_TIMEOUT = 600

# Syft
[testenv:syft]
deps =
    -e{toxinidir}/packages/syft[dev,data_science]
changedir = {toxinidir}/packages/syft
description = Syft
allowlist_externals =
    bash
commands =
    bash -c 'uv pip list || pip list'

# Syft Minimal - without dev+datascience packages
[testenv:syft-minimal]
deps =
    -e{toxinidir}/packages/syft
changedir = {toxinidir}/packages/syft
description = Syft
allowlist_externals =
    bash
commands =
    bash -c 'uv pip list || pip list'

[testenv:syftcli]
deps =
    -e{toxinidir}/packages/syftcli[dev]
changedir = {toxinidir}/packages/syftcli
description = Syft CLI
allowlist_externals =
    bash
commands =
    bash -c 'uv pip list || pip list'

[testenv:syft.publish]
changedir = {toxinidir}/packages/syft
description = Build and Publish Syft Wheel
deps =
    build
commands =
    python -c 'from shutil import rmtree; rmtree("build", True); rmtree("dist", True)'
    python -m build .


[testenv:syftcli.publish]
changedir = {toxinidir}/packages/syftcli
description = Build and Publish Syft CLI Wheel
deps =
    build
allowlist_externals =
    bash
commands =
    bash -c 'rm -rf build/ dist/ syftcli.egg-info/'
    python -m build .

[testenv:syftcli.build]
changedir = {toxinidir}/packages/syftcli
description = Build SyftCLI Binary for each platform
deps =
    -e{toxinidir}/packages/syftcli[build]
allowlist_externals =
    bash
setenv =
    SYFT_CLI_VERSION = {env:SYFT_CLI_VERSION}
commands =
    python -c 'from shutil import rmtree; rmtree("build", True); rmtree("dist", True)'


    ;Since we build universal binary for MacOS,we need to check the python is universal2 or not
    bash -c 'if [[ "$OSTYPE" == "darwin"* ]]; then \
        arch_info=$(lipo -info "$(which python3)"); \
        echo "Arch: $arch_info"; \
        if [[ "$arch_info" == *"Non-fat"* ]]; then \
            echo "Building on MacOS Requires Universal2 python"; \
            echo "Please install universal2 python from https://www.python.org/downloads/macos/"; \
            exit 1; \
        fi; \
    fi'

    ;check the platform and build accordingly by naming the binary as syftcli plus the extension
    ; Check if SYFT_CLI_VERSION is set or choosing the current version available
    bash -c 'if [ -z $SYFT_CLI_VERSION ]; then \
        echo "SYFT_CLI_VERSION is not set"; \
        SYFT_CLI_VERSION=$(python3 syftcli/version.py); \
        echo "Setting SYFT_CLI_VERSION to $SYFT_CLI_VERSION"; \
    else \
        echo "SYFT_CLI_VERSION is already set to $SYFT_CLI_VERSION"; \
    fi && \

    echo "Building syftcli-$SYFT_CLI_VERSION for $OSTYPE" && \

    if [[ "$OSTYPE" == "darwin"* ]]; then \
         pyinstaller --clean --onefile --name syftcli-v$SYFT_CLI_VERSION-macos-universal2 --distpath ./dist/cli syftcli/cli.py --target-arch universal2; \
    elif [[ "$OSTYPE" == "linux-gnu"* ]]; then \
        pyinstaller --clean --onefile --name syftcli-v$SYFT_CLI_VERSION-linux-x86_64 --distpath ./dist/cli syftcli/cli.py; \
    else \
        pyinstaller --clean --onefile --name syftcli-v$SYFT_CLI_VERSION-windows-x86_64  --distpath ./dist/cli syftcli/cli.py; \
    fi'


[testenv:lint]
description = Linting
allowlist_externals =
    bash
deps =
    black[python2]
    isort
    pre-commit
commands =
    black .
    isort .
    pre-commit run --all-files

[testenv:frontend.test.unit]
description = Frontend Unit Tests
allowlist_externals =
    docker
    bash
    pnpm
passenv=HOME, USER
changedir = {toxinidir}/packages/grid/frontend
setenv =
    DOCKER = {env:DOCKER:false}
commands =
    bash -c "echo Running with DOCKER=$DOCKER; date"

    bash -c 'if [[ "$DOCKER" == "false" ]]; then \
        bash ./scripts/check_pnpm.sh; \
        pnpm install; \
        pnpm run test:unit; \
    else \
        docker build --target syft-ui-tests -t ui-test -f frontend.dockerfile .; \
        docker run -t ui-test; \
    fi'



[testenv:syft.docs]
description = Build Docs for Syft
changedir = {toxinidir}/docs
deps =
    {[testenv:syft]deps}
    -r {toxinidir}/docs/requirements.txt
allowlist_externals =
    make
    echo
    cd
    rm
    ls
    xargs
    bash
commands =
    python --version
    bash -c "cd source/api_reference && ls | grep -v index.rst | xargs rm"
    sphinx-apidoc -f -M -d 2 -o ./source/api_reference/ ../packages/syft/src/syft
    make html
    echo "Open: {toxinidir}/docs/build/html/index.html"

[testenv:syft.jupyter]
description = Jupyter Notebook with Editable Syft
deps =
    {[testenv:syft]deps}
    jupyter
    jupyterlab
allowlist_externals =
    bash
commands =
    bash -c 'if [ -z "{posargs}" ]; then \
        jupyter lab --ip 0.0.0.0; \
        else \
        jupyter lab --ip 0.0.0.0 --ServerApp.token={posargs}; \
        fi'

[testenv:syft.protocol.check]
description = Syft Protocol Check
deps =
    {[testenv:syft-minimal]deps}
changedir = {toxinidir}/packages/syft
allowlist_externals =
    bash
setenv =
    BUMP = {env:BUMP:False}
commands =
    bash -c "echo Using BUMP=${BUMP}"
    python -c 'import syft as sy; sy.check_or_stage_protocol()'
    bash -c 'if [[ "$BUMP" != "False" ]]; then \
        python -c "import syft as sy; sy.bump_protocol_version()"; \
        fi'

[testenv:syft.api.snapshot]
description = Syft API Snapshot Check
deps =
    {[testenv:syft-minimal]deps}
changedir = {toxinidir}/packages/syft
allowlist_externals =
    bash
setenv =
    SAVE_SNAP = {env:SAVE_SNAP:False}
    STABLE_RELEASE = {env:STABLE_RELEASE:False}
commands =
    bash -c "echo Using SAVE_SNAP=${SAVE_SNAP}, STABLE_RELEASE=${STABLE_RELEASE}"
    python -c 'import syft as sy; sy.show_api_diff()'
    bash -c 'if [[ "$SAVE_SNAP" != "False" ]]; then \
        python -c "import syft as sy; sy.take_api_snapshot()"; \
        fi'


[testenv:syft.test.security]
description = Security Checks for Syft
changedir = {toxinidir}/packages/syft
deps =
    {[testenv:syft]deps}
commands =
    bandit -r src
    # restrictedpython 6.2
    # Temporarily ignore pytorch vulnerability warning here
    # https://data.safetycli.com/v/71670/97c
    # TODO: Remove `-i 71670` once torch is updated
    safety check -i 70612 -i 71670

[testenv:syft.test.unit]
description = Syft Unit Tests
deps =
    {[testenv:syft]deps}
allowlist_externals =
    bash
    uv
changedir = {toxinidir}/packages/syft
setenv =
    ENABLE_SIGNUP=False
commands =
    bash -c 'ulimit -n 4096 || true'
    pytest -n auto --dist loadgroup --durations=20 --disable-warnings

[testenv:syft.test.scenario]
description = Syft Scenario Tests
deps =
    -e{toxinidir}/packages/syft[dev,data_science]
    pytest-asyncio
    pytest-timeout
    anyio
    unsync
allowlist_externals =
    bash
    uv
changedir = {toxinidir}/tests/scenarios
setenv =
commands =
    pytest -s --disable-warnings

[testenv:syft.test.notebook]
description = Syft Notebook Tests
deps =
    -e{toxinidir}/packages/syft[dev,data_science]
    nbmake
changedir = {toxinidir}/notebooks
allowlist_externals =
    bash
setenv =
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:python}
    DEV_MODE = {env:DEV_MODE:True}
    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:api/0.8,tutorials}
    ENABLE_SIGNUP={env:ENABLE_SIGNUP:False}
    BUMP_PROTOCOL={env:BUMP_PROTOCOL:False}
commands =
    bash -c 'if [[ $BUMP_PROTOCOL == "True" ]]; then \
                python -c "import syft as sy; sy.bump_protocol_version()"; \
            fi;'
    bash -c "echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; ENABLE_SIGNUP=$ENABLE_SIGNUP; date"
    bash -c "for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' '); do \
    if [[ $subfolder == *tutorials* ]]; then \
        pytest -x --nbmake "$subfolder" \
            -p no:randomly \
            --ignore=tutorials/model-training \
            --ignore=tutorials/version-upgrades \
            -n $(python -c 'import multiprocessing; print(multiprocessing.cpu_count())') \
            -vvvv && \
        pytest -x --nbmake tutorials/model-training -p no:randomly -vvvv; \
    else \
        pytest -x --nbmake "$subfolder" -p no:randomly -k 'not 11-container-images-k8s.ipynb' -vvvv; \
    fi \
    done"
    ; pytest -x --nbmake api/0.8 -p no:randomly -vvvv
    ; pytest -x --nbmake api/0.9 -p no:randomly -vvvv
    ; pytest -x --nbmake tutorials -p no:randomly -vvvv
    ; pytest -x --nbmake tutorials/pandas-cookbook -p no:randomly -vvvv


# This is testing BQ without syncing and with in-memory python
[testenv:syft.test.notebook.scenario]
description = Syft Notebook Scenario Tests
deps =
    {[testenv:syft]deps}
    nbmake
    db-dtypes
    google-cloud-bigquery
    aiosmtpd
changedir = {toxinidir}/notebooks
allowlist_externals =
    bash
setenv =
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:python}
    DEV_MODE = {env:DEV_MODE:True}
    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:scenarios/bigquery}
    TEST_query_limit_size={env:test_query_limit_size:500000}
    SERVER_URL = {env:SERVER_URL:http://localhost}
    SERVER_PORT = {env:SERVER_PORT:8080}
    NUM_TEST_USERS = {env:NUM_TEST_USERS:5}
    NUM_TEST_JOBS = {env:NUM_TEST_JOBS:10}
commands =
    python --version
    bash -c "echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; date"
    bash -c "for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' ');\
    do \
        pytest -s -x --nbmake --nbmake-timeout=1000 "$subfolder" --ignore=scenarios/bigquery/sync -p no:randomly -vvvv --log-cli-level=DEBUG --capture=no;\
    done"

# This is testing BQ with syncing and with in-memory python
[testenv:syft.test.notebook.scenario.sync]
description = Syft Notebook Scenario Tests
deps =
    {[testenv:syft]deps}
    nbmake
    db-dtypes
    google-cloud-bigquery
changedir = {toxinidir}/notebooks
allowlist_externals =
    bash
setenv =
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:python}
    DEV_MODE = {env:DEV_MODE:True}
    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:scenarios/bigquery/sync}
    TEST_BIGQUERY_APIS_LIVE = {env:TEST_BIGQUERY_APIS_LIVE:false}
    TEST_query_limit_size={env:test_query_limit_size:500000}
    SERVER_URL = {env:SERVER_URL:http://localhost}
    SERVER_PORT = {env:SERVER_PORT:8080}
commands =

    bash -c "echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; date"
    bash -c "for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' ');\
    do \
        pytest -s -x --nbmake --nbmake-timeout=1000 "$subfolder" -p no:randomly -vvvv;\
    done"


# This is testing BQ without syncing over k8s
[testenv:stack.test.notebook.scenario.k8s]
description = Scenario Notebook Tests for Core Stack using K8s
deps =
    {[testenv:syft]deps}
    nbmake
    db-dtypes
    google-cloud-bigquery
    aiosmtpd
changedir = {toxinidir}
passenv=HOME, USER
allowlist_externals =
    devspace
    kubectl
    grep
    sleep
    bash
    k3d
    echo
    tox
setenv =
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}
    DEVSPACE_PROFILE = bigquery-scenario-tests
    GITHUB_CI = {env:GITHUB_CI:false}
    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}
    DATASITE_CLUSTER_NAME = {env:DATASITE_CLUSTER_NAME:bigquery-high}
    SERVER_URL = {env:SERVER_URL:http://localhost}
    SERVER_PORT = {env:SERVER_PORT:8080}
    TEST_EXTERNAL_REGISTRY = {env:TEST_EXTERNAL_REGISTRY:k3d-registry.localhost:5800}
    TEST_QUERY_LIMIT_SIZE={env:TEST_QUERY_LIMIT_SIZE:500000}
    TRACING={env:TRACING:False}
    NUM_TEST_USERS = {env:NUM_TEST_USERS:5}
    NUM_TEST_JOBS = {env:NUM_TEST_JOBS:10}
    NUM_TEST_WORKERS = {env:NUM_TEST_WORKERS:2}
commands =
    bash -c "python --version || true"
    bash -c "echo Running with GITHUB_CI=$GITHUB_CI; date"
    bash -c "echo Running with TEST_EXTERNAL_REGISTRY=$TEST_EXTERNAL_REGISTRY; date"
    python -c 'import syft as sy; sy.stage_protocol_changes()'
    k3d version

    # Deleting Old Cluster
    bash -c "k3d cluster delete ${DATASITE_CLUSTER_NAME} || true"

    # Deleting registry & volumes
    bash -c "k3d registry delete k3d-registry.localhost || true"
    bash -c "docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true"

    # Create registry
    tox -e dev.k8s.registry


    # Creating bigquery-high cluster on port SERVER_PORT
    bash -c '\
        export CLUSTER_NAME=${DATASITE_CLUSTER_NAME} CLUSTER_HTTP_PORT=${SERVER_PORT} && \
        tox -e dev.k8s.start && \
        tox -e dev.k8s.deploy'

    ; # free up build cache after build of images
    ; bash -c 'if [[ "$GITHUB_CI" != "false" ]]; then \
    ;     docker image prune --all --force; \
    ;     docker builder prune --all --force; \
    ; fi'

    ; sleep 30


    # wait for bigquery-high
    bash packages/grid/scripts/wait_for.sh service postgres --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service backend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service proxy --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service seaweedfs --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service frontend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash -c '(kubectl logs service/frontend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q -E "Network:\s+https?://[a-zA-Z0-9.-]+:[0-9]+/" || true'

    # Checking logs generated & startup of bigquery-high
    bash -c '(kubectl logs service/backend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q "Application startup complete" || true'

    bash -c "pytest -s -x --nbmake notebooks/scenarios/bigquery -p no:randomly --ignore=notebooks/scenarios/bigquery/sync -vvvv --nbmake-timeout=1000 --log-cli-level=DEBUG --capture=no;"

    # deleting clusters created
    bash -c "CLUSTER_NAME=${DATASITE_CLUSTER_NAME} tox -e dev.k8s.destroy || true"
    bash -c "k3d registry delete k3d-registry.localhost || true"
    bash -c "docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true"


# This is testing BQ with syncing over k8s
[testenv:stack.test.notebook.scenario.k8s.sync]
description = Syft Notebook Scenario Tests over k8s
deps =
    {[testenv:syft]deps}
    nbmake
    db-dtypes
    google-cloud-bigquery
changedir = {toxinidir}/notebooks
allowlist_externals =
    bash
setenv =
    DEV_MODE = {env:DEV_MODE:True}
    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:scenarios/bigquery/sync}
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}
    CLUSTER_NAME_HIGH = {env:CLUSTER_NAME_HIGH:bigquery-high}
    CLUSTER_NAME_LOW = {env:CLUSTER_NAME_LOW:bigquery-low}
    CLUSTER_HTTP_PORT_HIGH={env:CLUSTER_HTTP_PORT_HIGH:9081}
    CLUSTER_HTTP_PORT_LOW={env:CLUSTER_HTTP_PORT_LOW:9083}
    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}
    TEST_EXTERNAL_REGISTRY = {env:TEST_EXTERNAL_REGISTRY:k3d-registry.localhost:5800}
commands =
    bash -c "echo Running highlow with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; date"
    bash -c 'tox -e dev.k8s.destroy.datasite.highlow'
    bash -c "k3d registry delete k3d-registry.localhost || true"
    bash -c "docker volume rm k3d-${CLUSTER_NAME_HIGH}-images --force || true"
    bash -c "docker volume rm k3d-${CLUSTER_NAME_LOW}-images --force || true"

    # Now create everything
    bash -c 'tox -e dev.k8s.launch.datasite.highlow'

    bash -c "for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' ');\
    do \
        pytest -x --nbmake --nbmake-timeout=1000 "$subfolder" -p no:randomly -vvvv;\
    done"

    # Clean up again
    bash -c 'tox -e dev.k8s.destroy.datasite.highlow'
    bash -c "k3d registry delete k3d-registry.localhost || true"
    bash -c "docker volume rm k3d-${CLUSTER_NAME_HIGH}-images --force || true"
    bash -c "docker volume rm k3d-${CLUSTER_NAME_LOW}-images --force || true"


[testenv:single_container.launch]
description = Launch a single backend container using the dockerfile
changedir = {toxinidir}/packages
setenv =
    N_CONSUMERS = {env:N_CONSUMERS:1}
    SERVER_NAME = {env:SERVER_NAME:test_datasite_sc}
    SERVER_TYPE = {env:SERVER_TYPE:datasite}
    SERVER_PORT = {env:SERVER_PORT:8080}
allowlist_externals =
    bash
commands =
    bash -c 'tox -e single_container.destroy'
    bash -c 'docker build -f grid/backend/backend.dockerfile . -t openmined/syft-backend:local-dev'
    bash -c 'docker run -d \
    -e SERVER_NAME=${SERVER_NAME} \
    -e SERVER_TYPE=${SERVER_TYPE} \
    -e N_CONSUMERS=${N_CONSUMERS} \
    -e SINGLE_CONTAINER_MODE=true \
    -e CREATE_PRODUCER=true \
    -e INMEMORY_WORKERS=true \
    -p ${SERVER_PORT}:80 --add-host=host.docker.internal:host-gateway \
    --name ${SERVER_NAME} openmined/syft-backend:local-dev'

[testenv:single_container.destroy]
description = Destroy the single backend container run using single_container.launch
changedir = {toxinidir}/packages
setenv =
    SERVER_NAME = {env:SERVER_NAME:test_datasite_sc}
allowlist_externals =
    bash
commands =
    # Image is not cleaned up
    bash -c 'docker stop ${SERVER_NAME} || true'
    bash -c 'docker rm ${SERVER_NAME} || true'

[testenv:stack.test.notebook]
description = Stack Notebook Tests
deps =
    {[testenv:syft]deps}
    nbmake
changedir = {toxinidir}/notebooks
allowlist_externals =
    bash
setenv =
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}
    DEV_MODE = {env:DEV_MODE:True}
    TEST_NOTEBOOK_PATHS = {env:TEST_NOTEBOOK_PATHS:api/0.8}
    ENABLE_SIGNUP=True
    SERVER_URL = {env:SERVER_URL:http://localhost}
    SERVER_PORT = {env:SERVER_PORT:8080}
commands =

    # Volume cleanup
    bash -c 'docker volume rm -f $(docker volume ls -q --filter "label=orgs.openmined.syft") || true'
    bash -c 'docker volume rm -f $(docker volume ls -q --filter "label=com.docker.volume.anonymous") || true'
    bash -c 'docker network rm -f $(docker network ls -q --filter "label=orgs.openmined.syft") || true'

    bash -c "echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE DEV_MODE=$DEV_MODE TEST_NOTEBOOK_PATHS=$TEST_NOTEBOOK_PATHS; date"
    bash -c "for subfolder in $(echo ${TEST_NOTEBOOK_PATHS} | tr ',' ' ');\
    do \
        pytest -x --nbmake --nbmake-timeout=1000 "$subfolder" -p no:randomly -vvvv -k 'not 11-container-images-k8s.ipynb';\
    done"

    ; pytest -x --nbmake --nbmake-timeout=1000 api/0.8 -p no:randomly -vvvv
    ; pytest -x --nbmake --nbmake-timeout=1000 api/0.9 -p no:randomly -vvvv
    ; pytest -x --nbmake --nbmake-timeout=1000 tutorials -p no:randomly -vvvv
    ; pytest -x --nbmake --nbmake-timeout=1000 tutorials/pandas-cookbook -p no:randomly -vvvv

    bash -c 'docker volume rm -f $(docker volume ls -q --filter "label=orgs.openmined.syft") || true'
    bash -c 'docker volume rm -f $(docker volume ls -q --filter "label=com.docker.volume.anonymous") || true'
    bash -c 'docker network rm -f $(docker network ls -q --filter "label=orgs.openmined.syft") || true'



[testenv:frontend.generate.types]
description = Generate Types for Frontend
deps =
    {[testenv:syft]deps}
allowlist_externals =
    cd
    bash
    pnpm
changedir = {toxinidir}/packages/grid/frontend
passenv =
    PNPM_HOME
commands =
    bash -c ./scripts/check_pnpm.sh
    pnpm add -g json-schema-to-typescript

    ; clear the old ones
    bash -c 'rm -rf ./schema'
    bash -c 'rm -rf ./src/types/generated'

    ; generate new ones
    bash -c 'python3 -c "import syft as sy;sy.util.schema.generate_json_schemas()"'
    bash -c "json2ts -i './schema/**/*.json' -o ./src/types/generated"
    bash -c "python3 ./scripts/replace_imports.py ./src/types/generated"

[mypy]
python_version = 3.12
disable_error_code = attr-defined, valid-type, no-untyped-call, arg-type

[testenv:syft.test.integration]
description = Integration Tests for Syft Stack
deps =
    {[testenv:syft]deps}
changedir = {toxinidir}
passenv=HOME, USER
allowlist_externals =
    bash
setenv =
    PYTEST_MODULES = {env:PYTEST_MODULES:local_server}
    ASSOCIATION_REQUEST_AUTO_APPROVAL = {env:ASSOCIATION_REQUEST_AUTO_APPROVAL:true}
    PYTEST_FLAGS = {env:PYTEST_FLAGS:--ignore=tests/integration/local/job_test.py}
commands =
    python -c 'import syft as sy; sy.stage_protocol_changes()'

    # Run Integration Tests
    bash -c '\
        PYTEST_MODULES=($PYTEST_MODULES); \
        for i in "${PYTEST_MODULES[@]}"; do \
            echo "Starting test for $i"; date; \
            pytest tests/integration -m $i -vvvv -p no:randomly -p no:benchmark -o log_cli=True --capture=no $PYTEST_FLAGS; \
            return=$?; \
            echo "Finished $i"; \
            date; \
            if [[ $return -ne 0 ]]; then \
                exit $return; \
            fi; \
        done'

[testenv:stack.test.integration.k8s]
description = Integration Tests for Core Stack using K8s
deps =
    {[testenv:syft]deps}
changedir = {toxinidir}
passenv=HOME, USER, AZURE_BLOB_STORAGE_KEY
allowlist_externals =
    devspace
    kubectl
    grep
    sleep
    bash
    kubectx
    k3d
    echo
    tox
setenv =
    SERVER_PORT = {env:SERVER_PORT:9082}
    GITHUB_CI = {env:GITHUB_CI:false}
    PYTEST_MODULES = {env:PYTEST_MODULES:frontend network container_workload}
    DATASITE_CLUSTER_NAME = {env:DATASITE_CLUSTER_NAME:test-datasite-1}
    GATEWAY_CLUSTER_NAME = {env:GATEWAY_CLUSTER_NAME:test-gateway-1}
    ASSOCIATION_REQUEST_AUTO_APPROVAL = {env:ASSOCIATION_REQUEST_AUTO_APPROVAL:true}
    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}
commands =
    bash -c "echo Running with GITHUB_CI=$GITHUB_CI; date"
    python -c 'import syft as sy; sy.stage_protocol_changes()'
    k3d version

    # Deleting Old Cluster
    bash -c "k3d cluster delete ${DATASITE_CLUSTER_NAME} || true"
    bash -c "k3d cluster delete ${GATEWAY_CLUSTER_NAME} || true"

    # Deleting registry & volumes
    bash -c "k3d registry delete k3d-registry.localhost || true"
    bash -c "docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true"
    bash -c "docker volume rm k3d-${GATEWAY_CLUSTER_NAME}-images --force || true"

    # Create registry
    tox -e dev.k8s.registry

    # Creating test-gateway-1 cluster on port 9081
    bash -c '\
        export CLUSTER_NAME=${GATEWAY_CLUSTER_NAME} CLUSTER_HTTP_PORT=9081 DEVSPACE_PROFILE=gateway && \
        tox -e dev.k8s.start && \
            tox -e dev.k8s.deploy'

    # Creating test-datasite-1 cluster on port 9082
    bash -c '\
        export CLUSTER_NAME=${DATASITE_CLUSTER_NAME} CLUSTER_HTTP_PORT=9082 DEVSPACE_PROFILE=datasite-tunnel && \
        tox -e dev.k8s.start && \
        tox -e dev.k8s.deploy'

    # free up build cache after build of images
    bash -c 'if [[ "$GITHUB_CI" != "false" ]]; then \
        docker image prune --all --force; \
        docker builder prune --all --force; \
    fi'

    sleep 30

    # wait for test gateway 1
    bash packages/grid/scripts/wait_for.sh service postgres --context k3d-{env:GATEWAY_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service backend --context k3d-{env:GATEWAY_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service proxy --context k3d-{env:GATEWAY_CLUSTER_NAME} --namespace syft

    # wait for test datasite 1
    bash packages/grid/scripts/wait_for.sh service postgres --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service backend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service proxy --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service seaweedfs --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service frontend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash -c '(kubectl logs service/frontend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q -E "Network:\s+https?://[a-zA-Z0-9.-]+:[0-9]+/" || true'

    # Checking logs generated & startup of test-datasite 1
    bash -c '(kubectl logs service/backend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q "Application startup complete" || true'
    # Checking logs generated & startup of testgateway1
    bash -c '(kubectl logs service/backend --context k3d-${GATEWAY_CLUSTER_NAME} --namespace syft -f &) | grep -q "Application startup complete" || true'

    # Run Integration Tests
    bash -c '\
        PYTEST_MODULES=($PYTEST_MODULES); \
        for i in "${PYTEST_MODULES[@]}"; do \
            echo "Starting test for $i"; date; \
            pytest tests/integration -m $i -vvvv -p no:randomly -p no:benchmark -o log_cli=True --capture=no; \
            return=$?; \
            echo "Finished $i"; \
            date; \
            if [[ $return -ne 0 ]]; then \
                exit $return; \
            fi; \
        done'

    # deleting clusters created
    bash -c "CLUSTER_NAME=${DATASITE_CLUSTER_NAME} tox -e dev.k8s.destroy || true"
    bash -c "CLUSTER_NAME=${GATEWAY_CLUSTER_NAME} tox -e dev.k8s.destroy || true"
    bash -c "k3d registry delete k3d-registry.localhost || true"
    bash -c "docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true"
    bash -c "docker volume rm k3d-${GATEWAY_CLUSTER_NAME}-images --force || true"

[testenv:stack.test.notebook.k8s]
description = Notebook Tests for Core Stack using K8s
deps =
    {[testenv:syft]deps}
    nbmake
changedir = {toxinidir}
passenv=HOME, USER
allowlist_externals =
    devspace
    kubectl
    grep
    sleep
    bash
    k3d
    echo
    tox
setenv =
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}
    GITHUB_CI = {env:GITHUB_CI:false}
    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}
    DATASITE_CLUSTER_NAME = {env:DATASITE_CLUSTER_NAME:test-datasite-1}
    SERVER_URL = {env:SERVER_URL:http://localhost}
    SERVER_PORT = {env:SERVER_PORT:8080}
commands =
    bash -c "echo Running with GITHUB_CI=$GITHUB_CI; date"
    python -c 'import syft as sy; sy.stage_protocol_changes()'
    k3d version

    # Deleting Old Cluster
    bash -c "k3d cluster delete ${DATASITE_CLUSTER_NAME} || true"

    # Deleting registry & volumes
    bash -c "k3d registry delete k3d-registry.localhost || true"
    bash -c "docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true"

    # Create registry
    tox -e dev.k8s.registry


    # Creating test-datasite-1 cluster on port SERVER_PORT
    bash -c '\
        export CLUSTER_NAME=${DATASITE_CLUSTER_NAME} CLUSTER_HTTP_PORT=${SERVER_PORT} && \
        tox -e dev.k8s.start && \
        tox -e dev.k8s.deploy'

    # free up build cache after build of images
    bash -c 'if [[ "$GITHUB_CI" != "false" ]]; then \
        docker image prune --all --force; \
        docker builder prune --all --force; \
    fi'

    sleep 30

    # wait for test-datasite-1
    bash packages/grid/scripts/wait_for.sh service postgres --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service backend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service proxy --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service seaweedfs --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service frontend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash -c '(kubectl logs service/frontend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q -E "Network:\s+https?://[a-zA-Z0-9.-]+:[0-9]+/" || true'

    # Checking logs generated & startup of test-datasite 1
    bash -c '(kubectl logs service/backend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q "Application startup complete" || true'

    bash -c "pytest -x --nbmake notebooks/api/0.8 -p no:randomly -k 'not 14-container-images.ipynb' -vvvv --nbmake-timeout=1000"

    # deleting clusters created
    bash -c "CLUSTER_NAME=${DATASITE_CLUSTER_NAME} tox -e dev.k8s.destroy || true"
    bash -c "k3d registry delete k3d-registry.localhost || true"
    bash -c "docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true"


[testenv:syft.build.helm]
description = Build Helm Chart for Kubernetes
changedir = {toxinidir}
passenv=HOME, USER
allowlist_externals =
    bash
    helm
commands =
    bash -c 'cd packages/grid/helm && \
        python3 generate_helm_notes.py ./syft/templates'

    bash -c 'cd packages/grid/helm && \
        helm lint syft'


[testenv:syft.lint.helm]
description = Lint helm chart
changedir = {toxinidir}/packages/grid/helm
passenv=HOME, USER
allowlist_externals =
    bash
commands =
    bash -c 'kube-linter lint ./syft --config ./kubelinter-config.yaml'

[testenv:syft.package.helm]
description = Package Helm Chart for Kubernetes
deps =
changedir = {toxinidir}
passenv=HOME, USER
allowlist_externals =
    bash
    helm
commands =
    bash -c 'cd packages/grid/helm && \
        helm lint syft'

    bash -c 'cd packages/grid/helm/syft && \
        helm dependency update'

    bash -c 'cd packages/grid/helm && \
        helm package syft --destination repo'

    bash -c 'cd packages/grid/helm/repo && \
        helm repo index . --url https://openmined.github.io/PySyft/helm'


[testenv:dev.k8s.ready]
description = Check readiness of k8s deployement
changedir = {toxinidir}/packages/grid
allowlist_externals =
    bash
    tox
    curl
setenv =
    CLUSTER_NAME = {env:CLUSTER_NAME:syft}
    CLUSTER_HTTP_PORT = {env:SERVER_PORT:8080}
; Usage for posargs: names of the relevant services among {frontend backend proxy postgres seaweedfs registry}
commands =
    bash -c "env; date; k3d version"

    # Frontend
    bash -c "if echo '{posargs}' | grep -q 'frontend'; then \
            echo 'Checking readiness of frontend'; \
            ./scripts/wait_for.sh service frontend --context k3d-$CLUSTER_NAME --namespace syft && \
            (kubectl logs service/frontend --context k3d-$CLUSTER_NAME --namespace syft -f &) | grep -q -E 'Network:\s+https?://[a-zA-Z0-9.-]+:[0-9]+/' || true; \
            fi"

    # Backend
    bash -c "if echo '{posargs}' | grep -q 'backend'; then \
            echo 'Checking readiness of backend'; \
            ./scripts/wait_for.sh service backend --context k3d-$CLUSTER_NAME --namespace syft && \
            (kubectl logs service/backend --context k3d-$CLUSTER_NAME --namespace syft -f &) | grep -q 'Application startup complete' || true; \
            fi"

    # Mongo
    bash -c "if echo '{posargs}' | grep -q 'postgres'; then echo 'Checking readiness of Postgres'; ./scripts/wait_for.sh service postgres --context k3d-$CLUSTER_NAME --namespace syft; fi"

    # Proxy
    bash -c "if echo '{posargs}' | grep -q 'proxy'; then echo 'Checking readiness of proxy'; ./scripts/wait_for.sh service proxy --context k3d-$CLUSTER_NAME --namespace syft; fi"

    # Seaweedfs
    bash -c "if echo '{posargs}' | grep -q 'seaweedfs'; then echo 'Checking readiness of SeaweedFS'; ./scripts/wait_for.sh service seaweedfs --context k3d-$CLUSTER_NAME --namespace syft; fi"

    # Registry
    bash -c "if echo '{posargs}' | grep -q 'registry'; then echo 'Checking readiness of Registry'; ./scripts/wait_for.sh service registry --context k3d-$CLUSTER_NAME --namespace syft; fi"

    # Extra
    bash -c "curl http://localhost:${CLUSTER_HTTP_PORT}/api/v2/metadata"


[testenv:syft.test.helm]
description = Test Helm Chart for Kubernetes
changedir = {toxinidir}/packages/grid
passenv=HOME, USER, EXTERNAL_REGISTRY_USERNAME, EXTERNAL_REGISTRY_PASSWORD
allowlist_externals =
    bash
    tox
setenv =
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}
    SERVER_PORT = {env:SERVER_PORT:8080}
    SERVER_URL = {env:SERVER_URL:http://localhost}
    EXCLUDE_NOTEBOOKS = {env:EXCLUDE_NOTEBOOKS:not 14-container-images.ipynb}
    SYFT_VERSION = {env:SYFT_VERSION:local}
    EXTERNAL_REGISTRY = {env:EXTERNAL_REGISTRY:k3d-registry.localhost:5800}
    ; env vars for dev.k8s.start
    CLUSTER_NAME = testdatasite
    CLUSTER_HTTP_PORT = {env:SERVER_PORT:8080}
; Usage for posargs: if you pass override to this tox command, then resourcesPreset will be overridden
commands =
    bash -c "env; date; k3d version"

    bash -c "k3d cluster delete ${CLUSTER_NAME} || true"

    tox -e dev.k8s.start

    bash -c 'if [[ $SYFT_VERSION == "local" ]]; then \
        echo "Installing local helm charts"; \
        if [[ "{posargs}" == "override" ]]; then \
            echo "Overriding resourcesPreset"; \
            helm install ${CLUSTER_NAME} ./helm/syft -f ./helm/examples/dev/base.yaml --kube-context k3d-${CLUSTER_NAME} --namespace syft --create-namespace --set server.resourcesPreset=null --set seaweedfs.resourcesPreset=null --set postgres.resourcesPreset=null --set registry.resourcesPreset=null --set proxy.resourcesPreset=null --set frontend.resourcesPreset=null; \
        else \
            helm install ${CLUSTER_NAME} ./helm/syft -f ./helm/examples/dev/base.yaml --kube-context k3d-${CLUSTER_NAME} --namespace syft --create-namespace; \
        fi \
    else \
        echo "Installing helm charts from repo for syft version: ${SYFT_VERSION}"; \
        helm repo add openmined https://openmined.github.io/PySyft/helm; \
        helm repo update openmined; \
        if [[ "{posargs}" == "override" ]]; then \
            echo "Overriding resourcesPreset"; \
            helm install ${CLUSTER_NAME} openmined/syft --version=${SYFT_VERSION} -f ./helm/examples/dev/base.yaml --kube-context k3d-${CLUSTER_NAME} --namespace syft --create-namespace --set server.resourcesPreset=null --set seaweedfs.resourcesPreset=null --set postgres.resourcesPreset=null --set registry.resourcesPreset=null --set proxy.resourcesPreset=null --set frontend.resourcesPreset=null; \
        else \
            helm install ${CLUSTER_NAME} openmined/syft --version=${SYFT_VERSION} -f ./helm/examples/dev/base.yaml --kube-context k3d-${CLUSTER_NAME} --namespace syft --create-namespace; \
        fi \
    fi'

    ; wait for everything else to be loaded
    tox -e dev.k8s.ready -- frontend backend postgres proxy seaweedfs registry

    # Run Notebook tests
    tox -e e2e.test.notebook

    bash -c "k3d cluster delete ${CLUSTER_NAME} || true"

[testenv:syft.test.helm.upgrade]
description = Test helm upgrade
changedir = {toxinidir}/packages/grid/
passenv=HOME,USER,KUBE_CONTEXT
setenv =
    UPGRADE_TYPE = {env:UPGRADE_TYPE:ProdToBeta}
allowlist_externals =
    bash
commands =
    bash ./scripts/helm_upgrade.sh {env:UPGRADE_TYPE}

[testenv:syftcli.test.unit]
description = Syft CLI Unit Tests
deps =
    {[testenv:syftcli]deps}
changedir = {toxinidir}/packages/syftcli
allowlist_externals =
    uv
    pytest
commands =
    pytest

[testenv:dev.k8s.registry]
description = Start local Kubernetes registry with k3d
changedir = {toxinidir}
passenv=HOME,USER
allowlist_externals =
    bash
    sudo
commands =
    ; check k3d version
    bash -c 'k3d --version'

    ; create registry
    bash -c 'docker volume create k3d-registry-vol || true'
    bash -c 'k3d registry create registry.localhost --port 5800 -v k3d-registry-vol:/var/lib/registry --no-help || true'

    ; add patches to host
    bash -c 'if ! grep -q k3d-registry.localhost /etc/hosts; then sudo {envpython} scripts/patch_hosts.py --add-k3d-registry --fix-docker-hosts; fi'

    ; Fail this command if registry is not working
    bash -c 'curl --retry 5 --retry-all-errors http://k3d-registry.localhost:5800/v2/_catalog'

[testenv:dev.k8s.patch.coredns]
description = Patch CoreDNS to resolve k3d-registry.localhost
changedir = {toxinidir}
passenv=HOME,USER,CLUSTER_NAME
setenv=
    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}
allowlist_externals =
    bash
commands =
    ; patch coredns so k3d-registry.localhost works in k3d
    bash -c 'kubectl apply -f ./scripts/k8s-coredns-custom.yml --context k3d-${CLUSTER_NAME}'

    ; restarts coredns
    bash -c 'kubectl delete pod -n kube-system -l k8s-app=kube-dns --context k3d-${CLUSTER_NAME}'

[testenv:dev.k8s.install.signoz]
description = Install Signoz on local Kubernetes cluster
changedir = {toxinidir}
passenv=HOME,USER,CLUSTER_NAME
allowlist_externals=
    bash
setenv=
    CLUSTER_NAME = {env:CLUSTER_NAME:test-datasite-1}
    SIGNOZ_PORT = {env:SIGNOZ_PORT:3301}
commands=
    bash -c 'if [ "{posargs}" ]; then kubectl config use-context {posargs}; fi'
    bash -c 'helm repo add signoz https://charts.signoz.io && helm repo update'
    bash -c 'helm install syft signoz/signoz --namespace platform --create-namespace || true'
    # bash -c 'k3d cluster edit ${CLUSTER_NAME} --port-add "${SIGNOZ_PORT}:3301@loadbalancer"'
    ; bash packages/grid/scripts/wait_for.sh service syft-signoz-frontend --context k3d-{env:CLUSTER_NAME} --namespace platform


[testenv:dev.k8s.start]
description = Start local Kubernetes registry & cluster with k3d
changedir = {toxinidir}
passenv = HOME, USER
setenv =
    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}
    CLUSTER_HTTP_PORT = {env:CLUSTER_HTTP_PORT:8080}
    # SIGNOZ_PORT = {env:SIGNOZ_PORT:3301}
allowlist_externals =
    bash
    sleep
    tox
commands =
    ; start registry
    tox -e dev.k8s.registry

    ; for NodePort to work add the following --> -p "NodePort:NodePort@loadbalancer"
    bash -c 'k3d cluster create ${CLUSTER_NAME} \
            -p "${CLUSTER_HTTP_PORT}:80@loadbalancer" \
            --registry-use k3d-registry.localhost:5800 {posargs} && \
            kubectl --context k3d-${CLUSTER_NAME} create namespace syft || true'
    # -p "${SIGNOZ_PORT}:3301@loadbalancer" \
    ; patch coredns
    tox -e dev.k8s.patch.coredns

    ; dump cluster info
    tox -e dev.k8s.info

[testenv:dev.k8s.deploy]
description = Deploy Syft to a local Kubernetes cluster with Devspace
changedir = {toxinidir}/packages/grid
passenv = HOME, USER, DEVSPACE_PROFILE
setenv=
    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}
    TRACING = {env:TRACING:False}
allowlist_externals =
    bash
commands =
    ; deploy syft helm charts
    bash -c 'echo "profile=$DEVSPACE_PROFILE"'
    bash -c "echo Running with TRACING=$TRACING; date"
    bash -c '\
        if [[ -n "${DEVSPACE_PROFILE}" ]]; then export DEVSPACE_PROFILE="-p ${DEVSPACE_PROFILE}"; fi && \
        if [[ "${TRACING}" == "True" ]]; then DEVSPACE_PROFILE="${DEVSPACE_PROFILE} -p tracing"; fi && \
        if [[ "${TRACING}" == "True" ]]; then echo "TRACING PROFILE ENABLED"; fi && \
        devspace deploy -b --kube-context k3d-${CLUSTER_NAME} --no-warn ${DEVSPACE_PROFILE} --namespace syft --var CONTAINER_REGISTRY=k3d-registry.localhost:5800'

    # if TRACING is enabled start signoz
    ; bash -c 'if [[ "${TRACING}" == "True" ]]; then tox -e dev.k8s.install.signoz; fi'

[testenv:dev.k8s.hotreload]
description = Start development with hot-reload in Kubernetes
changedir = {toxinidir}/packages/grid
passenv = HOME, USER, DEVSPACE_PROFILE
setenv=
    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}
allowlist_externals =
    bash
commands =
    ; deploy syft helm charts with hot-reload
    bash -c '\
        if [[ -n "${DEVSPACE_PROFILE}" ]]; then export DEVSPACE_PROFILE="-p ${DEVSPACE_PROFILE}"; fi && \
        devspace dev --kube-context k3d-${CLUSTER_NAME} --no-warn ${DEVSPACE_PROFILE} --namespace syft --var CONTAINER_REGISTRY=k3d-registry.localhost:5800'

[testenv:dev.k8s.info]
description = Gather info about the localKubernetes cluster
passenv = HOME, USER
ignore_errors = True
allowlist_externals =
    k3d
    kubectl
commands =
    k3d cluster list
    kubectl cluster-info
    kubectl config current-context
    kubectl get namespaces

[testenv:dev.k8s.cleanup]
description = Cleanup Syft deployment and associated resources, but keep the cluster running
changedir = {toxinidir}/packages/grid
passenv=HOME, USER
setenv=
    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}
allowlist_externals =
    bash
commands =
    bash -c 'devspace purge --force-purge --kube-context k3d-${CLUSTER_NAME} --no-warn --namespace syft; sleep 3'
    bash -c 'devspace cleanup images --kube-context k3d-${CLUSTER_NAME} --no-warn --namespace syft --var CONTAINER_REGISTRY=k3d-registry.localhost:5800 || true'
    bash -c 'kubectl --context k3d-${CLUSTER_NAME} delete namespace syft --now=true || true'

[testenv:dev.k8s.render]
description = Dump devspace rendered chargs for debugging. Save in `packages/grid/out.render`
changedir = {toxinidir}/packages/grid
passenv = HOME, USER, DEVSPACE_PROFILE
setenv=
    OUTPUT_DIR = {env:OUTPUT_DIR:./.devspace/rendered}
allowlist_externals =
    bash
commands =
    bash -c '\
        if [[ -n "${DEVSPACE_PROFILE}" ]]; then export DEVSPACE_PROFILE="-p ${DEVSPACE_PROFILE}"; fi && \
        rm -rf ${OUTPUT_DIR} && \
        mkdir -p ${OUTPUT_DIR} && \
        echo "profile: $DEVSPACE_PROFILE" && \
        devspace print ${DEVSPACE_PROFILE} > ${OUTPUT_DIR}/config.txt && \
        devspace deploy --render --skip-build --no-warn ${DEVSPACE_PROFILE} --namespace syft --var CONTAINER_REGISTRY=k3d-registry.localhost:5800 > ${OUTPUT_DIR}/chart.yaml'

[testenv:dev.k8s.launch.gateway]
description = Launch a single gateway on K8s
passenv = HOME, USER
setenv=
    CLUSTER_NAME = {env:CLUSTER_NAME:test-gateway-1}
    CLUSTER_HTTP_PORT={env:CLUSTER_HTTP_PORT:9081}
    DEVSPACE_PROFILE=gateway
allowlist_externals =
    tox
commands =
    tox -e dev.k8s.start
    tox -e dev.k8s.{posargs:deploy}


[testenv:dev.k8s.launch.datasite.highlow]
description = Launch a high and a low side datasite on K8s
passenv = HOME, USER
setenv=
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}
    CLUSTER_NAME_HIGH = {env:CLUSTER_NAME_HIGH:test-datasite-high-1}
    CLUSTER_NAME_LOW = {env:CLUSTER_NAME_LOW:test-datasite-low-1}
    CLUSTER_HTTP_PORT_HIGH={env:CLUSTER_HTTP_PORT_HIGH:9081}
    CLUSTER_HTTP_PORT_LOW={env:CLUSTER_HTTP_PORT_LOW:9083}
    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}
    TEST_EXTERNAL_REGISTRY = {env:TEST_EXTERNAL_REGISTRY:k3d-registry.localhost:5800}
allowlist_externals =
    tox
    echo
    bash
commands =
    bash -c 'echo "Launching high datasite: $CLUSTER_NAME_HIGH" && \
    CLUSTER_NAME=$CLUSTER_NAME_HIGH CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_HIGH \
    tox -e dev.k8s.launch.datasite'

    bash -c 'packages/grid/scripts/wait_for.sh service backend --context k3d-{env:CLUSTER_NAME_HIGH} --namespace syft'

    bash -c 'echo "Launching low datasite: $CLUSTER_NAME_LOW" &&  \
    CLUSTER_NAME=$CLUSTER_NAME_LOW CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_LOW \
    DEVSPACE_PROFILE="datasite-low" tox -e dev.k8s.launch.datasite'

    bash -c 'echo "Waiting for services to be ready"'

    bash -c 'CLUSTER_NAME=$CLUSTER_NAME_HIGH CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_HIGH source ./scripts/get_k8s_secret_ci.sh \
        && CLUSTER_NAME=$CLUSTER_NAME_HIGH CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_HIGH ./scripts/display_credentials.sh'

    bash -c 'packages/grid/scripts/wait_for.sh service backend --context k3d-{env:CLUSTER_NAME_LOW} --namespace syft'

    bash -c 'CLUSTER_NAME=$CLUSTER_NAME_LOW CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_LOW source ./scripts/get_k8s_secret_ci.sh \
        && CLUSTER_NAME=$CLUSTER_NAME_LOW CLUSTER_HTTP_PORT=$CLUSTER_HTTP_PORT_LOW ./scripts/display_credentials.sh'


[testenv:dev.k8s.destroy.datasite.highlow]
description = Destroy a high and a low side datasite on K8s
passenv = HOME, USER
setenv=
    CLUSTER_NAME_HIGH = {env:CLUSTER_NAME_HIGH:test-datasite-high-1}
    CLUSTER_NAME_LOW = {env:CLUSTER_NAME_LOW:test-datasite-low-1}
allowlist_externals =
    tox
    echo
    bash
commands =
    bash -c 'echo "Destroying high datasite: $CLUSTER_NAME_HIGH" && \
    CLUSTER_NAME=$CLUSTER_NAME_HIGH tox -e dev.k8s.destroy'

    bash -c 'echo "Destroying low datasite: $CLUSTER_NAME_LOW" && \
    CLUSTER_NAME=$CLUSTER_NAME_LOW tox -e dev.k8s.destroy'


[testenv:dev.k8s.launch.datasite]
description = Launch a single datasite on K8s
passenv = HOME, USER
setenv=
    CLUSTER_NAME = {env:CLUSTER_NAME:test-datasite-1}
    CLUSTER_HTTP_PORT={env:CLUSTER_HTTP_PORT:9082}
    DEVSPACE_PROFILE={env:DEVSPACE_PROFILE}
allowlist_externals =
    tox
    bash
commands =
    bash -c "CLUSTER_NAME=${CLUSTER_NAME} tox -e dev.k8s.destroy"
    tox -e dev.k8s.start
    tox -e dev.k8s.{posargs:deploy}

[testenv:dev.k8s.launch.enclave]
description = Launch a single Enclave on K8s
passenv = HOME, USER
setenv=
    CLUSTER_NAME = {env:CLUSTER_NAME:test-enclave-1}
    CLUSTER_HTTP_PORT={env:CLUSTER_HTTP_PORT:9083}
    DEVSPACE_PROFILE=enclave
allowlist_externals =
    tox
commands =
    tox -e dev.k8s.start -- --volume /sys/kernel/security:/sys/kernel/security --volume /dev/tpmrm0:/dev/tpmrm0
    tox -e dev.k8s.{posargs:deploy}

[testenv:dev.k8s.destroy]
description = Destroy local Kubernetes cluster
changedir = {toxinidir}/packages/grid
passenv = HOME, USER
setenv=
    CLUSTER_NAME = {env:CLUSTER_NAME:syft-dev}
allowlist_externals =
    tox
    bash
commands =
    ; destroy cluster
    bash -c '\
        rm -rf .devspace; echo ""; \
        k3d cluster delete ${CLUSTER_NAME};'

[testenv:dev.k8s.destroyall]
description = Destroy both local Kubernetes cluster and registry
changedir = {toxinidir}
passenv = HOME, USER, CLUSTER_NAME
ignore_errors=True
allowlist_externals =
    bash
    tox
commands =
    ; destroy cluster
    tox -e dev.k8s.destroy

    ; destroy registry
    bash -c 'k3d registry delete registry.localhost || true'
    bash -c 'docker volume rm k3d-registry-vol --force || true'

[testenv:backend.test.basecpu]
description = Base CPU Docker Image Test
changedir = {toxinidir}/packages
allowlist_externals =
    docker
    bash
    env
setenv =
    PIP_PACKAGES = {env:PIP_PACKAGES:llama-index opendp}
    SYSTEM_PACKAGES = {env:SYSTEM_PACKAGES:curl wget}
    BUILD_PLATFORM = {env:BUILD_PLATFORM:linux/amd64}
commands =
    env

    ; Build the base image
    bash -c 'docker buildx build \
        --platform $BUILD_PLATFORM \
        -f ./grid/backend/grid/images/worker_cpu.dockerfile . \
        -t cpu-worker:latest'
    bash -c 'docker rmi cpu-worker:latest'

    bash -c '\
        docker buildx build \
            --platform $BUILD_PLATFORM \
            -f grid/backend/grid/images/worker_cpu.dockerfile . \
            -t cpu-worker:opendp \
            --build-arg PIP_PACKAGES="$PIP_PACKAGES" \
            --build-arg SYSTEM_PACKAGES="$SYSTEM_PACKAGES"'
    ; bash -c 'for pkg in $PIP_PACKAGES; do docker run --rm cpu-worker:opendp pip list | grep $pkg || uv pip list | grep $pkg; done'
    bash -c '\
        pip_pkgs=$(docker run --rm cpu-worker:opendp uv pip list || pip list); \
        for pkg in $PIP_PACKAGES; do echo $pip_pkgs | grep $pkg; done'
    bash -c 'for pkg in $SYSTEM_PACKAGES; do docker run --rm cpu-worker:opendp apk -e info "$pkg"; done'
    bash -c 'docker rmi cpu-worker:opendp'

    bash -c '\
        docker buildx build \
            --platform $BUILD_PLATFORM \
            -f grid/backend/grid/images/worker_cpu.dockerfile . \
            -t cpu-worker:custom-cmd  \
            --build-arg SYSTEM_PACKAGES="perl wget curl make " \
            --build-arg CUSTOM_CMD="""wget -O - "https://github.com/cowsay-org/cowsay/archive/refs/tags/v3.7.0.tar.gz" | tar xvzf - && cd cowsay-3.7.0 && make"""'
    bash -c 'for pkg in perl make curl wget; do docker run --rm cpu-worker:custom-cmd apk -e info "$pkg"; done'
    bash -c 'docker run --rm cpu-worker:custom-cmd bash -c "cd cowsay-3.7.0 && curl https://api.github.com/zen -s | ./cowsay"'
    bash -c 'docker rmi cpu-worker:custom-cmd'


# There are other adjacent notebook tests like
# stack.test.notebook, syft.test.notebook, etc.
# They could be modularized and reused.
# The below is the notebook test suite for point at external servers
[testenv:e2e.test.notebook]
description = E2E Notebook tests
changedir = {toxinidir}
deps =
    {[testenv:syft]deps}
    nbmake
allowlist_externals =
    bash
    pytest
passenv = EXTERNAL_REGISTRY,EXTERNAL_REGISTRY_USERNAME,EXTERNAL_REGISTRY_PASSWORD
setenv =
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}
    SERVER_PORT = {env:SERVER_PORT:8080}
    SERVER_URL = {env:SERVER_URL:http://localhost}
    EXCLUDE_NOTEBOOKS = {env:EXCLUDE_NOTEBOOKS:}
    SYFT_VERSION = {env:SYFT_VERSION:local}
commands =
    bash -c "echo Running with ORCHESTRA_DEPLOYMENT_TYPE=$ORCHESTRA_DEPLOYMENT_TYPE SERVER_PORT=$SERVER_PORT SERVER_URL=$SERVER_URL \
    Excluding notebooks: $EXCLUDE_NOTEBOOKS SYFT_VERSION=$SYFT_VERSION \
    EXTERNAL_REGISTRY=$EXTERNAL_REGISTRY; date"

    # Schema for EXLUDE_NOTEBOOKS is
    # for excluding
    # notebook1.ipynb, notebook2.ipynb
    # EXCLUDE_NOTEBOOKS=not notebook1.ipynb and not notebook2.ipynb

    # If the syft version is local install the local version
    # else install the version of syft specified
    bash -c "if [[ $SYFT_VERSION == 'local' ]]; then \
        echo 'Using local syft'; \
    else \
        echo 'Installing syft version: ${SYFT_VERSION}'; \
        uv pip install syft[data_science]==${SYFT_VERSION}; \
    fi"

    pytest -x --nbmake --nbmake-timeout=1000 notebooks/api/0.8 -p no:randomly -vvvv -k '{env:EXCLUDE_NOTEBOOKS:}'

[testenv:seaweedfs.test.unit]
description = Seaweedfs Unit Tests
deps =
    -r{toxinidir}/packages/grid/seaweedfs/requirements.txt
    -r{toxinidir}/packages/grid/seaweedfs/requirements.dev.txt
changedir = {toxinidir}/packages/grid/seaweedfs
allowlist_externals =
    bash
    pytest
commands =
    bash -c 'ulimit -n 4096 || true'
    pytest --disable-warnings

[testenv:migration.prepare]
description = Prepare Migration Data
pip_pre = True
setenv =
    MIGRATION_DATA_DIR = {env:MIGRATION_DATA_DIR:{temp_dir}/migration}
deps =
    nbmake
    requests
    syft==0.9.1
allowlist_externals =
    bash
    python
commands =
    ; Run notebooks to prepare migration data
    bash -c 'python -c "import syft as sy; print(\"Migrating from syft version:\", sy.__version__)"'
    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/0-prepare-migration-data.ipynb -vvvv
    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/1-dump-database-to-file.ipynb -vvvv
    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.blob'
    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.yaml'
    bash -c "echo 'Migration data prepared in ${MIGRATION_DATA_DIR}'"

[testenv:migration.test]
description = Migration Test
setenv =
    MIGRATION_DATA_DIR = {env:MIGRATION_DATA_DIR:{temp_dir}/migration}
deps =
    -e{toxinidir}/packages/syft[dev]
    nbmake
; changedir = {toxinidir}/packages/syft
allowlist_externals =
    bash
    tox
    pytest
commands =
    tox -e migration.prepare
    bash -c 'python -c "import syft as sy; print(\"Migrating to syft version:\", sy.__version__)"'
    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/2-migrate-from-file.ipynb -vvvv
commands_post =
    bash -c 'rm -f ${MIGRATION_DATA_DIR}/migration.blob'
    bash -c 'rm -f ${MIGRATION_DATA_DIR}/migration.yaml'

[testenv:migration.k8s.prepare]
description = Prepare migration data using a k8s cluster
changedir = {toxinidir}
passenv=HOME, USER, EXTERNAL_REGISTRY_USERNAME, EXTERNAL_REGISTRY_PASSWORD
deps =
    requests
    nbmake
    syft==0.9.0
allowlist_externals =
    bash
    tox
setenv =
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}
    SERVER_PORT = {env:SERVER_PORT:8080}
    SERVER_URL = {env:SERVER_URL:http://localhost}
    EXTERNAL_REGISTRY = {env:EXTERNAL_REGISTRY:k3d-registry.localhost:5800}
    ; env vars for dev.k8s.start
    CLUSTER_NAME = syft-migration-source
    CLUSTER_HTTP_PORT = {env:SERVER_PORT:8080}
    MIGRATION_DATA_DIR = {env:MIGRATION_DATA_DIR:{temp_dir}/migration}
    LATEST_SYFT_VERSION = 0.9.0
commands =
    bash -c "env; date; k3d version"
    bash -c "k3d cluster delete ${CLUSTER_NAME} || true"

    tox -e dev.k8s.start

    # Deploy cluster from latest stable syft version with Helm
    bash -c '\
        echo "Installing helm charts from repo for syft version: ${LATEST_SYFT_VERSION}"; \
        helm repo add openmined https://openmined.github.io/PySyft/helm; \
        helm repo update openmined; \
        helm install ${CLUSTER_NAME} openmined/syft --version ${LATEST_SYFT_VERSION} -f ./packages/grid/helm/examples/dev/base.yaml --kube-context k3d-${CLUSTER_NAME} --namespace syft --create-namespace; \
    '

    ; wait for everything else to be loaded
    tox -e dev.k8s.ready -- frontend backend postgres proxy seaweedfs registry

    bash -c 'python -c "import syft as sy; print(\"Migrating from syft version:\", sy.__version__)"'

    ; Run notebooks to prepare migration data
    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/0-prepare-migration-data.ipynb -vvvv
    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/1-dump-database-to-file.ipynb -vvvv
    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.blob'
    bash -c 'ls -l ${MIGRATION_DATA_DIR}/migration.yaml'
    bash -c "echo 'Migration data prepared in ${MIGRATION_DATA_DIR}'"

commands_post =
    bash -c "k3d cluster delete ${CLUSTER_NAME} || true"


[testenv:migration.k8s.test]
description = Migration Test on K8s
setenv =
    ORCHESTRA_DEPLOYMENT_TYPE = {env:ORCHESTRA_DEPLOYMENT_TYPE:remote}
    GITHUB_CI = {env:GITHUB_CI:false}
    SYFT_BASE_IMAGE_REGISTRY = {env:SYFT_BASE_IMAGE_REGISTRY:k3d-registry.localhost:5800}
    DATASITE_CLUSTER_NAME = {env:DATASITE_CLUSTER_NAME:test-datasite-1}
    SERVER_PORT = {env:SERVER_PORT:8081}
    MIGRATION_DATA_DIR = {env:MIGRATION_DATA_DIR:{temp_dir}/migration}
    TEST_EXTERNAL_REGISTRY = {env:TEST_EXTERNAL_REGISTRY:k3d-registry.localhost:5800}
deps =
    {[testenv:syft]deps}
    nbmake
changedir = {toxinidir}
passenv=HOME, USER
allowlist_externals =
    bash
    tox
    pytest
    devspace
    kubectl
    grep
    sleep
    k3d
    echo
commands =
    # create migration data files on previous syft version
    tox -e migration.k8s.prepare

    # Make migration.yaml available for devspace migration
    bash -c 'cp ${MIGRATION_DATA_DIR}/migration.yaml packages/grid/helm/examples/dev/migration.yaml'

    # Start the new cluster on syft version we're migrating to
    # set env variable for orchestra deployment type
    bash -c "echo Running with GITHUB_CI=$GITHUB_CI; date"
    python -c 'import syft as sy; sy.stage_protocol_changes()'
    k3d version

    # Deleting Old Cluster
    bash -c "k3d cluster delete ${DATASITE_CLUSTER_NAME} || true"

    # Deleting registry & volumes
    bash -c "k3d registry delete k3d-registry.localhost || true"
    bash -c "docker volume rm k3d-${DATASITE_CLUSTER_NAME}-images --force || true"

    # Create registry
    tox -e dev.k8s.registry

    # Creating test-datasite-1 cluster on port SERVER_PORT
    # NOTE set DEVSPACE_PROFILE=migrated-datasite will start the cluster with variables from migration.yaml
    bash -c '\
    export CLUSTER_NAME=${DATASITE_CLUSTER_NAME} \
    CLUSTER_HTTP_PORT=${SERVER_PORT} \
    DEVSPACE_PROFILE=migrated-datasite && \
    tox -e dev.k8s.start && \
    tox -e dev.k8s.deploy'

    # free up build cache after build of images
    bash -c 'if [[ "$GITHUB_CI" != "false" ]]; then \
        docker image prune --all --force; \
        docker builder prune --all --force; \
    fi'

    sleep 30

    ; # wait for test-datasite-1
    bash packages/grid/scripts/wait_for.sh service postgres --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service backend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service proxy --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service seaweedfs --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash packages/grid/scripts/wait_for.sh service frontend --context k3d-{env:DATASITE_CLUSTER_NAME} --namespace syft
    bash -c '(kubectl logs service/frontend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q -E "Network:\s+https?://[a-zA-Z0-9.-]+:[0-9]+/" || true'

    ; # Checking logs generated & startup of test-datasite 1
    bash -c '(kubectl logs service/backend --context k3d-${DATASITE_CLUSTER_NAME} --namespace syft -f &) | grep -q "Application startup complete" || true'

    # Run migration tests
    bash -c 'python -c "import syft as sy; print(\"Migrating to syft version:\", sy.__version__)"'
    pytest -x --nbmake --nbmake-timeout=1000 notebooks/tutorials/version-upgrades/2-migrate-from-file.ipynb -vvvv

commands_post =
    bash -c "CLUSTER_NAME=${DATASITE_CLUSTER_NAME} tox -e dev.k8s.destroy || true"
    bash -c 'rm -f ${MIGRATION_DATA_DIR}/migration.blob'
    bash -c 'rm -f ${MIGRATION_DATA_DIR}/migration.yaml'
