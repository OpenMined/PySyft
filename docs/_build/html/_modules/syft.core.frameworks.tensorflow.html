

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="python" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="python" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>syft.core.frameworks.tensorflow package &mdash; PySyft 0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/PySyft_docs.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PySyft
          

          
            
            <img src="../_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../notes/example.html">Example Note</a></li>
</ul>
<p class="caption"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../core/frameworks.html">Frameworks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../core/workers.html">Workers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mpc/interface.html">MPC Interface</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PySyft</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>syft.core.frameworks.tensorflow package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/_modules/syft.core.frameworks.tensorflow.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="syft-core-frameworks-tensorflow-package">
<h1>syft.core.frameworks.tensorflow package<a class="headerlink" href="#syft-core-frameworks-tensorflow-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-syft.core.frameworks.tensorflow.federated_averaging_optimizer">
<span id="syft-core-frameworks-tensorflow-federated-averaging-optimizer-module"></span><h2>syft.core.frameworks.tensorflow.federated_averaging_optimizer module<a class="headerlink" href="#module-syft.core.frameworks.tensorflow.federated_averaging_optimizer" title="Permalink to this headline">¶</a></h2>
<p>Synchronize replicas for FedAvg training.</p>
<dl class="class">
<dt id="syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer">
<em class="property">class </em><code class="descclassname">syft.core.frameworks.tensorflow.federated_averaging_optimizer.</code><code class="descname">FederatedAveragingOptimizer</code><span class="sig-paren">(</span><em>opt</em>, <em>replicas_to_aggregate</em>, <em>interval_steps</em>, <em>is_chief=False</em>, <em>total_num_replicas=None</em>, <em>device_setter=None</em>, <em>use_locking=False</em>, <em>name='fedAverage'</em><span class="sig-paren">)</span><a class="reference internal" href="syft/core/frameworks/tensorflow/federated_averaging_optimizer.html#FederatedAveragingOptimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">tensorflow.python.training.optimizer.Optimizer</span></code></p>
<p>Class to synchronize and aggregate model params.</p>
<p>In a typical synchronous training environment, gradients will be averaged each
step and then applied to the variables in one shot, after which replicas can
fetch the new variables and continue. In a federated average training environment,
model variables will be averaged every &#8216;interval_steps&#8217; steps, and then the
replicas will fetch the new variables and continue training locally. In the
interval between two average operations, there is no data transfer, which can
accelerate training.</p>
<p>The following accumulators/queue are created:
&lt;empty line&gt;
* N <cite>parameter accumulators</cite>, one per variable to train. Local variables are
pushed to them and the chief worker will wait until enough variables are
collected and then average them. The accumulator will drop all stale variables
(more details in the accumulator op).
* 1 <cite>token</cite> queue where the optimizer pushes the new global_step value after</p>
<blockquote>
<div>all variables are updated.</div></blockquote>
<p>The following local variable is created:
* <cite>global_step</cite>, one per replica. Updated after every average operation.</p>
<p>The optimizer adds nodes to the graph to collect local variables and pause
the trainers until variables are updated.
For the Parameter Server job:
&lt;empty line&gt;
1. An accumulator is created for each variable, and each replica pushes the</p>
<blockquote>
<div>local variables into the accumulators.</div></blockquote>
<ol class="arabic simple" start="2">
<li>Each accumulator averages once enough variables (replicas_to_aggregate)
have been accumulated.</li>
<li>Apply the averaged variables to global variables.</li>
<li>Only after all variables have been updated, increment the global step.</li>
<li>Only after step 4, pushes a token in the <cite>token_queue</cite>, once for
each worker replica. The workers can now fetch the token and start
the next round.</li>
</ol>
<p>For the replicas:
&lt;empty line&gt;
1. Start a training block: fetch variables and train for &#8220;interval_steps&#8221; steps.
2. Once the training block has been computed, push local variables into variable</p>
<blockquote>
<div>accumulators. Each accumulator will check the staleness and drop the stale.</div></blockquote>
<ol class="arabic simple" start="3">
<li>After pushing all the variables, dequeue a token from the token queue and
continue training. Note that this is effectively a barrier.</li>
<li>Fetch new variables and start the next block.</li>
</ol>
<p>### Usage</p>
<p><a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>python
# Create any optimizer to update the variables, say a simple SGD:
opt = GradientDescentOptimizer(learning_rate=0.1)</p>
<p># Wrap the optimizer with fed_avg_optimizer with 50 replicas: at each
# step the FederatedAveragingOptimizer collects &#8220;replicas_to_aggregate&#8221; variables
# before applying the average. Note that if you want to have 2 backup replicas,
# you can change total_num_replicas=52 and make sure this number matches how
# many physical replicas you started in your job.
opt = fed_avg_optimizer.FederatedAveragingOptimizer(opt,</p>
<blockquote>
<div>replicas_to_aggregate=50,
is_chief=True,
interval_steps=100,
device_setter)</div></blockquote>
<p># Some models have startup_delays to help stabilize the model but when using
# federated_average training, set it to 0.</p>
<p># Now you can call &#8216;minimize() normally&#8217;
# train_op = opt.minimize(loss, global_step=global_step)</p>
<p># And also, create the hook which handles initialization.
fed_avg_hook = opt.make_session_run_hook()
<a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<p>In the training program, every worker will run the train_op as if not
averaged or synchronized. Note that if you want to run other ops like
test op, you should use common session instead of MonitoredSession:</p>
<p><a href="#id9"><span class="problematic" id="id10">``</span></a><a href="#id11"><span class="problematic" id="id12">`</span></a>python
with training.MonitoredTrainingSession(</p>
<blockquote>
<div><blockquote>
<div>master=workers[worker_id].target,
hooks=[fed_avg_hook]) as mon_sess:</div></blockquote>
<dl class="docutils">
<dt>while not mon_sess.should_stop():</dt>
<dd>mon_sess.run(training_op)
sess = mon_sess._tf_sess()
sess.run(testing_op)</dd>
</dl>
</div></blockquote>
<p><a href="#id13"><span class="problematic" id="id14">``</span></a><a href="#id15"><span class="problematic" id="id16">`</span></a></p>
<dl class="method">
<dt id="syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.apply_gradients">
<code class="descname">apply_gradients</code><span class="sig-paren">(</span><em>grads_and_vars</em>, <em>global_step=None</em>, <em>name=None</em><span class="sig-paren">)</span><a class="reference internal" href="syft/core/frameworks/tensorflow/federated_averaging_optimizer.html#FederatedAveragingOptimizer.apply_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.apply_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply gradients to variables.
This contains most of the synchronization implementation.</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><p class="first">grads_and_vars: List of (local_vars, gradients) pairs.
global_step: Variable to increment by one after the variables have been
updated. We need it to check staleness.
name: Optional name for the returned operation. Default to the</p>
<blockquote class="last">
<div>name passed to the Optimizer constructor.</div></blockquote>
</dd>
<dt>Returns:</dt>
<dd>train_op: The op to dequeue a token so the replicas can exit this batch
and apply averages to local vars or an op to update vars locally.</dd>
<dt>Raises:</dt>
<dd><p class="first">ValueError: If the grads_and_vars is empty.
ValueError: If global step is not provided, the staleness cannot be</p>
<blockquote class="last">
<div>checked.</div></blockquote>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.make_session_run_hook">
<code class="descname">make_session_run_hook</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="syft/core/frameworks/tensorflow/federated_averaging_optimizer.html#FederatedAveragingOptimizer.make_session_run_hook"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#syft.core.frameworks.tensorflow.federated_averaging_optimizer.FederatedAveragingOptimizer.make_session_run_hook" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a hook to handle federated average init operations.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-syft.core.frameworks.tensorflow">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-syft.core.frameworks.tensorflow" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, OpenMined Contributors.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0.1',
            LANGUAGE:'python',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>