import torch
import syft


class FalconHelper:
    @classmethod
    def unfold(cls, image, kernel_size, padding):
        return cls.__switch_public_private(
            image, cls.__public_unfold, cls.__private_unfold, kernel_size, padding
        )

    @staticmethod
    def __public_unfold(image, kernel_size, padding):
        image = image.double()
        image = torch.nn.functional.unfold(image, kernel_size=kernel_size, padding=padding)
        image = image.long()
        return image

    @staticmethod
    def __private_unfold(image, kernel_size, padding):
        return image.unfold(kernel_size, padding)

    @staticmethod
    def xor(
        value: Union[int, ReplicatedSharingTensor], other: Union[int, ReplicatedSharingTensor]
    ) -> ReplicatedSharingTensor:
        """
        Compute the XOR value between value and other.
        If value and other are both ints we should use the "^" operator.

        Args:
            value (ReplicatedSharingTensor or Int): RST with ring size of 2 or Int value in {0, 1}
            other (ReplicatedSharingTensor or Int): RST with ring size of 2 or Int value in {0, 1}

        Returns:
            The XOR computation between value and other
        """

        if not any(isinstance(x, ReplicatedSharingTensor) for x in (value, other)):
            raise ValueError("One of the arguments should be a RST")

        if all((isinstance(x, ReplicatedSharingTensor) for x in (value, other))) and not all(
            (x.ring_size == FalconHelper.XOR_EXPECTED_RING_SIZE for x in (value, other))
        ):
            raise ValueError("If both arguments are RST then they should be in ring size 2")

        # Make sure we have a ReplicatedSharingTensor as value
        if isinstance(value, int):
            if 0 <= value <= 1:
                return FalconHelper.xor(other, value)
            else:
                raise ValueError("The integer value should be in {0, 1}")

>>>>>>> 97bd4cc7... Comments fix
        return value + other - (value * 2 * other)

    @staticmethod
    def __switch_public_private(value, public_function, private_function, *args, **kwargs):
        if isinstance(value, (int, float, torch.Tensor, syft.FixedPrecisionTensor)):
            return public_function(value, *args, **kwargs)
        elif isinstance(value, syft.ReplicatedSharingTensor):
            return private_function(value, *args, **kwargs)
        else:
            raise ValueError(
                "expected int, float, torch tensor, or ReplicatedSharingTensor "
                "but got {}".format(type(value))
            )

    @staticmethod
    def select_share(
        b: ReplicatedSharingTensor,
        x: ReplicatedSharingTensor,
        y: ReplicatedSharingTensor,
    ) -> ReplicatedSharingTensor:
        """Select x or y depending on b

        Args:
            x (ReplicatedSharingTensor): RST that will be selected if b reconstructed is 0
            y (ReplicatedSharingTensor): RST that will be selected if b reconstructed is 1
            b (ReplicatedSharingTensor): RST of a bit

        Return:
            x if b == 0 else y
        """

        ring_size = x.ring_size
        players = x.players

        # This should be generated by a crypto provider or create a 0 RST shared value and make
        # each worker add a random number at an offline step
        # (Idea): Use the correlated randomness shared at the beginning
        c = torch.randint(high=2, size=(1,))
        c_2 = c.share(*players, protocol="falcon", field=2)
        c_L = c.share(*players, protocol="falcon", field=ring_size)

        xor_b_c = FalconHelper.xor(b, c_2)

        # TODO: Think of a method to do this better (1 - c_share) without being to
        # "intrusive" with the operation
        ones = torch.ones(size=(1,)).share(*players, protocol="falcon", field=ring_size)
        if xor_b_c.reconstruct().item():
            d = ones - c_L
        else:
            d = c_L

        selected_val = (y - x) * d + x
        return selected_val

    def negate_cond(
        x: ReplicatedSharingTensor, beta: ReplicatedSharingTensor
    ) -> ReplicatedSharingTensor:
        """Performs the computation (-1)^beta * x

        Args:
            x (ReplicatedSharingTensor): RST to perform the computation on
            beta (ReplicatedSharingTensor): the reconstructed value should be in {0, 1}

        Return:
            A RST that is (-1)^beta * x
        """
        ring_size = x.ring_size
        players = x.players
        shape = x.shape

        pow_sh = (
            torch.ones(size=shape).share(*players, protocol="falcon", field=ring_size) - beta * 2
        )
        result = pow_sh * x

        return result
