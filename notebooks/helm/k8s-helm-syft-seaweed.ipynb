{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "import os\n",
    "from syft import ActionObject\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732a9097",
   "metadata": {},
   "source": [
    "Start this using"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d0a11e",
   "metadata": {},
   "source": [
    "```\n",
    "tox -e dev.k8s.start\n",
    "tox -e dev.k8s.deploy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc952d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = sy.login(url=\"http://localhost:8080\", email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a3c58d",
   "metadata": {},
   "source": [
    "# Mount storage container with Helm azure container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12255c2",
   "metadata": {},
   "source": [
    "# Start workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9e931-1857-4870-9476-eec4b2105094",
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_count = os.cpu_count() - 2\n",
    "worker_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.worker.start_workers(n=worker_count)\n",
    "# client.worker.start_workers(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cea5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.worker.list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2703f5a0",
   "metadata": {},
   "source": [
    "# Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f648da63-7c55-4e74-a513-ea9fa56c4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split.py\n",
    "# # stdlib\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "\n",
    "# def split_file(file_path, num_chunks):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         total_lines = sum(1 for line in file)\n",
    "\n",
    "#     lines_per_chunk = total_lines // num_chunks\n",
    "\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         for chunk in range(num_chunks):\n",
    "#             print(f\"Creating chunk {chunk}\")\n",
    "#             chunk_file_name = f\"{os.path.splitext(file_path)[0]}-chunk-{str(chunk).zfill(len(str(num_chunks)))}.jsonl\"\n",
    "#             with open(chunk_file_name, 'w', encoding='utf-8') as chunk_file:\n",
    "#                 for _ in range(lines_per_chunk):\n",
    "#                     line = file.readline()\n",
    "#                     if not line:\n",
    "#                         break\n",
    "#                     chunk_file.write(line)\n",
    "\n",
    "#                 # Handle any remaining lines for the last chunk\n",
    "#                 if chunk == num_chunks - 1:\n",
    "#                     for line in file:\n",
    "#                         chunk_file.write(line)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     if len(sys.argv) != 2:\n",
    "#         print(\"Usage: python script.py <filename>\")\n",
    "#         sys.exit(1)\n",
    "\n",
    "#     file_path = sys.argv[1]\n",
    "#     num_chunks = os.cpu_count() or 1  # Default to 1 if cpu_count is None\n",
    "#     split_file(file_path, num_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d282df34-b850-4d83-9200-dcd96b4ce3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download filtered_scenario_data_new.jsonl from azure\n",
    "# download train-00.jsonl from azure\n",
    "# run split.py train-00.jsonl\n",
    "# WARNING: bug where files around 2GB are causing issues with upload so try ~200mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d910168e-5912-4e9c-a54e-4ccf05134404",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/madhavajay/dev/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95564d7f-1300-45f7-8b47-ca226f34c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_path = f\"{data_dir}filtered_scenario_data_new.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb6f87b-5459-4f1b-8241-112c2f6f57b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-00-chunk-00.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67643068-4687-40f9-b0cf-c29342bfa37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_files = []\n",
    "import os\n",
    "for file in os.listdir(data_dir):\n",
    "    print(file)\n",
    "    if file.startswith(\"train-00-chunk\") and file.endswith(\".jsonl\"):\n",
    "        name = file.split(\".\")[0]\n",
    "        path = os.path.join(data_dir, file)\n",
    "        split_files.append((name, path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4bb3ce-900a-400f-a467-29f3aee468ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_file = sy.ActionObject.from_path(path=scenario_path).send(client).syft_action_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1680b51-5b98-4755-9c9e-2c3603b49668",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "for file in split_files:\n",
    "    path = file[1]\n",
    "    data = sy.ActionObject.from_path(path)\n",
    "    train_file = data.send(client).syft_action_data\n",
    "    train_files.append(train_file)\n",
    "    print(\"Added \", file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da9c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "helm_dataset = sy.Dataset(\n",
    "    name=\"Helm Dataset\",\n",
    "    asset_list=[\n",
    "        sy.Asset(\n",
    "            name=\"helm train data\",\n",
    "            data=ActionObject.from_obj(train_files),\n",
    "            mock=sy.ActionObject.empty()\n",
    "        ),\n",
    "        sy.Asset(\n",
    "            name=\"helm test data\",\n",
    "            data=ActionObject.from_obj([scenario_file]),\n",
    "            mock=sy.ActionObject.empty()\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4400f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_dataset(helm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842988d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "helm_ds = client.datasets[\"Helm Dataset\"]\n",
    "helm_train_files = helm_ds.assets[\"helm train data\"]\n",
    "helm_test_files = helm_ds.assets[\"helm test data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60b056",
   "metadata": {},
   "source": [
    "# Syft functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a5c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function()\n",
    "def compute_document_data_overlap(domain, scenario_file, input_files, n):\n",
    "    print(\"starting overlap computation\")\n",
    "\n",
    "    from nltk import ngrams\n",
    "    from collections import defaultdict\n",
    "    from string import punctuation\n",
    "    import re, json\n",
    "    import time\n",
    "\n",
    "    r = re.compile(r\"[\\s{}]+\".format(re.escape(punctuation)))\n",
    "    \n",
    "    def create_ngram_index(light_scenarios, n_values, stats_key_counts):\n",
    "        ngram_index = {n:{}  for n in n_values}\n",
    "        for i, scenario in enumerate(light_scenarios):\n",
    "            if i%20 == 0:\n",
    "                print(f\"n_gram indexing progress: {(i/len(light_scenarios))*100:.2f}%\")\n",
    "            for n in n_values:\n",
    "                stats_key = scenario['scenario_key'] + '_' + str(n)\n",
    "                stats_key_counts[stats_key] = len(scenario['instances'])\n",
    "                for instance in scenario['instances']:\n",
    "                    id = instance['id']                    \n",
    "                    input_tokens = r.split(instance['input'].lower())\n",
    "                    for input_ngram in ngrams(input_tokens, n):\n",
    "                        if input_ngram not in ngram_index[n]:\n",
    "                            ngram_index[n][input_ngram] = set()\n",
    "                        ngram_index[n][input_ngram].add(stats_key + '+' + id + '+' + 'input')\n",
    "\n",
    "                    # compute reference ngrams\n",
    "                    for reference in instance['references']:\n",
    "                        reference_unigrams = r.split(reference.lower())\n",
    "                        for reference_ngram in ngrams(reference_unigrams, n):\n",
    "                            if reference_ngram not in ngram_index[n]:\n",
    "                                ngram_index[n][reference_ngram] = set()\n",
    "                            ngram_index[n][reference_ngram].add(stats_key + '+' + id + '+' + 'references')\n",
    "        return ngram_index\n",
    "    \n",
    "    # SETUP\n",
    "    print(\"preparing scenarios and creating indexes\")\n",
    "    start = time.time()\n",
    "    light_scenarios = []\n",
    "    for i, (bytes_read, light_scenario_json) in enumerate(scenario_file.iter_lines(progress=True)):\n",
    "        if i % 20 == 0:\n",
    "            print(f\"scenario creation progress: {(bytes_read/scenario_file.file_size)*100:.2f}%\")\n",
    "\n",
    "        light_scenario_dict: dict = json.loads(light_scenario_json)\n",
    "\n",
    "        light_scenario_key_dict: dict = light_scenario_dict[\"scenario_key\"]\n",
    "        scenario_spec = str(light_scenario_key_dict[\"scenario_spec\"])\n",
    "\n",
    "        light_scenario_key = scenario_spec + '_' + light_scenario_key_dict[\"split\"]\n",
    "        light_instances = [\n",
    "            {\n",
    "                'input': instance_dict['input'], \n",
    "                'references': instance_dict['references'], \n",
    "                'id': instance_dict[\"id\"]\n",
    "            }\n",
    "            for instance_dict in light_scenario_dict[\"instances\"]\n",
    "        ]\n",
    "        light_scenarios.append({'scenario_key': light_scenario_key, 'instances': light_instances})\n",
    "    print(f\"Finished creating scenarios ({time.time()-start}s)\")\n",
    "    \n",
    "    print(\"Creating indexes\")\n",
    "    \n",
    "    start = time.time()\n",
    "    stats_key_counts = defaultdict(int)\n",
    "    ngram_index = create_ngram_index(\n",
    "        light_scenarios=light_scenarios, n_values=[n], stats_key_counts=stats_key_counts\n",
    "    )\n",
    "    print(f\"Finished creating indexes ({time.time()-start}s)\")\n",
    "        \n",
    "    \n",
    "    r = re.compile(r\"[\\s{}]+\".format(re.escape(punctuation)))\n",
    "    stats_key_to_input_ids = defaultdict(set)\n",
    "    stats_key_to_reference_ids = defaultdict(set)\n",
    "    print(\"computing overlap\")\n",
    "    start = time.time()\n",
    "    \n",
    "    domain.init_progress(input_files[0].file_size)\n",
    "\n",
    "    for input_file in input_files:\n",
    "        for i, (bytes_read, line) in enumerate(input_file.iter_lines(progress=True)):\n",
    "            if i%1000 == 0:\n",
    "                print(f\"computing overlap progress: {(bytes_read / input_file.file_size) * 100:.2f}%\")\n",
    "                domain.set_progress(bytes_read)\n",
    "            if i==10000:\n",
    "                break\n",
    "            document = json.loads(line)[\"text\"]\n",
    "            document_tokens = r.split(document.lower())\n",
    "            for n in ngram_index.keys():\n",
    "                for document_ngram in ngrams(document_tokens, n):\n",
    "                    if document_ngram in ngram_index[n]:\n",
    "                        for entry_overlap_key in ngram_index[n][document_ngram]:\n",
    "                            stats_key, id, part = entry_overlap_key.split(\"+\")\n",
    "                            if part == \"input\":\n",
    "                                stats_key_to_input_ids[stats_key].add(id)\n",
    "                            elif part == \"references\":\n",
    "                                stats_key_to_reference_ids[stats_key].add(id)\n",
    "    print(f\"Finished computing overlap ({time.time()-start}s)\")\n",
    "    print(\"done\")\n",
    "    \n",
    "    return stats_key_to_input_ids, stats_key_to_reference_ids, stats_key_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.code.submit(compute_document_data_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27be4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function_single_use(input_files=helm_train_files, scenario_files=helm_test_files)\n",
    "def main_function(domain, input_files, scenario_files):\n",
    "    N = [5, 9, 13]\n",
    "    jobs = []\n",
    "    for n in N[:1]:\n",
    "        for scenario_file in scenario_files:\n",
    "            for train_file in input_files:\n",
    "                batch_job = domain.launch_job(\n",
    "                    compute_document_data_overlap,\n",
    "                    scenario_file=scenario_file,\n",
    "                    input_files=[train_file],\n",
    "                    n=n\n",
    "                )\n",
    "                jobs.append(batch_job)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d92df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.code.request_code_execution(main_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee2790",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.requests[-1].approve(approve_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b084c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = client.code.main_function(input_files=helm_train_files,\n",
    "                                scenario_files=helm_test_files,\n",
    "                                blocking=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df60a45",
   "metadata": {},
   "source": [
    "# Inspect Jobs and get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3bee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d567f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.subjobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63089d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# job.wait().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852360ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.subjobs[0].logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5de7233",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [j.wait().get() for j in job.subjobs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a079df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stats_key_to_input_ids, stats_key_to_reference_ids, stats_key_counts\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe4daea",
   "metadata": {},
   "source": [
    "# Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5053b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_key_to_input_ids, stats_key_to_reference_ids, stats_key_counts = zip(*results)\n",
    "\n",
    "total_input_ids = defaultdict(set)\n",
    "total_reference_ids = defaultdict(set)\n",
    "total_stats_key_counts = defaultdict(int)\n",
    "\n",
    "for d in stats_key_counts:\n",
    "    for key, val in d.items():\n",
    "        total_stats_key_counts[key] += val\n",
    "\n",
    "\n",
    "for d in stats_key_to_input_ids:\n",
    "    for key in d:\n",
    "        new_set = set()\n",
    "        if key in total_input_ids:\n",
    "            new_set = total_input_ids[key]\n",
    "        new_set = new_set.union(d[key])\n",
    "        total_input_ids[key] = new_set\n",
    "\n",
    "for d in stats_key_to_reference_ids:\n",
    "    for key in d:\n",
    "        new_set = set()\n",
    "        if key in total_reference_ids:\n",
    "            new_set = total_reference_ids[key]\n",
    "        new_set = total_reference_ids[key].union(d[key])\n",
    "        total_reference_ids[key] = new_set\n",
    "\n",
    "all_data_overlap_stats = []\n",
    "for stats_key, count in total_stats_key_counts.items():\n",
    "    data_overlap_stats = {\n",
    "        'data_overlap_stats_key': None,\n",
    "        'num_instances': count,\n",
    "        'instance_ids_with_overlapping_input': sorted(total_input_ids[stats_key]),\n",
    "        'instance_ids_with_overlapping_reference': sorted(total_reference_ids[stats_key]),\n",
    "    }\n",
    "    subject, split, n_str = stats_key.rsplit('_', 2)\n",
    "    data_overlap_stats['data_overlap_stats_key'] = {\n",
    "        'light_scenario_key': {'scenario_spec': subject, 'split': split},\n",
    "        'overlap_protocol_spec': {'n': int(n_str)}\n",
    "    }\n",
    "    all_data_overlap_stats.append(data_overlap_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53c3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(all_data_overlap_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300abb87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
