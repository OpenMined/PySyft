{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emnist_train, emnist_test = tff.simulation.datasets.emnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_federated.proto.v0 import computation_pb2 as pb\n",
    "\n",
    "\n",
    "NUM_CLIENTS = 10\n",
    "NUM_EPOCHS = 5\n",
    "SHUFFLE_BUFFER = 100\n",
    "\n",
    "def preprocess(dataset):\n",
    "\n",
    "  def map_fn(element):\n",
    "    return collections.OrderedDict(\n",
    "        x=tf.reshape(element['pixels'], [-1, 784]),\n",
    "        y=tf.reshape(element['label'], [-1, 1]))\n",
    "\n",
    "  return dataset.repeat(NUM_EPOCHS).shuffle(SHUFFLE_BUFFER, seed=1).map(map_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = emnist_train.create_tf_dataset_for_client(\n",
    "    emnist_train.client_ids[0])\n",
    "preprocessed_example_dataset = preprocess(example_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataBackend(tff.framework.DataBackend):\n",
    "\n",
    "  async def materialize(self, data, type_spec):\n",
    "    client_id = int(data.uri[-1])\n",
    "    client_dataset = emnist_train.create_tf_dataset_for_client(\n",
    "        emnist_train.client_ids[client_id])\n",
    "    return preprocess(client_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_fn(\n",
    "    device: tf.config.LogicalDevice) -> tff.framework.DataExecutor:\n",
    "  return tff.framework.DataExecutor(\n",
    "      tff.framework.EagerTFExecutor(device),\n",
    "      data_backend=TestDataBackend())\n",
    "factory = tff.framework.local_executor_factory(leaf_executor_fn=ex_fn)\n",
    "ctx = tff.framework.ExecutionContext(executor_fn=factory)\n",
    "tff.framework.set_default_context(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model():\n",
    "  return tf.keras.models.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(784,)),\n",
    "      tf.keras.layers.Dense(10, kernel_initializer='zeros'),\n",
    "      tf.keras.layers.Softmax(),\n",
    "  ])\n",
    "  \n",
    "def model_fn():\n",
    "  keras_model = create_keras_model()\n",
    "  return tff.learning.from_keras_model(\n",
    "      keras_model,\n",
    "      input_spec=preprocessed_example_dataset.element_spec,\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_process = tff.learning.build_federated_averaging_process(\n",
    "    model_fn,\n",
    "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=0.02),\n",
    "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0))\n",
    "\n",
    "state = iterative_process.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_type = tff.types.StructWithPythonType(\n",
    "    preprocessed_example_dataset.element_spec,\n",
    "    container_type=collections.OrderedDict)\n",
    "dataset_type = tff.types.SequenceType(element_type)\n",
    "dataset_type_proto = tff.framework.serialize_type(dataset_type)\n",
    "# Sampling the first five clients in the EMNIST dataset.\n",
    "arguments = [\n",
    "  pb.Computation(data=pb.Data(uri=f'uri://{i}'), type=dataset_type_proto)\n",
    "  for i in range(5)\n",
    "]\n",
    "data_handle = tff.framework.DataDescriptor(None, arguments, \n",
    "  tff.FederatedType(dataset_type, tff.CLIENTS), len(arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.11625), ('loss', 12.6826515), ('num_examples', 2400), ('num_batches', 2400)]))])\n"
     ]
    }
   ],
   "source": [
    "state, metrics = iterative_process.next(state, data_handle)\n",
    "print('round 1, metrics={}'.format(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round  2, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.12375), ('loss', 10.283684), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
      "round  3, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.17791666), ('loss', 7.744127), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
      "round  4, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.275), ('loss', 5.888526), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
      "round  5, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.38708332), ('loss', 4.557935), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
      "round  6, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.46791667), ('loss', 3.7862585), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
      "round  7, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.5445833), ('loss', 3.1038892), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
      "round  8, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.57416666), ('loss', 2.962887), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
      "round  9, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.6329167), ('loss', 2.4559648), ('num_examples', 2400), ('num_batches', 2400)]))])\n",
      "round 10, metrics=OrderedDict([('broadcast', ()), ('aggregation', OrderedDict([('mean_value', ()), ('mean_weight', ())])), ('train', OrderedDict([('sparse_categorical_accuracy', 0.62708336), ('loss', 2.3828022), ('num_examples', 2400), ('num_batches', 2400)]))])\n"
     ]
    }
   ],
   "source": [
    "NUM_ROUNDS = 11\n",
    "for round_num in range(2, NUM_ROUNDS):\n",
    "  state, metrics = iterative_process.next(state, data_handle)\n",
    "  print('round {:2d}, metrics={}'.format(round_num, metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('x', TensorSpec(shape=(1, 784), dtype=tf.float32, name=None)),\n",
       "             ('y', TensorSpec(shape=(1, 1), dtype=tf.int32, name=None))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_example_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x7f3124301ac0>\n",
      "TensorSpec(shape=(1, 784), dtype=tf.float32, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `flat_predict_on_batch` contains input name(s) Identity, Identity_1 with unsupported characters which will be renamed to identity, identity_1 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp.txt/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tmp.txt/assets\n"
     ]
    }
   ],
   "source": [
    "model = create_keras_model()\n",
    "print(model)\n",
    "input_spec = [tf.TensorSpec(shape=(1,784), dtype=tf.float32, name=None), tf.TensorSpec(shape=(1,1), dtype=tf.int32, name=None)]\n",
    "functional_model = tff.learning.models.functional_model_from_keras(keras_model=model, loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(),input_spec=input_spec)\n",
    "print(functional_model.input_spec[0])\n",
    "tff.learning.models.save_functional_model(functional_model=functional_model, path='tmp.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/teo/PySyft/notebooks/PySyTFF/tmp.zip.zip'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive('tmp.zip', 'zip', 'tmp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('tmp.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(directory_to_extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow_federated.python.learning.models.serialization._LoadedFunctionalModel at 0x7f3144578850>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functional_model_reloaded = tff.learning.models.load_functional_model('tmp.txt')\n",
    "functional_model_reloaded"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67856bda971e2a654274dbee4a8f60d8877ae51b472a8acc4dc577a3ea0a55e3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('PySyTFF')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
