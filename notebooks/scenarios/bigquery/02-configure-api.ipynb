{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syft absolute\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# server = sy.orchestra.launch(\n",
    "#     name=\"bigquery-high\",\n",
    "#     dev_mode=False,\n",
    "#     server_side_type=\"high\",\n",
    "#     port=\"8080\",\n",
    "#     n_consumers=1, # How many workers to be spawned\n",
    "#     create_producer=True # Can produce more workers\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client = sy.login(\n",
    "    url=\"http://localhost:8080\", email=\"info@openmined.org\", password=\"changethis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your values to secrets.json in this folder\n",
    "secrets = sy.get_nb_secrets(\n",
    "    {\n",
    "        \"service_account_bigquery_private\": {},\n",
    "        \"service_account_bigquery_mock\": {},\n",
    "        \"region_bigquery\": \"\",\n",
    "        \"project_id\": \"\",\n",
    "        \"dataset_1\": \"dataset1\",\n",
    "        \"table_1\": \"table1\",\n",
    "        \"table_2\": \"table2\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.worker_pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list | grep bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install db-dtypes google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.api_endpoint_method(\n",
    "    settings={\n",
    "        \"credentials\": secrets[\"service_account_bigquery_private\"],\n",
    "        \"region\": secrets[\"region_bigquery\"],\n",
    "        \"project_id\": secrets[\"project_id\"],\n",
    "    }\n",
    ")\n",
    "def private_query_function(\n",
    "    context,\n",
    "    sql_query: str,\n",
    ") -> str:\n",
    "    # third party\n",
    "    from google.cloud import bigquery\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    # syft absolute\n",
    "    from syft.service.response import SyftError\n",
    "\n",
    "    # Auth for Bigquer based on the workload identity\n",
    "    credentials = service_account.Credentials.from_service_account_info(\n",
    "        context.settings[\"credentials\"]\n",
    "    )\n",
    "    scoped_credentials = credentials.with_scopes(\n",
    "        [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "    )\n",
    "\n",
    "    client = bigquery.Client(\n",
    "        credentials=scoped_credentials,\n",
    "        location=context.settings[\"region\"],\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        rows = client.query_and_wait(\n",
    "            sql_query,\n",
    "            project=context.settings[\"project_id\"],\n",
    "        )\n",
    "\n",
    "        if rows.total_rows > 1_000_000:\n",
    "            return SyftError(\n",
    "                message=\"Please only write queries that gather aggregate statistics\"\n",
    "            )\n",
    "\n",
    "        return rows.to_dataframe()\n",
    "    except Exception as e:\n",
    "        # We MUST handle the errors that we want to be visible to the data owners.\n",
    "        # Any exception not catched is visible only to the data owner.\n",
    "        # not a bigquery exception\n",
    "        if not hasattr(e, \"_errors\"):\n",
    "            output = f\"got exception e: {type(e)} {str(e)}\"\n",
    "            return SyftError(\n",
    "                message=f\"An error occured executing the API call {output}\"\n",
    "            )\n",
    "            # return SyftError(message=\"An error occured executing the API call, please contact the domain owner.\")\n",
    "\n",
    "        if e._errors[0][\"reason\"] in [\n",
    "            \"badRequest\",\n",
    "            \"blocked\",\n",
    "            \"duplicate\",\n",
    "            \"invalidQuery\",\n",
    "            \"invalid\",\n",
    "            \"jobBackendError\",\n",
    "            \"jobInternalError\",\n",
    "            \"notFound\",\n",
    "            \"notImplemented\",\n",
    "            \"rateLimitExceeded\",\n",
    "            \"resourceInUse\",\n",
    "            \"resourcesExceeded\",\n",
    "            \"tableUnavailable\",\n",
    "            \"timeout\",\n",
    "        ]:\n",
    "            return SyftError(\n",
    "                message=\"Error occured during the call: \" + e._errors[0][\"message\"]\n",
    "            )\n",
    "        else:\n",
    "            return SyftError(\n",
    "                message=\"An error occured executing the API call, please contact the domain owner.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any helper methods for our rate limiter\n",
    "\n",
    "\n",
    "def is_within_rate_limit(context):\n",
    "    \"\"\"Rate limiter for custom API calls made by users.\"\"\"\n",
    "    # stdlib\n",
    "    import datetime\n",
    "\n",
    "    state = context.state\n",
    "    settings = context.settings\n",
    "    email = context.user.email\n",
    "\n",
    "    current_time = datetime.datetime.now()\n",
    "    calls_last_min = [\n",
    "        1 if (current_time - call_time).seconds < 60 else 0\n",
    "        for call_time in state[email]\n",
    "    ]\n",
    "\n",
    "    return sum(calls_last_min) < settings[\"CALLS_PER_MIN\"]\n",
    "\n",
    "\n",
    "# Define a mock endpoint that the researchers can use for testing\n",
    "\n",
    "\n",
    "@sy.api_endpoint_method(\n",
    "    settings={\n",
    "        \"credentials\": secrets[\"service_account_bigquery_private\"],\n",
    "        \"region\": secrets[\"region_bigquery\"],\n",
    "        \"project_id\": secrets[\"project_id\"],\n",
    "        \"CALLS_PER_MIN\": 10,\n",
    "    },\n",
    "    helper_functions=[is_within_rate_limit],\n",
    ")\n",
    "def mock_query_function(\n",
    "    context,\n",
    "    sql_query: str,\n",
    ") -> str:\n",
    "    # stdlib\n",
    "    import datetime\n",
    "\n",
    "    # third party\n",
    "    from google.cloud import bigquery\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    # syft absolute\n",
    "    from syft.service.response import SyftError\n",
    "\n",
    "    # Auth for Bigquer based on the workload identity\n",
    "    credentials = service_account.Credentials.from_service_account_info(\n",
    "        context.settings[\"credentials\"]\n",
    "    )\n",
    "    scoped_credentials = credentials.with_scopes(\n",
    "        [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "    )\n",
    "\n",
    "    client = bigquery.Client(\n",
    "        credentials=scoped_credentials,\n",
    "        location=context.settings[\"region\"],\n",
    "    )\n",
    "\n",
    "    # Store a dict with the calltimes for each user, via the email.\n",
    "    if context.user.email not in context.state.keys():\n",
    "        context.state[context.user.email] = []\n",
    "\n",
    "    if not context.code.is_within_rate_limit(context):\n",
    "        return SyftError(message=\"Rate limit of calls per minute has been reached.\")\n",
    "\n",
    "    try:\n",
    "        context.state[context.user.email].append(datetime.datetime.now())\n",
    "\n",
    "        rows = client.query_and_wait(\n",
    "            sql_query,\n",
    "            project=context.settings[\"project_id\"],\n",
    "        )\n",
    "\n",
    "        if rows.total_rows > 1_000_000:\n",
    "            return SyftError(\n",
    "                message=\"Please only write queries that gather aggregate statistics\"\n",
    "            )\n",
    "\n",
    "        return rows.to_dataframe()\n",
    "\n",
    "    except Exception as e:\n",
    "        # not a bigquery exception\n",
    "        if not hasattr(e, \"_errors\"):\n",
    "            output = f\"got exception e: {type(e)} {str(e)}\"\n",
    "            return SyftError(\n",
    "                message=f\"An error occured executing the API call {output}\"\n",
    "            )\n",
    "            # return SyftError(message=\"An error occured executing the API call, please contact the domain owner.\")\n",
    "\n",
    "        # Treat all errors that we would like to be forwarded to the data scientists\n",
    "        # By default, any exception is only visible to the data owner.\n",
    "\n",
    "        if e._errors[0][\"reason\"] in [\n",
    "            \"badRequest\",\n",
    "            \"blocked\",\n",
    "            \"duplicate\",\n",
    "            \"invalidQuery\",\n",
    "            \"invalid\",\n",
    "            \"jobBackendError\",\n",
    "            \"jobInternalError\",\n",
    "            \"notFound\",\n",
    "            \"notImplemented\",\n",
    "            \"rateLimitExceeded\",\n",
    "            \"resourceInUse\",\n",
    "            \"resourcesExceeded\",\n",
    "            \"tableUnavailable\",\n",
    "            \"timeout\",\n",
    "        ]:\n",
    "            return SyftError(\n",
    "                message=\"Error occured during the call: \" + e._errors[0][\"message\"]\n",
    "            )\n",
    "        else:\n",
    "            return SyftError(\n",
    "                message=\"An error occured executing the API call, please contact the domain owner.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up the worker pools and identify the name of the one that has the required packages\n",
    "# After, bind the endpoint to that workerpool\n",
    "\n",
    "\n",
    "high_client.worker_pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_endpoint = sy.TwinAPIEndpoint(\n",
    "    path=\"bigquery.test_query\",\n",
    "    description=\"This endpoint allows to query Bigquery storage via SQL queries.\",\n",
    "    private_function=private_query_function,\n",
    "    mock_function=mock_query_function,\n",
    "    worker_pool=\"bigquery-pool\",\n",
    ")\n",
    "\n",
    "high_client.custom_api.add(endpoint=new_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we update the endpoint to timeout after 100s (rather the default of 60s)\n",
    "high_client.api.services.api.update(\n",
    "    endpoint_path=\"bigquery.test_query\", endpoint_timeout=120\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.api.services.api.update(\n",
    "    endpoint_path=\"bigquery.test_query\", hide_mock_definition=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mock version\n",
    "result = high_client.api.services.bigquery.test_query.mock(\n",
    "    sql_query=f\"SELECT * FROM {secrets['dataset_1']}.{secrets['table_1']} LIMIT 10\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.api_endpoint(\n",
    "    path=\"bigquery.schema\",\n",
    "    description=\"This endpoint allows for visualising the metadata of tables available in BigQuery.\",\n",
    "    settings={\n",
    "        \"credentials\": secrets[\"service_account_bigquery_mock\"],\n",
    "        \"region\": secrets[\"region_bigquery\"],\n",
    "        \"project_id\": secrets[\"project_id\"],\n",
    "        \"CALLS_PER_MIN\": 5,\n",
    "    },\n",
    "    helper_functions=[\n",
    "        is_within_rate_limit\n",
    "    ],  # Adds ratelimit as this is also a method available to data scientists\n",
    "    worker_pool=\"bigquery-pool\",\n",
    ")\n",
    "def schema_function(\n",
    "    context,\n",
    ") -> str:\n",
    "    # stdlib\n",
    "    import datetime\n",
    "\n",
    "    # third party\n",
    "    from google.oauth2 import service_account\n",
    "    import pandas as pd\n",
    "\n",
    "    # syft absolute\n",
    "    from syft.service.response import SyftError\n",
    "\n",
    "    # Auth for Bigquer based on the workload identity\n",
    "    credentials = service_account.Credentials.from_service_account_info(\n",
    "        context.settings[\"credentials\"]\n",
    "    )\n",
    "    scoped_credentials = credentials.with_scopes(\n",
    "        [\"https://www.googleapis.com/auth/cloud-platform\"]\n",
    "    )\n",
    "\n",
    "    client = bigquery.Client(\n",
    "        credentials=scoped_credentials,\n",
    "        location=context.settings[\"region\"],\n",
    "    )\n",
    "\n",
    "    if context.user.email not in context.state.keys():\n",
    "        context.state[context.user.email] = []\n",
    "\n",
    "    if not context.code.is_within_rate_limit(context):\n",
    "        return SyftError(message=\"Rate limit of calls per minute has been reached.\")\n",
    "\n",
    "    try:\n",
    "        context.state[context.user.email].append(datetime.datetime.now())\n",
    "\n",
    "        # Formats the data schema in a data frame format\n",
    "        # Warning: the only supported format types are primitives, np.ndarrays and pd.DataFrames\n",
    "\n",
    "        data_schema = []\n",
    "        for table_id in [\n",
    "            f\"{secrets['dataset_1']}.{secrets['table_1']}\",\n",
    "            f\"{secrets['dataset_1']}.{secrets['table_2']}\",\n",
    "        ]:\n",
    "            table = client.get_table(table_id)\n",
    "            for schema in table.schema:\n",
    "                data_schema.append(\n",
    "                    {\n",
    "                        \"project\": str(table.project),\n",
    "                        \"dataset_id\": str(table.dataset_id),\n",
    "                        \"table_id\": str(table.table_id),\n",
    "                        \"schema_name\": str(schema.name),\n",
    "                        \"schema_field\": str(schema.field_type),\n",
    "                        \"description\": str(table.description),\n",
    "                        \"num_rows\": str(table.num_rows),\n",
    "                    }\n",
    "                )\n",
    "        return pd.DataFrame(data_schema)\n",
    "\n",
    "    except Exception as e:\n",
    "        # not a bigquery exception\n",
    "        if not hasattr(e, \"_errors\"):\n",
    "            output = f\"got exception e: {type(e)} {str(e)}\"\n",
    "            return SyftError(\n",
    "                message=f\"An error occured executing the API call {output}\"\n",
    "            )\n",
    "            # return SyftError(message=\"An error occured executing the API call, please contact the domain owner.\")\n",
    "\n",
    "        # Should add appropriate error handling for what should be exposed to the data scientists.\n",
    "        return SyftError(\n",
    "            message=\"An error occured executing the API call, please contact the domain owner.\"\n",
    "        )\n",
    "\n",
    "\n",
    "high_client.custom_api.add(endpoint=schema_function)\n",
    "high_client.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.api_endpoint(\n",
    "    path=\"bigquery.submit_query\",\n",
    "    description=\"API endpoint that allows you to submit SQL queries to run on the private data.\",\n",
    "    worker_pool=\"bigquery-pool\",\n",
    ")\n",
    "def submit_query(\n",
    "    context,\n",
    "    func_name: str,\n",
    "    query: str,\n",
    ") -> str:\n",
    "    # stdlib\n",
    "    import hashlib\n",
    "\n",
    "    # syft absolute\n",
    "    import syft as sy\n",
    "\n",
    "    hash_object = hashlib.new(\"sha256\")\n",
    "\n",
    "    hash_object.update(context.user.email.encode(\"utf-8\"))\n",
    "    func_name = func_name + \"_\" + hash_object.hexdigest()[:6]\n",
    "\n",
    "    @sy.syft_function(\n",
    "        name=func_name,\n",
    "        input_policy=sy.MixedInputPolicy(\n",
    "            endpoint=sy.Constant(\n",
    "                val=context.admin_client.api.services.bigquery.test_query\n",
    "            ),\n",
    "            query=sy.Constant(val=query),\n",
    "            client=context.admin_client,\n",
    "        ),\n",
    "        worker_pool_name=\"bigquery-pool\",\n",
    "    )\n",
    "    def execute_query(query: str, endpoint):\n",
    "        res = endpoint(sql_query=query)\n",
    "        return res\n",
    "\n",
    "    request = context.user_client.code.request_code_execution(execute_query)\n",
    "    if isinstance(request, sy.SyftError):\n",
    "        return request\n",
    "    context.admin_client.requests.set_tags(request, [\"autosync\"])\n",
    "\n",
    "    return (\n",
    "        f\"Query submitted {request}. Use `client.code.{func_name}()` to run your query\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.custom_api.add(endpoint=submit_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.api.services.api.update(\n",
    "    endpoint_path=\"bigquery.submit_query\", hide_mock_definition=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.custom_api.api_endpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.api.services.bigquery.test_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_client.api.services.bigquery.submit_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mock version\n",
    "result = high_client.api.services.bigquery.test_query.mock(\n",
    "    sql_query=f\"SELECT * FROM {secrets['dataset_1']}.{secrets['table_1']} LIMIT 10\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test private version\n",
    "result = high_client.api.services.bigquery.test_query.private(\n",
    "    sql_query=f\"SELECT * FROM {secrets['dataset_1']}.{secrets['table_1']} LIMIT 10\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test mock version for wrong queries\n",
    "result = high_client.api.services.bigquery.test_query.mock(\n",
    "    sql_query=\"SELECT * FROM invalid_table LIMIT 1\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test private version\n",
    "result = high_client.api.services.bigquery.test_query.private(\n",
    "    sql_query=f\"SELECT * FROM {secrets['dataset_1']}.{secrets['table_1']} LIMIT 1\"\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the context state on an endpoint\n",
    "high_client.api.services.bigquery.test_query.mock.context.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = high_client.api.services.bigquery.submit_query(\n",
    "    func_name=\"my_func\",\n",
    "    query=f\"SELECT * FROM {secrets['dataset_1']}.{secrets['table_1']} LIMIT 1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
