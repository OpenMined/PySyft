{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teo/anaconda3/envs/PySyft/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-12-05 09:21:26.380031: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.5/lib64:/usr/local/cuda-11.5/lib64:\n",
      "2022-12-05 09:21:26.412308: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-12-05 09:21:27.146199: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.5/lib64:/usr/local/cuda-11.5/lib64:\n",
      "2022-12-05 09:21:27.147175: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.5/lib64:/usr/local/cuda-11.5/lib64:\n",
      "2022-12-05 09:21:27.147183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "import numpy as np\n",
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "9.0\n"
     ]
    }
   ],
   "source": [
    "def func1(x,y):\n",
    "    return 2*x + y\n",
    "\n",
    "def func2(x,y):\n",
    "    return func1(x,y) * x\n",
    "\n",
    "grad_fn1 = jax.grad(func1)\n",
    "grad_fn2 = jax.grad(func2)\n",
    "\n",
    "print(grad_fn1(2., 1.))\n",
    "print(grad_fn2(2., 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor annotated with DP Metadata!\n",
      "You can upload this Tensor to a domain node by calling `<domain_client>.load_dataset` and passing in this tensor as an asset.\n"
     ]
    }
   ],
   "source": [
    "data = np.array([2., 1.])\n",
    "phi_tensor = sy.Tensor(data).annotate_with_dp_metadata(lower_bound=1, upper_bound=2, data_subjects=\"ishan\")\n",
    "gamma_tensor = (phi_tensor[0] * 2 + phi_tensor[1]) * phi_tensor[0] \n",
    "# gamma_tensor = (phi_tensor * phi_tensor + phi_tensor) * phi_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as jnp\n",
    "from scipy.optimize import shgo\n",
    "from typing import Deque\n",
    "from collections import deque\n",
    "\n",
    "def create_new_lookup_tables(\n",
    "    dictionary: dict,\n",
    "):\n",
    "    index2key: Deque = deque()\n",
    "    key2index: dict = {}\n",
    "    index2values: Deque = (\n",
    "        deque()\n",
    "    )  # Note this maps to GammaTensor, not to GammaTensor.child as name may imply\n",
    "    index2size: Deque = deque()\n",
    "    for index, key in enumerate(dictionary.keys()):\n",
    "        key = str(key)\n",
    "        index2key.append(key)\n",
    "        key2index[key] = index\n",
    "        index2values.append(dictionary[key])\n",
    "        index2size.append(len(dictionary[key]))\n",
    "\n",
    "    return index2key, key2index, index2values, index2size\n",
    "\n",
    "def lipschitz_bound(tensor, data_subject) -> float:\n",
    "    # TODO: Check if there are any functions for which lipschitz bounds shouldn't be computed\n",
    "    # if dis(self.func) == dis(no_op):\n",
    "    #     raise Exception\n",
    "\n",
    "    print(\"Starting JAX JIT\")\n",
    "    # relative\n",
    "    from syft.core.tensor.autodp.gamma_functions import GAMMA_FUNC_MAPPER\n",
    "    from syft.core.tensor.autodp.gamma_tensor_ops import GAMMA_TENSOR_OP\n",
    "    from syft.core.adp.vectorized_publish import get_leaves_from_gamma_tensor_tree\n",
    "    \n",
    "    # TODO: this must be composed out of the whole computation tree\n",
    "    fn = jax.jit(GAMMA_FUNC_MAPPER[GAMMA_TENSOR_OP(tensor.func_str)])\n",
    "    # print(fn)\n",
    "    # print(\"Traced self.func with jax's jit, now calculating gradient\")\n",
    "    grad_fn = jax.grad(fn)\n",
    "    # phi_tensors = get_leaves_from_gamma_tensor_tree(tensor)\n",
    "    \n",
    "    # # this function will receive the data as one dimensional array\n",
    "    # def max_grad_fn(input_values: np.ndarray) -> float:\n",
    "    #     vectors = {}\n",
    "        \n",
    "    #     # we need to add the variable vectors belonging to the data_subject \n",
    "    #     # we are trying to minimize with regard to, and also the constant vectors\n",
    "    #     # that belong to the other data_subjects\n",
    "\n",
    "    #     grad_pred = grad_fn(vectors)\n",
    "\n",
    "    #     m = 0\n",
    "    #     for value in grad_pred.values():\n",
    "    #         m = max(m, jnp.max(value))\n",
    "\n",
    "    #     # return negative because we want to maximize instead of minimize\n",
    "    #     return -m\n",
    "\n",
    "    # print(\"starting SHGO\")\n",
    "    # res = shgo(max_grad_fn, bounds)\n",
    "    # print(\"Ran SHGO\")\n",
    "    # # return negative because we flipped earlier\n",
    "    # return -float(res.fun)\n",
    "    return grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([GammaTensor(child=5.0, data_subjects=DataSubjectArray: {'ishan'}, min_vals=<lazyrepeatarray data: 5.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 5.0 -> shape: ()>, is_linear=True, func_str='add', id='102850160', sources={'1216026388': GammaTensor(child=4.0, data_subjects=array(DataSubjectArray: {'ishan'}, dtype=object), min_vals=<lazyrepeatarray data: 4.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 4.0 -> shape: ()>, is_linear=True, func_str='noop', id='1216026388', sources={}), '276557268': GammaTensor(child=1.0, data_subjects=array(DataSubjectArray: {'ishan'}, dtype=object), min_vals=<lazyrepeatarray data: 1.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 1.0 -> shape: ()>, is_linear=True, func_str='noop', id='276557268', sources={})}), GammaTensor(child=2.0, data_subjects=array(DataSubjectArray: {'ishan'}, dtype=object), min_vals=<lazyrepeatarray data: 2.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 2.0 -> shape: ()>, is_linear=True, func_str='noop', id='929086921', sources={})])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from syft.core.tensor.autodp.gamma_functions import GAMMA_FUNC_MAPPER\n",
    "from syft.core.tensor.autodp.gamma_tensor_ops import GAMMA_TENSOR_OP\n",
    "gamma_tensor.child.sources.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.0, 1.0, 2.0]\n",
      "{'1216026388': 4.0, '276557268': 1.0, '929086921': 2.0}\n",
      "Starting JAX JIT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GammaTensor(child=10.0, data_subjects=DataSubjectArray: {'ishan'}, min_vals=<lazyrepeatarray data: 10.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 10.0 -> shape: ()>, is_linear=True, func_str='multiply', id='2081560700', sources={'102850160': GammaTensor(child=5.0, data_subjects=DataSubjectArray: {'ishan'}, min_vals=<lazyrepeatarray data: 5.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 5.0 -> shape: ()>, is_linear=True, func_str='add', id='102850160', sources={'1216026388': GammaTensor(child=4.0, data_subjects=array(DataSubjectArray: {'ishan'}, dtype=object), min_vals=<lazyrepeatarray data: 4.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 4.0 -> shape: ()>, is_linear=True, func_str='noop', id='1216026388', sources={}), '276557268': GammaTensor(child=1.0, data_subjects=array(DataSubjectArray: {'ishan'}, dtype=object), min_vals=<lazyrepeatarray data: 1.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 1.0 -> shape: ()>, is_linear=True, func_str='noop', id='276557268', sources={})}), '929086921': GammaTensor(child=2.0, data_subjects=array(DataSubjectArray: {'ishan'}, dtype=object), min_vals=<lazyrepeatarray data: 2.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 2.0 -> shape: ()>, is_linear=True, func_str='noop', id='929086921', sources={})})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from syft.core.adp.vectorized_publish import get_leaves_from_gamma_tensor_tree\n",
    "phi_tensors = get_leaves_from_gamma_tensor_tree(gamma_tensor.child)\n",
    "print([phi.child for phi in phi_tensors])\n",
    "vectors = {}\n",
    "for phi in phi_tensors:\n",
    "    vectors[phi.id] = phi.child\n",
    "print(vectors)\n",
    "fun = lipschitz_bound(gamma_tensor.child, None)\n",
    "fun._fun\n",
    "gamma_tensor.child\n",
    "# fun(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GammaTensor(child=4.0, data_subjects=array(DataSubjectArray: {'ishan'}, dtype=object), min_vals=<lazyrepeatarray data: 4.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 4.0 -> shape: ()>, is_linear=True, func_str='noop', id='1216026388', sources={}),\n",
       " GammaTensor(child=1.0, data_subjects=array(DataSubjectArray: {'ishan'}, dtype=object), min_vals=<lazyrepeatarray data: 1.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 1.0 -> shape: ()>, is_linear=True, func_str='noop', id='276557268', sources={}),\n",
       " GammaTensor(child=2.0, data_subjects=array(DataSubjectArray: {'ishan'}, dtype=object), min_vals=<lazyrepeatarray data: 2.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 2.0 -> shape: ()>, is_linear=True, func_str='noop', id='929086921', sources={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m~/anaconda3/envs/PySyft/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3398\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  Input \u001b[0;32mIn [11]\u001b[0;36m in \u001b[0;35m<cell line: 2>\u001b[0;36m\u001b[0m\n\u001b[0;31m    eval(s)\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m<string>:1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def f(x):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .gamma_tensor_ops import GAMMA_TENSOR_OP\n",
    "\n",
    "ops = {\n",
    "    GAMMA_TENSOR_OP.NOOP: lambda x: x,\n",
    "    GAMMA_TENSOR_OP.ADD: jnp.add,\n",
    "    GAMMA_TENSOR_OP.SUBTRACT: jnp.subtract,\n",
    "    GAMMA_TENSOR_OP.MULTIPLY: jnp.multiply,\n",
    "    GAMMA_TENSOR_OP.TRUE_DIVIDE: jnp.true_divide,\n",
    "    GAMMA_TENSOR_OP.MATMUL: jnp.matmul,\n",
    "    GAMMA_TENSOR_OP.RMATMUL: make_r_infix_op(jnp.matmul),\n",
    "    GAMMA_TENSOR_OP.GREATER: jnp.greater,\n",
    "    GAMMA_TENSOR_OP.GREATER_EQUAL: jnp.greater_equal,\n",
    "    GAMMA_TENSOR_OP.EQUAL: jnp.equal,\n",
    "    GAMMA_TENSOR_OP.NOT_EQUAL: jnp.not_equal,\n",
    "    GAMMA_TENSOR_OP.LESS: jnp.less,\n",
    "    GAMMA_TENSOR_OP.LESS_EQUAL: jnp.less_equal,\n",
    "    GAMMA_TENSOR_OP.LOG: jnp.log,\n",
    "    GAMMA_TENSOR_OP.TRANSPOSE: jnp.transpose,\n",
    "    GAMMA_TENSOR_OP.SUM: jnp.sum,\n",
    "    GAMMA_TENSOR_OP.ONES_LIKE: jnp.ones_like,\n",
    "    GAMMA_TENSOR_OP.ZEROS_LIKE: jnp.zeros_like,\n",
    "    GAMMA_TENSOR_OP.RAVEL: jnp.ravel,\n",
    "    GAMMA_TENSOR_OP.RESIZE: jnp.resize,\n",
    "    GAMMA_TENSOR_OP.RESHAPE: jnp.reshape,\n",
    "    GAMMA_TENSOR_OP.COMPRESS: jnp.compress,\n",
    "    GAMMA_TENSOR_OP.SQUEEZE: jnp.squeeze,\n",
    "    GAMMA_TENSOR_OP.ANY: jnp.any,\n",
    "    GAMMA_TENSOR_OP.ALL: jnp.all,\n",
    "    GAMMA_TENSOR_OP.LOGICAL_AND: jnp.logical_and,\n",
    "    GAMMA_TENSOR_OP.LOGICAL_OR: jnp.logical_or,\n",
    "    GAMMA_TENSOR_OP.POSITIVE: jnp.positive,\n",
    "    GAMMA_TENSOR_OP.NEGATIVE: jnp.negative,\n",
    "    GAMMA_TENSOR_OP.MEAN: jnp.mean,\n",
    "    GAMMA_TENSOR_OP.STD: jnp.std,\n",
    "    GAMMA_TENSOR_OP.VAR: jnp.var,\n",
    "    GAMMA_TENSOR_OP.DOT: jnp.dot,\n",
    "    GAMMA_TENSOR_OP.CUMSUM: jnp.cumsum,\n",
    "    GAMMA_TENSOR_OP.CUMPROD: jnp.cumprod,\n",
    "    GAMMA_TENSOR_OP.SQRT: jnp.sqrt,\n",
    "    GAMMA_TENSOR_OP.ABS: jnp.abs,\n",
    "    GAMMA_TENSOR_OP.CHOOSE: jnp.choose,\n",
    "    GAMMA_TENSOR_OP.CLIP: jnp.clip,\n",
    "    GAMMA_TENSOR_OP.COPY: jnp.copy,\n",
    "    GAMMA_TENSOR_OP.TAKE: jnp.take,\n",
    "    GAMMA_TENSOR_OP.PUT: jnp.put,\n",
    "    GAMMA_TENSOR_OP.ARGMAX: jnp.argmax,\n",
    "    GAMMA_TENSOR_OP.ARGMIN: jnp.argmin,\n",
    "    GAMMA_TENSOR_OP.PTP: jnp.ptp,\n",
    "    GAMMA_TENSOR_OP.MOD: jnp.mod,\n",
    "    GAMMA_TENSOR_OP.SWAPAXES: jnp.swapaxes,\n",
    "    GAMMA_TENSOR_OP.NONZERO: jnp.nonzero,\n",
    "    GAMMA_TENSOR_OP.PROD: jnp.prod,\n",
    "    GAMMA_TENSOR_OP.FLOOR_DIVIDE: jnp.floor_divide,\n",
    "    GAMMA_TENSOR_OP.POWER: jnp.power,\n",
    "    GAMMA_TENSOR_OP.TRACE: jnp.trace,\n",
    "    GAMMA_TENSOR_OP.MIN: jnp.min,\n",
    "    GAMMA_TENSOR_OP.MAX: jnp.max,\n",
    "    GAMMA_TENSOR_OP.LSHIFT: jnp.left_shift,\n",
    "    GAMMA_TENSOR_OP.RSHIFT: jnp.right_shift,\n",
    "    GAMMA_TENSOR_OP.XOR: jnp.bitwise_xor,\n",
    "    GAMMA_TENSOR_OP.ROUND: jnp.round,\n",
    "    GAMMA_TENSOR_OP.SORT: jnp.sort,\n",
    "    GAMMA_TENSOR_OP.ARGSORT: jnp.argsort,\n",
    "    GAMMA_TENSOR_OP.REPEAT: jnp.repeat,\n",
    "    GAMMA_TENSOR_OP.DIVMOD: jnp.divmod,\n",
    "    GAMMA_TENSOR_OP.FLATTEN_C: get_flatten_type(order=\"C\"),\n",
    "    GAMMA_TENSOR_OP.FLATTEN_F: get_flatten_type(order=\"F\"),\n",
    "    GAMMA_TENSOR_OP.FLATTEN_A: get_flatten_type(order=\"A\"),\n",
    "    GAMMA_TENSOR_OP.FLATTEN_K: get_flatten_type(order=\"K\"),\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('PySyft')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4793a45c21657cf3d03167fad0b0286070df6dcd03825f60c6ea99cbb4eef702"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
