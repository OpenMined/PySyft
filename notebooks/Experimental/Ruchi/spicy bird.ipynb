{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa928c-5eff-49dc-b174-007f05fcb234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🌶 🍗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e8e5d-2c46-40da-8c90-269990b08ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Dict\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytest\n",
    "\n",
    "# syft absolute\n",
    "import syft as sy\n",
    "from syft import Domain\n",
    "from syft.core.adp.entity import DataSubject\n",
    "from syft.util import download_file\n",
    "from syft.util import get_root_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e33a85-c6f7-487f-b51a-694f67f31b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_spicy_bird_benchmark() -> Tuple[Dict[str, Path], List[str]]:\n",
    "    file_suffix = \"_rows_dataset_sample.parquet\"\n",
    "    BASE_URL = \"https://raw.githubusercontent.com/madhavajay/datasets/main/spicy_bird/\"\n",
    "    sizes = [\"100K\", \"250K\", \"500K\", \"750K\", \"1M\"]\n",
    "    folder_name = \"spicy_bird\"\n",
    "    dataset_path = get_root_data_path() / folder_name\n",
    "    paths = []\n",
    "    for size in sizes:\n",
    "        filename = f\"{size}{file_suffix}\"\n",
    "        url = f\"{BASE_URL}{filename}\"\n",
    "        print(url)\n",
    "        path = download_file(url=url, full_path=dataset_path / filename)\n",
    "        paths.append(path)\n",
    "    return dict(zip(sizes, paths)), sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03955087-5acf-430f-bd0a-7eac9e8ad4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_subset(\n",
    "    domain: Domain,\n",
    "    df: pd.DataFrame,\n",
    "    size_name: str,\n",
    "    unique_key: str,\n",
    "    start_index: int,\n",
    "    end_index: int,\n",
    "    count: int,\n",
    ") -> None:\n",
    "    name = f\"Tweets - {size_name} - {unique_key} - {count}\"\n",
    "    impressions = ((np.array(list(df[\"impressions\"][start_index:end_index])))).astype(\n",
    "        np.int32\n",
    "    )\n",
    "    publication_title = list(df[\"publication_title\"][start_index:end_index])\n",
    "\n",
    "    entities = list()\n",
    "    for i in range(len(publication_title)):\n",
    "        entities.append(DataSubject(name=publication_title[i]))\n",
    "\n",
    "    tweets_data = sy.Tensor(impressions).private(\n",
    "        min_val=0, max_val=30, entities=entities\n",
    "    )\n",
    "\n",
    "    # blocking\n",
    "    domain.load_dataset(\n",
    "        assets={f\"{size_name}_tweets\": tweets_data},\n",
    "        name=name,\n",
    "        description=f\"{name} - {datetime.now()}\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a3139e-087c-4a13-8dc0-e75f6a9e3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_upload(\n",
    "    domain: Domain,\n",
    "    size_name: str,\n",
    "    unique_key: str,\n",
    "    df: pd.DataFrame,\n",
    "    chunk_size: int = 250_000,\n",
    ") -> float:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # iterate over number of chunks - 1\n",
    "    count = 0\n",
    "    last_val = 0\n",
    "    for i in range(0, df.shape[0] - chunk_size, chunk_size):\n",
    "        count = count + 1\n",
    "        last_val += chunk_size\n",
    "        upload_subset(\n",
    "            domain=domain,\n",
    "            df=df,\n",
    "            size_name=size_name,\n",
    "            unique_key=unique_key,\n",
    "            start_index=i,\n",
    "            end_index=i + chunk_size,\n",
    "            count=count,\n",
    "        )\n",
    "\n",
    "    # upload final chunk\n",
    "    upload_subset(\n",
    "        domain=domain,\n",
    "        df=df,\n",
    "        size_name=size_name,\n",
    "        unique_key=unique_key,\n",
    "        start_index=last_val,\n",
    "        end_index=df.shape[0],\n",
    "        count=count + 1,\n",
    "    )\n",
    "    return time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4501d4f-7e04-4180-b86f-4fcb7f97baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_sum(\n",
    "    domain: Domain, chunk_indexes: List[int], size_name: str, timeout: int = 999\n",
    ") -> float:\n",
    "    start_time = time.time()\n",
    "\n",
    "    res = None\n",
    "    for chunk_index in chunk_indexes:\n",
    "        # get the dataset asset for size_name at chunk_index\n",
    "        dataset = domain.datasets[chunk_index][f\"{size_name}_tweets\"]\n",
    "        if res is None:\n",
    "            res = dataset.sum(axis=0)\n",
    "        else:\n",
    "            res += dataset.sum(axis=0)\n",
    "\n",
    "    # make sure to block\n",
    "    res.block_with_timeout(timeout)\n",
    "\n",
    "    return time.time() - start_time\n",
    "\n",
    "\n",
    "def get_all_chunks(domain: Domain, unique_key: str) -> List[int]:\n",
    "    ids = []\n",
    "    for i in domain.datasets:\n",
    "        if unique_key in i.name:\n",
    "            ids.append(i.key)\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1925f-f686-4886-b74b-b9f2f3ac1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DOMAIN1_PORT = 9082\n",
    "DOMAIN1_PORT = 8081\n",
    "\n",
    "files, ordered_sizes = download_spicy_bird_benchmark()\n",
    "domain = sy.login(\n",
    "    email=\"info@openmined.org\", password=\"changethis\", port=DOMAIN1_PORT\n",
    ")\n",
    "\n",
    "benchmark_report = {}\n",
    "for size_name in ordered_sizes:\n",
    "    unique_key = str(hash(time.time()))\n",
    "    benchmark_report[size_name] = {}\n",
    "    df = pd.read_parquet(files[size_name])\n",
    "\n",
    "    # make smaller\n",
    "    # df = df[0:10000]\n",
    "\n",
    "    upload_time = time_upload(\n",
    "        domain=domain, size_name=size_name, unique_key=unique_key, df=df\n",
    "    )\n",
    "    benchmark_report[size_name][\"upload_secs\"] = upload_time\n",
    "    all_chunks = get_all_chunks(domain=domain, unique_key=unique_key)\n",
    "    sum_time = time_sum(\n",
    "        domain=domain, chunk_indexes=all_chunks, size_name=size_name\n",
    "    )\n",
    "    benchmark_report[size_name][\"sum_secs\"] = sum_time\n",
    "    break\n",
    "\n",
    "print(benchmark_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
