{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reqs\n",
    "\n",
    "SPEED:\n",
    "- automatic testing of fast serde of 10M tensor\n",
    "- automatic testing of fast .private() creation\n",
    "- direct converstion between primes created into numpy array and use here.\n",
    "- automatic testing of fast converstion between Phi and Gamma\n",
    "- testing of compatibility with jax?\n",
    "\n",
    "CORRECTNESS:\n",
    "- automatic testing that polynomial evaluation is correct\n",
    "- automatic testing that derivative calculation is correct (or perhaps figure out a way to use jax's jacobian function?)\n",
    "- automatic testing that DP guarantees seem to work (min/max extremes and inner samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyterflame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "from numpy.typing import ArrayLike\n",
    "from typing import List\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "from primesieve import primes\n",
    "import syft as sy\n",
    "import time\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrimeFactory:\n",
    "    \"\"\"IMPORTANT: it's very important that two tensors be able to tell that\n",
    "    they are indeed referencing the EXACT same PrimeFactory. At present this is done\n",
    "    by ensuring that it is literally the same python object. In the future, we will\n",
    "    probaby need to formalize this. However, the main way this could go wrong is if we\n",
    "    created some alternate way for checking to see if two prime factories 'sortof looked\n",
    "    the same' but which in fact weren't the EXACT same object. This could lead to\n",
    "    security leaks wherein two tensors think two different symbols in fact are the\n",
    "    same symbol.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.exp = 2\n",
    "        self._prime_number_cache: np.ndarray = primes(10**self.exp)\n",
    "\n",
    "    def __getitem__(self, indices):\n",
    "        \n",
    "        if isinstance(indices, slice):\n",
    "            max_index = indices.stop\n",
    "            \n",
    "        elif isinstance(indices, int):\n",
    "            max_index = indices\n",
    "\n",
    "        while max_index > len(self._prime_number_cache) - 1:\n",
    "            self.exp += 1\n",
    "            self._prime_number_cache = primes(10**self.exp)\n",
    "\n",
    "        return np.array(self._prime_number_cache.__getitem__(indices))\n",
    "    \n",
    "pf = PrimeFactory()\n",
    "sy.pf = pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool\n",
    "\n",
    "# out = 5\n",
    "\n",
    "# def f(x):\n",
    "#     return x*x + out\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     with Pool(5) as p:\n",
    "#         print(p.map(f, [1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = {\"3\":3,\"4\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mp.Value('lookup', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSubjectRegistry():\n",
    "    \"\"\"This registry needs to run on the Domain server with the accountant. It is assumed that there\n",
    "    is only one globally maintained list of entities, mapped to primes.\"\"\"\n",
    "    \n",
    "    def __init__(self, prime_factory=None):\n",
    "        \n",
    "        if prime_factory is None:\n",
    "            prime_factory = sy.pf\n",
    "        \n",
    "        # tries to make it as efficient as possible to fetch arrays of primes\n",
    "        self.prime_factory = prime_factory\n",
    "        \n",
    "        # maps indices in the prime_factory to the data subject strings uniquely identifying them\n",
    "        self.prime_index2data_subjects = list()\n",
    "        self.data_subjects_set = set()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.prime_index2data_subjects = list()\n",
    "        self.data_subjects_set = set()\n",
    "        \n",
    "    def __getitem__(self, data_subject_strings):\n",
    "\n",
    "        if isinstance(data_subject_strings, str):\n",
    "            data_subject_strings = [data_subject_strings]\n",
    "        \n",
    "        if isinstance(data_subject_strings, list):\n",
    "            if len(self.prime_index2data_subjects) == 0:\n",
    "                unique_data_subject_strings = set(data_subject_strings)\n",
    "                if len(unique_data_subject_strings) == len(data_subject_strings):\n",
    "                    self.prime_index2data_subjects = data_subject_strings\n",
    "                    self.data_subjects_set = unique_data_subject_strings\n",
    "                    return self.prime_factory[0:len(data_subject_strings)]\n",
    "                else:\n",
    "                    self.prime_index2data_subjects = list(unique_data_subject_strings)\n",
    "                    self.data_subjects_set = unique_data_subject_strings\n",
    "                    \n",
    "                    relevant_primes = self.prime_factory[0:len(data_subject_strings)]\n",
    "                    \n",
    "                    \n",
    "            \n",
    "            return self.prime_factory[starting_len:starting_len+len(data_subject_strings)]\n",
    "        \n",
    "        raise Exception(\"don't know how to handle this input type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lazyrepeatarray:\n",
    "    \"\"\"when data is repeated along one or more dimensions, store it using this lazyrepeatarray so that \n",
    "    you can save on RAM and CPU when computing with it.\"\"\"\n",
    "    \n",
    "    def __init__(self, data, shape):\n",
    "        \"\"\"\n",
    "        data: the raw data values without repeats\n",
    "        shape: the shape of 'data' if repeats were included\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        # NOTE: all additional arguments are assumed to be broadcast if dims are shorter\n",
    "        # than that of data. Example: if data.shape == (2,3,4) and min_vals.shape == (2,3), \n",
    "        # then it's assumed that the full min_vals.shape is actually (2,3,4) where the last\n",
    "        # dim is simply copied. Example2: if data.shape == (2,3,4) and min_vals.shape == (2,1,4),\n",
    "        #Â then the middle dimension is supposed to be copied to be min_vals.shape == (2,3,4) if\n",
    "        # necessary. This is just to keep the memory footprint (and computation) as small as\n",
    "        # possible.        \n",
    "        \n",
    "        \n",
    "        if isinstance(data, (bool, int, float)):\n",
    "            data = np.array(data)\n",
    "        \n",
    "        self.data = data\n",
    "        self.shape = shape\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"\"\"\n",
    "        THIS MIGHT LOOK LIKE COPY-PASTED CODE!\n",
    "        Don't touch it. It's going to get more complicated.\n",
    "        \"\"\"        \n",
    "        if self.shape != other.shape:\n",
    "            raise Exception(\"cannot subtract tensors with different shapes\")\n",
    "        \n",
    "        if self.data.shape == other.data.shape:\n",
    "            return lazyrepeatarray(data = self.data - other.data, shape=self.shape)\n",
    "        \n",
    "        raise Exception(\"not sure how to do this yet\")\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        THIS MIGHT LOOK LIKE COPY-PASTED CODE!\n",
    "        Don't touch it. It's going to get more complicated.\n",
    "        \"\"\"\n",
    "        if self.shape != other.shape:\n",
    "            raise Exception(\"cannot subtract tensors with different shapes\")\n",
    "        \n",
    "        if self.data.shape == other.data.shape:\n",
    "            return lazyrepeatarray(data = self.data * other.data, shape=self.shape)\n",
    "        \n",
    "        raise Exception(\"not sure how to do this yet\")\n",
    "    \n",
    "    def __pow__(self, exponent):\n",
    "        if exponent == 2:\n",
    "            return self * self\n",
    "        raise Exception(\"not sure how to do this yet\")\n",
    "    \n",
    "    def simple_assets_for_serde(self):\n",
    "        return [self.data, self.shape]\n",
    "    \n",
    "    @staticmethod\n",
    "    def deserialize_from_simple_assets(assets):\n",
    "        return lazyrepeatarray(data=assets[0], \n",
    "                              shape=assets[1])\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        return np.prod(self.shape)\n",
    "        \n",
    "    def sum(self, axis=None):\n",
    "        if axis is None:\n",
    "            if self.data.size == 1:\n",
    "                return np.array(self.data * self.size).flatten()\n",
    "            else:\n",
    "                raise Exception(\"not sure how to do this yet\")\n",
    "        else:\n",
    "            raise Exception(\"not sure how to do this yet\")\n",
    "\n",
    "x = lazyrepeatarray(2, shape=(1000000,4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class array:\n",
    "    \n",
    "    def __init__(self, data: ArrayLike):\n",
    "        \n",
    "        if isinstance(data, list):\n",
    "            data = np.array(data)\n",
    "        \n",
    "        self.child = data\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.child.shape\n",
    "    \n",
    "    def private(self, \n",
    "                min_vals: ArrayLike,\n",
    "                max_vals: ArrayLike,\n",
    "                data_subjects: Union[str,List[str]]):\n",
    "        \n",
    "        ####################################################################\n",
    "        # Step 1: Ensure data_subjects are ready to used to create phi array\n",
    "        ####################################################################\n",
    "        \n",
    "        # if it's a string - convert to list of length 1\n",
    "        if isinstance(data_subjects, str):\n",
    "            data_subjects = [data_subjects]\n",
    "\n",
    "        # for every data subject, get its affiliated prime\n",
    "        data_subjects = dsr[data_subjects]\n",
    "        \n",
    "        ######################################################################\n",
    "        # Step 2: Store min_vals and max_vals as lazyarray objects if possible\n",
    "        ######################################################################\n",
    "        \n",
    "        if isinstance(min_vals, (bool, int, float)):\n",
    "            min_vals = np.array(min_vals)\n",
    "            \n",
    "        if isinstance(max_vals, (bool, int, float)):\n",
    "            max_vals = np.array(max_vals)            \n",
    "        \n",
    "        if min_vals.shape != self.child.shape:\n",
    "            min_vals = lazyrepeatarray(min_vals, self.child.shape)\n",
    "            \n",
    "        if max_vals.shape != self.child.shape:\n",
    "            max_vals = lazyrepeatarray(max_vals, self.child.shape)            \n",
    "        \n",
    "        # entire tensor refers to one entity\n",
    "        if len(data_subjects) == 1:\n",
    "            return pharray(data=self.child,\n",
    "                           min_vals=min_vals,\n",
    "                           max_vals=max_vals,\n",
    "                           data_subject_prime=data_subjects)\n",
    "        \n",
    "        # each row corresponds to a unique entity\n",
    "        elif len(data_subjects) == len(self.child):\n",
    "            return rowarray(rows=self.child,\n",
    "                            min_vals=min_vals,\n",
    "                            max_vals=max_vals,\n",
    "                            data_subjects=data_subjects,\n",
    "                            row_type=pharray)\n",
    "        else:\n",
    "            raise Exception(\"not sure how to initialize\")\n",
    "        \n",
    "sy.array = array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pharray:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data: np.ndarray, \n",
    "                 min_vals: np.ndarray,\n",
    "                 max_vals: np.ndarray,\n",
    "                 data_subject_prime: np.ndarray,\n",
    "                 ignore_minmax_checks=True):\n",
    "        \n",
    "        if isinstance(min_vals, (bool, int, float)):\n",
    "            min_vals = np.array(min_vals)\n",
    "            \n",
    "        if isinstance(max_vals, (bool, int, float)):\n",
    "            max_vals = np.array(max_vals)            \n",
    "            \n",
    "        if isinstance(data_subject_prime, (bool, int, float)):\n",
    "            data_subject_prime = np.array(data_subject_prime)                        \n",
    "        \n",
    "        if not ignore_minmax_checks:\n",
    "            assert (data >= min_vals).all()\n",
    "            assert (data <= max_vals).all()\n",
    "        \n",
    "        self.data = data\n",
    "        self.minv = min_vals\n",
    "        self.maxv = max_vals\n",
    "        self.sub = data_subject_prime\n",
    "        \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        shape = self.data.shape\n",
    "        size = self.data.size\n",
    "        term = input2scalarprime.copy()\n",
    "        return gmarray(input2value=self.data,\n",
    "                       input2minval=self.minv,\n",
    "                       input2maxval=self.maxv,\n",
    "                       input2subjectprime=x.sub)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lazyprimearray:\n",
    "    \n",
    "    def __init__(self, start, stop, shape):\n",
    "        \"\"\" an array made of primes which is lazily evaluated\n",
    "        \n",
    "        start: the prime index this array begins with\n",
    "        end: the prime inddex this array ends with\n",
    "        shape: the shape of the array\n",
    "        \"\"\"\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.shape = shape\n",
    "        self._data_cache = None\n",
    "        \n",
    "    def reshape(self, *new_shape):\n",
    "        if np.prod(self.shape) == np.prod(new_shape):\n",
    "            \n",
    "            result = lazyprimearray(start=self.start,\n",
    "                                    stop=self.stop,\n",
    "                                    shape=new_shape)\n",
    "            return result\n",
    "        else:\n",
    "            raise Exception(\"New shape not compatible\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "class rowarray:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 rows,\n",
    "                 min_vals,\n",
    "                 max_vals,\n",
    "                 data_subjects,\n",
    "                 row_type=pharray):\n",
    "        \n",
    "        self.rows = rows\n",
    "        self.minv = min_vals\n",
    "        self.maxv = max_vals\n",
    "        self.subs = data_subjects\n",
    "        self.row_type = row_type\n",
    "    \n",
    "    def sum(self, axis=None):\n",
    "        return self.gamma.sum(axis=axis)\n",
    "    \n",
    "    def serialize(self):\n",
    "        assets = [self.rows, \n",
    "                  self.minv.simple_assets_for_serde(), \n",
    "                  self.maxv.simple_assets_for_serde(), \n",
    "                  self.subs, \n",
    "                  self.row_type]\n",
    "        return pyarrow.serialize(assets).to_buffer()\n",
    "    \n",
    "    @staticmethod\n",
    "    def deserialize(blob):\n",
    "        assets = pyarrow.deserialize(blob)\n",
    "        rows = assets[0]\n",
    "        minv = lazyrepeatarray.deserialize_from_simple_assets(assets[1])\n",
    "        maxv = lazyrepeatarray.deserialize_from_simple_assets(assets[2])\n",
    "        subs = assets[3]\n",
    "        row_type = assets[4]\n",
    "        return rowarray(rows=rows,\n",
    "                        min_vals=minv,\n",
    "                        max_vals=maxv,\n",
    "                        data_subjects=subs,\n",
    "                        row_type=row_type)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.rows.shape\n",
    "    \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        if self.row_type == pharray:\n",
    "            shape = self.rows.shape\n",
    "            size = self.rows.size\n",
    "            return gmarray(input2value=self.rows,\n",
    "                           input2minval=self.minv,\n",
    "                           input2maxval=self.maxv,\n",
    "                           input2subjectprime=self.subs,\n",
    "                           shape=self.shape,\n",
    "                           is_linear=True)\n",
    "        else:\n",
    "            raise Exception(\"Sorry don't know how to convert this to gamma yet.\")\n",
    "            \n",
    "            \n",
    "class gmarray:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input2value,                 \n",
    "                 input2minval,\n",
    "                 input2maxval,                 \n",
    "                 input2subjectprime,\n",
    "                 shape,\n",
    "                 is_linear,\n",
    "                 input2scalarprime=None,          \n",
    "                 input2scalarprime_id=None,\n",
    "                 value_cache=None,\n",
    "                 minval_cache=None,\n",
    "                 maxval_cache=None,\n",
    "                 term=None, \n",
    "                 coeff=None, \n",
    "                 bias=None):\n",
    "\n",
    "        # REPLACING SCALAR MANAGER ARE THE FOLLOWING NDARRAY LOOKUP TABLES\n",
    "        self.input2value = input2value\n",
    "        self.input2minval = input2minval\n",
    "        self.input2maxval = input2maxval \n",
    "        self.shape = shape\n",
    "        \n",
    "        if input2subjectprime.shape == self.input2value.shape:\n",
    "            input2subjectprime = input2subjectprime.reshape(input2subjectprime.shape + [1])\n",
    "        \n",
    "        # if an integer, it's assumed to be elementwise\n",
    "        self.input2subjectprime = input2subjectprime \n",
    "        \n",
    "        # None == elementwise, unique primes for freshly created gammatensor, starting at 1\n",
    "        self.input2scalarprime = input2scalarprime\n",
    "        \n",
    "        if input2scalarprime_id is None:\n",
    "            # given no i2s id, ASSUME we're initializing this tensor for the first time!\n",
    "            # which means all the caches are just copies of the data\n",
    "            input2scalarprime_id = sy.core.common.UID()\n",
    "            value_cache=input2value\n",
    "            minval_cache=input2minval\n",
    "            maxval_cache=input2maxval\n",
    "            is_linear=True\n",
    "            \n",
    "        self.input2scalarprime_id = input2scalarprime_id\n",
    "        \n",
    "        self.value_cache = value_cache\n",
    "        self.minval_cache = minval_cache\n",
    "        self.maxval_cache = maxval_cache\n",
    "        \n",
    "        self.is_linear=is_linear\n",
    "        \n",
    "        # tensor of polynomial terms - primes representing variables\n",
    "        # None == elementwise, unique primes for freshly created gammatensor, starting at 1\n",
    "        self._term = term\n",
    "        \n",
    "        # a tensor of coefficients - the floats which multiply by variables in polys\n",
    "        # None == np.ones_like(term)\n",
    "        self._coeff = coeff \n",
    "        \n",
    "        # a tensor of bias terms - scalars which are added to polys\n",
    "        # None == np.zeros_like(term)\n",
    "        self._bias = bias\n",
    "    \n",
    "    def serialize(self):\n",
    "        assets = list()\n",
    "        assets.append(self.input2value)\n",
    "        assets.append(self.input2minval.simple_assets_for_serde())\n",
    "        assets.append(self.input2maxval.simple_assets_for_serde())\n",
    "        assets.append(self.shape)\n",
    "        assets.append(self.input2subjectprime)\n",
    "        assets.append(self.input2scalarprime)\n",
    "#         assets.append(self.input2scalarprime_id)        \n",
    "        assets.append(self.value_cache)        \n",
    "        assets.append(self.minval_cache)        \n",
    "        assets.append(self.maxval_cache)        \n",
    "        assets.append(self._term)                \n",
    "        assets.append(self._coeff)                \n",
    "        assets.append(self._bias)    \n",
    "        return pyarrow.serialize(assets).to_buffer()        \n",
    "        \n",
    "    @property\n",
    "    def size(self):\n",
    "        return np.prod(self.shape)\n",
    "    \n",
    "    @property\n",
    "    def term(self):\n",
    "        if self._term is None:\n",
    "            self._term = lazyprimearray(start=0, stop=np.prod(self.shape), shape=list(self.shape)+[1])\n",
    "        return self._term\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def coeff(self):\n",
    "        if self._coeff is None:\n",
    "            self._coeff = lazyrepeatarray(data=1, shape=self.shape)\n",
    "        return self._coeff\n",
    "    \n",
    "    @property\n",
    "    def bias(self):\n",
    "        if self._bias is None:\n",
    "            self._bias = lazyrepeatarray(data=0, shape=self.shape)\n",
    "        return self._bias\n",
    "        \n",
    "    def sum(self, axis=None):\n",
    "        if axis is None:\n",
    "            return gmarray(input2value = self.input2value,\n",
    "                            input2minval = self.input2minval,\n",
    "                            input2maxval = self.input2maxval,\n",
    "                            input2subjectprime = self.input2subjectprime,\n",
    "                            shape=(),\n",
    "                            is_linear=self.is_linear,\n",
    "                            input2scalarprime = self.input2scalarprime,\n",
    "                            input2scalarprime_id = self.input2scalarprime_id,\n",
    "                            value_cache=self.value_cache.sum(),\n",
    "                            minval_cache=self.minval_cache.sum(),\n",
    "                            maxval_cache=self.maxval_cache.sum(),\n",
    "                            term=self.term.reshape(1,self.size),\n",
    "                            coeff = None if self._coeff is None else self.coeff.reshape(1, self.size),\n",
    "                            bias = None if self._bias is None else self.bias.sum())\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Not sure how to run this yet\")\n",
    "    \n",
    "    def deriv(self, inputs, input_mask=None):\n",
    "\n",
    "        assert inputs.shape == self.input2value.shape\n",
    "        \n",
    "        # if someone doesn't pass in a mask we assume they\n",
    "        # want to use all the inputs they're passing in\n",
    "        if input_mask is None:\n",
    "            input_mask = np.zeros_like(inputs)\n",
    "        else:\n",
    "            \"\"\n",
    "            # if they do pass in a mask then 1s correspond\n",
    "            # to data passed in and 0s to values from self.input2value\n",
    "        \n",
    "        assert inputs.shape == self.input2value.shape\n",
    "        \n",
    "        if self.is_linear:\n",
    "            \n",
    "            # TODO: lazyarray should know how to find the max coeff very\n",
    "            # efficient instead of needing to hardcode this here\n",
    "            if self._coeff is None:\n",
    "                return np.ones(self.shape)\n",
    "        \n",
    "        raise Exception(\"Ooops... can't compute max deriv of this yet...\")\n",
    "            \n",
    "    def max_deriv(self, inputs, input_mask=None):\n",
    "        \n",
    "        assert inputs.shape == self.input2value.shape\n",
    "        \n",
    "        # if someone doesn't pass in a mask we assume they\n",
    "        # want to use all the inputs they're passing in\n",
    "        if input_mask is None:\n",
    "            input_mask = np.zeros_like(inputs)\n",
    "        else:\n",
    "            \"\"\n",
    "            # if they do pass in a mask then 1s correspond\n",
    "            # to data passed in and 0s to values from self.input2value\n",
    "        \n",
    "        if self.is_linear:\n",
    "            \n",
    "            # TODO: lazyarray should know how to find the max coeff very\n",
    "            # efficient instead of needing to hardcode this here\n",
    "            if self._coeff is None:\n",
    "                return np.ones(self.shape)\n",
    "        \n",
    "        raise Exception(\"Ooops... can't compute max deriv of this yet...\")\n",
    "        \n",
    "#     def max_deriv_wrt_entity(self, entity_prime):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "data_subjects = list(map(lambda x:str(x),range(0,5)))\n",
    "data = sy.array(np.random.rand(5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsr = DataSubjectRegistry()\n",
    "x = data.private(min_vals=0, \n",
    "                 max_vals=1, \n",
    "                 data_subjects=data_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.deriv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(1.)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.max_deriv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.core.adp.idp_gaussian_mechanism import iDPGaussianMechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 3\n",
    "squared_l2_norm = np.sum(out.input2value**2)\n",
    "squared_l2_norm_upper_bound = ((out.input2maxval - out.input2minval)**2).sum()\n",
    "L = out.max_deriv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def max_deriv_wrt_entity(self, entity_prime):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_prime = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = iDPGaussianMechanism(sigma=sigma,\n",
    "                         squared_l2_norm=squared_l2_norm,\n",
    "                         squared_l2_norm_upper_bound=squared_l2_norm_upper_bound,\n",
    "                         L=L,\n",
    "                         entity_name=\"dunno\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  [0. 1. 2.]\n",
      "f(x):  5.0\n",
      "grad_f(x): [0. 2. 4.]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "def f(x):\n",
    "    return jnp.sum(x**2) # identical to numpy syntax\n",
    "grad_f = jax.grad(f) # compute the gradient function\n",
    "x = jnp.array([0., 1., 2.]) # use JAX arrays!\n",
    "print('x: ', x)\n",
    "print('f(x): ', f(x))\n",
    "print('grad_f(x):', grad_f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
