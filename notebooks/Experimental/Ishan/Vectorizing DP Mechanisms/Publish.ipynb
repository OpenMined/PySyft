{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b0505e-7294-4880-98a1-18084fd47a9c",
   "metadata": {},
   "source": [
    "### data_subject_ledger.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf403c42-ebb4-449b-b23e-2e4b84614103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class LedgerUpdate:\n",
    "    def __init__(self, sigmas, l2_norms, l2_norm_bounds, Ls, coeffs, entity_ids, update_number, timestamp):\n",
    "        self.sigmas = sigmas\n",
    "        self.l2_norms = l2_norms\n",
    "        self.l2_norm_bounds = l2_norm_bounds \n",
    "        self.Ls = Ls\n",
    "        self.coeffs = coeffs \n",
    "        self.entity_ids = entity_ids\n",
    "        self.update_number = update_number\n",
    "        self.timestamp = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d6e571d6-df49-4fcd-9304-86077b28910f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar\n",
    "import time\n",
    "\n",
    "class DataSubjectLedger:\n",
    "    \"\"\"for a particular data subject, this is the list\n",
    "    of all mechanisms releasing informationo about this\n",
    "    particular subject, stored in a vectorized form\"\"\"\n",
    "    \n",
    "    def __init__(self, default_cache_size=1e3):\n",
    "        \n",
    "        self.delta = 1e-6  # WARNING: CHANGING DELTA INVALIDATES THE CACHE\n",
    "        self.reset()\n",
    "        self.cache_constant2epsilon = list()\n",
    "        self.increase_max_cache(int(default_cache_size))\n",
    "        \n",
    "        # save initial size (number of rows from DB) when deserialized\n",
    "        self.known_db_size = 0\n",
    "        self.update_number = 0\n",
    "        self.timestamp_of_last_update = None\n",
    "    \n",
    "    def write_to_db(self):\n",
    "        self.update_number += 1\n",
    "    \n",
    "        result = LedgerUpdate(sigmas=self.sigmas[self.known_db_size:],\n",
    "                            l2_norms=self.l2_norms[self.known_db_size:],\n",
    "                            l2_norm_bounds=self.l2_norms[self.known_db_size:],\n",
    "                            Ls=self.Ls[self.known_db_size:],\n",
    "                            coeffs=self.coeffs[self.known_db_size:],\n",
    "                            entity_ids=self.entity_ids[self.known_db_size:],\n",
    "                            update_number=self.update_number,\n",
    "                            timestamp=time.time()\n",
    "                           )\n",
    "        self.known_db_size += len(self.sigmas)\n",
    "        return result\n",
    "\n",
    "    def read_from_db(self, update: LedgerUpdate):\n",
    "        if update.update_number == self.update_number + 1:\n",
    "            if self.timestamp_of_last_update is not None and update.timestamp < self.timestamp:\n",
    "                raise Exception(\"It appears that updates were created out of order.\" +  \n",
    "                \"This is probably due to multiple python threads creating updates- which should NOT happen.\" + \n",
    "                \"This is a very serious error- please contact OpenMined immediately.\" + \"Thank you!\")\n",
    "            self.sigmas = np.concatenate([self.sigmas, update.sigmas])\n",
    "            self.l2_norms = np.concatenate([self.l2_norms, update.l2_norms])\n",
    "            self.l2_norm_bounds = np.concatenate([self.l2_norm_bounds, update.l2_norm_bounds]) \n",
    "            self.Ls = np.concatenate([self.Ls, update.Ls])\n",
    "            self.coeffs = np.concatenate([self.coeffs, update.coeffs]) \n",
    "            self.entity_ids = np.concatenate([self.entity_ids, update.entity_ids])\n",
    "            self.update_number = update.update_number\n",
    "            self.timestamp = update.timestamp\n",
    "        else:\n",
    "            raise Exception(\"Cannot add update to Ledger\")\n",
    "                                                   \n",
    "                                                   \n",
    "    def reset(self):\n",
    "        self.sigmas = np.array([])\n",
    "        self.l2_norms = np.array([])\n",
    "        self.l2_norm_bounds = np.array([])\n",
    "        self.Ls = np.array([])\n",
    "        self.coeffs = np.array([])\n",
    "        self.entity_ids = np.array([])\n",
    "        self.entity2budget = np.array([])\n",
    "        \n",
    "    def batch_append(self, \n",
    "                     sigmas: np.ndarray, \n",
    "                     l2_norms: np.ndarray, \n",
    "                     l2_norm_bounds: np.ndarray, \n",
    "                     Ls: np.ndarray, \n",
    "                     coeffs: np.ndarray, \n",
    "                     entity_ids: np.ndarray):\n",
    "        self.sigmas = np.concatenate([self.sigmas, sigmas])\n",
    "        self.l2_norms = np.concatenate([self.l2_norms, l2_norms])        \n",
    "        self.l2_norm_bounds = np.concatenate([self.l2_norm_bounds, l2_norm_bounds])        \n",
    "        self.Ls = np.concatenate([self.Ls, Ls])        \n",
    "        self.coeffs = np.concatenate([self.coeffs, coeffs])               \n",
    "        self.entity_ids = np.concatenate([self.entity_ids, entity_ids])\n",
    "        \n",
    "    def increase_max_cache(self, new_size):\n",
    "        new_entries = []\n",
    "        current_size = len(self.cache_constant2epsilon)\n",
    "        for i in range(new_size - current_size):\n",
    "            alpha, eps = self.get_optimal_alpha_for_constant(i+1 + current_size)\n",
    "            new_entries.append(eps)\n",
    "        self.cache_constant2epsilon = np.concatenate([self.cache_constant2epsilon, np.array(new_entries)])\n",
    "        # print(self.cache_constant2epsilon)\n",
    "        \n",
    "    def get_fake_rdp_func(self, constant):\n",
    "        \n",
    "        def func(alpha):\n",
    "            return alpha * constant\n",
    "        \n",
    "        return func\n",
    "\n",
    "    def get_alpha_search_function(self, rdp_compose_func):\n",
    "            \n",
    "        # if len(self.deltas) > 0:\n",
    "            # delta = np.max(self.deltas)\n",
    "        # else:\n",
    "        log_delta = np.log(self.delta)\n",
    "        \n",
    "        def fun(alpha):  # the input is the RDP's \\alpha\n",
    "            \n",
    "            if alpha <= 1:\n",
    "                return np.inf\n",
    "            else:\n",
    "                alpha_minus_1 = alpha-1\n",
    "                return np.maximum(rdp_compose_func(alpha) + np.log(alpha_minus_1/alpha)\n",
    "                                  - (log_delta + np.log(alpha))/alpha_minus_1, 0)\n",
    "        return fun    \n",
    "    \n",
    "    def get_optimal_alpha_for_constant(self, constant=3):\n",
    "        \n",
    "        f = self.get_fake_rdp_func(constant)\n",
    "        f2 = self.get_alpha_search_function(rdp_compose_func=f)\n",
    "        results = minimize_scalar(f2, method='Brent', bracket=(1,2), bounds=[1, np.inf])\n",
    "        \n",
    "        return results.x, results.fun\n",
    "\n",
    "        \n",
    "    def get_batch_rdp_constants(self, entity_ids_query, private=True):\n",
    "        \n",
    "        # get indices for all ledger rows corresponding to any of the entities in entity_ids_query\n",
    "        indices_batch = np.where(np.in1d(self.entity_ids, entity_ids_query))[0]\n",
    "        \n",
    "        # use the indices to get a \"batch\" of the full ledger. this is the only part\n",
    "        # of the ledger we care about (the entries corresponding to specific entities)\n",
    "        batch_sigmas = self.sigmas.take(indices_batch)\n",
    "        batch_Ls = self.Ls.take(indices_batch)\n",
    "        batch_l2_norms = self.l2_norms.take(indices_batch)\n",
    "        batch_l2_norm_bounds = self.l2_norm_bounds.take(indices_batch)\n",
    "        batch_coeffs = self.coeffs.take(indices_batch)\n",
    "        batch_entity_ids = self.entity_ids.take(indices_batch).astype(np.int64)\n",
    "        \n",
    "        squared_Ls = batch_Ls**2\n",
    "        squared_sigma = batch_sigmas**2\n",
    "        \n",
    "        if private:\n",
    "            squared_L2_norms = batch_l2_norms**2\n",
    "            constant = (squared_Ls * squared_L2_norms / (2 * squared_sigma)) * batch_coeffs\n",
    "            constant = np.bincount(batch_entity_ids, weights=constant).take(entity_ids_query)\n",
    "            return constant\n",
    "        else:\n",
    "            squared_L2_norm_bounds = batch_l2_norm_bounds**2\n",
    "            constant = (squared_Ls * squared_L2_norm_bounds / (2 * squared_sigma)) * batch_coeffs\n",
    "            constant = np.bincount(batch_entity_ids, weights=constant).take(entity_ids_query)\n",
    "            return constant\n",
    "        \n",
    "    def get_epsilon_spend(self, entity_ids_query):\n",
    "        rdp_constants = self.get_batch_rdp_constants(entity_ids_query=entity_ids_query).astype(np.int64)\n",
    "        rdp_constants_lookup = rdp_constants - 1\n",
    "        try:\n",
    "            eps_spend = self.cache_constant2epsilon.take(rdp_constants_lookup)\n",
    "        except IndexError:\n",
    "            self.increase_max_cache(int(max(rdp_constants_lookup) * 1.1))\n",
    "            eps_spend = self.cache_constant2epsilon.take(rdp_constants_lookup)\n",
    "        return eps_spend\n",
    "    \n",
    "    def get_overbudgeted_entities(self, user_budget: float, unique_entity_ids_query): \n",
    "        \"\"\" TODO: \n",
    "        In our current implementation, user_budget is obtained by querying the Adversarial Accountant's entity2ledger with the Data Scientist's User Key.\n",
    "        When we replace the entity2ledger with something else, we could perhaps directly add it into this method\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the privacy budget spent by all the entities\n",
    "        epsilon_spent = self.get_epsilon_spend(unique_entity_ids_query.astype(np.int64))\n",
    "        # print(np.mean(epsilon_spent))\n",
    "        \n",
    "        # Create a mask\n",
    "        is_overbudget = np.ones_like(epsilon_spent) * user_budget < epsilon_spent\n",
    "        return is_overbudget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d86548a-1653-47f4-95af-67f11d904b3d",
   "metadata": {},
   "source": [
    "### gamma_tensor.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161e732-56a8-47bc-9372-46cf3aaef375",
   "metadata": {},
   "source": [
    "### vectorized_publish.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f68566f-618d-4551-a026-674806d8abb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from syft.core.adp.entity_list import DataSubjectList\n",
    "from typing import Optional\n",
    "from typing import Callable\n",
    "from random import gauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e1924df-3547-4bf5-9c99-e0eb9ef4f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bounds_for_mechanism(value_array, min_val_array, max_val_array):\n",
    "    \"\"\"Calculates the squared L2 norm values needed to create a Mechanism, and calculate privacy budget + spend\"\"\"\n",
    "    \"\"\" If you calculate the privacy budget spend with the worst case bound, you can show this number to the D.S.\n",
    "    If you calculate it with the regular value (the value computed below when public_only = False, you cannot show the \n",
    "    privacy budget to the DS because this violates privacy.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Double check whether the iDPGaussianMechanism class squares its squared_l2_norm values!!\n",
    "    worst_case_l2_norm = np.sqrt(np.sum(np.square(max_val_array - min_val_array))) * np.ones_like(value_array)\n",
    "    l2_norm = np.sqrt(np.sum(np.square(value_array))) * np.ones_like(value_array)\n",
    "    # print(l2_norm.shape, worst_case_l2_norm.shape)\n",
    "    # print(l2_norm.shape)\n",
    "    return l2_norm, worst_case_l2_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94656a7f-ecac-4e04-b602-fdbb2d375c2f",
   "metadata": {},
   "source": [
    "A lot of the \"reshape\" calls are because `values` returns a single number for sum, and thus calling `ones_like` returns a dimensionless np array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e880ffaa-6a75-40e2-a42e-3f2577b6a252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_publish(\n",
    "    min_vals: np.ndarray, \n",
    "    max_vals: np.ndarray, \n",
    "    values: np.ndarray, \n",
    "    data_subjects: DataSubjectList, \n",
    "    is_linear: bool = True, \n",
    "    ledger: Optional[DataSubjectLedger] = None, \n",
    "    data_scientist_budget: float = 675, \n",
    "    sigma: float = 1.5, \n",
    "    output_func: Callable = np.sum\n",
    "    # private: bool = False\n",
    "):\n",
    "    print(\"Starting vectorized publish\")\n",
    "    # Get all unique entities\n",
    "    unique_data_subjects = data_subjects.one_hot_lookup\n",
    "    unique_data_subject_indices = np.arange(len(unique_data_subjects)) # because unique_data_subjects returns an array, but we need indices\n",
    "    \n",
    "    print(\"Obtained data subject indices\")\n",
    "    \n",
    "    # Calculate everything needed for RDP\n",
    "    sigmas = np.reshape(np.ones_like(values) * sigma, -1)\n",
    "    coeffs = np.ones_like(values).reshape(-1)\n",
    "    l2_norms, l2_norm_bounds = calculate_bounds_for_mechanism(value_array=values, min_val_array=min_vals, max_val_array=max_vals)\n",
    "    \n",
    "    if is_linear:\n",
    "        lipschitz_bounds = np.ones_like(values).reshape(-1)\n",
    "    else:\n",
    "        raise Exception(\"gamma_tensor.lipschitz_bound property would be used here\")\n",
    "    \n",
    "    input_entities = data_subjects.entities_indexed[0].reshape(-1)\n",
    "    \n",
    "    print(\"Obtained all parameters for RDP\")\n",
    "    \n",
    "    if ledger is None:\n",
    "        ledger = DataSubjectLedger()\n",
    "    print(\"Initialized ledger!\")\n",
    "    \n",
    "    ledger.reset()\n",
    "    # Get the Ledger started\n",
    "    ledger.batch_append(\n",
    "        sigmas=sigmas,\n",
    "        l2_norms=l2_norms,\n",
    "        l2_norm_bounds=l2_norm_bounds,\n",
    "        Ls=lipschitz_bounds,\n",
    "        coeffs=coeffs,\n",
    "        entity_ids=input_entities\n",
    "    )\n",
    "    \n",
    "    print(\"Concluded batch append\")\n",
    "    \n",
    "    # Query budget spend of all unique entities\n",
    "    mask = ledger.get_overbudgeted_entities(user_budget=data_scientist_budget, unique_entity_ids_query=input_entities) #unique_data_subject_indices)\n",
    "    \n",
    "    print(\"Obtained overbudgeted entity mask\")\n",
    "    \n",
    "    # TODO: Send this LedgerUpdate to the actual database\n",
    "    # update = ledger.write_to_db()\n",
    "    \n",
    "    print(\"Written to DB!\")\n",
    "    \n",
    "    # Filter results\n",
    "    filtered_inputs = values * (mask^1) #+ gauss(0, sigma)  # Double check that noise has mean of 0\n",
    "    return output_func(filtered_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44dc1fd-f7fc-4a57-8ccf-a76d2fa98e9e",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb1cc29a-c074-4f03-a16b-73b949f64795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft as sy\n",
    "from syft.core.adp.entity import DataSubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16717113-0bac-437d-ab2e-39ad4cd0504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10**3\n",
    "unique_data_subject_count = int(size/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed248cd8-a907-4169-91e2-ecf2b66bb5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.59 ms, sys: 0 ns, total: 1.59 ms\n",
      "Wall time: 1.09 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_subjects = np.random.choice([DataSubject(str(i)) for i in range(unique_data_subject_count)], size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abc21818-3c08-41b3-95de-ac09967cc844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_subjects)/size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c49be1fa-ae85-4077-92bc-85e5cb7e0ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sy.Tensor(np.ones(size, dtype=np.int32)).private(min_val=0, max_val=2, entities=data_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "330ed0bd-4e83-48fb-91db-e08c4a70205c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Data: GammaTensor(value=1000, data_subjects=<syft.core.adp.entity_list.DataSubjectList object at 0x7f419a562f40>, min_val=0, max_val=2000, is_linear=True, func=<function no_op at 0x7f41a8676280>, id='1750640486', state={'0': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)}) ,type: <class 'syft.core.tensor.autodp.gamma_tensor.GammaTensor'> must be list or nd.array ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [63]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/PySyft/packages/syft/src/syft/core/tensor/passthrough.py:542\u001b[0m, in \u001b[0;36mPassthroughTensor.sum\u001b[0;34m(self, axis)\u001b[0m\n\u001b[1;32m    540\u001b[0m     tensor\u001b[38;5;241m.\u001b[39mchild \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensor\n\u001b[0;32m--> 542\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/PySyft/packages/syft/src/syft/core/tensor/tensor.py:367\u001b[0m, in \u001b[0;36mTensor.__init__\u001b[0;34m(self, child, public_shape, public_dtype)\u001b[0m\n\u001b[1;32m    361\u001b[0m     child \u001b[38;5;241m=\u001b[39m to32bit(child\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, PassthroughTensor) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    364\u001b[0m     child, np\u001b[38;5;241m.\u001b[39mndarray\n\u001b[1;32m    365\u001b[0m ):\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchild\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ,type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(child)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be list or nd.array \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    369\u001b[0m     )\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child, (np\u001b[38;5;241m.\u001b[39mndarray, PassthroughTensor)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(child, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mint32, np\u001b[38;5;241m.\u001b[39mbool_]\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(child, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    374\u001b[0m ):\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou tried to pass an a tensor of type:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    377\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mtype\u001b[39m(child))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe adding support for more types very soon!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m     )\n",
      "\u001b[0;31mException\u001b[0m: Data: GammaTensor(value=1000, data_subjects=<syft.core.adp.entity_list.DataSubjectList object at 0x7f419a562f40>, min_val=0, max_val=2000, is_linear=True, func=<function no_op at 0x7f41a8676280>, id='1750640486', state={'0': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)}) ,type: <class 'syft.core.tensor.autodp.gamma_tensor.GammaTensor'> must be list or nd.array "
     ]
    }
   ],
   "source": [
    "# TO FIX- calling Sum on Sy.Tensor when child is NDEPT/GammaTEnsor\n",
    "data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c12ae347-eb70-4244-a940-582cb3f0f8b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syft.core.tensor.autodp.ndim_entity_phi.PhiTensor"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data.child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ced18dd-a63c-4be9-a0cd-2148df9a7fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_to_publish = data.child.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b653806-5a50-4082-beb3-055f72b737bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "syft.core.tensor.autodp.gamma_tensor.GammaTensor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(result_to_publish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3bb70242-64c2-457b-89dc-a4116c152e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting vectorized publish\n",
      "Obtained data subject indices\n",
      "Obtained all parameters for RDP\n",
      "Initialized ledger!\n",
      "Concluded batch append\n",
      "672.0822908587811\n",
      "Obtained overbudgeted entity mask\n",
      "Written to DB!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/PycharmProjects/PySyft/.tox/syft.jupyter/lib/python3.8/site-packages/scipy/optimize/_optimize.py:2782: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  w = xb - ((xb - xc) * tmp2 - (xb - xa) * tmp1) / denom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "554"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_publish(\n",
    "    min_vals=result_to_publish.min_val,\n",
    "    max_vals=result_to_publish.max_val,\n",
    "    values=result_to_publish.state['0'],\n",
    "    data_subjects=result_to_publish.data_subjects,\n",
    "    sigma=size/100,\n",
    "    output_func=np.sum\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ac0e2c5-d776-45c2-8f5b-08867e32076a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_to_publish.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a8f9c9-b40b-40a0-b66a-fa29541c50ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
