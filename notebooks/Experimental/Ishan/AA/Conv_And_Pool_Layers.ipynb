{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10e63b02",
   "metadata": {},
   "source": [
    "### Conv Numpy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0879767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/e/anaconda3/envs/Hagrid/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "\n",
    "# relative\n",
    "from syft.core.tensor.nn import activations\n",
    "from syft.core.tensor.autodp.gamma_tensor import GammaTensor\n",
    "from syft.core.tensor.autodp.phi_tensor import PhiTensor\n",
    "from syft.core.tensor.nn.initializations import XavierInitialization\n",
    "from syft.core.tensor.nn.utils import dp_zeros\n",
    "from syft.core.tensor.nn.layers.base import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ffa0210",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + 2 * padding - field_height) % stride == 0\n",
    "    assert (W + 2 * padding - field_height) % stride == 0\n",
    "    out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
    "    out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fcd046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft import DataSubjectList\n",
    "from syft import lazyrepeatarray\n",
    "\n",
    "def dp_pad(input: Union[PhiTensor, GammaTensor], width, padding_mode=\"constant\", **kwargs):\n",
    "    \n",
    "    data = input.child    \n",
    "    output_data = np.pad(data, width, mode=padding_mode, **kwargs)\n",
    "    min_v = lazyrepeatarray(data=min(input.min_vals.data.min(), output_data.min()), shape=output_data.shape)\n",
    "    max_v = lazyrepeatarray(data=min(input.max_vals.data.max(), output_data.max()), shape=output_data.shape)\n",
    "    \n",
    "    output_data_subjects=DataSubjectList(\n",
    "        one_hot_lookup=input.data_subjects.one_hot_lookup,\n",
    "        data_subjects_indexed=np.pad(input.data_subjects.data_subjects_indexed, width, mode=padding_mode, **kwargs)\n",
    "    )\n",
    "\n",
    "    if isinstance(input, PhiTensor):\n",
    "        return PhiTensor(\n",
    "            child=output_data,\n",
    "            data_subjects=output_data_subjects,\n",
    "            min_vals=min_v,\n",
    "            max_vals=max_v\n",
    "        )\n",
    "    elif isinstance(input, GammaTensor):\n",
    "        return GammaTensor(\n",
    "            child=output_data,\n",
    "            data_subjects=output_data_subjects,\n",
    "            min_vals=min_v,\n",
    "            max_vals=max_v,\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Padding is not implemented for Input Type: {type(input)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93819291",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "C_in = 3\n",
    "H_in = 50\n",
    "W_in = 50\n",
    "\n",
    "\n",
    "input_shape = (N, C_in, H_in, W_in)\n",
    "x = PhiTensor(child=np.random.randint(low=0, high=255, size=input_shape),\n",
    "              data_subjects=np.zeros(input_shape),\n",
    "              min_vals=0,\n",
    "              max_vals=255\n",
    "             )\n",
    "\n",
    "# TEST Padding\n",
    "# dp_pad(x, ((0, 0), (0, 0), (p, p), (p, p)), padding_mode=\"constant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9ba6fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col_indices_dp(x, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    x_padded = dp_pad(x, ((0, 0), (0, 0), (p, p), (p, p)), padding_mode='constant')\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose((1, 2, 0)).reshape((field_height * field_width * C, -1))\n",
    "    return cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77c03bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy implementation\n",
      "(3072, 5290)\n",
      "dp implementation\n",
      "(3072, 529, 10) (3072, 5290) (529, 3072, 10)\n",
      "(3072, 5290)\n"
     ]
    }
   ],
   "source": [
    "print(\"numpy implementation\")\n",
    "print(im2col_indices(x.child, 32, 32, padding=2, stride=1).shape)\n",
    "print(\"dp implementation\")\n",
    "print(im2col_indices_dp(x, 32, 32, padding=2, stride=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93417d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_forward(X, W, b, stride=1, padding=1):\n",
    "    cache = W, b, stride, padding\n",
    "    n_filters, d_filter, h_filter, w_filter = W.shape\n",
    "    n_x, d_x, h_x, w_x = X.shape\n",
    "    h_out = (h_x - h_filter + 2 * padding) / stride + 1\n",
    "    w_out = (w_x - w_filter + 2 * padding) / stride + 1\n",
    "\n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        raise Exception('Invalid output dimension!')\n",
    "\n",
    "    h_out, w_out = int(h_out), int(w_out)\n",
    "    print(h_out, w_out)\n",
    "\n",
    "    X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    W_col = W.reshape(n_filters, -1)\n",
    "    \n",
    "    print(W_col.shape)\n",
    "    print(X_col.shape)\n",
    "    #return W_col,X_col\n",
    "    \n",
    "\n",
    "    out = (W_col @ X_col).T + b\n",
    "    out = out.reshape(n_filters, h_out, w_out, n_x)\n",
    "    out = out.transpose(3, 0, 1, 2)\n",
    "\n",
    "    cache = (X, W, b, stride, padding, X_col)\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df381dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.core.tensor.nn.utils import dp_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2b723de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 52\n",
      "(32, 27)\n",
      "(27, 27040)\n"
     ]
    }
   ],
   "source": [
    "from syft.core.tensor.nn.initializations import XavierInitialization\n",
    "\n",
    "nb_filter = 32\n",
    "filter_height = filter_width = 3\n",
    "nb_batch, pre_nb_filter, pre_height, pre_width = x.shape\n",
    "W = XavierInitialization()((nb_filter, pre_nb_filter, filter_height, filter_width))\n",
    "W = PhiTensor(\n",
    "    child=W, \n",
    "    data_subjects=DataSubjectList(one_hot_lookup=x.data_subjects.one_hot_lookup, data_subjects_indexed=np.zeros_like(W)),\n",
    "    min_vals=W.min(),\n",
    "    max_vals=W.max()\n",
    ")\n",
    "\n",
    "b = dp_zeros(shape=(nb_filter, ), data_subjects=x.data_subjects)\n",
    "\n",
    "out, c = conv_forward(x.child, W.child, b.child, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d546fd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 32, 52, 52)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56c2cfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_forward_dp(X, W, b, stride=1, padding=1):\n",
    "    cache = W, b, stride, padding\n",
    "    n_filters, d_filter, h_filter, w_filter = W.shape\n",
    "    n_x, d_x, h_x, w_x = X.shape\n",
    "    h_out = (h_x - h_filter + 2 * padding) / stride + 1\n",
    "    w_out = (w_x - w_filter + 2 * padding) / stride + 1\n",
    "\n",
    "    if not h_out.is_integer() or not w_out.is_integer():\n",
    "        raise Exception('Invalid output dimension!')\n",
    "\n",
    "    h_out, w_out = int(h_out), int(w_out)\n",
    "    \n",
    "    print(h_out, w_out)\n",
    "\n",
    "    X_col = im2col_indices_dp(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    print(n_filters)\n",
    "    W_col = W.reshape((n_filters, -1))\n",
    "    \n",
    "    #return W_col, X_col\n",
    "    \n",
    "    \n",
    "    out = X_col.transpose() @ W_col.T + b\n",
    "    out = out.reshape((n_filters, h_out, w_out, n_x))\n",
    "    out = out.transpose((3, 0, 1, 2))\n",
    "\n",
    "    cache = (X, W, b, stride, padding, X_col)\n",
    "\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85d79355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 52\n",
      "(27, 2704, 10) (27, 27040) (2704, 27, 10)\n",
      "32\n",
      "(27, 27040)\n",
      "27 (27040,)\n",
      "(27040, 32) (32, 52, 52, 10) (27, 27040)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 730080 into shape (27,32,52,52,10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35755/1063993470.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout_dp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_forward_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_35755/1072092139.py\u001b[0m in \u001b[0;36mconv_forward_dp\u001b[0;34m(X, W, b, stride, padding)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mW_col\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/PySyft/packages/syft/src/syft/core/tensor/autodp/phi_tensor.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(self, *shape)\u001b[0m\n\u001b[1;32m   1216\u001b[0m             output_ds = DataSubjectList(\n\u001b[1;32m   1217\u001b[0m                 \u001b[0mone_hot_lookup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_subjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_lookup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1218\u001b[0;31m                 data_subjects_indexed=self.data_subjects.data_subjects_indexed.reshape(\n\u001b[0m\u001b[1;32m   1219\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_subjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m                 )\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 730080 into shape (27,32,52,52,10)"
     ]
    }
   ],
   "source": [
    "out_dp, cache_dp = conv_forward_dp(x, W.child, b.child, padding=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d428c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1,\n",
    "                   stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "\n",
    "def conv_backward(dout, cache):\n",
    "    X, W, b, stride, padding, X_col = cache\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "    db = np.sum(dout, axis=(0, 2, 3))\n",
    "    db = db.reshape(n_filter, -1)\n",
    "\n",
    "    dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "    dW = dout_reshaped @ X_col.T\n",
    "    dW = dW.reshape(W.shape)\n",
    "\n",
    "    W_reshape = W.reshape(n_filter, -1)\n",
    "    dX_col = W_reshape.T @ dout_reshaped\n",
    "    print(\"dX_col\", dX_col.shape)\n",
    "    dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "    return dX, dW, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df891a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dX, dW, db = conv_backward(out, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dX.shape, dW.shape, db.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_add_at(a, indices, b):\n",
    "    data_a = a.child\n",
    "    data_b = b.child\n",
    "    np.add.at(data_a,indices, data_b)\n",
    "    return PhiTensor(\n",
    "        child=data_a,\n",
    "        data_subjects=a.data_subjects,\n",
    "        min_vals=data_a.min(),\n",
    "        max_vals=data_a.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e231b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im_indices_dp(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = dp_zeros((N, C, H_padded, W_padded), data_subjects=cols.data_subjects)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape((C * field_height * field_width, -1, N))\n",
    "    cols_reshaped = cols_reshaped.transpose((2, 0, 1))\n",
    "    x_padded = dp_add_at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7751999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward_dp(dout, cache):\n",
    "    X, W, b, stride, padding, X_col = cache\n",
    "    n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "    db = dout.sum(axis=(0, 2, 3))\n",
    "    db = db.reshape((n_filter, -1))\n",
    "\n",
    "    dout_reshaped = dout.transpose((1, 2, 3, 0)).reshape((n_filter, -1))\n",
    "    \n",
    "    dW = dout_reshaped @ X_col.T\n",
    "    dW = dW.reshape(W.shape)\n",
    "\n",
    "    W_reshape = W.reshape(n_filter, -1)\n",
    "    dX_col = dout_reshaped.transpose() @ W_reshape\n",
    "    dX = col2im_indices_dp(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5391ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_dp, dW_dp, db_dp = conv_backward_dp(out_dp, cache_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26767e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dX_dp.shape, dW_dp.shape, db_dp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6647836b",
   "metadata": {},
   "source": [
    "### Conv PhiTensor Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec11af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "from typing import Dict\n",
    "from typing import Optional\n",
    "from typing import Tuple\n",
    "from typing import Union\n",
    "\n",
    "# third party\n",
    "import numpy as np\n",
    "\n",
    "# relative\n",
    "from syft.core.tensor.nn import activations\n",
    "from syft.core.tensor.autodp.gamma_tensor import GammaTensor\n",
    "from syft.core.tensor.autodp.phi_tensor import PhiTensor\n",
    "from syft.core.tensor.nn.initializations import XavierInitialization\n",
    "from syft.core.tensor.nn.utils import dp_zeros\n",
    "from syft.core.tensor.nn.layers.base import Layer\n",
    "\n",
    "\n",
    "class Convolution(Layer):\n",
    "    \"\"\"\n",
    "    If this is the first layer in a model, provide the keyword argument `input_shape`\n",
    "    (tuple of integers, does NOT include the sample axis, N.),\n",
    "    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n",
    "    \"\"\"\n",
    "    __name__ = \"ConvPointer\"\n",
    "    __module__ = \"syft.core.tensor.nn.layers.convolution\"\n",
    "    __attr_allowlist__ = [\n",
    "        \"nb_filter\",\n",
    "        \"filter_size\",\n",
    "        \"input_shape\",\n",
    "        \"stride\",\n",
    "        \"W\",\n",
    "        \"b\",\n",
    "        \"dW\",\n",
    "        \"db\",\n",
    "        \"out_shape\",\n",
    "        \"last_output\",\n",
    "        \"last_input\"\n",
    "    ]\n",
    "\n",
    "    def __init__(self, nb_filter, filter_size, input_shape: Optional[Tuple]=None, stride: int=1, padding: int=0, activation: Optional[activations.Activation]=None):\n",
    "        self.nb_filter = nb_filter\n",
    "        self.filter_size = filter_size\n",
    "        self.input_shape = input_shape\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.W, self.dW = None, None\n",
    "        self.b, self.db = None, None\n",
    "        self.out_shape = None\n",
    "        self.last_output = None\n",
    "        self.last_input = None\n",
    "\n",
    "        self.init = XavierInitialization()\n",
    "        self.activation = activations.get(activation)\n",
    "\n",
    "    def connect_to(self, prev_layer: Optional[Layer]=None):\n",
    "        if prev_layer is None:\n",
    "            assert self.input_shape is not None\n",
    "            input_shape = self.input_shape\n",
    "        else:\n",
    "            input_shape = prev_layer.out_shape\n",
    "\n",
    "        # input_shape: (batch size, num input feature maps, image height, image width)\n",
    "        assert len(input_shape) == 4\n",
    "\n",
    "        nb_batch, pre_nb_filter, pre_height, pre_width = input_shape\n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        height = (pre_height - filter_height + 2 * self.padding) // self.stride + 1\n",
    "        width = (pre_width - filter_width +  2 * self.padding) // self.stride + 1\n",
    "\n",
    "        # output shape\n",
    "        self.out_shape = (nb_batch, self.nb_filter, height, width)\n",
    "\n",
    "        # filters\n",
    "        self.W = self.init((self.nb_filter, pre_nb_filter, filter_height, filter_width))\n",
    "        self.b = np.zeros((self.nb_filter,))\n",
    "        \n",
    "        \n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        self.last_input = input\n",
    "        \n",
    "        n_filters, d_filter, h_filter, w_filter = self.W.shape\n",
    "        n_x, d_x, h_x, w_x = input.shape\n",
    "        \n",
    "        _, _, h_out, w_out, = self.out_shape\n",
    "\n",
    "        self.X_col = im2col_indices_dp(input, h_filter, w_filter, padding=self.padding, stride=self.stride)\n",
    "\n",
    "        W_col = self.W.reshape((n_filters, -1))\n",
    "\n",
    "        out = self.X_col.transpose() @ W_col.T + self.b\n",
    "        out = out.reshape((n_filters, h_out, w_out, n_x))\n",
    "        out = out.transpose((3, 0, 1, 2))\n",
    "        \n",
    "        self.last_output = self.activation.forward(out) if self.activation is not None else out\n",
    "        return out\n",
    "    \n",
    "    def backward(self, pre_grads, *args, **kwargs):\n",
    "        n_filter, d_filter, h_filter, w_filter = self.W.shape\n",
    "        \n",
    "        pre_grads = (pre_grads * self.activation.derivative(pre_grads)) if self.activation is not None else pre_grad\n",
    "        db = pre_grads.sum(axis=(0, 2, 3))\n",
    "        self.db = db.reshape((n_filter, -1))\n",
    "\n",
    "        pre_grads_reshaped = pre_grads.transpose((1, 2, 3, 0)).reshape((n_filter, -1))\n",
    "\n",
    "        dW = pre_grads_reshaped @ self.X_col.T\n",
    "        self.dW = dW.reshape(W.shape)\n",
    "\n",
    "        W_reshape = self.W.reshape(n_filter, -1)\n",
    "        dX_col = pre_grads_reshaped.transpose() @ W_reshape\n",
    "        dX = col2im_indices_dp(dX_col, self.input_shape, h_filter, w_filter, padding=self.padding, stride=self.stride)\n",
    "        return dX\n",
    "\n",
    "\n",
    "#     def forward(self, input: Union[PhiTensor, GammaTensor], *args: Tuple, **kwargs: Dict):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "#         # TODO: This could fail if the DP Tensor has < 4 dimensions\n",
    "\n",
    "#         # shape\n",
    "#         nb_batch, input_depth, old_img_h, old_img_w = input.shape\n",
    "#         if isinstance(self.filter_size, tuple):\n",
    "#             filter_height, filter_width = self.filter_size\n",
    "#         elif isinstance(self.filter_size, int):\n",
    "#             filter_height = filter_width = self.filter_size\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#         new_img_h, new_img_w = self.out_shape[2:]\n",
    "\n",
    "#         # init\n",
    "#         outputs = dp_zeros((nb_batch, self.nb_filter, new_img_h, new_img_w), input.data_subjects)\n",
    "\n",
    "#         # convolution operation\n",
    "#         for x in np.arange(nb_batch):\n",
    "#             for y in np.arange(self.nb_filter):\n",
    "#                 for h in np.arange(new_img_h):\n",
    "#                     for w in np.arange(new_img_w):\n",
    "#                         h_shift, w_shift = h * self.stride, w * self.stride\n",
    "#                         # patch: (input_depth, filter_h, filter_w)\n",
    "#                         patch = input[x, :, h_shift: h_shift + filter_height, w_shift: w_shift + filter_width]\n",
    "#                         outputs[x, y, h, w] = (patch * self.W[y]).sum() + self.b[y]\n",
    "\n",
    "#         # nonlinear activation\n",
    "#         # self.last_output: (nb_batch, output_depth, image height, image width)\n",
    "\n",
    "#         # TODO: Min/max vals are direct function of private data- fix this when we have time\n",
    "\n",
    "#         self.last_output = self.activation.forward(outputs) if self.activation is not None else outputs\n",
    "\n",
    "#         return self.last_output\n",
    "\n",
    "#     def backward(self, pre_grad, *args, **kwargs):\n",
    "\n",
    "#         # shape\n",
    "#         assert pre_grad.shape == self.last_output.shape\n",
    "#         nb_batch, input_depth, old_img_h, old_img_w = self.last_input.shape\n",
    "#         new_img_h, new_img_w = self.out_shape[2:]\n",
    "\n",
    "#         if isinstance(self.filter_size, tuple):\n",
    "#             filter_height, filter_width = self.filter_size\n",
    "#         elif isinstance(self.filter_size, int):\n",
    "#             filter_height = filter_width = self.filter_size\n",
    "#         else:\n",
    "#             raise NotImplementedError\n",
    "\n",
    "#         #         filter_h, filter_w = self.filter_size\n",
    "#         old_img_h, old_img_w = self.last_input.shape[-2:]\n",
    "\n",
    "#         # gradients\n",
    "#         # TODO: Decide if dW and db needs to be DP Tensors or can they live as numpy arrays\n",
    "#         self.dW = np.zeros((self.W.shape))\n",
    "#         self.db = np.zeros((self.b.shape))\n",
    "#         delta = (pre_grad * self.activation.derivative()) if self.activation is not None else pre_grad\n",
    "\n",
    "#         # dW\n",
    "#         for r in np.arange(self.nb_filter):\n",
    "#             for t in np.arange(input_depth):\n",
    "#                 for h in np.arange(filter_height):\n",
    "#                     for w in np.arange(filter_width):\n",
    "#                         input_window = self.last_input[:, t,\n",
    "#                                        h:old_img_h - filter_height + h + 1:self.stride,\n",
    "#                                        w:old_img_w - filter_width + w + 1:self.stride]\n",
    "#                         delta_window = delta[:, r]\n",
    "#                         self.dW[r, t, h, w] = ((input_window * delta_window).sum() * (1/nb_batch)).child\n",
    "#         # db\n",
    "#         for r in np.arange(self.nb_filter):\n",
    "#             self.db[r] = (delta[:, r].sum() * (1/nb_batch)).child\n",
    "\n",
    "#         # dX\n",
    "#         if not self.first_layer:\n",
    "#             layer_grads = self.last_input.zeros_like()\n",
    "#             for b in np.arange(nb_batch):\n",
    "#                 for r in np.arange(self.nb_filter):\n",
    "#                     for t in np.arange(input_depth):\n",
    "#                         for h in np.arange(new_img_h):\n",
    "#                             for w in np.arange(new_img_w):\n",
    "#                                 h_shift, w_shift = h * self.stride, w * self.stride\n",
    "#                                 temp = layer_grads[b, t, h_shift:h_shift + filter_height, w_shift:w_shift + filter_width]\n",
    "#                                 layer_grads[b, t, h_shift:h_shift + filter_height, w_shift:w_shift + filter_width] = temp+ (delta[b, r, h, w] * self.W[r, t])\n",
    "\n",
    "#             return layer_grads\n",
    "\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        return self.dW, self.db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6018966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.core.tensor.nn import Model, leaky_ReLU, Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d25f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30358bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.add(Convolution(32, (3, 3), input_shape=(10, 3, 50, 50), padding=2, activation=leaky_ReLU()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32673e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94d6a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = net.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8595121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebcb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0fb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conv.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5318670",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.backward(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26f1561",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.b.shape, conv.W.shape, conv.db.shape, conv.dW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d31bf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft import PhiTensor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "C_in = 3\n",
    "H_in = 50\n",
    "W_in = 50\n",
    "\n",
    "\n",
    "input_shape = (N, C_in, H_in, W_in)\n",
    "x = PhiTensor(child=np.random.randint(low=0, high=255, size=input_shape),\n",
    "              data_subjects=np.zeros(input_shape),\n",
    "              min_vals=0,\n",
    "              max_vals=255\n",
    "             )\n",
    "\n",
    "# TEST Padding\n",
    "# dp_pad(x, ((0, 0), (0, 0), (p, p), (p, p)), padding_mode=\"constant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c72e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft.core.tensor.nn import Convolution, Model, leaky_ReLU, MaxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a03bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model()\n",
    "net.add(Convolution(32, (3, 3), input_shape=(10, 3, 50, 50), padding=2, activation=leaky_ReLU()))\n",
    "net.add(MaxPool(pool_size=2, stride=2))\n",
    "net.compile()\n",
    "conv = net.layers[0]\n",
    "out = conv.forward(x)\n",
    "print(\"Forward pass\", out.shape)\n",
    "conv.backward(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_col, max_idx, h_out, w_out, n, d = net.layers[1].forward(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7694cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_col.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d20552",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_col.data_subjects.data_subjects_indexed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf936df",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = X_col.child[max_idx, range(max_idx.size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314bedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.reshape((h_out, w_out, n, d)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acd2c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_grad = outputs.reshape((h_out, w_out, n, d)).transpose((2, 3, 0, 1))\n",
    "pre_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd497c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_grad_dp = PhiTensor(pre_grad, data_subjects=out.data_subjects, min_vals=pre_grad.min(), max_vals=pre_grad.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8658e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.layers[1].backward(pre_grad_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a452aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
