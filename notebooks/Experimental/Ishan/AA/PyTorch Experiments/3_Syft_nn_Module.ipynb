{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90222193-5eb7-4036-9a9a-b22847f94608",
   "metadata": {},
   "source": [
    "To Do:\n",
    "\n",
    "I need to create a Syft equivalent of nn.Module that lets you define a model if you initilize it with all the layers it will use, and define its forward pass using the activation functions.\n",
    "\n",
    "SyMPC seems to make a Module equivalent but for each type of layer- i.e. Conv2d is implemented as a Module, and (I think) their MPCTensor tracks all its gradients.\n",
    "\n",
    "Perhaps this will come down to modifying the loss function to take the DP Tensor as input, but I think if we use `publish` in order to do the conversion from DP Tensor -> array/tensor, we wouldn't have to do this modification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d485eeb8-ed85-4fc9-b3da-53b6d94bd5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda3/envs/syft/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import syft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fc76e4b-e3d7-47de-b055-e89109b0fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965bcdac-edf2-409d-965d-6414f64e7376",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adfe944c-f7f2-45cc-a5e4-966178ba909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft.core.tensor.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59af5cb6-8b32-4ee1-af31-bc1ae5d04a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from typing import Sequence\n",
    "from typing import Tuple, Optional\n",
    "from syft import PhiTensor\n",
    "from syft.core.tensor.nn.conv_layers import child_to_torch\n",
    "from torch import Tensor\n",
    "from syft.core.adp.data_subject_list import DataSubjectList as DSL\n",
    "\n",
    "class Conv2d(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels:int, out_channels:int, kernel_size: Union[int, Sequence[int]], padding:int):\n",
    "        super(Conv2d, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.func = torch.nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_size, padding=self.padding)\n",
    "        \n",
    "    def forward(self, image: PhiTensor):\n",
    "        # TODO: This conversion is only required the first time and not after that.\n",
    "        torch_tensor = child_to_torch(image)\n",
    "        data = self.func(torch_tensor)\n",
    "        data_array = data.detach().numpy()\n",
    "\n",
    "        return PhiTensor(\n",
    "            child=data_array,\n",
    "            data_subjects=DSL(\n",
    "                one_hot_lookup=image.data_subjects.one_hot_lookup,\n",
    "                data_subjects_indexed=np.zeros_like(data_array),\n",
    "            ),\n",
    "            min_vals=data_array.min(),\n",
    "            max_vals=data_array.max(),\n",
    "        )\n",
    "#             return nn.Conv2d(\n",
    "#                 image=x, \n",
    "#                 in_channels=self.in_channels, \n",
    "#                 out_channels=self.out_channels, \n",
    "#                 kernel_size=self.kernel_size, \n",
    "#                 padding=self.padding\n",
    "#             )\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.func.parameters()\n",
    "    \n",
    "    \n",
    "class BatchNorm2d(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, eps=1e-05, momentum=0.1, affine=True):\n",
    "        super(BatchNorm2d, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.func = torch.nn.BatchNorm2d(num_features=self.num_features, \n",
    "            eps=self.eps, \n",
    "            momentum=self.momentum, \n",
    "            affine=self.affine\n",
    "        )\n",
    "        \n",
    "    def forward(self, image: PhiTensor):\n",
    "#         return nn.BatchNorm2d(\n",
    "#             image=x, \n",
    "#             num_features=self.num_features, \n",
    "#             eps=self.eps, \n",
    "#             momentum=self.momentum, \n",
    "#             affine=self.affine\n",
    "#         )\n",
    "        data = self.func(Tensor(image.child.decode()))\n",
    "        minv, maxv = image.min_vals.data, image.max_vals.data\n",
    "        public_avg = 0.5 * (maxv + minv)\n",
    "        # Assumption: max and min are equally distant from public_avg -> var = 0.25 * (max - min)**2\n",
    "        public_var = 0.25 * (maxv - minv) ** 2\n",
    "\n",
    "        return PhiTensor(\n",
    "            child=data.detach().numpy(),\n",
    "            data_subjects=image.data_subjects,\n",
    "            min_vals=(minv - public_avg) / np.sqrt(public_var + self.eps),\n",
    "            max_vals=(maxv - public_avg) / np.sqrt(public_var + self.eps),\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.func.parameters()\n",
    "\n",
    "class MaxPool2d(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, kernel_size: Union[int, Tuple[int, int]],\n",
    "                 stride: Optional[Union[int, Tuple[int, int]]] = None,\n",
    "                 padding: Union[int, Tuple[int, int]] = 0,\n",
    "                 dilation: int = 1,\n",
    "                 return_indices: bool = False,\n",
    "                 ceil_mode: bool = False,\n",
    "                ):\n",
    "        super(MaxPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.return_indices = return_indices\n",
    "        self.ceil_mode = ceil_mode\n",
    "        self.func = torch.nn.MaxPool2d(kernel_size=self.kernel_size, \n",
    "            stride=self.stride, \n",
    "            padding=self.padding, \n",
    "            return_indices=self.return_indices, \n",
    "            ceil_mode=self.ceil_mode)\n",
    "        \n",
    "    def forward(self, image: PhiTensor):\n",
    "        data = self.func(Tensor(image.child.decode())).detach().numpy()\n",
    "        return PhiTensor(\n",
    "            child=data,\n",
    "            data_subjects=DSL(\n",
    "                one_hot_lookup=image.data_subjects.one_hot_lookup,\n",
    "                data_subjects_indexed=np.zeros_like(data),\n",
    "            ),\n",
    "            min_vals=np.ones_like(data) * image.min_vals.data,\n",
    "            max_vals=np.ones_like(data) * image.max_vals.data,\n",
    "        )\n",
    "#         return nn.MaxPool2d(\n",
    "#             image=x, \n",
    "#             kernel_size=self.kernel_size, \n",
    "#             stride=self.stride, \n",
    "#             padding=self.padding, \n",
    "#             return_indices=self.return_indices, \n",
    "#             ceil_mode=self.ceil_mode\n",
    "#         )\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.func.parameters()\n",
    "    \n",
    "    \n",
    "class AvgPool2d(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        kernel_size: Union[int, Tuple[int, int]], \n",
    "        stride: Optional[Union[int, Tuple[int, int]]] = None, \n",
    "        padding: Union[int, Tuple[int, int]] = 0\n",
    "    ):\n",
    "        super(AvgPool2d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.func = torch.nn.AvgPool2d(kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "        \n",
    "    def forward(self, image: PhiTensor):\n",
    "        data = self.func(Tensor(image.child.decode())).detach().numpy()\n",
    "        return PhiTensor(\n",
    "            child=data,\n",
    "            data_subjects=DSL(\n",
    "                one_hot_lookup=image.data_subjects.one_hot_lookup,\n",
    "                data_subjects_indexed=np.zeros_like(data),\n",
    "            ),\n",
    "            min_vals=np.ones_like(data) * image.min_vals.data,\n",
    "            max_vals=np.ones_like(data) * image.max_vals.data,\n",
    "        )\n",
    "        \n",
    "#         return nn.AvgPool2d(image=x, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.func.parameters()\n",
    "\n",
    "    \n",
    "class Linear(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.func = torch.nn.Linear(in_features=self.in_features, out_features=self.out_features, bias=self.bias)\n",
    "    \n",
    "    def forward(self, image: PhiTensor):\n",
    "        image_asarray = image.child.decode()\n",
    "        data = self.func(Tensor(image_asarray)).detach().numpy()\n",
    "        minv = (\n",
    "            self.func(Tensor(np.ones_like(image_asarray) * image.min_vals.data))\n",
    "            .detach()\n",
    "            .numpy()\n",
    "        )\n",
    "        maxv = (\n",
    "            self.func(Tensor(np.ones_like(image_asarray) * image.max_vals.data))\n",
    "            .detach()\n",
    "            .numpy()\n",
    "        )\n",
    "\n",
    "        return PhiTensor(\n",
    "            child=data,\n",
    "            data_subjects=DSL(\n",
    "                one_hot_lookup=image.data_subjects.one_hot_lookup,\n",
    "                data_subjects_indexed=np.zeros_like(data),\n",
    "            ),\n",
    "            min_vals=minv,\n",
    "            max_vals=maxv,\n",
    "        )\n",
    "        \n",
    "#         return nn.Linear(image=x, in_features=self.in_features, out_features=self.out_features, bias=self.bias)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.func.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e854d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(torch.nn.Module):\n",
    "    def __init__(self, weight: Optional[Tensor] = None, size_average=None, ignore_index: int = -100,\n",
    "                 reduce=None, reduction: str = 'mean'):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.size_average = size_average\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduce = reduce\n",
    "        self.reduction = reduction\n",
    "        self.func = torch.nn.CrossEntropyLoss(\n",
    "            self.weight, self.size_average, self.ignore_index,\n",
    "            self.reduce, self.reduction\n",
    "        )\n",
    "        \n",
    "    def forward(self, input: PhiTensor, target: PhiTensor):\n",
    "        input_asarray = input.child.decode()\n",
    "        target_asarray = target.child.decode()\n",
    "        \n",
    "        data = self.func(Tensor(input_asarray), Tensor(target_asarray).long()).detach().numpy()\n",
    "        \n",
    "        minv = (\n",
    "            self.func(\n",
    "                Tensor(np.ones_like(input_asarray) * input.min_vals.data),\n",
    "                Tensor(np.ones_like(target_asarray) * target.min_vals.data).long(),\n",
    "            )\n",
    "            .detach()\n",
    "            .numpy()\n",
    "        )\n",
    "        maxv = (\n",
    "            self.func(\n",
    "                Tensor(np.ones_like(input_asarray) * input.max_vals.data),\n",
    "                Tensor(np.ones_like(target_asarray) * target.max_vals.data).long(),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return PhiTensor(\n",
    "            child=data,\n",
    "            data_subjects=DSL(\n",
    "                one_hot_lookup=input.data_subjects.one_hot_lookup,\n",
    "                data_subjects_indexed=np.zeros_like(data),\n",
    "            ),\n",
    "            min_vals=minv,\n",
    "            max_vals=maxv,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f779d70b-f285-4f1e-98a6-96f2c52fe4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=2)\n",
    "        self.conv2 = Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=2)\n",
    "        self.conv3 = Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=2)\n",
    "        self.conv4 = Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=2)\n",
    "        self.conv5 = Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=2)\n",
    "        self.bn1 = BatchNorm2d(32)\n",
    "        self.bn2 = BatchNorm2d(64)\n",
    "        self.bn3 = BatchNorm2d(128)\n",
    "        self.bn4 = BatchNorm2d(256)\n",
    "        self.bn5 = BatchNorm2d(512)\n",
    "        self.pool = MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.avg = AvgPool2d(3)\n",
    "        self.fc = Linear(512 * 1 * 1, 2)\n",
    "        \n",
    "    def forward(self, x: PhiTensor):\n",
    "        # First layer of CNN - running 1 at a time to debug and see if any individual componenet is failing\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = F.leaky_relu(x)\n",
    "#         x = self.pool(x)\n",
    "        \n",
    "        # Subsequent layers\n",
    "        x = self.pool(F.leaky_relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.leaky_relu(self.bn4(self.conv4(x))))\n",
    "        x = self.pool(F.leaky_relu(self.bn5(self.conv5(x))))\n",
    "        x = self.avg(x)\n",
    "        x = x.reshape((-1, 512 * 1 * 1))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82bfdf71-1b51-43be-a0b3-5b66aaee7c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0a39762-66c6-4965-8bce-0402fa99cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft import PhiTensor\n",
    "import numpy as np\n",
    "\n",
    "N = 10\n",
    "C_in = 3\n",
    "H_in = 50\n",
    "W_in = 50\n",
    "\n",
    "\n",
    "input_shape = (N, C_in, H_in, W_in)\n",
    "x = PhiTensor(child=np.random.randint(low=0, high=255, size=input_shape),\n",
    "              data_subjects=np.zeros(input_shape),\n",
    "              min_vals=0,\n",
    "              max_vals=255\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de6ec849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_phi_tensor(input_shape):\n",
    "    y = PhiTensor(child=np.random.randint(low=0, high=2, size=input_shape),\n",
    "                  data_subjects=np.zeros(input_shape),\n",
    "                  min_vals=0,\n",
    "                  max_vals=1\n",
    "             )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88e5984a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=47774), min_vals=<lazyrepeatarray data: 0.7607353329658508 -> shape: ()>, max_vals=<lazyrepeatarray data: 0.3790375888347626 -> shape: ()>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = CrossEntropyLoss()\n",
    "prediction = cnn_model(x)\n",
    "\n",
    "\n",
    "target = create_target_phi_tensor(10)\n",
    "loss_fn(prediction, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f73108e-4e30-4c38-88fa-fffa623c3887",
   "metadata": {},
   "source": [
    "Alright so the results are terrible, but since it's random information I guess that's fine..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5cf700f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=FixedPrecisionTensor(child=[[-11553   6784]\n",
       " [ -1671   8769]\n",
       " [ -2155  14245]\n",
       " [ -7785   6681]\n",
       " [ -5030  14044]\n",
       " [ -4728  12926]\n",
       " [ -1904   8442]\n",
       " [  2326  15468]\n",
       " [ -3033  17263]\n",
       " [ -7450  12054]]), min_vals=<lazyrepeatarray data: [[-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]] -> shape: (10, 2)>, max_vals=<lazyrepeatarray data: [[-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]] -> shape: (10, 2)>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e20de325-c638-4d16-b0b5-5adba5f927c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lazyrepeatarray data: [[-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]\n",
       " [-0.01747882  0.11341693]] -> shape: (10, 2)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.min_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c346d0e-a086-4169-a6c3-fee99c43eca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lazyrepeatarray data: [[-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]\n",
       " [-0.08483684  0.68978536]] -> shape: (10, 2)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.max_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78ab54b8-f903-4f4f-b331-90dc6d232b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.17628479,  0.10351562],\n",
       "       [-0.02549744,  0.13380432],\n",
       "       [-0.03288269,  0.21736145],\n",
       "       [-0.11878967,  0.10194397],\n",
       "       [-0.07675171,  0.21429443],\n",
       "       [-0.07214355,  0.19723511],\n",
       "       [-0.02905273,  0.1288147 ],\n",
       "       [ 0.03549194,  0.23602295],\n",
       "       [-0.04627991,  0.26341248],\n",
       "       [-0.11367798,  0.18392944]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.child.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4e052a5-28b5-464a-9e06-d0876f9937b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "classes = 2\n",
    "batch_size = 128\n",
    "alpha = 0.002\n",
    "device = 'cpu'\n",
    "\n",
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eba615e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6b49256",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adamax(model.parameters(), lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5603dff-49e7-463b-99b6-a7d54d23decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phi_tensor():\n",
    "    return PhiTensor(\n",
    "        child=np.random.randint(0, 255, (50, 50, 3)),\n",
    "        data_subjects=np.ones((50, 50, 3)) * np.random.choice([0, 1]),\n",
    "        min_vals=0,\n",
    "        max_vals=255\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "70688784-1023-48eb-b4b1-6ad066387c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = [(create_phi_tensor(), create_target_phi_tensor(1)) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ebd8588-7ef7-4f63-80bf-6beb268b7bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc938670-3d8e-4fac-9f0e-1ead3deb0fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ledger\n"
     ]
    }
   ],
   "source": [
    "from syft.core.adp.data_subject_ledger import DataSubjectLedger\n",
    "from syft.core.adp.ledger_store import DictLedgerStore\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "ledger_store = DictLedgerStore()\n",
    "user_key = b\"1231\"\n",
    "ledger = DataSubjectLedger.get_or_create(store=ledger_store, user_key=user_key)\n",
    "\n",
    "def get_budget_for_user(*args: Any, **kwargs: Any) -> float:\n",
    "    return 999999\n",
    "\n",
    "def deduct_epsilon_for_user(*args: Any, **kwargs: Any) -> bool:\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "086c4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_loss_func = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "22aed3a8-ab54-491f-8267-04fdd323bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.5390472412109375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PhiTensor' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#         published_output = outputs.publish(\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#             get_budget_for_user=get_budget_for_user, \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#             deduct_epsilon_for_user=deduct_epsilon_for_user, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     26\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n\u001b[1;32m     28\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PhiTensor' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "total_step = len(loader_train)\n",
    "published_output_list = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in tqdm(enumerate(loader_train)):        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = pb_loss_func(outputs, labels) # we shouldn't need a [0]  here\n",
    "        print(\"Loss: \",loss.decode())\n",
    "        \n",
    "        \n",
    "#         published_output = outputs.publish(\n",
    "#             get_budget_for_user=get_budget_for_user, \n",
    "#             deduct_epsilon_for_user=deduct_epsilon_for_user, \n",
    "#             ledger=ledger, \n",
    "#             sigma=1000\n",
    "#         ).decode()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # REMOVE THIS PART- I put it here just to see if the rest would work\n",
    "        # published_output /= published_output.max()\n",
    "        #loss = criterion(torch.Tensor(published_output), labels) # we shouldn't need a [0]  here\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38012323-d6ef-411a-a82b-6e63ab3bfc59",
   "metadata": {},
   "source": [
    "## THIS IS THE SOURCE OF THE ERROR:\n",
    "https://stackoverflow.com/questions/48377214/runtimeerror-dimension-out-of-range-expected-to-be-in-range-of-1-0-but-go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f6a42-12ca-42c9-be25-fc03b3f5d78d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
