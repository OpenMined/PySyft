{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99346c73",
   "metadata": {},
   "source": [
    "#### TODO:\n",
    "- add .backward to PhiTensor\n",
    "- fix data subjects for CrossEntropyLoss\n",
    "- implement optimizer similar using nn.Module overriding forward to accept DP Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d0c427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Hagrid/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import syft\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84e32414",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "096ee254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import syft.core.tensor.nn.functional as F\n",
    "from syft import PhiTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e886ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=2)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=2)\n",
    "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm2d(512)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.avg = nn.AvgPool2d(3)\n",
    "        self.fc = nn.Linear(512 * 1 * 1, 2)\n",
    "        \n",
    "    def forward(self, x: PhiTensor):\n",
    "        # First layer of CNN - running 1 at a time to debug and see if any individual componenet is failing\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.bn1(x)\n",
    "#         x = F.leaky_relu(x)\n",
    "#         x = self.pool(x)\n",
    "        \n",
    "        # Subsequent layers\n",
    "        x = self.pool(F.leaky_relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.leaky_relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.leaky_relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.leaky_relu(self.bn4(self.conv4(x))))\n",
    "        x = self.pool(F.leaky_relu(self.bn5(self.conv5(x))))\n",
    "        x = self.avg(x)\n",
    "        x = x.reshape((-1, 512 * 1 * 1))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6c2a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = ConvNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e5252e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from syft import PhiTensor\n",
    "import numpy as np\n",
    "\n",
    "N = 10\n",
    "C_in = 3\n",
    "H_in = 50\n",
    "W_in = 50\n",
    "\n",
    "\n",
    "input_shape = (N, C_in, H_in, W_in)\n",
    "x = PhiTensor(child=np.random.randint(low=0, high=255, size=input_shape),\n",
    "              data_subjects=np.zeros(input_shape),\n",
    "              min_vals=0,\n",
    "              max_vals=255\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908bdcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_phi_tensor():\n",
    "    return PhiTensor(\n",
    "        child=np.random.randint(0, 255, (50, 50, 3)),\n",
    "        data_subjects=np.ones((50, 50, 3)) * np.random.choice([0, 1]),\n",
    "        min_vals=0,\n",
    "        max_vals=255\n",
    "    )\n",
    "\n",
    "def create_target_phi_tensor(input_shape):\n",
    "    y = PhiTensor(child=np.random.randint(low=0, high=2, size=input_shape),\n",
    "                  data_subjects=np.zeros(input_shape),\n",
    "                  min_vals=0,\n",
    "                  max_vals=1\n",
    "             )\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "087d1fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=0.7510945200920105, min_vals=<lazyrepeatarray data: 0.6495890021324158 -> shape: ()>, max_vals=<lazyrepeatarray data: 1.1558825969696045 -> shape: ()>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "prediction = cnn_model(x)\n",
    "\n",
    "\n",
    "target = create_target_phi_tensor(10)\n",
    "loss_fn(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf53cb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = [(create_phi_tensor(), create_target_phi_tensor(1)) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdaf9c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "classes = 2\n",
    "batch_size = 128\n",
    "alpha = 0.002\n",
    "device = 'cpu'\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "pb_loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adamax(model.parameters(), lr=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9962abbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.1823441\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PhiTensor' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#         published_output = outputs.publish(\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#             get_budget_for_user=get_budget_for_user, \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#             deduct_epsilon_for_user=deduct_epsilon_for_user, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n\u001b[1;32m     24\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PhiTensor' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_step = len(loader_train)\n",
    "published_output_list = []\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in tqdm(enumerate(loader_train)):        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = pb_loss_func(outputs, labels)\n",
    "        print(\"Loss: \",loss.child)\n",
    "        \n",
    "        \n",
    "#         published_output = outputs.publish(\n",
    "#             get_budget_for_user=get_budget_for_user, \n",
    "#             deduct_epsilon_for_user=deduct_epsilon_for_user, \n",
    "#             ledger=ledger, \n",
    "#             sigma=1000\n",
    "#         ).decode()\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf7049-354a-47e0-a62e-b97e96ea50d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b86e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222b4444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hagrid",
   "language": "python",
   "name": "hagrid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
