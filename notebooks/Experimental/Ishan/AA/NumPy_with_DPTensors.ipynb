{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c74cc9-fb3f-4872-8e62-ef181c4a3011",
   "metadata": {},
   "source": [
    "To Do:\n",
    "- Implement LeakyRelu activation function --> Done\n",
    "    - Accidentally added it to Conv layer; we need it on the BatchNorm layer for our model\n",
    "- Implement forward pass & backward pass for every layer of our CNN in pure NumPy: --> Done\n",
    "    - conv2d --> Done\n",
    "    - batchnorm2d  --> Done\n",
    "    - avgpool2d  --> Done\n",
    "    - maxpool2d  --> Done\n",
    "    - linear  --> Done\n",
    "- Implement AdaMax optimizer in pure NumPy --> Done\n",
    "- Implement Cross-Entropy Loss in pure NumPy --> Done\n",
    "- Implement Model class in pure NumPy --> Done\n",
    "- Verify pure NumPy model training works --> Done\n",
    "- Modify to work with DP Tensors instead of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c771e5-8f3b-4158-8dad-b40a55c49f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Hagrid/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from syft import PhiTensor\n",
    "from syft.core.adp.data_subject_list import DataSubjectList\n",
    "from syft.core.tensor.lazy_repeat_array import lazyrepeatarray as lra\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9570ecfe-9aeb-4c4d-9f4c-b5e250993cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_leakyrelu(dp_tensor: PhiTensor, slope: float=0.01) -> PhiTensor:\n",
    "    # TODO: Should we have an index in DSLs that corresponds to no data?\n",
    "    \n",
    "    gt = (dp_tensor.child > 0)\n",
    "    return PhiTensor(\n",
    "        child= gt * dp_tensor.child + (1 - gt) * dp_tensor.child * slope,\n",
    "        data_subjects=dp_tensor.data_subjects,\n",
    "        min_vals= lra(data=dp_tensor.min_vals.data * slope, shape=dp_tensor.min_vals.shape), \n",
    "        max_vals= lra(data=dp_tensor.max_vals.data * slope, shape=dp_tensor.max_vals.shape),\n",
    "    )\n",
    "\n",
    "\n",
    "class leaky_ReLU():\n",
    "\n",
    "    def __init__(self, slope=0.01):\n",
    "        super(leaky_ReLU, self).__init__()\n",
    "        self.slope = slope\n",
    "\n",
    "    def forward(self, input_array: PhiTensor):\n",
    "        # Last image that has been forward passed through this activation function\n",
    "        self.last_forward = input_array        \n",
    "        return dp_leakyrelu(dp_tensor=input_array, slope=self.slope)\n",
    "\n",
    "    def derivative(self, input_array: Optional[PhiTensor]=None):\n",
    "        last_forward = input_array if input_array else self.last_forward\n",
    "        res = np.ones(last_forward.shape)\n",
    "        idx = last_forward <= 0\n",
    "        res[idx.child] = self.slope\n",
    "        \n",
    "        return PhiTensor(child=res,\n",
    "                         data_subjects=last_forward.data_subjects,\n",
    "                         min_vals=last_forward.min_vals*0,\n",
    "                         max_vals = last_forward.max_vals*1)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6fd353b-f0fe-4423-b919-9961f86e46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Uniform():\n",
    "    def __init__(self, scale=0.05):\n",
    "        self.scale = scale\n",
    "        \n",
    "    def __call__(self, size):\n",
    "        return self.call(size)\n",
    "\n",
    "    def call(self, size):\n",
    "        return np.array(np.random.uniform(-self.scale, self.scale, size=size))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fcc3c8d-5f35-41ae-9cfd-dee729717732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_size(size):\n",
    "    if len(size) == 2:\n",
    "        fan_in = size[0]\n",
    "        fan_out = size[1]\n",
    "\n",
    "    elif len(size) == 4 or len(size) == 5:\n",
    "        respective_field_size = np.prod(size[2:])\n",
    "        fan_in = size[1] * respective_field_size\n",
    "        fan_out = size[0] * respective_field_size\n",
    "\n",
    "    else:\n",
    "        fan_in = fan_out = int(np.sqrt(np.prod(size)))\n",
    "\n",
    "    return fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f67671-2b55-43d4-91a2-7165e50601f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitialization():\n",
    "    def __call__(self, size):\n",
    "        return self.call(size)\n",
    "    \n",
    "    def call(self, size):\n",
    "        fan_in, fan_out = decompose_size(size)\n",
    "        return Uniform(np.sqrt(6 / (fan_in + fan_out)))(size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a656e1d-8003-4529-a985-ff7885cd553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    Subclassed when implementing new types of layers.\n",
    "    \n",
    "    Each layer can keep track of the layer(s) feeding into it, a\n",
    "    network's output :class:`Layer` instance can double as a handle to the full\n",
    "    network.\n",
    "    \"\"\"\n",
    "\n",
    "    first_layer = False\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\" Layer parameters. \n",
    "        \n",
    "        Returns a list of numpy.array variables or expressions that\n",
    "        parameterize the layer.\n",
    "        Returns\n",
    "        -------\n",
    "        list of numpy.array variables or expressions\n",
    "            A list of variables that parameterize the layer\n",
    "        Notes\n",
    "        -----\n",
    "        For layers without any parameters, this will return an empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        \"\"\" Get layer parameter gradients as calculated from backward(). \"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def param_grads(self):\n",
    "        \"\"\" Layer parameters and corresponding gradients. \"\"\"\n",
    "        return list(zip(self.params, self.grads))\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449fb14f-9793-49f5-aaa8-498c43e86e71",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**Note:**\n",
    "\n",
    "In the Convolution layer below, W, dw, b, db all start as numpy arrays.\n",
    "They **should** be PhiTensors- if someone wants to print them, they should have to spend PB.\n",
    "\n",
    "However, when they're initialized in the Convolution layer, they have to be initialized as \n",
    "np arrays because the initial values are public information. But as soon as they are exposed to a DP Tensor, they should convert to a PhiTensor.\n",
    "\n",
    "However we should be able to pass the data to and from various layers to each other.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbcf3cc8-0c49-452f-9c05-ebffded0b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution(Layer):\n",
    "    \"\"\"\n",
    "    If this is the first layer in a model, provide the keyword argument `input_shape`\n",
    "    (tuple of integers, does NOT include the sample axis, N.),\n",
    "    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nb_filter, filter_size, input_shape=None, stride=1):\n",
    "        self.nb_filter = nb_filter\n",
    "        self.filter_size = filter_size\n",
    "        self.input_shape = input_shape\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.W, self.dW = None, None\n",
    "        self.b, self.db = None, None\n",
    "        self.out_shape = None\n",
    "        self.last_output = None\n",
    "        self.last_input = None\n",
    "\n",
    "        self.init = XavierInitialization()\n",
    "        self.activation = leaky_ReLU()\n",
    "\n",
    "    def connect_to(self, prev_layer=None):\n",
    "        if prev_layer is None:\n",
    "            assert self.input_shape is not None\n",
    "            input_shape = self.input_shape\n",
    "        else:\n",
    "            input_shape = prev_layer.out_shape\n",
    "\n",
    "        # input_shape: (batch size, num input feature maps, image height, image width)\n",
    "        assert len(input_shape) == 4\n",
    "\n",
    "        nb_batch, pre_nb_filter, pre_height, pre_width = input_shape\n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        height = (pre_height - filter_height) // self.stride + 1\n",
    "        width = (pre_width - filter_width) // self.stride + 1\n",
    "\n",
    "        # output shape\n",
    "        self.out_shape = (nb_batch, self.nb_filter, height, width)\n",
    "\n",
    "        # filters\n",
    "        self.W = self.init((self.nb_filter, pre_nb_filter, filter_height, filter_width))\n",
    "        self.b = np.zeros((self.nb_filter,))\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "\n",
    "        self.last_input = input\n",
    "        \n",
    "        # TODO: This could fail if the DP Tensor has < 4 dimensions\n",
    "        \n",
    "        # shape\n",
    "        nb_batch, input_depth, old_img_h, old_img_w = input.shape\n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        new_img_h, new_img_w = self.out_shape[2:]\n",
    "\n",
    "        # init\n",
    "        outputs = np.zeros((nb_batch, self.nb_filter, new_img_h, new_img_w))\n",
    "        \n",
    "        \n",
    "\n",
    "        # convolution operation\n",
    "        for x in np.arange(nb_batch):\n",
    "            for y in np.arange(self.nb_filter):\n",
    "                for h in np.arange(new_img_h):\n",
    "                    for w in np.arange(new_img_w):\n",
    "                        h_shift, w_shift = h * self.stride, w * self.stride\n",
    "                        # patch: (input_depth, filter_h, filter_w)\n",
    "                        patch = input[x, :, h_shift: h_shift + filter_height, w_shift: w_shift + filter_width]\n",
    "                        outputs[x, y, h, w] = np.sum(patch.child * self.W[y]) + self.b[y]\n",
    "\n",
    "        # nonlinear activation\n",
    "        # self.last_output: (nb_batch, output_depth, image height, image width)\n",
    "        \n",
    "        # TODO: Min/max vals are direct function of private data- fix this when we have time\n",
    "        outputs = PhiTensor(\n",
    "            child=outputs,data_subjects=np.zeros_like(outputs), \n",
    "            min_vals=outputs.min(), max_vals=outputs.max()\n",
    "        )\n",
    "        self.last_output = self.activation.forward(outputs)\n",
    "\n",
    "        return self.last_output\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "\n",
    "        # shape\n",
    "        assert pre_grad.shape == self.last_output.shape\n",
    "        nb_batch, input_depth, old_img_h, old_img_w = self.last_input.shape\n",
    "        new_img_h, new_img_w = self.out_shape[2:]\n",
    "        \n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "#         filter_h, filter_w = self.filter_size\n",
    "        old_img_h, old_img_w = self.last_input.shape[-2:]\n",
    "\n",
    "        # gradients\n",
    "        self.dW = np.zeros((self.W.shape))\n",
    "        self.db = np.zeros((self.b.shape))\n",
    "        delta = pre_grad * self.activation.derivative()\n",
    "\n",
    "        # dW\n",
    "        for r in np.arange(self.nb_filter):\n",
    "            for t in np.arange(input_depth):\n",
    "                for h in np.arange(filter_height):\n",
    "                    for w in np.arange(filter_width):\n",
    "                        input_window = self.last_input[:, t,\n",
    "                                       h:old_img_h - filter_height + h + 1:self.stride,\n",
    "                                       w:old_img_w - filter_width + w + 1:self.stride]\n",
    "                        delta_window = delta[:, r]\n",
    "                        self.dW[r, t, h, w] = ((input_window * delta_window).sum() * (1/nb_batch)).child\n",
    "        # db\n",
    "        for r in np.arange(self.nb_filter):\n",
    "            self.db[r] = (delta[:, r].sum() * (1/nb_batch)).child\n",
    "        \n",
    "        \n",
    "        # dX\n",
    "        \n",
    "        \n",
    "        if not self.first_layer:\n",
    "            layer_grads = self.last_input.zeros_like()\n",
    "            for b in np.arange(nb_batch):\n",
    "                for r in np.arange(self.nb_filter):\n",
    "                    for t in np.arange(input_depth):\n",
    "                        for h in np.arange(new_img_h):\n",
    "                            for w in np.arange(new_img_w):\n",
    "                                h_shift, w_shift = h * self.stride, w * self.stride\n",
    "                                temp = layer_grads[b, t, h_shift:h_shift + filter_height, w_shift:w_shift + filter_width]\n",
    "                                layer_grads[b, t, h_shift:h_shift + filter_height, w_shift:w_shift + filter_width] = temp+ (delta[b, r, h, w] * self.W[r, t])\n",
    "                              \n",
    "        return layer_grads\n",
    "                         \n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        return self.dW, self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "df10cfa0-63cd-431b-bccd-d5c6b72516cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    def __init__(self, epsilon=1e-6, momentum=0.9, axis=0):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "\n",
    "        self.beta, self.dbeta = None, None\n",
    "        self.gamma, self.dgamma = None, None\n",
    "        self.cache = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        n_in = prev_layer.out_shape[-1]\n",
    "        self.beta = np.zeros((n_in,))\n",
    "        self.gamma = np.ones((n_in,))\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "        # N, D = x.shape\n",
    "        self.out_shape = input.shape\n",
    "        print(\"Starting BN forward pass, printing shapes:\")\n",
    "\n",
    "        # step1: calculate the mean\n",
    "        xmu = input - input.mean(axis=0)\n",
    "        print(\"Subtracted mean\")\n",
    "        print(\"xmu shape: \", xmu.shape)\n",
    "        # step3:\n",
    "        var = xmu.std(axis=0)\n",
    "        sqrtvar = (var + self.epsilon).sqrt()\n",
    "    \n",
    "        print(\"square rooted\")\n",
    "        print(\"var shape: \", var.shape, \" sqrtvar shape: \", sqrtvar.shape)\n",
    "        ivar = sqrtvar.reciprocal()\n",
    "        print(\"got past reciprocal, shape: \", ivar.shape)\n",
    "\n",
    "\n",
    "        # step5: normalization->x^\n",
    "        xhat = xmu * ivar\n",
    "        print(\"got xhat of shape: \", xhat.shape)\n",
    "\n",
    "        # step6: scale and shift\n",
    "        gammax = xhat * self.gamma\n",
    "        print(\"got gammax of shape: \", gammax.shape)\n",
    "        out = gammax + self.beta\n",
    "        print(\"got out of shape: \", out.shape)\n",
    "\n",
    "        self.cache = (xhat, xmu, ivar, sqrtvar, var)\n",
    "        print(\"cached\")\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        If you get stuck, here's a resource:\n",
    "        https://kratzert.github.io/2016/02/12/understanding-the-\n",
    "        gradient-flow-through-the-batch-normalization-layer.html\n",
    "        \n",
    "        Note: \n",
    "            - I removed the np.ones() at a few places where I \n",
    "               thought it wasn't making a difference\n",
    "            - I occasionally have kernel crashes on my 8GB machine\n",
    "            when running this. Perhaps too many large temp vars?\n",
    "            could also be due to too many large numbers.\n",
    "        \"\"\"\n",
    "        \n",
    "        xhat, xmu, ivar, sqrtvar, var = self.cache\n",
    "\n",
    "        N, D,x,y = pre_grad.shape\n",
    "#         print(f\"input shape of (N,D,x,y) = {(N, D, x, y)}\")\n",
    "\n",
    "        # step6\n",
    "        self.dbeta = pre_grad.sum(axis=0)\n",
    "        dgammax = pre_grad\n",
    "        self.dgamma = (dgammax * xhat).sum( axis=0)\n",
    "        dxhat = dgammax * self.gamma\n",
    "#         print(f\"step 6: shaep of dbeta = {self.dbeta.shape}\")\n",
    "#         print(f\"step 6: shaep of dgamma = {self.dgamma.shape}\")\n",
    "#         print(f\"step 6: shaep of dxhat = {dxhat.shape}\")\n",
    "\n",
    "        # step5\n",
    "        divar = (dxhat * xmu).sum(axis=0)\n",
    "        dxmu1 = dxhat * ivar \n",
    "\n",
    "        # step4\n",
    "        dsqrtvar = -1. / (sqrtvar * sqrtvar) * divar\n",
    "#         print(f\"step 4: shaep of dsqrtvar = {dsqrtvar.shape}\")\n",
    "        inv_var_eps_sqrt = (var + self.epsilon).sqrt().reciprocal()\n",
    "        \n",
    "#         print(f\"var + eps shape:\", inv_var_eps_sqrt.shape)\n",
    "        dvar = dsqrtvar * 0.5 * inv_var_eps_sqrt\n",
    "#         print(f\"dvar shape:\", dvar.shape)\n",
    "\n",
    "        \n",
    "        # step3\n",
    "        dxmu2 = xmu * dvar * (2/N)\n",
    "\n",
    "        # step2, \n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "\n",
    "#         # step1, \n",
    "        dmu = (dxmu1 + dxmu2).sum(axis=0) * -1\n",
    "        dx2 = dmu * (1/N)\n",
    "\n",
    "        # step0 done!\n",
    "        dx = dx1 + dx2\n",
    "\n",
    "        return dx\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.beta, self.gamma\n",
    "\n",
    "    @property\n",
    "    def grades(self):\n",
    "        return self.dbeta, self.dgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8e178933-c93b-4580-880f-22709d5527f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bnc():\n",
    "    shape = (1, 1, 50, 50)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "\n",
    "    c = Convolution(3, 3, input_shape=shape)\n",
    "    c.connect_to()\n",
    "    bn = BatchNorm()\n",
    "    bn.connect_to(c)\n",
    "    c_out = c.forward(smol_data)\n",
    "    output = bn.forward(c_out)\n",
    "    grad_signal = PhiTensor(\n",
    "    child=np.random.random((1,1,48,48))*255, data_subjects=np.zeros((1,1, 48, 48)), min_vals=0, max_vals=255\n",
    ")\n",
    "    return bn.backward(grad_signal)\n",
    "    \n",
    "# test_bnc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "18e479d9-e0e3-49a8-8f0a-5560a8f4c573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BN forward pass, printing shapes:\n",
      "Subtracted mean\n",
      "xmu shape:  (1, 3, 48, 48)\n",
      "square rooted\n",
      "var shape:  (3, 48, 48)  sqrtvar shape:  (3, 48, 48)\n",
      "got past reciprocal, shape:  (3, 48, 48)\n",
      "got xhat of shape:  (1, 3, 48, 48)\n",
      "got gammax of shape:  (1, 3, 48, 48)\n",
      "got out of shape:  (1, 3, 48, 48)\n",
      "cached\n",
      "input shape of (N,D,x,y) = (1, 1, 48, 48)\n",
      "step 6: shaep of dbeta = ()\n",
      "step 6: shaep of dgamma = ()\n",
      "step 6: shaep of dxhat = (1, 1, 48, 48)\n",
      "step 4: shaep of dsqrtvar = (3, 48, 48)\n",
      "var + eps shape: (3, 48, 48)\n",
      "dvar shape: (3, 48, 48)\n",
      "Lazy Repeat adding with mismatched shapes\n",
      "Lazy Repeat adding with mismatched shapes\n",
      "Lazy Repeat adding with mismatched shapes\n",
      "Lazy Repeat adding with mismatched shapes\n",
      "Lazy Repeat adding with mismatched shapes\n",
      "Lazy Repeat adding with mismatched shapes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=[[[[-1.27818086e+08 -1.27836122e+08 -1.27829598e+08 ... -1.27817908e+08\n",
       "    -1.27807745e+08 -1.27822273e+08]\n",
       "   [-1.27838403e+08 -1.27822666e+08 -1.27803974e+08 ... -1.27823675e+08\n",
       "    -1.27810432e+08 -1.27831174e+08]\n",
       "   [-1.27809633e+08 -1.27838436e+08 -1.27806700e+08 ... -1.27818540e+08\n",
       "    -1.27807721e+08 -1.27809712e+08]\n",
       "   ...\n",
       "   [-1.27836826e+08 -1.27817454e+08 -1.27807245e+08 ... -1.27828262e+08\n",
       "    -1.27832084e+08 -1.27827981e+08]\n",
       "   [-1.27801766e+08 -1.27836213e+08 -1.27806511e+08 ... -1.27807336e+08\n",
       "    -1.27804070e+08 -1.27827610e+08]\n",
       "   [-1.27838108e+08 -1.27811919e+08 -1.27818333e+08 ... -1.27822847e+08\n",
       "    -1.27817802e+08 -1.27831417e+08]]\n",
       "\n",
       "  [[-1.27818086e+08 -1.27836122e+08 -1.27829598e+08 ... -1.27817908e+08\n",
       "    -1.27807745e+08 -1.27822273e+08]\n",
       "   [-1.27838403e+08 -1.27822666e+08 -1.27803974e+08 ... -1.27823675e+08\n",
       "    -1.27810432e+08 -1.27831174e+08]\n",
       "   [-1.27809633e+08 -1.27838436e+08 -1.27806700e+08 ... -1.27818540e+08\n",
       "    -1.27807721e+08 -1.27809712e+08]\n",
       "   ...\n",
       "   [-1.27836826e+08 -1.27817454e+08 -1.27807245e+08 ... -1.27828262e+08\n",
       "    -1.27832084e+08 -1.27827981e+08]\n",
       "   [-1.27801766e+08 -1.27836213e+08 -1.27806511e+08 ... -1.27807336e+08\n",
       "    -1.27804070e+08 -1.27827610e+08]\n",
       "   [-1.27838108e+08 -1.27811919e+08 -1.27818333e+08 ... -1.27822847e+08\n",
       "    -1.27817802e+08 -1.27831417e+08]]\n",
       "\n",
       "  [[-1.27818086e+08 -1.27836122e+08 -1.27829598e+08 ... -1.27817908e+08\n",
       "    -1.27807745e+08 -1.27822273e+08]\n",
       "   [-1.27838403e+08 -1.27822666e+08 -1.27803974e+08 ... -1.27823675e+08\n",
       "    -1.27810432e+08 -1.27831174e+08]\n",
       "   [-1.27809633e+08 -1.27838436e+08 -1.27806700e+08 ... -1.27818540e+08\n",
       "    -1.27807721e+08 -1.27809712e+08]\n",
       "   ...\n",
       "   [-1.27836826e+08 -1.27817454e+08 -1.27807245e+08 ... -1.27828262e+08\n",
       "    -1.27832084e+08 -1.27827981e+08]\n",
       "   [-1.27801766e+08 -1.27836213e+08 -1.27806511e+08 ... -1.27807336e+08\n",
       "    -1.27804070e+08 -1.27827610e+08]\n",
       "   [-1.27838108e+08 -1.27811919e+08 -1.27818333e+08 ... -1.27822847e+08\n",
       "    -1.27817802e+08 -1.27831417e+08]]]], min_vals=<lazyrepeatarray data: [-4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19\n",
       " -4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19\n",
       " -4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19\n",
       " -4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19\n",
       " -4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19\n",
       " -4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19\n",
       " -4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19\n",
       " -4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19\n",
       " -4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19\n",
       " -4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19\n",
       " -4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19\n",
       " -4.87008203e+19 -4.87008203e+19 -4.87008203e+19 -4.87008203e+19] -> shape: (1, 1, 48, 48)>, max_vals=<lazyrepeatarray data: [4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19\n",
       " 4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19\n",
       " 4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19\n",
       " 4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19\n",
       " 4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19\n",
       " 4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19\n",
       " 4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19\n",
       " 4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19\n",
       " 4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19\n",
       " 4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19\n",
       " 4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19\n",
       " 4.87008203e+19 4.87008203e+19 4.87008203e+19 4.87008203e+19] -> shape: (1, 1, 48, 48)>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_bnc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00049879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BN forward pass, printing shapes:\n",
      "Subtracted mean\n",
      "xmu shape:  (1, 3, 1, 1)\n",
      "square rooted\n",
      "var shape:  (3, 1, 1)  sqrtvar shape:  (3, 1, 1)\n",
      "got past reciprocal, shape:  (3, 1, 1)\n",
      "got xhat of shape:  (1, 3, 1, 1)\n",
      "got gammax of shape:  (1, 3, 1, 1)\n",
      "got out of shape:  (1, 3, 1, 1)\n",
      "cached\n",
      "input shape of (N,D,x,y) = (1, 1, 3, 3)\n",
      "step 6: shaep of dbeta = ()\n",
      "step 6: shaep of dgamma = ()\n",
      "step 6: shaep of dxhat = (1, 1, 3, 3)\n",
      "step 4: shaep of dsqrtvar = (3, 1, 1)\n",
      "var + eps shape: (3, 1, 1)\n",
      "dvar shape: (3, 1, 1)\n",
      "Lazy Repeat adding with mismatched shapes\n",
      "Lazy Repeat adding with mismatched shapes\n",
      "Lazy Repeat adding with mismatched shapes\n",
      "Lazy Repeat adding with mismatched shapes\n",
      "Lazy Repeat adding with mismatched shapes\n",
      "Lazy Repeat adding with mismatched shapes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=[[[[-1678.93750228 -1803.55571043 -1803.92403539]\n",
       "   [-1742.8405872  -1784.98142077 -1770.41024839]\n",
       "   [-1703.56360028 -1781.86262897 -1670.54380956]]\n",
       "\n",
       "  [[-1678.93750228 -1803.55571043 -1803.92403539]\n",
       "   [-1742.8405872  -1784.98142077 -1770.41024839]\n",
       "   [-1703.56360028 -1781.86262897 -1670.54380956]]\n",
       "\n",
       "  [[-1678.93750228 -1803.55571043 -1803.92403539]\n",
       "   [-1742.8405872  -1784.98142077 -1770.41024839]\n",
       "   [-1703.56360028 -1781.86262897 -1670.54380956]]]], min_vals=<lazyrepeatarray data: [-1512007.47766757] -> shape: (1, 1, 3, 3)>, max_vals=<lazyrepeatarray data: [1504007.47766757] -> shape: (1, 1, 3, 3)>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_bnc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a16e99e-20fa-4702-bf4a-afed82b43525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool(Layer):\n",
    "    \"\"\"Average pooling operation for spatial data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pool_size : tuple of 2 integers,\n",
    "        factors by which to downscale (vertical, horizontal).\n",
    "        (2, 2) will halve the image in each dimension.\n",
    "    Returns\n",
    "    -------\n",
    "    4D numpy.array \n",
    "        with shape `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        self.out_shape = 0\n",
    "        self.out_shape = None\n",
    "        self.input_shape = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        assert 5 > len(prev_layer.out_shape) >= 3\n",
    "\n",
    "        old_h, old_w = prev_layer.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = old_h // pool_h, old_w // pool_w\n",
    "\n",
    "        assert old_h % pool_h == old_w % pool_w == 0\n",
    "\n",
    "        self.out_shape = prev_layer.out_shape[:-2] + (new_h, new_w)\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "\n",
    "        # shape\n",
    "        self.input_shape = input.shape\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "\n",
    "        # forward\n",
    "        outputs = np.zeros(self.input_shape[:-2] + self.out_shape[-2:])\n",
    "        outputs = PhiTensor(child=outputs, data_subjects=np.zeros_like(outputs), min_vals=0, max_vals=1)\n",
    "        \n",
    "        ndim = len(input.shape)\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            outputs[a, b, h, w] = input[a, b, h:h + pool_h, w:w + pool_w].mean()\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        outputs[a, h, w] = np.mean(input[a, h:h + pool_h, w:w + pool_w])\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, pre_grad: PhiTensor, *args, **kwargs):\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        length = np.prod(self.pool_size)\n",
    "\n",
    "        layer_grads = np.zeros(self.input_shape)\n",
    "        layer_grads = PhiTensor(child=layer_grads, data_subjects=np.zeros_like(layer_grads), min_vals=0, max_vals=1)\n",
    "        \n",
    "        ndim = len(pre_grad.shape)\n",
    "\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            h_shift, w_shift = h * pool_h, w * pool_w\n",
    "                            layer_grads[a, b, h_shift: h_shift + pool_h, w_shift: w_shift + pool_w] = \\\n",
    "                                pre_grad[a, b, h, w] * (1/length)\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        h_shift, w_shift = h * pool_h, w * pool_w\n",
    "                        layer_grads[a, h_shift: h_shift + pool_h, w_shift: w_shift + pool_w] = \\\n",
    "                            pre_grad[a, h, w] * (1/length)\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return layer_grads\n",
    "\n",
    "\n",
    "class MaxPool(Layer):\n",
    "    \"\"\"Max pooling operation for spatial data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pool_size : tuple of 2 integers,\n",
    "        factors by which to downscale (vertical, horizontal).\n",
    "        (2, 2) will halve the image in each dimension.\n",
    "    Returns\n",
    "    -------\n",
    "    4D numpy.array \n",
    "        with shape `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        self.input_shape = None\n",
    "        self.out_shape = None\n",
    "        self.last_input = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        # prev_layer.out_shape: (nb_batch, ..., height, width)\n",
    "        assert len(prev_layer.out_shape) >= 3\n",
    "\n",
    "        old_h, old_w = prev_layer.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = old_h // pool_h, old_w // pool_w\n",
    "\n",
    "        assert old_h % pool_h == old_w % pool_w == 0\n",
    "\n",
    "        self.out_shape = prev_layer.out_shape[:-2] + (new_h, new_w)\n",
    "\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        # shape\n",
    "        self.input_shape = input.shape\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "\n",
    "        # forward\n",
    "        self.last_input = input\n",
    "        outputs = np.zeros(self.input_shape[:-2] + self.out_shape[-2:])\n",
    "        outputs = PhiTensor(child=outputs, data_subjects=np.zeros_like(outputs), min_vals=0, max_vals=1)\n",
    "        \n",
    "        ndim = len(input.shape)\n",
    "\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            outputs[a, b, h, w] = input[a, b, h:h + pool_h, w:w + pool_w].max()\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        outputs[a, h, w] = input[a, h:h + pool_h, w:w + pool_w].max()\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "\n",
    "        layer_grads = np.zeros(self.input_shape)\n",
    "        layer_grads = PhiTensor(child=layer_grads, data_subjects=np.zeros_like(layer_grads), min_vals=0, max_vals=1)\n",
    "        \n",
    "        ndim = len(pre_grad.shape)\n",
    "\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            patch = self.last_input[a, b, h:h + pool_h, w:w + pool_w]\n",
    "                            max_idx = np.unravel_index(patch.argmax(), patch.shape)\n",
    "                            h_shift, w_shift = h * pool_h + max_idx[0], w * pool_w + max_idx[1]\n",
    "                            layer_grads[a, b, h_shift, w_shift] = pre_grad[a, b, a, w]\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        patch = self.last_input[a, h:h + pool_h, w:w + pool_w]\n",
    "                        max_idx = np.unravel_index(patch.argmax(), patch.shape)\n",
    "                        h_shift, w_shift = h * pool_h + max_idx[0], w * pool_w + max_idx[1]\n",
    "                        layer_grads[a, h_shift, w_shift] = pre_grad[a, a, w]\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a508b16d-6fa3-425d-9898-84171cd41258",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = AvgPool((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ab5506a6-4225-4d5d-b121-406d41b930d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_c_bn_avg():\n",
    "    shape = (10, 3, 6, 6)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "\n",
    "    c = Convolution(3, 3, input_shape=shape)\n",
    "    c.connect_to()\n",
    "    bn = BatchNorm()\n",
    "    bn.connect_to(c)\n",
    "    c_out = c.forward(smol_data)\n",
    "    bn_out = bn.forward(c_out)\n",
    "    avg = AvgPool((2,2))\n",
    "    avg.connect_to(bn)\n",
    "    return avg.forward(bn_out)\n",
    "    \n",
    "# test_c_bn_avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e97b8c41-ef8b-421a-bc4e-52ea7f7ad342",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxp = MaxPool((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4f035d2e-ecae-4fad-985e-4c9fa6a683bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_c_bn_max():\n",
    "    shape = (10, 3, 6, 6)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "\n",
    "    c = Convolution(3, 3, input_shape=shape)\n",
    "    c.connect_to()\n",
    "    bn = BatchNorm()\n",
    "    bn.connect_to(c)\n",
    "    c_out = c.forward(smol_data)\n",
    "    bn_out = bn.forward(c_out)\n",
    "    maxp = MaxPool((2,2))\n",
    "    maxp.connect_to(bn)\n",
    "    return maxp.forward(bn_out)\n",
    "    \n",
    "# test_c_bn_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b41a5cc-2098-438e-9e7c-d9978fadcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, n_out, n_in=None):\n",
    "        self.n_out = n_out\n",
    "        self.n_in = n_in\n",
    "        self.out_shape = (None, n_out)\n",
    "\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.last_input = None\n",
    "        self.init = XavierInitialization()\n",
    "\n",
    "    def connect_to(self, prev_layer=None):\n",
    "        if prev_layer is None:\n",
    "            assert self.n_in is not None\n",
    "            n_in = self.n_in\n",
    "        else:\n",
    "            assert len(prev_layer.out_shape) == 2\n",
    "            n_in = prev_layer.out_shape[-1]\n",
    "\n",
    "        self.W = self.init((n_in, self.n_out))\n",
    "        self.b = np.zeros((self.n_out,))\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "        self.last_input = input\n",
    "        return input.dot(self.W) + self.b\n",
    "\n",
    "    def backward(self, pre_grad: PhiTensor, *args, **kwargs):\n",
    "        self.dW = self.last_input.T.dot(pre_grad)\n",
    "        self.db = pre_grad.mean(axis=0)\n",
    "        if not self.first_layer:\n",
    "            return pre_grad.dot(self.W.T)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        return self.dW, self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f0d766d-997b-4360-bd26-77ca4b512447",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = Linear(n_out=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28e785a6-4581-464a-8cab-e411bbbe6f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=[[[[   5.24833433  -75.84949229]\n",
       "   [  10.24342301  -38.9142909 ]\n",
       "   [ 368.68715765  171.71721353]\n",
       "   [ 155.06380828  -59.8949137 ]\n",
       "   [ 378.82537868   61.49110329]\n",
       "   [ 253.60563567  -25.09348321]]\n",
       "\n",
       "  [[ 257.52941686  173.11077525]\n",
       "   [ 166.44139151  -20.3353861 ]\n",
       "   [ 234.0051232   170.22089581]\n",
       "   [ 153.04403779  -63.69885478]\n",
       "   [ 170.72483013  127.35582891]\n",
       "   [ 187.83306639   16.02506172]]\n",
       "\n",
       "  [[  31.4683409  -112.98179052]\n",
       "   [ 188.43870286   66.00767805]\n",
       "   [ 290.78516731   10.1833828 ]\n",
       "   [ 230.55195987   70.04463432]\n",
       "   [  22.06618054   25.69595126]\n",
       "   [ 207.67907493   24.20454383]]]\n",
       "\n",
       "\n",
       " [[[ 248.92589215  -50.54530723]\n",
       "   [ 246.47872062   15.59708157]\n",
       "   [ 242.76760928   44.91433977]\n",
       "   [ 313.99163716   28.95963354]\n",
       "   [  51.15047689  -49.06108945]\n",
       "   [ 288.88619756  118.5549757 ]]\n",
       "\n",
       "  [[ 356.90329665   35.5240908 ]\n",
       "   [ 178.34281198 -102.69864148]\n",
       "   [ 181.3230046   -74.18619988]\n",
       "   [ 316.11192439   10.23971865]\n",
       "   [ 116.06056845   14.59730063]\n",
       "   [ 127.80934666   54.12808728]]\n",
       "\n",
       "  [[ 144.54325905   -8.97871406]\n",
       "   [ 314.36035025  162.37270027]\n",
       "   [ 189.00909388  147.03580932]\n",
       "   [ 153.79110049   80.09236269]\n",
       "   [ 274.3911258   207.31416898]\n",
       "   [ 133.36338037  -53.50380327]]]\n",
       "\n",
       "\n",
       " [[[ 219.86605503  167.41027627]\n",
       "   [ 193.13610091  -42.16956369]\n",
       "   [  62.05412458   66.91632524]\n",
       "   [ 309.89061671  146.52973291]\n",
       "   [  82.82060597  -32.34274054]\n",
       "   [ 322.52735862   47.89406196]]\n",
       "\n",
       "  [[ 308.64657582  136.99742105]\n",
       "   [ 396.86177065   44.853983  ]\n",
       "   [ 108.6245524  -131.13846932]\n",
       "   [ 165.70661371   15.83592386]\n",
       "   [ 192.72806411  165.54572302]\n",
       "   [ 129.73330655  115.90163103]]\n",
       "\n",
       "  [[ 160.75258653   89.54942104]\n",
       "   [ 254.16262429   91.39419135]\n",
       "   [ 207.69123142    5.27713224]\n",
       "   [ 167.62121645  143.19106832]\n",
       "   [ 330.15036996  114.94392004]\n",
       "   [ 293.73300608  112.6053227 ]]]\n",
       "\n",
       "\n",
       " [[[ 314.6747786   162.12536655]\n",
       "   [ 223.24385753   54.16568073]\n",
       "   [ 254.40479789  196.11681679]\n",
       "   [ 136.65664375   98.2597644 ]\n",
       "   [ 243.82316973  184.07132014]\n",
       "   [ 223.01962548  147.18704731]]\n",
       "\n",
       "  [[ 140.43543314   71.02602256]\n",
       "   [  45.16101147   48.94184419]\n",
       "   [ 290.37025251  214.50500347]\n",
       "   [ 283.11493306   50.34572216]\n",
       "   [  35.45073139   39.47263928]\n",
       "   [ 104.8971306  -117.29739708]]\n",
       "\n",
       "  [[ 100.13090793   16.29144804]\n",
       "   [ 368.98659164  196.140689  ]\n",
       "   [ 331.15506604   67.87891055]\n",
       "   [ 120.44009479 -110.46752734]\n",
       "   [ 347.08965227  175.37935705]\n",
       "   [ 182.30308937   74.31502297]]]\n",
       "\n",
       "\n",
       " [[[ 179.48966235  139.66441804]\n",
       "   [  62.02193835  134.78831587]\n",
       "   [ 344.29180721   82.09443371]\n",
       "   [ 263.27268987  227.78528724]\n",
       "   [  97.03195902  -81.0813457 ]\n",
       "   [ 302.96397609  -14.19490716]]\n",
       "\n",
       "  [[ 444.0029844    65.25774168]\n",
       "   [ 256.25143003   25.21722531]\n",
       "   [ 315.57280486   65.78326663]\n",
       "   [ 256.15808932  -52.43205441]\n",
       "   [ 267.35188937  129.01760697]\n",
       "   [ 265.12929325  119.05905916]]\n",
       "\n",
       "  [[ 103.07224257   15.94396872]\n",
       "   [ 263.26897767  266.29662038]\n",
       "   [ 149.63767828  -79.58978708]\n",
       "   [ 338.78687256  247.33818131]\n",
       "   [ 281.29439108  114.40189059]\n",
       "   [ 215.44535999   71.1274358 ]]]\n",
       "\n",
       "\n",
       " [[[  91.33197063   29.03633921]\n",
       "   [ 349.01378004  203.2455326 ]\n",
       "   [ 105.56214814  -13.59124236]\n",
       "   [ 313.4737125    -7.31495967]\n",
       "   [ 139.52760881   31.45662951]\n",
       "   [   8.03176053 -137.4101555 ]]\n",
       "\n",
       "  [[ 159.2269013   124.6434055 ]\n",
       "   [ 151.62050034   13.24994526]\n",
       "   [ 314.08239312  154.34696938]\n",
       "   [ 290.75486257  172.75337927]\n",
       "   [  58.60072857   56.17748185]\n",
       "   [ 232.32105009  122.6390933 ]]\n",
       "\n",
       "  [[ 251.96239626   71.29951962]\n",
       "   [ 192.00989699   48.535779  ]\n",
       "   [ 205.81435007  156.23508009]\n",
       "   [ 303.02564224  150.43124217]\n",
       "   [ 212.69003156   53.11638996]\n",
       "   [ 306.47162403   56.41361702]]]\n",
       "\n",
       "\n",
       " [[[ 237.7684997     7.22188335]\n",
       "   [ 312.40783491   51.54360828]\n",
       "   [ 133.71535797   65.09181159]\n",
       "   [ 181.38102947   23.66026883]\n",
       "   [ 333.12453223   41.74135805]\n",
       "   [ 235.37624912  -33.26196966]]\n",
       "\n",
       "  [[ 195.60549989   32.60474955]\n",
       "   [ 254.34951307  130.69762754]\n",
       "   [ 195.44662999  234.17625623]\n",
       "   [ 298.07789059   11.8325247 ]\n",
       "   [ -23.92391413  -21.03154901]\n",
       "   [ 168.91276245  -42.83567041]]\n",
       "\n",
       "  [[ 307.42068947  214.54707588]\n",
       "   [ 340.05563846  250.03734445]\n",
       "   [ 441.28989885   80.05501239]\n",
       "   [ 231.76516108  -84.6222142 ]\n",
       "   [ 265.81209268  127.885206  ]\n",
       "   [ 222.93419915   80.67046236]]]\n",
       "\n",
       "\n",
       " [[[ -31.81694158    5.73821263]\n",
       "   [ 206.21382176  187.10184869]\n",
       "   [ 196.56247899   80.04126137]\n",
       "   [  96.29044654   96.03974456]\n",
       "   [ 226.81350841   40.84759517]\n",
       "   [ 166.78231889  -68.6045227 ]]\n",
       "\n",
       "  [[ 272.84061168    6.27146412]\n",
       "   [ 377.70647356   76.04537723]\n",
       "   [  50.67705354    2.97680364]\n",
       "   [ 145.6608653   -49.95760561]\n",
       "   [ 386.69880293  138.19897562]\n",
       "   [ 227.61014552   58.16072354]]\n",
       "\n",
       "  [[ 413.38638235  231.79327419]\n",
       "   [  23.01212059   55.66663403]\n",
       "   [   1.07335054   28.59337106]\n",
       "   [ 461.62528021  190.38416008]\n",
       "   [ 195.00487096   70.39562804]\n",
       "   [ 123.02585894  -82.55281987]]]\n",
       "\n",
       "\n",
       " [[[ 276.92069054  146.30287855]\n",
       "   [  95.61581638   26.158708  ]\n",
       "   [ 243.71402286   78.66142083]\n",
       "   [ 356.04761819  106.59752138]\n",
       "   [ 100.59838999  -73.30574104]\n",
       "   [ 181.2141474    -2.18548556]]\n",
       "\n",
       "  [[  95.71003538   15.95391279]\n",
       "   [ 181.94415003   -9.52680899]\n",
       "   [ 383.97672949  128.52714395]\n",
       "   [ 219.03558992   -7.51103289]\n",
       "   [ 367.63695676   64.16651929]\n",
       "   [ 287.42897339  144.44877561]]\n",
       "\n",
       "  [[ 381.28599321   96.40362108]\n",
       "   [ 178.18332796   60.0668786 ]\n",
       "   [ 164.05556583  -70.9033623 ]\n",
       "   [ 199.37805451   16.89218116]\n",
       "   [ 386.24603514   60.46300296]\n",
       "   [ 300.00210147   47.74866423]]]\n",
       "\n",
       "\n",
       " [[[ 291.34818401  245.73323267]\n",
       "   [ 227.97597069   16.80459282]\n",
       "   [  93.64634205  -83.71264651]\n",
       "   [ 256.63946398    2.42313279]\n",
       "   [ 143.91481341    6.21498396]\n",
       "   [ 112.22643202   91.88292982]]\n",
       "\n",
       "  [[ 146.88733124   19.67312016]\n",
       "   [ 190.09179636  127.5990355 ]\n",
       "   [ -34.56152456 -222.40313332]\n",
       "   [ 106.44973279  145.32251165]\n",
       "   [ 222.340419     84.8550331 ]\n",
       "   [  87.55881333   71.74933884]]\n",
       "\n",
       "  [[ 387.6224945   219.82725294]\n",
       "   [  72.9955164    76.52038319]\n",
       "   [ 283.25799349   64.23124534]\n",
       "   [ 226.53061551   -8.81303395]\n",
       "   [ 109.56523728  -45.81592376]\n",
       "   [ 268.95406308  170.79082827]]]], min_vals=<lazyrepeatarray data: [[<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]] -> shape: (10, 3, 6, 2)>, max_vals=<lazyrepeatarray data: [[<lazyrepeatarray data: 48.74820051058566 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: -120.16353177402547 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: -2.8613798061943463 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 206.48412387383007 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: -181.3314460704663 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: -153.2375032358752 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 174.3420541258565 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 43.94589675487487 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 210.6555651207431 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 110.15045057529052 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 185.340875494333 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 42.22065827974946 -> shape: (10, 3, 6, 6)>]] -> shape: (10, 3, 6, 2)>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_lin():\n",
    "    shape = (10, 3, 6, 6)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "    \n",
    "    lin = Linear(n_out=2)\n",
    "    lin.n_in = 6\n",
    "    lin.connect_to()\n",
    "    return lin.forward(smol_data)\n",
    "    \n",
    "test_lin()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2814059b-a806-4aa6-b8b9-6542bd98344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    \"\"\"Abstract optimizer base class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clip : float\n",
    "        If smaller than 0, do not apply parameter clip.\n",
    "    lr : float\n",
    "        The learning rate controlling the size of update steps\n",
    "    decay : float\n",
    "        Decay parameter for the moving average. Must lie in [0, 1) where\n",
    "        lower numbers means a shorter memory.\n",
    "    lr_min : float\n",
    "        When adapting step rates, do not move below this value. Default is 0.\n",
    "    lr_max : float\n",
    "        When adapting step rates, do not move above this value. Default is inf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, clip=-1, decay=0., lr_min=0., lr_max=np.inf):\n",
    "        self.lr = lr\n",
    "        self.clip = clip\n",
    "        self.decay = decay\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "\n",
    "        self.iterations = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.iterations += 1\n",
    "\n",
    "        self.lr *= (1. / 1 + self.decay * self.iterations)\n",
    "        self.lr = np.clip(self.lr, self.lr_min, self.lr_max)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db57ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_maximum(x, y):\n",
    "    x_data = x.child\n",
    "    y_data = y.child if hasattr(y, \"child\") else y\n",
    "    \n",
    "    output = np.maximum(x_data, y_data)\n",
    "    min_v, max_v = output.min(), output.max()\n",
    "    dsl = DataSubjectList(\n",
    "            one_hot_lookup=x.data_subjects.one_hot_lookup,\n",
    "            data_subjects_indexed=np.zeros_like(output)\n",
    "        )\n",
    "    return PhiTensor(\n",
    "        child=output,\n",
    "        data_subjects=dsl,\n",
    "        min_vals=min_v,\n",
    "        max_vals=max_v,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f17351b5-fd14-4dce-910a-a7591f7f548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adamax(Optimizer):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta1 : float\n",
    "        Exponential decay rate for the first moment estimates.\n",
    "    beta2 : float\n",
    "        Exponential decay rate for the second moment estimates.\n",
    "    epsilon : float\n",
    "        Constant for numerical stability.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kingma, Diederik, and Jimmy Ba (2014):\n",
    "           Adam: A Method for Stochastic Optimization.\n",
    "           arXiv preprint arXiv:1412.6980.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta1=0.9, beta2=0.999, epsilon=1e-8, *args, **kwargs):\n",
    "        super(Adamax, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.ms = None\n",
    "        self.vs = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        # init\n",
    "        self.iterations += 1\n",
    "        a_t = self.lr / (1 - np.power(self.beta1, self.iterations))\n",
    "        if self.ms is None:\n",
    "            self.ms = [p.zeros_like() for p in params]\n",
    "        if self.vs is None:\n",
    "            self.vs = [p.zeros_like() for p in params]\n",
    "\n",
    "        # update parameters\n",
    "        for i, (m, v, p, g) in enumerate(zip(self.ms, self.vs, params, grads)):\n",
    "            m = m * self.beta1 + g * (1 - self.beta1)\n",
    "            v = dp_maximum(v * self.beta2, g.abs())\n",
    "            p = p - m * (v + self.epsilon).reciprocal() * a_t\n",
    "\n",
    "            self.ms[i] = m\n",
    "            self.vs[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0146b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adamax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13c615e2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mb_data\u001b[49m, b_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'b_data' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer.update(b_data, b_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba13f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a671665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_log(input: PhiTensor):\n",
    "    data = input.child\n",
    "    \n",
    "    output = np.log(data)\n",
    "    min_v, max_v = output.min(), output.max()\n",
    "    dsl = DataSubjectList(\n",
    "            one_hot_lookup=input.data_subjects.one_hot_lookup,\n",
    "            data_subjects_indexed=np.zeros_like(output)\n",
    "        )\n",
    "    return PhiTensor(\n",
    "        child=output,\n",
    "        data_subjects=dsl,\n",
    "        min_vals=min_v,\n",
    "        max_vals=max_v,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735de6c7-43c7-49ec-aa2b-b40106a63452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy():\n",
    "    def __init__(self, epsilon=1e-11):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        .. math:: L = -t \\\\log(p) - (1 - t) \\\\log(1 - p)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : numpy.array\n",
    "            Predictions in (0, 1), such as sigmoidal output of a neural network.\n",
    "        targets : numpy.array\n",
    "            Targets in [0, 1], such as ground truth labels.\n",
    "        \"\"\"\n",
    "        outputs = outputs.clip(self.epsilon, 1 - self.epsilon)\n",
    "        log_loss = targets * dp_log(outputs) + ((targets * -1) + 1) * dp_log((outputs * -1) + 1)\n",
    "        log_loss = log_loss.sum(axis=1) * -1\n",
    "        return log_loss.mean()\n",
    "\n",
    "    def backward(self, outputs: PhiTensor, targets: PhiTensor):\n",
    "        \"\"\"Backward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : numpy.array\n",
    "            Predictions in (0, 1), such as sigmoidal output of a neural network.\n",
    "        targets : numpy.array\n",
    "            Targets in [0, 1], such as ground truth labels.\n",
    "        \"\"\"\n",
    "        outputs = outputs.clip(self.epsilon, 1 - self.epsilon)\n",
    "        divisor = dp_maximum(outputs * ((outputs * -1) + 1), self.epsilon)\n",
    "        return (outputs - targets) * divisor.reciprocal()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745f76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = BinaryCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ecf885",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (2, 10)\n",
    "target = PhiTensor(child=np.random.randint(low=0, high=2, size=input_shape),\n",
    "              data_subjects=np.zeros(input_shape),\n",
    "              min_vals=0,\n",
    "              max_vals=1\n",
    "         )\n",
    "\n",
    "prediction = PhiTensor(child=np.random.rand(*input_shape),\n",
    "              data_subjects=np.zeros(input_shape),\n",
    "              min_vals=0,\n",
    "              max_vals=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df88e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn.forward(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c880527",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn.backward(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0d5b1-0240-4871-91d1-aa0797b1595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, layers=None):\n",
    "        self.layers = [] if layers is None else layers\n",
    "\n",
    "        self.loss = None\n",
    "        self.optimizer = Adamax\n",
    "\n",
    "    def add(self, layer):\n",
    "        assert isinstance(layer, Layer), \"PySyft doesn't recognize this kind of layer.\"\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, loss=BinaryCrossEntropy(), optimizer=Adamax()):\n",
    "        self.layers[0].first_layer = True\n",
    "\n",
    "        next_layer = None\n",
    "        for layer in self.layers:\n",
    "            layer.connect_to(next_layer)\n",
    "            next_layer = layer\n",
    "\n",
    "        self.loss = BinaryCrossEntropy()\n",
    "        self.optimizer = Adamax()\n",
    "\n",
    "    def fit(self, X, Y, max_iter=100, batch_size=64, shuffle=True,\n",
    "            validation_split=0., validation_data=None):\n",
    "\n",
    "        # prepare data\n",
    "        train_X = X #.astype(get_dtype()) if np.issubdtype(np.float64, X.dtype) else X\n",
    "        train_Y = Y #.astype(get_dtype()) if np.issubdtype(np.float64, Y.dtype) else Y\n",
    "\n",
    "        if 1. > validation_split > 0.:\n",
    "            split = int(train_Y.shape[0] * validation_split)\n",
    "            valid_X, valid_Y = train_X[-split:], train_Y[-split:]\n",
    "            train_X, train_Y = train_X[:-split], train_Y[:-split]\n",
    "        elif validation_data is not None:\n",
    "            valid_X, valid_Y = validation_data\n",
    "        else:\n",
    "            valid_X, valid_Y = None, None\n",
    "\n",
    "        iter_idx = 0\n",
    "        while iter_idx < max_iter:\n",
    "            iter_idx += 1\n",
    "\n",
    "            # shuffle\n",
    "            if shuffle:\n",
    "                seed = np.random.randint(111, 1111111)\n",
    "                np.random.seed(seed)\n",
    "                np.random.shuffle(train_X)\n",
    "                np.random.seed(seed)\n",
    "                np.random.shuffle(train_Y)\n",
    "\n",
    "            # train\n",
    "            train_losses, train_predicts, train_targets = [], [], []\n",
    "            for b in range(train_Y.shape[0] // batch_size):\n",
    "                batch_begin = b * batch_size\n",
    "                batch_end = batch_begin + batch_size\n",
    "                x_batch = train_X[batch_begin:batch_end]\n",
    "                y_batch = train_Y[batch_begin:batch_end]\n",
    "\n",
    "                # forward propagation\n",
    "                y_pred = self.predict(x_batch)\n",
    "\n",
    "                # backward propagation\n",
    "                next_grad = self.loss.backward(y_pred, y_batch)\n",
    "                for layer in self.layers[::-1]:\n",
    "                    next_grad = layer.backward(next_grad)\n",
    "\n",
    "                # get parameter and gradients\n",
    "                params = []\n",
    "                grads = []\n",
    "                for layer in self.layers:\n",
    "                    params += layer.params\n",
    "                    grads += layer.grads\n",
    "\n",
    "                # update parameters\n",
    "                self.optimizer.update(params, grads)\n",
    "\n",
    "                # got loss and predict\n",
    "                train_losses.append(self.loss.forward(y_pred, y_batch))\n",
    "                train_predicts.extend(y_pred)\n",
    "                train_targets.extend(y_batch)\n",
    "\n",
    "            # output train status\n",
    "            runout = \"iter %d, train-[loss %.4f, acc %.4f]; \" % (\n",
    "                iter_idx, float(np.mean(train_losses)), float(self.accuracy(train_predicts, train_targets)))\n",
    "\n",
    "            # runout = \"iter %d, train-[loss %.4f, ]; \" % (\n",
    "            #     iter_idx, float(np.mean(train_losses)))\n",
    "\n",
    "            if valid_X is not None and valid_Y is not None:\n",
    "                # valid\n",
    "                valid_losses, valid_predicts, valid_targets = [], [], []\n",
    "                for b in range(valid_X.shape[0] // batch_size):\n",
    "                    batch_begin = b * batch_size\n",
    "                    batch_end = batch_begin + batch_size\n",
    "                    x_batch = valid_X[batch_begin:batch_end]\n",
    "                    y_batch = valid_Y[batch_begin:batch_end]\n",
    "\n",
    "                    # forward propagation\n",
    "                    y_pred = self.predict(x_batch)\n",
    "\n",
    "                    # got loss and predict\n",
    "                    valid_losses.append(self.loss.forward(y_pred, y_batch))\n",
    "                    valid_predicts.extend(y_pred)\n",
    "                    valid_targets.extend(y_batch)\n",
    "\n",
    "                # output valid status\n",
    "                runout += \"valid-[loss %.4f, acc %.4f]; \" % (\n",
    "                    float(np.mean(valid_losses)), float(self.accuracy(valid_predicts, valid_targets)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        x_next = X\n",
    "        for layer in self.layers[:]:\n",
    "            x_next = layer.forward(x_next)\n",
    "        y_pred = x_next\n",
    "        return y_pred\n",
    "\n",
    "    def accuracy(self, outputs, targets):\n",
    "        y_predicts = np.argmax(outputs, axis=1)\n",
    "        y_targets = np.argmax(targets, axis=1)\n",
    "        acc = y_predicts == y_targets\n",
    "        return np.mean(acc)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dce0b9-cd55-45f3-9603-03024945cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93cd4e-5503-4431-978b-c0074840a58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hagrid",
   "language": "python",
   "name": "hagrid"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
