{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7c74cc9-fb3f-4872-8e62-ef181c4a3011",
   "metadata": {},
   "source": [
    "To Do:\n",
    "- Implement LeakyRelu activation function --> Done\n",
    "    - Accidentally added it to Conv layer; we need it on the BatchNorm layer for our model\n",
    "- Implement forward pass & backward pass for every layer of our CNN in pure NumPy: --> Done\n",
    "    - conv2d --> Done\n",
    "    - batchnorm2d  --> Done\n",
    "    - avgpool2d  --> Done\n",
    "    - maxpool2d  --> Done\n",
    "    - linear  --> Done\n",
    "- Implement AdaMax optimizer in pure NumPy --> Done\n",
    "- Implement Cross-Entropy Loss in pure NumPy --> Done\n",
    "- Implement Model class in pure NumPy --> Done\n",
    "- Verify pure NumPy model training works --> Done\n",
    "- Modify to work with DP Tensors instead of numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46c771e5-8f3b-4158-8dad-b40a55c49f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shubham/anaconda3/envs/syft/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from syft import PhiTensor\n",
    "from syft.core.adp.data_subject_list import DataSubjectList\n",
    "from syft.core.tensor.lazy_repeat_array import lazyrepeatarray as lra\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9570ecfe-9aeb-4c4d-9f4c-b5e250993cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_leakyrelu(dp_tensor: PhiTensor, slope: float=0.01) -> PhiTensor:\n",
    "    # TODO: Should we have an index in DSLs that corresponds to no data?\n",
    "    \n",
    "    gt = (dp_tensor.child > 0)\n",
    "    return PhiTensor(\n",
    "        child= gt * dp_tensor.child + (1 - gt) * dp_tensor.child * slope,\n",
    "        data_subjects=dp_tensor.data_subjects,\n",
    "        min_vals= lra(data=dp_tensor.min_vals.data * slope, shape=dp_tensor.min_vals.shape), \n",
    "        max_vals= lra(data=dp_tensor.max_vals.data * slope, shape=dp_tensor.max_vals.shape),\n",
    "    )\n",
    "\n",
    "\n",
    "class leaky_ReLU():\n",
    "\n",
    "    def __init__(self, slope=0.01):\n",
    "        super(leaky_ReLU, self).__init__()\n",
    "        self.slope = slope\n",
    "\n",
    "    def forward(self, input_array: PhiTensor):\n",
    "        # Last image that has been forward passed through this activation function\n",
    "        self.last_forward = input_array        \n",
    "        return dp_leakyrelu(dp_tensor=input_array, slope=self.slope)\n",
    "\n",
    "    def derivative(self, input_array: Optional[PhiTensor]=None):\n",
    "        last_forward = input_array if input_array else self.last_forward\n",
    "        res = np.ones(last_forward.shape)\n",
    "        idx = last_forward <= 0\n",
    "        res[idx.child] = self.slope\n",
    "        \n",
    "        return PhiTensor(child=res,\n",
    "                         data_subjects=last_forward.data_subjects,\n",
    "                         min_vals=last_forward.min_vals*0,\n",
    "                         max_vals = last_forward.max_vals*1)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6fd353b-f0fe-4423-b919-9961f86e46f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Uniform():\n",
    "    def __init__(self, scale=0.05):\n",
    "        self.scale = scale\n",
    "        \n",
    "    def __call__(self, size):\n",
    "        return self.call(size)\n",
    "\n",
    "    def call(self, size):\n",
    "        return np.array(np.random.uniform(-self.scale, self.scale, size=size))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fcc3c8d-5f35-41ae-9cfd-dee729717732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_size(size):\n",
    "    if len(size) == 2:\n",
    "        fan_in = size[0]\n",
    "        fan_out = size[1]\n",
    "\n",
    "    elif len(size) == 4 or len(size) == 5:\n",
    "        respective_field_size = np.prod(size[2:])\n",
    "        fan_in = size[1] * respective_field_size\n",
    "        fan_out = size[0] * respective_field_size\n",
    "\n",
    "    else:\n",
    "        fan_in = fan_out = int(np.sqrt(np.prod(size)))\n",
    "\n",
    "    return fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43f67671-2b55-43d4-91a2-7165e50601f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitialization():\n",
    "    def __call__(self, size):\n",
    "        return self.call(size)\n",
    "    \n",
    "    def call(self, size):\n",
    "        fan_in, fan_out = decompose_size(size)\n",
    "        return Uniform(np.sqrt(6 / (fan_in + fan_out)))(size)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a656e1d-8003-4529-a985-ff7885cd553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"\n",
    "    Subclassed when implementing new types of layers.\n",
    "    \n",
    "    Each layer can keep track of the layer(s) feeding into it, a\n",
    "    network's output :class:`Layer` instance can double as a handle to the full\n",
    "    network.\n",
    "    \"\"\"\n",
    "\n",
    "    first_layer = False\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\" Layer parameters. \n",
    "        \n",
    "        Returns a list of numpy.array variables or expressions that\n",
    "        parameterize the layer.\n",
    "        Returns\n",
    "        -------\n",
    "        list of numpy.array variables or expressions\n",
    "            A list of variables that parameterize the layer\n",
    "        Notes\n",
    "        -----\n",
    "        For layers without any parameters, this will return an empty list.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        \"\"\" Get layer parameter gradients as calculated from backward(). \"\"\"\n",
    "        return []\n",
    "\n",
    "    @property\n",
    "    def param_grads(self):\n",
    "        \"\"\" Layer parameters and corresponding gradients. \"\"\"\n",
    "        return list(zip(self.params, self.grads))\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449fb14f-9793-49f5-aaa8-498c43e86e71",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "**Note:**\n",
    "\n",
    "In the Convolution layer below, W, dw, b, db all start as numpy arrays.\n",
    "They **should** be PhiTensors- if someone wants to print them, they should have to spend PB.\n",
    "\n",
    "However, when they're initialized in the Convolution layer, they have to be initialized as \n",
    "np arrays because the initial values are public information. But as soon as they are exposed to a DP Tensor, they should convert to a PhiTensor.\n",
    "\n",
    "However we should be able to pass the data to and from various layers to each other.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbcf3cc8-0c49-452f-9c05-ebffded0b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution(Layer):\n",
    "    \"\"\"\n",
    "    If this is the first layer in a model, provide the keyword argument `input_shape`\n",
    "    (tuple of integers, does NOT include the sample axis, N.),\n",
    "    e.g. `input_shape=(3, 128, 128)` for 128x128 RGB pictures.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nb_filter, filter_size, input_shape=None, stride=1):\n",
    "        self.nb_filter = nb_filter\n",
    "        self.filter_size = filter_size\n",
    "        self.input_shape = input_shape\n",
    "        self.stride = stride\n",
    "        \n",
    "        self.W, self.dW = None, None\n",
    "        self.b, self.db = None, None\n",
    "        self.out_shape = None\n",
    "        self.last_output = None\n",
    "        self.last_input = None\n",
    "\n",
    "        self.init = XavierInitialization()\n",
    "        self.activation = leaky_ReLU()\n",
    "\n",
    "    def connect_to(self, prev_layer=None):\n",
    "        if prev_layer is None:\n",
    "            assert self.input_shape is not None\n",
    "            input_shape = self.input_shape\n",
    "        else:\n",
    "            input_shape = prev_layer.out_shape\n",
    "\n",
    "        # input_shape: (batch size, num input feature maps, image height, image width)\n",
    "        assert len(input_shape) == 4\n",
    "\n",
    "        nb_batch, pre_nb_filter, pre_height, pre_width = input_shape\n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        height = (pre_height - filter_height) // self.stride + 1\n",
    "        width = (pre_width - filter_width) // self.stride + 1\n",
    "\n",
    "        # output shape\n",
    "        self.out_shape = (nb_batch, self.nb_filter, height, width)\n",
    "        print(self.out_shape)\n",
    "\n",
    "        # filters\n",
    "        self.W = self.init((self.nb_filter, pre_nb_filter, filter_height, filter_width))\n",
    "        self.b = np.zeros((self.nb_filter,))\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "\n",
    "        self.last_input = input\n",
    "        \n",
    "        # TODO: This could fail if the DP Tensor has < 4 dimensions\n",
    "        \n",
    "        # shape\n",
    "        nb_batch, input_depth, old_img_h, old_img_w = input.shape\n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        new_img_h, new_img_w = self.out_shape[2:]\n",
    "\n",
    "        # init\n",
    "        outputs = np.zeros((nb_batch, self.nb_filter, new_img_h, new_img_w))\n",
    "        \n",
    "        \n",
    "\n",
    "        # convolution operation\n",
    "        for x in np.arange(nb_batch):\n",
    "            for y in np.arange(self.nb_filter):\n",
    "                for h in np.arange(new_img_h):\n",
    "                    for w in np.arange(new_img_w):\n",
    "                        h_shift, w_shift = h * self.stride, w * self.stride\n",
    "                        # patch: (input_depth, filter_h, filter_w)\n",
    "                        patch = input[x, :, h_shift: h_shift + filter_height, w_shift: w_shift + filter_width]\n",
    "                        outputs[x, y, h, w] = np.sum(patch.child * self.W[y]) + self.b[y]\n",
    "\n",
    "        # nonlinear activation\n",
    "        # self.last_output: (nb_batch, output_depth, image height, image width)\n",
    "        \n",
    "        # TODO: Min/max vals are direct function of private data- fix this when we have time\n",
    "        outputs = PhiTensor(\n",
    "            child=outputs,data_subjects=np.zeros_like(outputs), \n",
    "            min_vals=outputs.min(), max_vals=outputs.max()\n",
    "        )\n",
    "        self.last_output = self.activation.forward(outputs)\n",
    "\n",
    "        return self.last_output\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "\n",
    "        # shape\n",
    "        assert pre_grad.shape == self.last_output.shape\n",
    "        nb_batch, input_depth, old_img_h, old_img_w = self.last_input.shape\n",
    "        new_img_h, new_img_w = self.out_shape[2:]\n",
    "        \n",
    "        if isinstance(self.filter_size, tuple):\n",
    "            filter_height, filter_width = self.filter_size\n",
    "        elif isinstance(self.filter_size, int):\n",
    "            filter_height = filter_width = self.filter_size\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "#         filter_h, filter_w = self.filter_size\n",
    "        old_img_h, old_img_w = self.last_input.shape[-2:]\n",
    "\n",
    "        # gradients\n",
    "        self.dW = np.zeros((self.W.shape))\n",
    "        self.db = np.zeros((self.b.shape))\n",
    "        delta = pre_grad * self.activation.derivative()\n",
    "\n",
    "        # dW\n",
    "        for r in np.arange(self.nb_filter):\n",
    "            for t in np.arange(input_depth):\n",
    "                for h in np.arange(filter_height):\n",
    "                    for w in np.arange(filter_width):\n",
    "                        input_window = self.last_input[:, t,\n",
    "                                       h:old_img_h - filter_height + h + 1:self.stride,\n",
    "                                       w:old_img_w - filter_width + w + 1:self.stride]\n",
    "                        delta_window = delta[:, r]\n",
    "                        self.dW[r, t, h, w] = ((input_window * delta_window).sum() * (1/nb_batch)).child\n",
    "        # db\n",
    "        for r in np.arange(self.nb_filter):\n",
    "            self.db[r] = (delta[:, r].sum() * (1/nb_batch)).child\n",
    "        \n",
    "        \n",
    "        # dX\n",
    "        \n",
    "        \n",
    "        if not self.first_layer:\n",
    "            layer_grads = self.last_input.zeros_like()\n",
    "            for b in np.arange(nb_batch):\n",
    "                for r in np.arange(self.nb_filter):\n",
    "                    for t in np.arange(input_depth):\n",
    "                        for h in np.arange(new_img_h):\n",
    "                            for w in np.arange(new_img_w):\n",
    "                                h_shift, w_shift = h * self.stride, w * self.stride\n",
    "                                temp = layer_grads[b, t, h_shift:h_shift + filter_height, w_shift:w_shift + filter_width]\n",
    "                                layer_grads[b, t, h_shift:h_shift + filter_height, w_shift:w_shift + filter_width] = temp+ (delta[b, r, h, w] * self.W[r, t])\n",
    "                              \n",
    "        return layer_grads\n",
    "                         \n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        return self.dW, self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0bfa725-40be-4c48-9cd8-0c583a34c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Convolution(3, 3, input_shape=(1, 3, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6480805b-c623-4f5a-83b8-7bdc0dd3b242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "c.connect_to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "454d6256-d500-4669-a8f4-61d96c4d4879",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PhiTensor(\n",
    "    child=np.random.rand(1, 3, 10, 10), \n",
    "    data_subjects=np.zeros((1,3,10,10)), min_vals=0, max_vals=1)\n",
    "\n",
    "res = c.forward(input=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a24b8863-393e-4346-ad8c-c7e30a7fb045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 8, 8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.last_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b06a0337-2b43-42d1-901c-07c6b81d52f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b_data = PhiTensor(\n",
    "    child=np.random.rand(1,3,8,8), \n",
    "    data_subjects=np.zeros((1,3,8,8)), min_vals=0, max_vals=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fa44336-f164-4ce5-a101-7eb26921fb2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=[[[[ 9.31492822e-04 -1.38889747e-03  5.33828255e-02  5.83014325e-02\n",
       "    -8.29619733e-02  2.38273373e-03 -1.68378156e-03 -4.49446154e-02\n",
       "    -7.78756565e-02  6.91329573e-02]\n",
       "   [-1.03477958e-01 -1.56885281e-01  6.28764538e-02 -3.81822918e-03\n",
       "    -7.82274717e-02  7.36921695e-02  1.30323469e-03 -3.76276973e-02\n",
       "     3.54259435e-02  7.61455258e-02]\n",
       "   [-1.18583400e-01  1.78628719e-02  2.62480935e-01 -2.16422928e-01\n",
       "    -2.53147397e-01  4.79376737e-01 -2.33778440e-01 -4.85660513e-01\n",
       "     1.40010759e-02  1.81417288e-01]\n",
       "   [-6.89776734e-02 -4.07526307e-02 -9.85178242e-02 -2.26671654e-01\n",
       "    -1.27837341e-01  1.45286183e-01 -2.23442271e-01  1.91276698e-01\n",
       "     3.71560401e-01  1.75162334e-01]\n",
       "   [-4.08306633e-01  1.88315575e-01  1.33294275e-01 -3.86724603e-01\n",
       "    -2.22658464e-01  6.12843569e-01 -3.35134431e-01 -9.25257891e-01\n",
       "    -2.53803080e-01  2.00972795e-01]\n",
       "   [ 7.70153500e-02 -3.71166434e-01 -3.81760796e-01  1.67626895e-01\n",
       "    -3.28597139e-01 -4.99608899e-01  1.01830449e-01  8.45637591e-02\n",
       "     7.48134131e-02  1.62474929e-01]\n",
       "   [-1.34152109e-01 -2.53574542e-01  3.84023122e-01  1.21593434e-01\n",
       "    -1.74478551e-02  2.35937190e-01 -7.86547802e-02 -3.03603721e-01\n",
       "     2.41835297e-02  5.21490549e-02]\n",
       "   [-1.27051110e-01 -2.70523567e-01 -2.23900755e-02 -3.75571994e-02\n",
       "    -2.95372533e-01 -3.05783523e-01 -3.42967927e-01  3.54193090e-01\n",
       "     3.14700542e-01 -1.73484398e-01]\n",
       "   [-1.82275407e-01 -1.19450801e-01 -2.15005410e-01  1.13622353e-01\n",
       "     3.14172522e-01 -9.91763014e-02 -2.50835025e-01 -2.66860927e-01\n",
       "     1.64241720e-01  5.51436228e-03]\n",
       "   [ 3.50411490e-04 -1.85589535e-03 -9.73131665e-02 -2.29789581e-01\n",
       "    -3.23752430e-02 -1.01350123e-01 -1.06922386e-01  1.42521462e-01\n",
       "    -4.62609031e-02 -1.09410494e-01]]\n",
       "\n",
       "  [[-4.25018831e-03  1.00536849e-03 -6.63987974e-02  9.40193129e-03\n",
       "    -1.69944585e-02 -2.78473414e-03 -1.62608862e-03 -2.91813908e-02\n",
       "     4.45407194e-02  1.83263298e-02]\n",
       "   [-6.34465998e-02  8.84364292e-02  1.15557841e-02 -7.63819772e-02\n",
       "     7.12092436e-02  1.37252162e-02 -5.19980384e-03 -6.72393071e-02\n",
       "     4.08758597e-02  1.58448715e-03]\n",
       "   [-1.53083389e-01  1.14597421e-01  2.16828986e-02 -1.55470566e-01\n",
       "     2.22339506e-01 -1.37017408e-01 -1.37226137e-02 -1.09471928e-01\n",
       "     1.48472481e-01  4.88522740e-02]\n",
       "   [-2.77761266e-01  6.36509393e-02 -2.39637951e-02 -2.95883454e-01\n",
       "     1.94622111e-01 -2.48444145e-01 -3.88225004e-01 -1.28900881e-01\n",
       "     1.15504192e-01  1.10239976e-02]\n",
       "   [-1.71503405e-01 -1.05719626e-01  1.00586360e-01 -1.42181885e-01\n",
       "    -9.05301785e-02 -1.55620177e-01  1.60461726e-01 -2.56162941e-02\n",
       "     4.77880896e-03  4.11655298e-02]\n",
       "   [ 3.29594583e-02 -1.43602514e-01  3.07197303e-01  9.15066679e-02\n",
       "    -9.03452374e-02  3.35338395e-02  6.28749794e-02 -2.81187378e-02\n",
       "     9.93724727e-02  3.59724480e-03]\n",
       "   [-8.85879256e-02 -1.54027775e-01  2.12961803e-01 -5.48225972e-03\n",
       "    -2.79011935e-01 -1.52941492e-02  6.36089295e-03  2.62631441e-01\n",
       "    -2.23035895e-02 -7.80941861e-03]\n",
       "   [-1.55843472e-01  1.22374997e-01 -3.29685251e-01  1.57577528e-01\n",
       "     2.60129911e-02 -1.64188371e-01 -1.43100907e-01  8.73697852e-03\n",
       "     4.03007971e-02 -2.42124607e-02]\n",
       "   [ 4.21963864e-02 -8.54059805e-02 -2.10316845e-01  1.02784023e-01\n",
       "     6.78194917e-02 -9.41063766e-02  1.94677611e-01 -1.40888619e-01\n",
       "    -8.71451612e-02  5.84584300e-02]\n",
       "   [ 3.79286372e-04  1.96275521e-04  5.52944009e-02 -9.77828907e-02\n",
       "     7.10745303e-02  2.60869778e-02 -4.65806045e-02  1.15904285e-02\n",
       "    -5.21710669e-03  9.66610883e-02]]\n",
       "\n",
       "  [[ 3.16864329e-03  3.14963074e-04 -7.30605574e-02 -4.56951176e-02\n",
       "     4.41675813e-02 -3.62114349e-04 -2.20639901e-03  6.27765447e-02\n",
       "    -4.40803854e-02 -7.64071200e-02]\n",
       "   [ 1.34794821e-01 -9.85371514e-02 -2.18840852e-01  3.23705210e-02\n",
       "    -7.04151912e-02 -8.08968703e-02 -4.51459440e-03  7.35753869e-02\n",
       "     2.21852200e-02  2.55618424e-02]\n",
       "   [ 2.01662149e-01  1.31770767e-02  6.19939401e-02  3.49399410e-01\n",
       "    -1.40001079e-01 -2.82208282e-02  3.82676368e-01 -5.36126035e-02\n",
       "    -4.35983914e-01 -2.11452444e-01]\n",
       "   [-7.67350592e-02 -2.81916768e-01 -2.98973113e-03  4.24377026e-01\n",
       "     1.23208251e-02 -9.77054892e-02  2.81422510e-01  1.15698731e-01\n",
       "     1.60817119e-01  6.40001072e-02]\n",
       "   [-7.95534808e-02 -1.12227237e-01 -4.19402664e-02  2.03038320e-01\n",
       "    -4.60385894e-02 -3.93933984e-01 -1.99539725e-01  3.82046286e-01\n",
       "    -3.10276256e-01 -2.52305000e-01]\n",
       "   [ 2.63981133e-01  3.95521038e-01 -3.19266772e-01 -2.38398629e-01\n",
       "     5.30467319e-02 -7.34414539e-02 -3.85393643e-01 -6.09436837e-02\n",
       "     5.24387750e-02  5.95306284e-02]\n",
       "   [ 1.86458469e-01  4.88220287e-02 -1.63150742e-01  1.53688889e-01\n",
       "     2.61038399e-02  2.70400565e-01  5.43803497e-01 -7.98181747e-02\n",
       "    -2.70142934e-01 -8.44176385e-02]\n",
       "   [ 1.98499121e-01  3.16373392e-02 -3.12831056e-02 -3.43167530e-01\n",
       "    -2.07920314e-01 -1.49765589e-02 -5.35430491e-02 -2.76331384e-01\n",
       "    -2.74056450e-02  9.86226950e-02]\n",
       "   [ 3.93372175e-02  6.17763388e-04  9.73614848e-02 -2.26244236e-02\n",
       "     1.66850713e-01  2.47218352e-01  2.86998888e-02 -1.57912200e-01\n",
       "    -1.71021650e-01 -3.75880373e-02]\n",
       "   [ 8.84406095e-04  1.27126717e-03  1.41128167e-01  3.79828381e-02\n",
       "    -1.32684355e-01  1.89222120e-02 -4.19463591e-02  7.51913277e-02\n",
       "     1.39425402e-01 -4.59072923e-02]]]], min_vals=<lazyrepeatarray data: -0.9252578911351779 -> shape: (1, 3, 10, 10)>, max_vals=<lazyrepeatarray data: 0.6164228365612443 -> shape: (1, 3, 10, 10)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.backward(b_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df10cfa0-63cd-431b-bccd-d5c6b72516cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(Layer):\n",
    "    def __init__(self, epsilon=1e-6, momentum=0.9, axis=0):\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.axis = axis\n",
    "\n",
    "        self.beta, self.dbeta = None, None\n",
    "        self.gamma, self.dgamma = None, None\n",
    "        self.cache = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        n_in = prev_layer.out_shape[-1]\n",
    "        self.beta = np.zeros((n_in,))\n",
    "        self.gamma = np.ones((n_in,))\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "        # N, D = x.shape\n",
    "        self.out_shape = input.shape\n",
    "\n",
    "        # step1: calculate the mean\n",
    "        xmu = input - input.mean(axis=0)\n",
    "        print(\"Subtracted mean\")\n",
    "        # step3:        \n",
    "        var = xmu.std(axis=0)\n",
    "        sqrtvar = (var + self.epsilon).sqrt()\n",
    "    \n",
    "        print(\"square rooted\")\n",
    "        ivar = sqrtvar.reciprocal()\n",
    "#         ivar = PhiTensor(\n",
    "#             child=sqrtvar.child ** -1,\n",
    "#             data_subjects=sqrtvar.data_subjects,\n",
    "#             min_vals=lra(data=1/sqrtvar.min_vals.data,shape=sqrtvar.shape),\n",
    "#             max_vals=lra(data=1/sqrtvar.max_vals.data,shape=sqrtvar.shape)\n",
    "#         )\n",
    "\n",
    "        \n",
    "        print(\"got past reciprocal\")\n",
    "\n",
    "\n",
    "        # step5: normalization->x^\n",
    "        xhat = xmu * ivar\n",
    "        print(\"got xhat\")\n",
    "\n",
    "        # step6: scale and shift\n",
    "        gammax = xhat * self.gamma\n",
    "        print(\"got gammax\")\n",
    "        out = gammax + self.beta\n",
    "        print(\"got out\")\n",
    "\n",
    "        self.cache = (xhat, xmu, ivar, sqrtvar, var)\n",
    "        print(\"cached\")\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        xhat, xmu, ivar, sqrtvar, var = self.cache\n",
    "\n",
    "        N, D = pre_grad.shape\n",
    "\n",
    "        # step6\n",
    "        self.dbeta = np.sum(pre_grad, axis=0)\n",
    "        dgammax = pre_grad\n",
    "        self.dgamma = np.sum(dgammax * xhat, axis=0)\n",
    "        dxhat = dgammax * self.gamma\n",
    "\n",
    "        # step5\n",
    "        divar = np.sum(dxhat * xmu, axis=0)\n",
    "        dxmu1 = dxhat * ivar \n",
    "\n",
    "        # step4\n",
    "        dsqrtvar = -1. / (sqrtvar ** 2) * divar\n",
    "        dvar = 0.5 * 1. / np.sqrt(var + self.epsilon) * dsqrtvar\n",
    "\n",
    "        # step3\n",
    "        dsq = 1. / N * np.ones((N, D)) * dvar\n",
    "        dxmu2 = 2 * xmu * dsq  \n",
    "\n",
    "        # step2, \n",
    "        dx1 = (dxmu1 + dxmu2)\n",
    "\n",
    "        # step1, \n",
    "        dmu = -1 * np.sum(dxmu1 + dxmu2, axis=0)\n",
    "        dx2 = 1. / N * np.ones((N, D)) * dmu\n",
    "\n",
    "        # step0 done!\n",
    "        dx = dx1 + dx2\n",
    "\n",
    "        return dx\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.beta, self.gamma\n",
    "\n",
    "    @property\n",
    "    def grades(self):\n",
    "        return self.dbeta, self.dgamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e178933-c93b-4580-880f-22709d5527f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bnc():\n",
    "    shape = (10, 3, 50, 50)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "\n",
    "    c = Convolution(3, 3, input_shape=shape)\n",
    "    c.connect_to()\n",
    "    bn = BatchNorm()\n",
    "    bn.connect_to(c)\n",
    "    c_out = c.forward(smol_data)\n",
    "    return bn.forward(c_out), bn\n",
    "    \n",
    "# test_bnc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00049879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 48, 48)\n",
      "Subtracted mean\n",
      "square rooted\n",
      "got past reciprocal\n",
      "got xhat\n",
      "got gammax\n",
      "got out\n",
      "cached\n"
     ]
    }
   ],
   "source": [
    "output, bn = test_bnc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "207fbf1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "backward() missing 1 required positional argument: 'pre_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: backward() missing 1 required positional argument: 'pre_grad'"
     ]
    }
   ],
   "source": [
    "bn.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a16e99e-20fa-4702-bf4a-afed82b43525",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgPool(Layer):\n",
    "    \"\"\"Average pooling operation for spatial data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pool_size : tuple of 2 integers,\n",
    "        factors by which to downscale (vertical, horizontal).\n",
    "        (2, 2) will halve the image in each dimension.\n",
    "    Returns\n",
    "    -------\n",
    "    4D numpy.array \n",
    "        with shape `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        self.out_shape = 0\n",
    "        self.out_shape = None\n",
    "        self.input_shape = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        assert 5 > len(prev_layer.out_shape) >= 3\n",
    "\n",
    "        old_h, old_w = prev_layer.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = old_h // pool_h, old_w // pool_w\n",
    "\n",
    "        assert old_h % pool_h == old_w % pool_w == 0\n",
    "\n",
    "        self.out_shape = prev_layer.out_shape[:-2] + (new_h, new_w)\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "\n",
    "        # shape\n",
    "        self.input_shape = input.shape\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "\n",
    "        # forward\n",
    "        outputs = np.zeros(self.input_shape[:-2] + self.out_shape[-2:])\n",
    "        outputs = PhiTensor(child=outputs, data_subjects=np.zeros_like(outputs), min_vals=0, max_vals=1)\n",
    "        \n",
    "        ndim = len(input.shape)\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            outputs[a, b, h, w] = input[a, b, h:h + pool_h, w:w + pool_w].mean()\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        outputs[a, h, w] = np.mean(input[a, h:h + pool_h, w:w + pool_w])\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        length = np.prod(self.pool_size)\n",
    "\n",
    "        layer_grads = np.zeros(self.input_shape)\n",
    "        \n",
    "        ndim = len(pre_grad.shape)\n",
    "\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            h_shift, w_shift = h * pool_h, w * pool_w\n",
    "                            layer_grads[a, b, h_shift: h_shift + pool_h, w_shift: w_shift + pool_w] = \\\n",
    "                                pre_grad[a, b, h, w] / length\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        h_shift, w_shift = h * pool_h, w * pool_w\n",
    "                        layer_grads[a, h_shift: h_shift + pool_h, w_shift: w_shift + pool_w] = \\\n",
    "                            pre_grad[a, h, w] / length\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return layer_grads\n",
    "\n",
    "\n",
    "class MaxPool(Layer):\n",
    "    \"\"\"Max pooling operation for spatial data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    pool_size : tuple of 2 integers,\n",
    "        factors by which to downscale (vertical, horizontal).\n",
    "        (2, 2) will halve the image in each dimension.\n",
    "    Returns\n",
    "    -------\n",
    "    4D numpy.array \n",
    "        with shape `(nb_samples, channels, pooled_rows, pooled_cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, pooled_rows, pooled_cols, channels)` if dim_ordering='tf'.\n",
    "    \"\"\"\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "        self.input_shape = None\n",
    "        self.out_shape = None\n",
    "        self.last_input = None\n",
    "\n",
    "    def connect_to(self, prev_layer):\n",
    "        # prev_layer.out_shape: (nb_batch, ..., height, width)\n",
    "        assert len(prev_layer.out_shape) >= 3\n",
    "\n",
    "        old_h, old_w = prev_layer.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = old_h // pool_h, old_w // pool_w\n",
    "\n",
    "        assert old_h % pool_h == old_w % pool_w == 0\n",
    "\n",
    "        self.out_shape = prev_layer.out_shape[:-2] + (new_h, new_w)\n",
    "\n",
    "    def forward(self, input, *args, **kwargs):\n",
    "        # shape\n",
    "        self.input_shape = input.shape\n",
    "        pool_h, pool_w = self.pool_size\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "\n",
    "        # forward\n",
    "        self.last_input = input\n",
    "        outputs = np.zeros(self.input_shape[:-2] + self.out_shape[-2:])\n",
    "        outputs = PhiTensor(child=outputs, data_subjects=np.zeros_like(outputs), min_vals=0, max_vals=1)\n",
    "        \n",
    "        ndim = len(input.shape)\n",
    "\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            outputs[a, b, h, w] = input[a, b, h:h + pool_h, w:w + pool_w].max()\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = input.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        outputs[a, h, w] = input[a, h:h + pool_h, w:w + pool_w].max()\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        new_h, new_w = self.out_shape[-2:]\n",
    "        pool_h, pool_w = self.pool_size\n",
    "\n",
    "        layer_grads = np.zeros(self.input_shape)\n",
    "        \n",
    "        ndim = len(pre_grad.shape)\n",
    "\n",
    "        if ndim == 4:\n",
    "            nb_batch, nb_axis, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for b in np.arange(nb_axis):\n",
    "                    for h in np.arange(new_h):\n",
    "                        for w in np.arange(new_w):\n",
    "                            patch = self.last_input[a, b, h:h + pool_h, w:w + pool_w]\n",
    "                            max_idx = np.unravel_index(patch.argmax(), patch.shape)\n",
    "                            h_shift, w_shift = h * pool_h + max_idx[0], w * pool_w + max_idx[1]\n",
    "                            layer_grads[a, b, h_shift, w_shift] = pre_grad[a, b, a, w]\n",
    "\n",
    "        elif ndim == 3:\n",
    "            nb_batch, _, _ = pre_grad.shape\n",
    "\n",
    "            for a in np.arange(nb_batch):\n",
    "                for h in np.arange(new_h):\n",
    "                    for w in np.arange(new_w):\n",
    "                        patch = self.last_input[a, h:h + pool_h, w:w + pool_w]\n",
    "                        max_idx = np.unravel_index(patch.argmax(), patch.shape)\n",
    "                        h_shift, w_shift = h * pool_h + max_idx[0], w * pool_w + max_idx[1]\n",
    "                        layer_grads[a, h_shift, w_shift] = pre_grad[a, a, w]\n",
    "\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "        return layer_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a508b16d-6fa3-425d-9898-84171cd41258",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = AvgPool((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab5506a6-4225-4d5d-b121-406d41b930d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_c_bn_avg():\n",
    "    shape = (10, 3, 6, 6)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "\n",
    "    c = Convolution(3, 3, input_shape=shape)\n",
    "    c.connect_to()\n",
    "    bn = BatchNorm()\n",
    "    bn.connect_to(c)\n",
    "    c_out = c.forward(smol_data)\n",
    "    bn_out = bn.forward(c_out)\n",
    "    avg = AvgPool((2,2))\n",
    "    avg.connect_to(bn)\n",
    "    return avg.forward(bn_out)\n",
    "    \n",
    "# test_c_bn_avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e97b8c41-ef8b-421a-bc4e-52ea7f7ad342",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxp = MaxPool((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4f035d2e-ecae-4fad-985e-4c9fa6a683bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_c_bn_max():\n",
    "    shape = (10, 3, 6, 6)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "\n",
    "    c = Convolution(3, 3, input_shape=shape)\n",
    "    c.connect_to()\n",
    "    bn = BatchNorm()\n",
    "    bn.connect_to(c)\n",
    "    c_out = c.forward(smol_data)\n",
    "    bn_out = bn.forward(c_out)\n",
    "    maxp = MaxPool((2,2))\n",
    "    maxp.connect_to(bn)\n",
    "    return maxp.forward(bn_out)\n",
    "    \n",
    "# test_c_bn_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b41a5cc-2098-438e-9e7c-d9978fadcdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self, n_out, n_in=None):\n",
    "        self.n_out = n_out\n",
    "        self.n_in = n_in\n",
    "        self.out_shape = (None, n_out)\n",
    "\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.last_input = None\n",
    "        self.init = XavierInitialization()\n",
    "\n",
    "    def connect_to(self, prev_layer=None):\n",
    "        if prev_layer is None:\n",
    "            assert self.n_in is not None\n",
    "            n_in = self.n_in\n",
    "        else:\n",
    "            assert len(prev_layer.out_shape) == 2\n",
    "            n_in = prev_layer.out_shape[-1]\n",
    "\n",
    "        self.W = self.init((n_in, self.n_out))\n",
    "        self.b = np.zeros((self.n_out,))\n",
    "\n",
    "    def forward(self, input: PhiTensor, *args, **kwargs):\n",
    "        self.last_input = input\n",
    "        return input.dot(self.W) + self.b\n",
    "\n",
    "    def backward(self, pre_grad, *args, **kwargs):\n",
    "        self.dW = np.dot(self.last_input.T, pre_grad)\n",
    "        self.db = np.mean(pre_grad, axis=0)\n",
    "        if not self.first_layer:\n",
    "            return np.dot(pre_grad, self.W.T)\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        return self.W, self.b\n",
    "\n",
    "    @property\n",
    "    def grads(self):\n",
    "        return self.dW, self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f0d766d-997b-4360-bd26-77ca4b512447",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = Linear(n_out=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28e785a6-4581-464a-8cab-e411bbbe6f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=[[[[ -90.47214248   23.3897904 ]\n",
       "   [ 111.51206997 -203.96759178]\n",
       "   [ 142.02613993  -53.87684007]\n",
       "   [ 214.40639819 -209.31801496]\n",
       "   [ -71.65385195 -161.70682198]\n",
       "   [ -73.31078007  -40.74926085]]\n",
       "\n",
       "  [[-181.03601562  -20.9746452 ]\n",
       "   [-140.80802528   63.11897089]\n",
       "   [ -93.8570165    51.12933977]\n",
       "   [  30.83755387   60.42247999]\n",
       "   [  59.35318544  -76.5892349 ]\n",
       "   [  17.70826383 -191.34744185]]\n",
       "\n",
       "  [[ 140.59425713  -88.6172595 ]\n",
       "   [  38.88695864 -180.40664227]\n",
       "   [-159.14259351 -124.513612  ]\n",
       "   [ -91.64542839 -101.8473201 ]\n",
       "   [  16.92237531 -135.41852746]\n",
       "   [ 171.64210411  -68.96951773]]]\n",
       "\n",
       "\n",
       " [[[-110.7066096     8.07421268]\n",
       "   [  11.05696826 -179.9241417 ]\n",
       "   [ -21.16225862 -172.0154797 ]\n",
       "   [ -62.45333344 -161.48257961]\n",
       "   [  49.27924808  -53.67392981]\n",
       "   [ 228.08001412 -154.54258873]]\n",
       "\n",
       "  [[-142.82245352  -39.25095826]\n",
       "   [ 105.45209226  -67.13951662]\n",
       "   [  26.0520003  -211.37744826]\n",
       "   [ 126.9127082  -131.25920061]\n",
       "   [-121.12972737  -88.56196074]\n",
       "   [ -24.94867344 -114.01410356]]\n",
       "\n",
       "  [[ -22.92391162 -180.73835237]\n",
       "   [ 134.83234799 -169.10877529]\n",
       "   [  48.59827493 -118.22319998]\n",
       "   [ 126.83191676 -105.77138665]\n",
       "   [ -46.54877136   58.44196277]\n",
       "   [ 211.58183194 -224.80525631]]]\n",
       "\n",
       "\n",
       " [[[-126.4040694  -115.94553895]\n",
       "   [  89.8227781  -200.64635282]\n",
       "   [ 107.64746798 -240.72625704]\n",
       "   [  75.86812014 -187.02128645]\n",
       "   [ -62.44549435 -135.2714754 ]\n",
       "   [ -60.64375485   46.42812874]]\n",
       "\n",
       "  [[ 117.86315204 -116.65306062]\n",
       "   [  38.52989165   25.33109682]\n",
       "   [ 136.07549761 -145.1719777 ]\n",
       "   [  -7.16539777 -117.67031522]\n",
       "   [ -91.67291117 -182.15291064]\n",
       "   [-144.24832173  -46.30998837]]\n",
       "\n",
       "  [[ 107.46746111 -271.3688289 ]\n",
       "   [ -76.85987903 -105.29937835]\n",
       "   [-208.73451153  -53.32229942]\n",
       "   [ 102.27582998 -149.75986764]\n",
       "   [ -51.02045161  -97.65742468]\n",
       "   [  92.80271221  -75.39520853]]]\n",
       "\n",
       "\n",
       " [[[ -96.59501296 -168.4675242 ]\n",
       "   [ 102.30553837 -200.61425073]\n",
       "   [  59.74100189 -138.16996902]\n",
       "   [ 151.14136445 -233.86544357]\n",
       "   [   1.72683967  -94.74859475]\n",
       "   [-124.44021127  -72.09873437]]\n",
       "\n",
       "  [[-125.45725092  -69.66051755]\n",
       "   [ -42.30472554   22.43466611]\n",
       "   [ -97.18921321 -109.77339789]\n",
       "   [  66.10443985  -80.63156591]\n",
       "   [ 123.5664402    -2.20950851]\n",
       "   [  64.55918756 -118.05602703]]\n",
       "\n",
       "  [[-277.87481685   -9.36574371]\n",
       "   [  18.24232722  -14.67259044]\n",
       "   [ 129.93055882  -70.59093535]\n",
       "   [  75.98540376 -142.13753789]\n",
       "   [  33.19685879 -186.21469712]\n",
       "   [ -59.48962113  -73.393637  ]]]\n",
       "\n",
       "\n",
       " [[[ -67.27332376 -191.87572665]\n",
       "   [ 177.60716023 -235.24360146]\n",
       "   [  -2.76671125 -135.98664298]\n",
       "   [ -48.09420837  -65.57021562]\n",
       "   [  70.95263944  -66.32805113]\n",
       "   [ 161.51352372 -165.39882101]]\n",
       "\n",
       "  [[ -73.5069564  -148.49101845]\n",
       "   [-168.04497339  -23.76007579]\n",
       "   [  43.76785635 -188.23597958]\n",
       "   [  52.37256465   57.12312754]\n",
       "   [  51.17434917 -158.2841806 ]\n",
       "   [ 194.4791274  -163.02484659]]\n",
       "\n",
       "  [[  36.28813732  -66.45322288]\n",
       "   [-107.37794671   77.97912528]\n",
       "   [ -47.80797033 -147.12936756]\n",
       "   [  71.90818402   15.01344434]\n",
       "   [ 163.92710036  -71.35864447]\n",
       "   [ 108.67557345 -258.04503993]]]\n",
       "\n",
       "\n",
       " [[[  28.99091555   -4.94262799]\n",
       "   [  98.29538745  -63.64936582]\n",
       "   [ -30.46144073   77.61641057]\n",
       "   [  -9.54488179  -29.93528501]\n",
       "   [ -20.30774675 -135.71306344]\n",
       "   [ -25.63507822  -96.78547048]]\n",
       "\n",
       "  [[ -77.3251173   -97.73596341]\n",
       "   [ 165.73707866 -121.12388308]\n",
       "   [  80.53945773 -197.95313695]\n",
       "   [ 120.70278115  -26.55859442]\n",
       "   [ -24.30419902 -107.89830449]\n",
       "   [  -8.92977443 -130.26707219]]\n",
       "\n",
       "  [[ -37.56294064  -40.43509583]\n",
       "   [-243.17242825  -45.08077026]\n",
       "   [ -52.22725874 -104.53355314]\n",
       "   [ -25.49313374    3.9756897 ]\n",
       "   [ 129.74550872  -87.64549687]\n",
       "   [-109.52174667  -42.0747862 ]]]\n",
       "\n",
       "\n",
       " [[[  32.25733467  -14.66890463]\n",
       "   [ -37.15575856 -220.69905405]\n",
       "   [  67.943184     -6.35356309]\n",
       "   [ -11.22764294  -30.9426923 ]\n",
       "   [ -35.21196839 -167.25503836]\n",
       "   [ 125.96924624 -111.37913366]]\n",
       "\n",
       "  [[ -61.50674854   29.04679439]\n",
       "   [  64.40955549 -194.9171157 ]\n",
       "   [ 206.60926875  -66.33744958]\n",
       "   [ -33.66664871 -147.56940205]\n",
       "   [ 130.54807434  -48.46986783]\n",
       "   [ 130.91683731 -167.17193495]]\n",
       "\n",
       "  [[ -32.40753575 -206.23737879]\n",
       "   [  33.93961143  -95.77508246]\n",
       "   [-102.15265778  -67.82634932]\n",
       "   [  94.45272943  -62.17686765]\n",
       "   [ 202.18103434  -48.90840656]\n",
       "   [ 149.94120874 -100.42396747]]]\n",
       "\n",
       "\n",
       " [[[ 131.5065167  -153.88759966]\n",
       "   [ 121.73763903 -110.80498739]\n",
       "   [  81.06439924  -28.18340374]\n",
       "   [  42.05395228 -226.88729944]\n",
       "   [ -31.84412672 -102.66879957]\n",
       "   [   4.01301032 -137.02298968]]\n",
       "\n",
       "  [[  24.72832399  -75.08620947]\n",
       "   [ 206.93072359 -192.00709281]\n",
       "   [ 111.53412373  -99.23043329]\n",
       "   [ -13.51687153  -81.18309602]\n",
       "   [ -84.75599175 -173.94321463]\n",
       "   [ -80.14755866  -52.26954504]]\n",
       "\n",
       "  [[ 192.41644359 -155.0080762 ]\n",
       "   [   3.96725542  -85.13583437]\n",
       "   [  65.1017714   -60.04698632]\n",
       "   [ -78.14888343  -25.03027261]\n",
       "   [-176.40082783  -91.59219977]\n",
       "   [ -27.39601896  -29.16766927]]]\n",
       "\n",
       "\n",
       " [[[-126.91555133   38.18612344]\n",
       "   [ -95.35657217 -143.86762295]\n",
       "   [  54.71866046  -85.79968638]\n",
       "   [  61.72759322   52.75678552]\n",
       "   [  32.97119501 -218.58413157]\n",
       "   [ -93.18701153 -131.68585906]]\n",
       "\n",
       "  [[   8.54699929 -187.57115701]\n",
       "   [ -49.76160319 -135.91436549]\n",
       "   [ 228.36134456 -185.68546612]\n",
       "   [  -0.60151833 -168.15630473]\n",
       "   [  55.19248171 -114.52276273]\n",
       "   [-165.04572567    1.9336041 ]]\n",
       "\n",
       "  [[-100.85653156  -48.11551565]\n",
       "   [  40.589449    -83.27898807]\n",
       "   [ -67.29944062    4.35767608]\n",
       "   [ 162.5952208  -141.47156583]\n",
       "   [ -35.64019755 -156.74532972]\n",
       "   [ 117.97347894 -295.26316357]]]\n",
       "\n",
       "\n",
       " [[[  20.27766724  -54.40875253]\n",
       "   [ -61.23595446   -2.10141632]\n",
       "   [-161.65731971   69.52254239]\n",
       "   [ -34.30987356  -54.72688646]\n",
       "   [ 165.84082636  -23.63284941]\n",
       "   [ -63.03704647    2.34982599]]\n",
       "\n",
       "  [[ -60.22650762  -63.53796938]\n",
       "   [ -86.85992104 -179.84399814]\n",
       "   [ 149.91208622  -18.08675382]\n",
       "   [ -72.48770325  -38.53755866]\n",
       "   [ -28.26495039 -147.83940713]\n",
       "   [ -61.87017859  -26.30796372]]\n",
       "\n",
       "  [[ -78.20558027  -65.81859892]\n",
       "   [-120.41682525  -81.13134849]\n",
       "   [ 132.60281726  -74.30386022]\n",
       "   [ 109.5770202   -34.68761964]\n",
       "   [ -74.87605046 -103.54200001]\n",
       "   [ 160.36756902 -155.73750113]]]], min_vals=<lazyrepeatarray data: [[<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 0.0 -> shape: (10, 3, 6, 6)>]] -> shape: (10, 3, 6, 2)>, max_vals=<lazyrepeatarray data: [[<lazyrepeatarray data: 21.06145515204588 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: -97.91327377928256 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 194.8517983198912 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: -174.16399640529295 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: 218.8138549072655 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: -51.27025720995975 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: -192.1965655545029 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 74.19432430436072 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: -183.43369993174738 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: -93.10354498488516 -> shape: (10, 3, 6, 6)>]\n",
       " [<lazyrepeatarray data: -21.39820494788025 -> shape: (10, 3, 6, 6)>\n",
       "  <lazyrepeatarray data: 150.65694764236653 -> shape: (10, 3, 6, 6)>]] -> shape: (10, 3, 6, 2)>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_lin():\n",
    "    shape = (10, 3, 6, 6)\n",
    "\n",
    "    smol_data = PhiTensor(\n",
    "        child=np.random.rand(*shape)*255, \n",
    "        data_subjects=np.zeros(shape), min_vals=0, max_vals=255)\n",
    "    \n",
    "    lin = Linear(n_out=2)\n",
    "    lin.n_in = 6\n",
    "    lin.connect_to()\n",
    "    return lin.forward(smol_data)\n",
    "    \n",
    "test_lin()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2814059b-a806-4aa6-b8b9-6542bd98344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    \"\"\"Abstract optimizer base class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clip : float\n",
    "        If smaller than 0, do not apply parameter clip.\n",
    "    lr : float\n",
    "        The learning rate controlling the size of update steps\n",
    "    decay : float\n",
    "        Decay parameter for the moving average. Must lie in [0, 1) where\n",
    "        lower numbers means a shorter memory.\n",
    "    lr_min : float\n",
    "        When adapting step rates, do not move below this value. Default is 0.\n",
    "    lr_max : float\n",
    "        When adapting step rates, do not move above this value. Default is inf.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, clip=-1, decay=0., lr_min=0., lr_max=np.inf):\n",
    "        self.lr = lr\n",
    "        self.clip = clip\n",
    "        self.decay = decay\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "\n",
    "        self.iterations = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.iterations += 1\n",
    "\n",
    "        self.lr *= (1. / 1 + self.decay * self.iterations)\n",
    "        self.lr = np.clip(self.lr, self.lr_min, self.lr_max)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db57ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_maximum(x, y):\n",
    "    x_data = x.child\n",
    "    y_data = y.child if hasattr(y, \"child\") else y\n",
    "    \n",
    "    output = np.maximum(x_data, y_data)\n",
    "    min_v, max_v = output.min(), output.max()\n",
    "    dsl = DataSubjectList(\n",
    "            one_hot_lookup=x.data_subjects.one_hot_lookup,\n",
    "            data_subjects_indexed=np.zeros_like(output)\n",
    "        )\n",
    "    return PhiTensor(\n",
    "        child=output,\n",
    "        data_subjects=dsl,\n",
    "        min_vals=min_v,\n",
    "        max_vals=max_v,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f17351b5-fd14-4dce-910a-a7591f7f548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adamax(Optimizer):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    beta1 : float\n",
    "        Exponential decay rate for the first moment estimates.\n",
    "    beta2 : float\n",
    "        Exponential decay rate for the second moment estimates.\n",
    "    epsilon : float\n",
    "        Constant for numerical stability.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] Kingma, Diederik, and Jimmy Ba (2014):\n",
    "           Adam: A Method for Stochastic Optimization.\n",
    "           arXiv preprint arXiv:1412.6980.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, beta1=0.9, beta2=0.999, epsilon=1e-8, *args, **kwargs):\n",
    "        super(Adamax, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.ms = None\n",
    "        self.vs = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        # init\n",
    "        self.iterations += 1\n",
    "        a_t = self.lr / (1 - np.power(self.beta1, self.iterations))\n",
    "        if self.ms is None:\n",
    "            self.ms = [p.zeros_like() for p in params]\n",
    "        if self.vs is None:\n",
    "            self.vs = [p.zeros_like() for p in params]\n",
    "\n",
    "        # update parameters\n",
    "        for i, (m, v, p, g) in enumerate(zip(self.ms, self.vs, params, grads)):\n",
    "            m = m * self.beta1 + g * (1 - self.beta1)\n",
    "            v = dp_maximum(v * self.beta2, g.abs())\n",
    "            p = p - m * (v + self.epsilon).reciprocal() * a_t\n",
    "\n",
    "            self.ms[i] = m\n",
    "            self.vs[i] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0146b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adamax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13c615e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.update(b_data, b_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ba13f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhiTensor(child=[[[0.07334236 0.04888557 0.05925477 0.01540591 0.05235092 0.05017593\n",
       "    0.00738318 0.09245465]\n",
       "   [0.09062626 0.01186943 0.03049573 0.06551239 0.09921528 0.03092944\n",
       "    0.05517609 0.03263456]\n",
       "   [0.08760598 0.02943837 0.09207491 0.08242253 0.01906886 0.07011938\n",
       "    0.07991179 0.02517367]\n",
       "   [0.02085738 0.02195942 0.07643256 0.02554922 0.00688869 0.09684498\n",
       "    0.09395584 0.0524662 ]\n",
       "   [0.0843923  0.01962767 0.08700034 0.05530068 0.08786439 0.05875747\n",
       "    0.06293604 0.04402708]\n",
       "   [0.00383091 0.00345442 0.04663927 0.05150891 0.01207804 0.08751121\n",
       "    0.01334211 0.002317  ]\n",
       "   [0.0629998  0.0451505  0.01376046 0.0850425  0.0085138  0.01939398\n",
       "    0.07948224 0.04319725]\n",
       "   [0.01518107 0.02053355 0.03974134 0.01992281 0.03085114 0.09713843\n",
       "    0.05893768 0.08017457]]\n",
       " \n",
       "  [[0.04832742 0.01265369 0.02798774 0.06815135 0.06394597 0.06861843\n",
       "    0.06842293 0.00170035]\n",
       "   [0.07607203 0.08886999 0.08561758 0.06191978 0.00772804 0.08650645\n",
       "    0.01018768 0.0413997 ]\n",
       "   [0.06703149 0.0861902  0.08131514 0.09448637 0.06771183 0.05932877\n",
       "    0.08638086 0.0968151 ]\n",
       "   [0.09960105 0.03673101 0.05806405 0.03259458 0.08693456 0.08803146\n",
       "    0.05852643 0.00380336]\n",
       "   [0.07534974 0.09377882 0.02043865 0.06131589 0.05814848 0.06274285\n",
       "    0.0408272  0.07543119]\n",
       "   [0.05547314 0.04415108 0.07945596 0.00629276 0.07648227 0.07039625\n",
       "    0.07794584 0.07868242]\n",
       "   [0.04666486 0.03401251 0.00762476 0.07149256 0.05590091 0.04113231\n",
       "    0.07726703 0.06133625]\n",
       "   [0.0399589  0.0420583  0.04571435 0.09999158 0.04530032 0.02630251\n",
       "    0.05716434 0.0564579 ]]\n",
       " \n",
       "  [[0.09847269 0.02162832 0.014408   0.08723735 0.0059658  0.0720172\n",
       "    0.00871812 0.0245496 ]\n",
       "   [0.05163996 0.0201681  0.07969659 0.0272013  0.01310837 0.07977479\n",
       "    0.00849073 0.02145721]\n",
       "   [0.01725207 0.059442   0.00063743 0.07692137 0.09433882 0.03952115\n",
       "    0.09458925 0.05776598]\n",
       "   [0.03824323 0.04618841 0.08942101 0.04385812 0.00431481 0.01175737\n",
       "    0.01783342 0.00253205]\n",
       "   [0.0508743  0.05673156 0.07083307 0.00686304 0.09861043 0.02781418\n",
       "    0.0209404  0.05388719]\n",
       "   [0.00801172 0.08130216 0.02658721 0.0726996  0.05893828 0.06028006\n",
       "    0.05104776 0.04933033]\n",
       "   [0.06320595 0.00762364 0.03082791 0.08745782 0.04154974 0.0370354\n",
       "    0.06711082 0.04506564]\n",
       "   [0.03372594 0.05763373 0.06573994 0.09840303 0.04689387 0.03483718\n",
       "    0.08041248 0.06047197]]], min_vals=<lazyrepeatarray data: [[[0.07334236 0.04888557 0.05925477 0.01540591 0.05235092 0.05017593\n",
       "    0.00738318 0.09245465]\n",
       "   [0.09062626 0.01186943 0.03049573 0.06551239 0.09921528 0.03092944\n",
       "    0.05517609 0.03263456]\n",
       "   [0.08760598 0.02943837 0.09207491 0.08242253 0.01906886 0.07011938\n",
       "    0.07991179 0.02517367]\n",
       "   [0.02085738 0.02195942 0.07643256 0.02554922 0.00688869 0.09684498\n",
       "    0.09395584 0.0524662 ]\n",
       "   [0.0843923  0.01962767 0.08700034 0.05530068 0.08786439 0.05875747\n",
       "    0.06293604 0.04402708]\n",
       "   [0.00383091 0.00345442 0.04663927 0.05150891 0.01207804 0.08751121\n",
       "    0.01334211 0.002317  ]\n",
       "   [0.0629998  0.0451505  0.01376046 0.0850425  0.0085138  0.01939398\n",
       "    0.07948224 0.04319725]\n",
       "   [0.01518107 0.02053355 0.03974134 0.01992281 0.03085114 0.09713843\n",
       "    0.05893768 0.08017457]]\n",
       " \n",
       "  [[0.04832742 0.01265369 0.02798774 0.06815135 0.06394597 0.06861843\n",
       "    0.06842293 0.00170035]\n",
       "   [0.07607203 0.08886999 0.08561758 0.06191978 0.00772804 0.08650645\n",
       "    0.01018768 0.0413997 ]\n",
       "   [0.06703149 0.0861902  0.08131514 0.09448637 0.06771183 0.05932877\n",
       "    0.08638086 0.0968151 ]\n",
       "   [0.09960105 0.03673101 0.05806405 0.03259458 0.08693456 0.08803146\n",
       "    0.05852643 0.00380336]\n",
       "   [0.07534974 0.09377882 0.02043865 0.06131589 0.05814848 0.06274285\n",
       "    0.0408272  0.07543119]\n",
       "   [0.05547314 0.04415108 0.07945596 0.00629276 0.07648227 0.07039625\n",
       "    0.07794584 0.07868242]\n",
       "   [0.04666486 0.03401251 0.00762476 0.07149256 0.05590091 0.04113231\n",
       "    0.07726703 0.06133625]\n",
       "   [0.0399589  0.0420583  0.04571435 0.09999158 0.04530032 0.02630251\n",
       "    0.05716434 0.0564579 ]]\n",
       " \n",
       "  [[0.09847269 0.02162832 0.014408   0.08723735 0.0059658  0.0720172\n",
       "    0.00871812 0.0245496 ]\n",
       "   [0.05163996 0.0201681  0.07969659 0.0272013  0.01310837 0.07977479\n",
       "    0.00849073 0.02145721]\n",
       "   [0.01725207 0.059442   0.00063743 0.07692137 0.09433882 0.03952115\n",
       "    0.09458925 0.05776598]\n",
       "   [0.03824323 0.04618841 0.08942101 0.04385812 0.00431481 0.01175737\n",
       "    0.01783342 0.00253205]\n",
       "   [0.0508743  0.05673156 0.07083307 0.00686304 0.09861043 0.02781418\n",
       "    0.0209404  0.05388719]\n",
       "   [0.00801172 0.08130216 0.02658721 0.0726996  0.05893828 0.06028006\n",
       "    0.05104776 0.04933033]\n",
       "   [0.06320595 0.00762364 0.03082791 0.08745782 0.04154974 0.0370354\n",
       "    0.06711082 0.04506564]\n",
       "   [0.03372594 0.05763373 0.06573994 0.09840303 0.04689387 0.03483718\n",
       "    0.08041248 0.06047197]]] -> shape: (3, 8, 8)>, max_vals=<lazyrepeatarray data: [[[0.07334236 0.04888557 0.05925477 0.01540591 0.05235092 0.05017593\n",
       "    0.00738318 0.09245465]\n",
       "   [0.09062626 0.01186943 0.03049573 0.06551239 0.09921528 0.03092944\n",
       "    0.05517609 0.03263456]\n",
       "   [0.08760598 0.02943837 0.09207491 0.08242253 0.01906886 0.07011938\n",
       "    0.07991179 0.02517367]\n",
       "   [0.02085738 0.02195942 0.07643256 0.02554922 0.00688869 0.09684498\n",
       "    0.09395584 0.0524662 ]\n",
       "   [0.0843923  0.01962767 0.08700034 0.05530068 0.08786439 0.05875747\n",
       "    0.06293604 0.04402708]\n",
       "   [0.00383091 0.00345442 0.04663927 0.05150891 0.01207804 0.08751121\n",
       "    0.01334211 0.002317  ]\n",
       "   [0.0629998  0.0451505  0.01376046 0.0850425  0.0085138  0.01939398\n",
       "    0.07948224 0.04319725]\n",
       "   [0.01518107 0.02053355 0.03974134 0.01992281 0.03085114 0.09713843\n",
       "    0.05893768 0.08017457]]\n",
       " \n",
       "  [[0.04832742 0.01265369 0.02798774 0.06815135 0.06394597 0.06861843\n",
       "    0.06842293 0.00170035]\n",
       "   [0.07607203 0.08886999 0.08561758 0.06191978 0.00772804 0.08650645\n",
       "    0.01018768 0.0413997 ]\n",
       "   [0.06703149 0.0861902  0.08131514 0.09448637 0.06771183 0.05932877\n",
       "    0.08638086 0.0968151 ]\n",
       "   [0.09960105 0.03673101 0.05806405 0.03259458 0.08693456 0.08803146\n",
       "    0.05852643 0.00380336]\n",
       "   [0.07534974 0.09377882 0.02043865 0.06131589 0.05814848 0.06274285\n",
       "    0.0408272  0.07543119]\n",
       "   [0.05547314 0.04415108 0.07945596 0.00629276 0.07648227 0.07039625\n",
       "    0.07794584 0.07868242]\n",
       "   [0.04666486 0.03401251 0.00762476 0.07149256 0.05590091 0.04113231\n",
       "    0.07726703 0.06133625]\n",
       "   [0.0399589  0.0420583  0.04571435 0.09999158 0.04530032 0.02630251\n",
       "    0.05716434 0.0564579 ]]\n",
       " \n",
       "  [[0.09847269 0.02162832 0.014408   0.08723735 0.0059658  0.0720172\n",
       "    0.00871812 0.0245496 ]\n",
       "   [0.05163996 0.0201681  0.07969659 0.0272013  0.01310837 0.07977479\n",
       "    0.00849073 0.02145721]\n",
       "   [0.01725207 0.059442   0.00063743 0.07692137 0.09433882 0.03952115\n",
       "    0.09458925 0.05776598]\n",
       "   [0.03824323 0.04618841 0.08942101 0.04385812 0.00431481 0.01175737\n",
       "    0.01783342 0.00253205]\n",
       "   [0.0508743  0.05673156 0.07083307 0.00686304 0.09861043 0.02781418\n",
       "    0.0209404  0.05388719]\n",
       "   [0.00801172 0.08130216 0.02658721 0.0726996  0.05893828 0.06028006\n",
       "    0.05104776 0.04933033]\n",
       "   [0.06320595 0.00762364 0.03082791 0.08745782 0.04154974 0.0370354\n",
       "    0.06711082 0.04506564]\n",
       "   [0.03372594 0.05763373 0.06573994 0.09840303 0.04689387 0.03483718\n",
       "    0.08041248 0.06047197]]] -> shape: (3, 8, 8)>)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a671665c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_log(input: PhiTensor):\n",
    "    data = input.child\n",
    "    \n",
    "    output = np.log(data)\n",
    "    min_v, max_v = output.min(), output.max()\n",
    "    dsl = DataSubjectList(\n",
    "            one_hot_lookup=input.data_subjects.one_hot_lookup,\n",
    "            data_subjects_indexed=np.zeros_like(output)\n",
    "        )\n",
    "    return PhiTensor(\n",
    "        child=output,\n",
    "        data_subjects=dsl,\n",
    "        min_vals=min_v,\n",
    "        max_vals=max_v,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "735de6c7-43c7-49ec-aa2b-b40106a63452",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryCrossEntropy():\n",
    "    def __init__(self, epsilon=1e-11):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        .. math:: L = -t \\\\log(p) - (1 - t) \\\\log(1 - p)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : numpy.array\n",
    "            Predictions in (0, 1), such as sigmoidal output of a neural network.\n",
    "        targets : numpy.array\n",
    "            Targets in [0, 1], such as ground truth labels.\n",
    "        \"\"\"\n",
    "        outputs = outputs.clip(self.epsilon, 1 - self.epsilon)\n",
    "        log_loss = targets * dp_log(outputs) + ((targets * -1) + 1) * dp_log((outputs * -1) + 1)\n",
    "        log_loss = log_loss.sum(axis=1) * -1\n",
    "        return log_loss.mean()\n",
    "\n",
    "    def backward(self, outputs: PhiTensor, targets: PhiTensor):\n",
    "        \"\"\"Backward pass.\n",
    "        Parameters\n",
    "        ----------\n",
    "        outputs : numpy.array\n",
    "            Predictions in (0, 1), such as sigmoidal output of a neural network.\n",
    "        targets : numpy.array\n",
    "            Targets in [0, 1], such as ground truth labels.\n",
    "        \"\"\"\n",
    "        outputs = outputs.clip(self.epsilon, 1 - self.epsilon)\n",
    "        divisor = dp_maximum(outputs * ((outputs * -1) + 1), self.epsilon)\n",
    "        return (outputs - targets) * divisor.reciprocal()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "745f76fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = BinaryCrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58ecf885",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (2, 10)\n",
    "target = PhiTensor(child=np.random.randint(low=0, high=2, size=input_shape),\n",
    "              data_subjects=np.zeros(input_shape),\n",
    "              min_vals=0,\n",
    "              max_vals=1\n",
    "         )\n",
    "\n",
    "prediction = PhiTensor(child=np.random.rand(*input_shape),\n",
    "              data_subjects=np.zeros(input_shape),\n",
    "              min_vals=0,\n",
    "              max_vals=1\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "df88e59d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=11.460410106128375, min_vals=<lazyrepeatarray data: -0.0 -> shape: ()>, max_vals=<lazyrepeatarray data: 113.88888779867897 -> shape: ()>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn.forward(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2c880527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiTensor(child=[[-3.92378976  1.24498393 -1.38794205  5.90297321 -1.59037078  1.65571697\n",
       "  -1.1607311   1.02053309  1.55317498 -1.57865786]\n",
       " [ 1.29174864 -3.96418975  1.00400083  1.16634607  2.25411203  1.14705988\n",
       "  -2.01264545 -1.69485681  3.3113709  -1.71785996]], min_vals=<lazyrepeatarray data: -42.18098972602649 -> shape: (2, 10)>, max_vals=<lazyrepeatarray data: 42.18098972602649 -> shape: (2, 10)>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn.backward(prediction, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0d5b1-0240-4871-91d1-aa0797b1595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, layers=None):\n",
    "        self.layers = [] if layers is None else layers\n",
    "\n",
    "        self.loss = None\n",
    "        self.optimizer = Adamax\n",
    "\n",
    "    def add(self, layer):\n",
    "        assert isinstance(layer, Layer), \"PySyft doesn't recognize this kind of layer.\"\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def compile(self, loss=BinaryCrossEntropy(), optimizer=Adamax()):\n",
    "        self.layers[0].first_layer = True\n",
    "\n",
    "        next_layer = None\n",
    "        for layer in self.layers:\n",
    "            layer.connect_to(next_layer)\n",
    "            next_layer = layer\n",
    "\n",
    "        self.loss = BinaryCrossEntropy()\n",
    "        self.optimizer = Adamax()\n",
    "\n",
    "    def fit(self, X, Y, max_iter=100, batch_size=64, shuffle=True,\n",
    "            validation_split=0., validation_data=None):\n",
    "\n",
    "        # prepare data\n",
    "        train_X = X #.astype(get_dtype()) if np.issubdtype(np.float64, X.dtype) else X\n",
    "        train_Y = Y #.astype(get_dtype()) if np.issubdtype(np.float64, Y.dtype) else Y\n",
    "\n",
    "        if 1. > validation_split > 0.:\n",
    "            split = int(train_Y.shape[0] * validation_split)\n",
    "            valid_X, valid_Y = train_X[-split:], train_Y[-split:]\n",
    "            train_X, train_Y = train_X[:-split], train_Y[:-split]\n",
    "        elif validation_data is not None:\n",
    "            valid_X, valid_Y = validation_data\n",
    "        else:\n",
    "            valid_X, valid_Y = None, None\n",
    "\n",
    "        iter_idx = 0\n",
    "        while iter_idx < max_iter:\n",
    "            iter_idx += 1\n",
    "\n",
    "            # shuffle\n",
    "            if shuffle:\n",
    "                seed = np.random.randint(111, 1111111)\n",
    "                np.random.seed(seed)\n",
    "                np.random.shuffle(train_X)\n",
    "                np.random.seed(seed)\n",
    "                np.random.shuffle(train_Y)\n",
    "\n",
    "            # train\n",
    "            train_losses, train_predicts, train_targets = [], [], []\n",
    "            for b in range(train_Y.shape[0] // batch_size):\n",
    "                batch_begin = b * batch_size\n",
    "                batch_end = batch_begin + batch_size\n",
    "                x_batch = train_X[batch_begin:batch_end]\n",
    "                y_batch = train_Y[batch_begin:batch_end]\n",
    "\n",
    "                # forward propagation\n",
    "                y_pred = self.predict(x_batch)\n",
    "\n",
    "                # backward propagation\n",
    "                next_grad = self.loss.backward(y_pred, y_batch)\n",
    "                for layer in self.layers[::-1]:\n",
    "                    next_grad = layer.backward(next_grad)\n",
    "\n",
    "                # get parameter and gradients\n",
    "                params = []\n",
    "                grads = []\n",
    "                for layer in self.layers:\n",
    "                    params += layer.params\n",
    "                    grads += layer.grads\n",
    "\n",
    "                # update parameters\n",
    "                self.optimizer.update(params, grads)\n",
    "\n",
    "                # got loss and predict\n",
    "                train_losses.append(self.loss.forward(y_pred, y_batch))\n",
    "                train_predicts.extend(y_pred)\n",
    "                train_targets.extend(y_batch)\n",
    "\n",
    "            # output train status\n",
    "            runout = \"iter %d, train-[loss %.4f, acc %.4f]; \" % (\n",
    "                iter_idx, float(np.mean(train_losses)), float(self.accuracy(train_predicts, train_targets)))\n",
    "\n",
    "            # runout = \"iter %d, train-[loss %.4f, ]; \" % (\n",
    "            #     iter_idx, float(np.mean(train_losses)))\n",
    "\n",
    "            if valid_X is not None and valid_Y is not None:\n",
    "                # valid\n",
    "                valid_losses, valid_predicts, valid_targets = [], [], []\n",
    "                for b in range(valid_X.shape[0] // batch_size):\n",
    "                    batch_begin = b * batch_size\n",
    "                    batch_end = batch_begin + batch_size\n",
    "                    x_batch = valid_X[batch_begin:batch_end]\n",
    "                    y_batch = valid_Y[batch_begin:batch_end]\n",
    "\n",
    "                    # forward propagation\n",
    "                    y_pred = self.predict(x_batch)\n",
    "\n",
    "                    # got loss and predict\n",
    "                    valid_losses.append(self.loss.forward(y_pred, y_batch))\n",
    "                    valid_predicts.extend(y_pred)\n",
    "                    valid_targets.extend(y_batch)\n",
    "\n",
    "                # output valid status\n",
    "                runout += \"valid-[loss %.4f, acc %.4f]; \" % (\n",
    "                    float(np.mean(valid_losses)), float(self.accuracy(valid_predicts, valid_targets)))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Calculate an output Y for the given input X. \"\"\"\n",
    "        x_next = X\n",
    "        for layer in self.layers[:]:\n",
    "            x_next = layer.forward(x_next)\n",
    "        y_pred = x_next\n",
    "        return y_pred\n",
    "\n",
    "    def accuracy(self, outputs, targets):\n",
    "        y_predicts = np.argmax(outputs, axis=1)\n",
    "        y_targets = np.argmax(targets, axis=1)\n",
    "        acc = y_predicts == y_targets\n",
    "        return np.mean(acc)\n",
    "\n",
    "    def evaluate(self, X, Y):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dce0b9-cd55-45f3-9603-03024945cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d93cd4e-5503-4431-978b-c0074840a58a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
