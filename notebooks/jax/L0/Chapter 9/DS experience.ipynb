{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax Level 0 Data Scientist Experience - Chapter 9 - Getting started with Jax MLPs, CNNs, and RNNs\n",
    "\n",
    "Link to the original blog post by Robert Tjarko Lange: https://roberttlange.com/posts/2020/03/blog-post-10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kj/filesystem-disk-unix.c++:1703: warning: PWD environment variable doesn't match current directory; pwd = /\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… The installed version of syft==0.8.1b1 matches the requirement >=0.8b0\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import syft as sy\n",
    "sy.requires(\">=0.8-beta\")\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - User login and code execution requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite Store Path:\n",
      "!open file:///var/folders/sz/hkfsnn612hq56r7cs5rd540r0000gn/T/7bca415d13ed1ec841f0d0aede098dbb.sqlite\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SyftClient - test-domain-1 <7bca415d13ed1ec841f0d0aede098dbb>: PythonConnection>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register a client to the domain\n",
    "node = sy.orchestra.launch(name=\"test-domain-1\")\n",
    "guest_domain_client = node.client\n",
    "guest_domain_client.register(name=\"Jane Doe\", email=\"jane@caltech.edu\", password=\"abc123\", institution=\"Caltech\", website=\"https://www.caltech.edu/\")\n",
    "guest_domain_client.login(email=\"jane@caltech.edu\", password=\"abc123\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this JAX thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for code execution\n",
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_dot_time_comparison():\n",
    "    # Note: using different naming conventions for numpy and jax\n",
    "    # compared to the original blog post, i.e. onp => np, np => jnp.\n",
    "    import numpy as np\n",
    "    import jax.numpy as jnp\n",
    "    from jax import random\n",
    "\n",
    "    # Generate key which is used to generate random numbers\n",
    "    key = random.PRNGKey(1)\n",
    "\n",
    "    # Generate a random matrix\n",
    "    x = random.uniform(key, (1000, 1000))\n",
    "    # Compare running times of 3 different matrix multiplications\n",
    "    %time y = np.dot(x, x)\n",
    "    %time y = jnp.dot(x, x)\n",
    "    %time y = jnp.dot(x, x).block_until_ready()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Few Basic Concepts & Conventions - jit, grad & vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_jit():\n",
    "    # Note: using different naming conventions for numpy and jax\n",
    "    # compared to the original blog post, i.e. onp => np, np => jnp.\n",
    "    import numpy as np\n",
    "    import jax.numpy as jnp\n",
    "    from jax import random, jit, grad\n",
    "\n",
    "    def ReLU(x):\n",
    "        \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "        return jnp.maximum(0, x)\n",
    "    \n",
    "    key = random.PRNGKey(1)\n",
    "    x = random.uniform(key, (1000, 1000))\n",
    "\n",
    "    jit_ReLU = jit(ReLU)\n",
    "\n",
    "    %time out = ReLU(x).block_until_ready()\n",
    "    # Call jitted version to compile for evaluation time!\n",
    "    %time jit_ReLU(x).block_until_ready()\n",
    "    %time out = jit_ReLU(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_grad():\n",
    "    # Note: using different naming conventions for numpy and jax\n",
    "    # compared to the original blog post, i.e. onp => np, np => jnp.\n",
    "    import numpy as np\n",
    "    import jax.numpy as jnp\n",
    "    from jax import random, jit, grad\n",
    "\n",
    "    # NOTE: using the same ReLU function as in the previous example\n",
    "    def ReLU(x):\n",
    "        \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "        return jnp.maximum(0, x)\n",
    "    \n",
    "    def FiniteDiffGrad(x):\n",
    "        \"\"\" Compute the finite difference derivative approx for the ReLU\"\"\"\n",
    "        return np.array((ReLU(x + 1e-3) - ReLU(x - 1e-3)) / (2 * 1e-3))\n",
    "    \n",
    "    key = random.PRNGKey(1)\n",
    "    x = random.uniform(key, (1000, 1000))\n",
    "\n",
    "    # Compare the Jax gradient with a finite difference approximation\n",
    "    print(\"Jax Grad: \", jit(grad(jit(ReLU)))(2.))\n",
    "    print(\"FD Gradient:\", FiniteDiffGrad(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_vmap():\n",
    "    # Note: using different naming conventions for numpy and jax\n",
    "    # compared to the original blog post, i.e. onp => np, np => jnp.\n",
    "    import numpy as np\n",
    "    import jax.numpy as jnp\n",
    "    from jax import random, jit, vmap\n",
    "\n",
    "    batch_dim = 32\n",
    "    feature_dim = 100\n",
    "    hidden_dim = 512\n",
    "\n",
    "    # Generate a batch of vectors to process\n",
    "    key = random.PRNGKey(1)\n",
    "    X = random.normal(key, (batch_dim, feature_dim))\n",
    "\n",
    "    # Generate Gaussian weights and biases\n",
    "    params = [random.normal(key, (hidden_dim, feature_dim)),\n",
    "            random.normal(key, (hidden_dim, ))]\n",
    "    \n",
    "    # NOTE: using the same ReLU function as in the previous example\n",
    "    def ReLU(x):\n",
    "        \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "        return jnp.maximum(0, x)\n",
    "\n",
    "    def relu_layer(params, x):\n",
    "        \"\"\" Simple ReLu layer for single sample \"\"\"\n",
    "        return ReLU(np.dot(params[0], x) + params[1])\n",
    "\n",
    "    def batch_version_relu_layer(params, x):\n",
    "        \"\"\" Error prone batch version \"\"\"\n",
    "        return ReLU(np.dot(X, params[0].T) + params[1])\n",
    "\n",
    "    def vmap_relu_layer(params, x):\n",
    "        \"\"\" vmap version of the ReLU layer \"\"\"\n",
    "        return jit(vmap(relu_layer, in_axes=(None, 0), out_axes=0))\n",
    "\n",
    "    out = np.stack([relu_layer(params, X[i, :]) for i in range(X.shape[0])])\n",
    "    out = batch_version_relu_layer(params, X)\n",
    "    out = vmap_relu_layer(params, X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNOTE: running time comparison results differ from the ones on the blog post:\\nCPU times: user 45.3 ms, sys: 6.12 ms, total: 51.5 ms\\nWall time: 16 ms\\nCPU times: user 24.1 ms, sys: 3.28 ms, total: 27.4 ms\\nWall time: 6.85 ms\\nCPU times: user 149 ms, sys: 23 ms, total: 172 ms\\nWall time: 30.4 ms\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test our functions locally\n",
    "# func_dot_time_comparison()\n",
    "\"\"\"\n",
    "NOTE: running time comparison results differ from the ones on the blog post:\n",
    "CPU times: user 45.3 ms, sys: 6.12 ms, total: 51.5 ms\n",
    "Wall time: 16 ms\n",
    "CPU times: user 24.1 ms, sys: 3.28 ms, total: 27.4 ms\n",
    "Wall time: 6.85 ms\n",
    "CPU times: user 149 ms, sys: 23 ms, total: 172 ms\n",
    "Wall time: 30.4 ms\n",
    "\"\"\"\n",
    "\n",
    "# func_jit()\n",
    "# func_grad()\n",
    "# func_vmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert-danger\" style=\"padding:5px;\"><strong>SyftError</strong>: Duplication Key Error: syft.service.code.user_code.UserCode</div><br />"
      ],
      "text/plain": [
       "<class 'syft.service.response.SyftError'>: Duplication Key Error: syft.service.code.user_code.UserCode"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit the function for code execution\n",
    "# guest_domain_client.api.services.code.request_code_execution(func_dot_time_comparison)\n",
    "# guest_domain_client.api.services.code.request_code_execution(func_jit)\n",
    "# guest_domain_client.api.services.code.request_code_execution(func_grad)\n",
    "guest_domain_client.api.services.code.request_code_execution(func_vmap)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "### Training a MNIST Multilayer Perceptron in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from seaborn) (1.24.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/antti/.pyenv/versions/3.10.4/envs/jax_1/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# seaborn used by helpers.py\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = guest_domain_client.api.services.dataset.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = results[0]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock = dataset.assets[0].mock\n",
    "mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from helpers import plot_mnist_examples\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 100\n",
    "\n",
    "# TODO: use the received dataset\n",
    "train_data = None\n",
    "train_labels = None\n",
    "test_data = None\n",
    "test_labels = None\n",
    "\n",
    "# Shuffle the training data\n",
    "shuffle_indices = np.random.permutation(len(train_data))\n",
    "train_data = train_data[shuffle_indices]\n",
    "train_labels = train_labels[shuffle_indices]\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_train_batches = len(train_data) // batch_size\n",
    "num_test_batches = len(test_data) // batch_size\n",
    "\n",
    "# Create a function to retrieve a batch of data\n",
    "def get_batch(data, labels, batch_idx, batch_size):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = (batch_idx + 1) * batch_size\n",
    "    return data[start_idx:end_idx], labels[start_idx:end_idx]\n",
    "\n",
    "# Plot MNIST examples\n",
    "plot_mnist_examples(train_data, train_labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function(input_policy=sy.ExactMatch(\n",
    "                    data=mock),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_mlp(data):\n",
    "    # Import some additional JAX\n",
    "    from jax.scipy.special import logsumexp\n",
    "    from jax.experimental import optimizers\n",
    "    from jax import random, jit, grad, vmap, value_and_grad\n",
    "\n",
    "    key = random.PRNGKey(1)\n",
    "\n",
    "    def ReLU(x):\n",
    "        \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_layer(params, x):\n",
    "        \"\"\" Simple ReLu layer for single sample \"\"\"\n",
    "        return ReLU(np.dot(params[0], x) + params[1])\n",
    "\n",
    "    def initialize_mlp(sizes, key):\n",
    "        \"\"\" Initialize the weights of all layers of a linear layer network \"\"\"\n",
    "        keys = random.split(key, len(sizes))\n",
    "        # Initialize a single layer with Gaussian weights -  helper function\n",
    "        def initialize_layer(m, n, key, scale=1e-2):\n",
    "            w_key, b_key = random.split(key)\n",
    "            return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "        return [initialize_layer(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "    layer_sizes = [784, 512, 512, 10]\n",
    "    # Return a list of tuples of layer weights\n",
    "    params = initialize_mlp(layer_sizes, key)\n",
    "\n",
    "    num_epochs = 10\n",
    "    num_classes = 10\n",
    "\n",
    "    def forward_pass(params, in_array):\n",
    "        \"\"\" Compute the forward pass for each example individually \"\"\"\n",
    "        activations = in_array\n",
    "\n",
    "        # Loop over the ReLU hidden layers\n",
    "        for w, b in params[:-1]:\n",
    "            activations = relu_layer([w, b], activations)\n",
    "\n",
    "        # Perform final trafo to logits\n",
    "        final_w, final_b = params[-1]\n",
    "        logits = np.dot(final_w, activations) + final_b\n",
    "        return logits - logsumexp(logits)\n",
    "\n",
    "    # Make a batched version of the `predict` function\n",
    "    batch_forward = vmap(forward_pass, in_axes=(None, 0), out_axes=0)\n",
    "\n",
    "    # For logging purposes we compute the accuracy as well\n",
    "    def one_hot(x, k, dtype=np.float32):\n",
    "        \"\"\"Create a one-hot encoding of x of size k \"\"\"\n",
    "        return np.array(x[:, None] == np.arange(k), dtype)\n",
    "\n",
    "    def loss(params, in_arrays, targets):\n",
    "        \"\"\" Compute the multi-class cross-entropy loss \"\"\"\n",
    "        preds = batch_forward(params, in_arrays)\n",
    "        return -np.sum(preds * targets)\n",
    "\n",
    "    def accuracy(params, data, labels):\n",
    "        \"\"\" Compute the accuracy for a provided dataloader \"\"\"\n",
    "        acc_total = 0\n",
    "        num_samples = len(data)\n",
    "        for i in range(num_samples):\n",
    "            images = data.reshape(data.shape[0], 28*28)\n",
    "            target = labels[i]\n",
    "            targets = one_hot(np.array(target), num_classes)\n",
    "\n",
    "            target_class = np.argmax(targets, axis=1)\n",
    "            predicted_class = np.argmax(batch_forward(params, images), axis=1)\n",
    "            acc_total += np.sum(predicted_class == target_class)\n",
    "        return acc_total/num_samples\n",
    "    \n",
    "    @jit\n",
    "    def update(params, x, y, opt_state):\n",
    "        \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "        value, grads = value_and_grad(loss)(params, x, y)\n",
    "        opt_state = opt_update(0, grads, opt_state)\n",
    "        return get_params(opt_state), opt_state, value\n",
    "\n",
    "    # Defining an optimizer in Jax\n",
    "    step_size = 1e-3\n",
    "    opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "    def run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\"):\n",
    "        \"\"\" Implements a learning loop over epochs. \"\"\"\n",
    "        # Initialize placeholder for loggin\n",
    "        log_acc_train, log_acc_test, train_loss = [], [], []\n",
    "\n",
    "        # Get the initial set of parameters\n",
    "        params = get_params(opt_state)\n",
    "\n",
    "        # Get initial accuracy after random init\n",
    "        train_acc = accuracy(params, train_data, train_labels)\n",
    "        test_acc = accuracy(params, test_data, test_labels)\n",
    "        log_acc_train.append(train_acc)\n",
    "        log_acc_test.append(test_acc)\n",
    "\n",
    "        # Loop over the training epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            num_train_samples = len(train_data)\n",
    "            for i in range(num_train_batches):\n",
    "                # Get a batch of data\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = (i + 1) * batch_size\n",
    "                data = train_data[start_idx:end_idx]\n",
    "                target = train_labels[start_idx:end_idx]\n",
    "\n",
    "                if net_type == \"MLP\":\n",
    "                    # Flatten the image into 784 vectors for the MLP\n",
    "                    x = np.array(data).reshape(data.size(0), 28*28)\n",
    "                elif net_type == \"CNN\":\n",
    "                    # No flattening of the input required for the CNN\n",
    "                    x = np.array(data)\n",
    "                y = one_hot(np.array(target), num_classes)\n",
    "                params, opt_state, loss = update(params, x, y, opt_state)\n",
    "                train_loss.append(loss)\n",
    "\n",
    "            epoch_time = time.time() - start_time\n",
    "            train_acc = accuracy(params, train_data, train_labels)\n",
    "            test_acc = accuracy(params, test_data, test_labels)\n",
    "            log_acc_train.append(train_acc)\n",
    "            log_acc_test.append(test_acc)\n",
    "            print(\"Epoch {} | T: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,\n",
    "                                                                        train_acc, test_acc))\n",
    "\n",
    "        return train_loss, log_acc_train, log_acc_test\n",
    "\n",
    "\n",
    "    train_loss, train_log, test_log = run_mnist_training_loop(num_epochs,\n",
    "                                                            opt_state,\n",
    "                                                            net_type=\"MLP\")\n",
    "    \n",
    "\n",
    "# from helpers import plot_mnist_performance\n",
    "# plot_mnist_performance(train_loss, train_log, test_log,\n",
    "#                        \"MNIST MLP Performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our functions locally\n",
    "func_mlp(data=mock)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5\n",
    "### Using the stax API to build Sequential Models - Case Study: A CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (695301393.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    def update(params, x, y, opt_state):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "@sy.syft_function(input_policy=sy.ExactMatch(\n",
    "                    data=mock),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_cnn(data):\n",
    "  from jax import random, value_and_grad, jit\n",
    "  from jax.experimental import stax, optimizers\n",
    "  from jax.experimental.stax import (BatchNorm, Conv, Dense, Flatten,\n",
    "                                    Relu, LogSoftmax)\n",
    "  \n",
    "  key = random.PRNGKey(1)\n",
    "  num_classes = 10\n",
    "  \n",
    "  # NOTE: ** copy&pasted from func_mlp. Any way to share code?\n",
    "  def one_hot(x, k, dtype=np.float32):\n",
    "        \"\"\"Create a one-hot encoding of x of size k \"\"\"\n",
    "        return np.array(x[:, None] == np.arange(k), dtype)\n",
    "  \n",
    "  @jit\n",
    "  def update(params, x, y, opt_state):\n",
    "        \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "        value, grads = value_and_grad(loss)(params, x, y)\n",
    "        opt_state = opt_update(0, grads, opt_state)\n",
    "        return get_params(opt_state), opt_state, value\n",
    "  \n",
    "  \n",
    "  def run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\"):\n",
    "        \"\"\" Implements a learning loop over epochs. \"\"\"\n",
    "        # Initialize placeholder for loggin\n",
    "        log_acc_train, log_acc_test, train_loss = [], [], []\n",
    "\n",
    "        # Get the initial set of parameters\n",
    "        params = get_params(opt_state)\n",
    "\n",
    "        # Get initial accuracy after random init\n",
    "        train_acc = accuracy(params, train_data, train_labels)\n",
    "        test_acc = accuracy(params, test_data, test_labels)\n",
    "        log_acc_train.append(train_acc)\n",
    "        log_acc_test.append(test_acc)\n",
    "\n",
    "        # Loop over the training epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            num_train_samples = len(train_data)\n",
    "            for i in range(num_train_batches):\n",
    "                # Get a batch of data\n",
    "                start_idx = i * batch_size\n",
    "                end_idx = (i + 1) * batch_size\n",
    "                data = train_data[start_idx:end_idx]\n",
    "                target = train_labels[start_idx:end_idx]\n",
    "\n",
    "                if net_type == \"MLP\":\n",
    "                    # Flatten the image into 784 vectors for the MLP\n",
    "                    x = np.array(data).reshape(data.size(0), 28*28)\n",
    "                elif net_type == \"CNN\":\n",
    "                    # No flattening of the input required for the CNN\n",
    "                    x = np.array(data)\n",
    "                y = one_hot(np.array(target), num_classes)\n",
    "                params, opt_state, loss = update(params, x, y, opt_state)\n",
    "                train_loss.append(loss)\n",
    "\n",
    "            epoch_time = time.time() - start_time\n",
    "            train_acc = accuracy(params, train_data, train_labels)\n",
    "            test_acc = accuracy(params, test_data, test_labels)\n",
    "            log_acc_train.append(train_acc)\n",
    "            log_acc_test.append(test_acc)\n",
    "            print(\"Epoch {} | T: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,\n",
    "                                                                        train_acc, test_acc))\n",
    "\n",
    "        return train_loss, log_acc_train, log_acc_test\n",
    "  # end of copy&paste **\n",
    "  \n",
    "  init_fun, conv_net = stax.serial(Conv(32, (5, 5), (2, 2), padding=\"SAME\"),\n",
    "                                 BatchNorm(), Relu,\n",
    "                                 Conv(32, (5, 5), (2, 2), padding=\"SAME\"),\n",
    "                                 BatchNorm(), Relu,\n",
    "                                 Conv(10, (3, 3), (2, 2), padding=\"SAME\"),\n",
    "                                 BatchNorm(), Relu,\n",
    "                                 Conv(10, (3, 3), (2, 2), padding=\"SAME\"), Relu,\n",
    "                                 Flatten,\n",
    "                                 Dense(num_classes),\n",
    "                                 LogSoftmax)\n",
    "\n",
    "  _, params = init_fun(key, (batch_size, 1, 28, 28))\n",
    "\n",
    "  def accuracy(params, data_loader):\n",
    "    \"\"\" Compute the accuracy for the CNN case (no flattening of input)\"\"\"\n",
    "    acc_total = 0\n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        images = np.array(data)\n",
    "        targets = one_hot(np.array(target), num_classes)\n",
    "\n",
    "        target_class = np.argmax(targets, axis=1)\n",
    "        predicted_class = np.argmax(conv_net(params, images), axis=1)\n",
    "        acc_total += np.sum(predicted_class == target_class)\n",
    "    return acc_total/len(data_loader.dataset)\n",
    "\n",
    "  def loss(params, images, targets):\n",
    "      preds = conv_net(params, images)\n",
    "      return -np.sum(preds * targets)\n",
    "  \n",
    "  step_size = 1e-3\n",
    "  opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "  opt_state = opt_init(params)\n",
    "  num_epochs = 10\n",
    "\n",
    "  train_loss, train_log, test_log = run_mnist_training_loop(num_epochs,\n",
    "                                                            opt_state,\n",
    "                                                            net_type=\"CNN\")\n",
    "\n",
    "\n",
    "# Plot the loss curve over time\n",
    "# from helpers import plot_mnist_performance\n",
    "# plot_mnist_performance(train_loss, train_log, test_log,\n",
    "#                        \"MNIST MLP Performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
