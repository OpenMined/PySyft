{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax Level 0 Data Scientist Experience - Chapter 9 - Getting started with Jax MLPs, CNNs, and RNNs\n",
    "\n",
    "Link to the original blog post by Robert Tjarko Lange: https://roberttlange.com/posts/2020/03/blog-post-10/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ The installed version of syft==0.8.1b3 matches the requirement >=0.8b0\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import syft as sy\n",
    "sy.requires(\">=0.8-beta\")\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - User login and code execution requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite Store Path:\n",
      "!open file:///var/folders/sz/hkfsnn612hq56r7cs5rd540r0000gn/T/7bca415d13ed4ec881f0d0aede098dbb.sqlite\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<SyftClient - test-domain-1 <7bca415d13ed4ec881f0d0aede098dbb>: PythonConnection>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Register a client to the domain\n",
    "node = sy.orchestra.launch(name=\"test-domain-1\")\n",
    "guest_domain_client = node.client\n",
    "guest_domain_client.register(name=\"Jane Doe\", email=\"jane@caltech.edu\", password=\"abc123\", institution=\"Caltech\", website=\"https://www.caltech.edu/\")\n",
    "guest_domain_client.login(email=\"jane@caltech.edu\", password=\"abc123\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this JAX thing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for code execution\n",
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_dot_time_comparison():\n",
    "    # Note: using different naming conventions for numpy and jax\n",
    "    # compared to the original blog post, i.e. onp => np, np => jnp.\n",
    "    import numpy as np\n",
    "    import jax.numpy as jnp\n",
    "    from jax import random\n",
    "\n",
    "    # Generate key which is used to generate random numbers\n",
    "    key = random.PRNGKey(1)\n",
    "\n",
    "    # Generate a random matrix\n",
    "    x = random.uniform(key, (1000, 1000))\n",
    "    # Compare running times of 3 different matrix multiplications\n",
    "    %time y = np.dot(x, x)\n",
    "    %time y = jnp.dot(x, x)\n",
    "    %time y = jnp.dot(x, x).block_until_ready()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Few Basic Concepts & Conventions - jit, grad & vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_jit():\n",
    "    # Note: using different naming conventions for numpy and jax\n",
    "    # compared to the original blog post, i.e. onp => np, np => jnp.\n",
    "    import numpy as np\n",
    "    import jax.numpy as jnp\n",
    "    from jax import random, jit, grad\n",
    "\n",
    "    def ReLU(x):\n",
    "        \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "        return jnp.maximum(0, x)\n",
    "    \n",
    "    key = random.PRNGKey(1)\n",
    "    x = random.uniform(key, (1000, 1000))\n",
    "\n",
    "    jit_ReLU = jit(ReLU)\n",
    "\n",
    "    %time out = ReLU(x).block_until_ready()\n",
    "    # Call jitted version to compile for evaluation time!\n",
    "    %time jit_ReLU(x).block_until_ready()\n",
    "    %time out = jit_ReLU(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_grad():\n",
    "    # Note: using different naming conventions for numpy and jax\n",
    "    # compared to the original blog post, i.e. onp => np, np => jnp.\n",
    "    import numpy as np\n",
    "    import jax.numpy as jnp\n",
    "    from jax import random, jit, grad\n",
    "\n",
    "    # NOTE: using the same ReLU function as in the previous example\n",
    "    def ReLU(x):\n",
    "        \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "        return jnp.maximum(0, x)\n",
    "    \n",
    "    def FiniteDiffGrad(x):\n",
    "        \"\"\" Compute the finite difference derivative approx for the ReLU\"\"\"\n",
    "        return jnp.array((ReLU(x + 1e-3) - ReLU(x - 1e-3)) / (2 * 1e-3))\n",
    "    \n",
    "    key = random.PRNGKey(1)\n",
    "    x = random.uniform(key, (1000, 1000))\n",
    "\n",
    "    # Compare the Jax gradient with a finite difference approximation\n",
    "    print(\"Jax Grad: \", jit(grad(jit(ReLU)))(2.))\n",
    "    print(\"FD Gradient:\", FiniteDiffGrad(2.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_vmap():\n",
    "    # Note: using different naming conventions for numpy and jax\n",
    "    # compared to the original blog post, i.e. onp => np, np => jnp.\n",
    "    import numpy as np\n",
    "    import jax.numpy as jnp\n",
    "    from jax import random, jit, vmap\n",
    "\n",
    "    batch_dim = 32\n",
    "    feature_dim = 100\n",
    "    hidden_dim = 512\n",
    "\n",
    "    # Generate a batch of vectors to process\n",
    "    key = random.PRNGKey(1)\n",
    "    X = random.normal(key, (batch_dim, feature_dim))\n",
    "\n",
    "    # Generate Gaussian weights and biases\n",
    "    params = [random.normal(key, (hidden_dim, feature_dim)),\n",
    "            random.normal(key, (hidden_dim, ))]\n",
    "    \n",
    "    # NOTE: using the same ReLU function as in the previous example\n",
    "    def ReLU(x):\n",
    "        \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "        return jnp.maximum(0, x)\n",
    "\n",
    "    def relu_layer(params, x):\n",
    "        \"\"\" Simple ReLu layer for single sample \"\"\"\n",
    "        return ReLU(jnp.dot(params[0], x) + params[1])\n",
    "\n",
    "    def batch_version_relu_layer(params, x):\n",
    "        \"\"\" Error prone batch version \"\"\"\n",
    "        return ReLU(jnp.dot(X, params[0].T) + params[1])\n",
    "\n",
    "    def vmap_relu_layer(params, x):\n",
    "        \"\"\" vmap version of the ReLU layer \"\"\"\n",
    "        return jit(vmap(relu_layer, in_axes=(None, 0), out_axes=0))\n",
    "\n",
    "    out = jnp.stack([relu_layer(params, X[i, :]) for i in range(X.shape[0])])\n",
    "    out = batch_version_relu_layer(params, X)\n",
    "    out = vmap_relu_layer(params, X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.18 ms, sys: 94 µs, total: 3.27 ms\n",
      "Wall time: 1.04 ms\n",
      "CPU times: user 34.3 ms, sys: 30.3 ms, total: 64.6 ms\n",
      "Wall time: 245 ms\n",
      "CPU times: user 4.27 ms, sys: 634 µs, total: 4.9 ms\n",
      "Wall time: 1.63 ms\n",
      "Jax Grad:  1.0\n",
      "FD Gradient: 0.9999999999998899\n"
     ]
    }
   ],
   "source": [
    "# Test our functions locally\n",
    "# func_dot_time_comparison()\n",
    "\"\"\"\n",
    "NOTE: running time comparison results differ from the ones on the blog post:\n",
    "CPU times: user 45.3 ms, sys: 6.12 ms, total: 51.5 ms\n",
    "Wall time: 16 ms\n",
    "CPU times: user 24.1 ms, sys: 3.28 ms, total: 27.4 ms\n",
    "Wall time: 6.85 ms\n",
    "CPU times: user 149 ms, sys: 23 ms, total: 172 ms\n",
    "Wall time: 30.4 ms\n",
    "\"\"\"\n",
    "\n",
    "func_jit()\n",
    "func_grad()\n",
    "func_vmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"alert-danger\" style=\"padding:5px;\"><strong>SyftError</strong>: Duplication Key Error: syft.service.code.user_code.UserCode</div><br />"
      ],
      "text/plain": [
       "<class 'syft.service.response.SyftError'>: Duplication Key Error: syft.service.code.user_code.UserCode"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit the function for code execution\n",
    "guest_domain_client.api.services.code.request_code_execution(func_dot_time_comparison)\n",
    "guest_domain_client.api.services.code.request_code_execution(func_jit)\n",
    "guest_domain_client.api.services.code.request_code_execution(func_grad)\n",
    "guest_domain_client.api.services.code.request_code_execution(func_vmap)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3\n",
    "### Training a MNIST Multilayer Perceptron in JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from seaborn) (3.7.1)\n",
      "Requirement already satisfied: pandas>=0.25 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from seaborn) (1.5.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from seaborn) (1.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.39.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/antti/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Users/antti/.pyenv/versions/3.10.4/envs/jax_1/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# seaborn used by helpers.py\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "List - Size: 1\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>syft.service.dataset.dataset.Dataset</td>\n",
       "      <td>3c161f87791244f3a749dc49c4530b4f</td>\n",
       "      <td>mnist</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "[syft.service.dataset.dataset.Dataset]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = guest_domain_client.api.services.dataset.get_all()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "Syft Dataset: mnist\n",
       "Assets:\n",
       "\ttrain images: None\n",
       "\ttest images: None\n",
       "\ttrain labels: None\n",
       "\ttest labels: None\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "syft.service.dataset.dataset.Dataset"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = results[0]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_mock = dataset.assets[0].mock_data\n",
    "test_images_mock = dataset.assets[1].mock_data\n",
    "train_labels_mock = dataset.assets[2].mock_data\n",
    "test_labels_mock = dataset.assets[3].mock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_asset = dataset.assets[0]\n",
    "test_images_asset = dataset.assets[1]\n",
    "train_labels_asset = dataset.assets[2]\n",
    "test_labels_asset = dataset.assets[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.dtype[int64]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_labels_mock.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot MNIST examples\n",
    "from helpers import plot_mnist_examples\n",
    "plot_mnist_examples(train_images_mock, train_labels_mock)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function(input_policy=sy.ExactMatch(\n",
    "                    train_images=train_images_asset.pointer, test_images=test_images_asset.pointer,\n",
    "                    train_labels=train_labels_asset.pointer, test_labels=test_labels_asset.pointer),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_mlp(train_images, test_images, train_labels, test_labels):\n",
    "    # Import some additional JAX\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from jax.scipy.special import logsumexp\n",
    "    from jax.experimental import optimizers\n",
    "    from jax import random, jit, grad, vmap, value_and_grad\n",
    "\n",
    "    key = random.PRNGKey(1)\n",
    "\n",
    "    # Set the batch size\n",
    "    batch_size = 100\n",
    "\n",
    "    # Shuffle the training data\n",
    "    shuffle_indices = np.random.permutation(len(train_images))\n",
    "    train_data = train_images[shuffle_indices]\n",
    "    train_labels = train_labels[shuffle_indices]\n",
    "    test_data = test_images\n",
    "\n",
    "    def ReLU(x):\n",
    "        \"\"\" Rectified Linear Unit (ReLU) activation function \"\"\"\n",
    "        return jnp.maximum(0, x)\n",
    "\n",
    "    def relu_layer(params, x):\n",
    "        \"\"\" Simple ReLu layer for single sample \"\"\"\n",
    "        return ReLU(jnp.dot(params[0], x) + params[1])\n",
    "\n",
    "    def initialize_mlp(sizes, key):\n",
    "        \"\"\" Initialize the weights of all layers of a linear layer network \"\"\"\n",
    "        keys = random.split(key, len(sizes))\n",
    "        # Initialize a single layer with Gaussian weights -  helper function\n",
    "        def initialize_layer(m, n, key, scale=1e-2):\n",
    "            w_key, b_key = random.split(key)\n",
    "            return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "        return [initialize_layer(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "\n",
    "    layer_sizes = [784, 512, 512, 10]\n",
    "    # Return a list of tuples of layer weights\n",
    "    params = initialize_mlp(layer_sizes, key)\n",
    "\n",
    "    num_epochs = 10\n",
    "    num_classes = 10\n",
    "\n",
    "    def forward_pass(params, in_array):\n",
    "        \"\"\" Compute the forward pass for each example individually \"\"\"\n",
    "        activations = in_array\n",
    "\n",
    "        # Loop over the ReLU hidden layers\n",
    "        for w, b in params[:-1]:\n",
    "            activations = relu_layer([w, b], activations)\n",
    "\n",
    "        # Perform final trafo to logits\n",
    "        final_w, final_b = params[-1]\n",
    "        logits = jnp.dot(final_w, activations) + final_b\n",
    "        return logits - logsumexp(logits)\n",
    "\n",
    "    # Make a batched version of the `predict` function\n",
    "    batch_forward = vmap(forward_pass, in_axes=(None, 0), out_axes=0)\n",
    "\n",
    "    # For logging purposes we compute the accuracy as well\n",
    "    def one_hot(x, k, dtype=jnp.float32):\n",
    "        \"\"\"Create a one-hot encoding of x of size k \"\"\"\n",
    "        return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "    def loss(params, in_arrays, targets):\n",
    "        \"\"\" Compute the multi-class cross-entropy loss \"\"\"\n",
    "        preds = batch_forward(params, in_arrays)\n",
    "        return -jnp.sum(preds * targets)\n",
    "\n",
    "    def accuracy(params, data, labels):\n",
    "        \"\"\" Compute the accuracy for a provided dataloader \"\"\"\n",
    "        acc_total = 0\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            data_batch = data[i:i+batch_size]\n",
    "            labels_batch = labels[i:i+batch_size]\n",
    "\n",
    "            images = data_batch.reshape(data_batch.shape[0], 28*28)\n",
    "            targets = one_hot(jnp.array(labels_batch), num_classes)\n",
    "\n",
    "            target_class = jnp.argmax(targets, axis=1)\n",
    "            predicted_class = jnp.argmax(batch_forward(params, images), axis=1)\n",
    "            acc_total += jnp.sum(predicted_class == target_class)\n",
    "        return acc_total/len(data)\n",
    "    \n",
    "    @jit\n",
    "    def update(params, x, y, opt_state):\n",
    "        \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "        value, grads = value_and_grad(loss)(params, x, y)\n",
    "        opt_state = opt_update(0, grads, opt_state)\n",
    "        return get_params(opt_state), opt_state, value\n",
    "\n",
    "    # Defining an optimizer in Jax\n",
    "    step_size = 1e-3\n",
    "    opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "    opt_state = opt_init(params)\n",
    "\n",
    "    def run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\"):\n",
    "        \"\"\" Implements a learning loop over epochs. \"\"\"\n",
    "        # Initialize placeholder for loggin\n",
    "        log_acc_train, log_acc_test, train_loss = [], [], []\n",
    "\n",
    "        # Get the initial set of parameters\n",
    "        params = get_params(opt_state)\n",
    "\n",
    "        # Get initial accuracy after random init\n",
    "        train_acc = accuracy(params, train_data, train_labels)\n",
    "        test_acc = accuracy(params, test_data, test_labels)\n",
    "        log_acc_train.append(train_acc)\n",
    "        log_acc_test.append(test_acc)\n",
    "\n",
    "        # Loop over the training epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            for i in range(0, len(train_data), batch_size):\n",
    "                data = train_data[i:i+batch_size]\n",
    "                target = train_labels[i:i+batch_size]\n",
    "                if net_type == \"MLP\":\n",
    "                    # Flatten the image into 784 vectors for the MLP\n",
    "                    x = jnp.array(data).reshape(data.shape[0], 28*28)\n",
    "                elif net_type == \"CNN\":\n",
    "                    # No flattening of the input required for the CNN\n",
    "                    x = jnp.array(data)\n",
    "                y = one_hot(jnp.array(target), num_classes)\n",
    "                params, opt_state, loss = update(params, x, y, opt_state)\n",
    "                train_loss.append(loss)\n",
    "\n",
    "            epoch_time = time.time() - start_time\n",
    "            train_acc = accuracy(params, train_data, train_labels)\n",
    "            test_acc = accuracy(params, test_data, test_labels)\n",
    "            log_acc_train.append(train_acc)\n",
    "            log_acc_test.append(test_acc)\n",
    "            print(\"Epoch {} | T: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,\n",
    "                                                                        train_acc, test_acc))\n",
    "\n",
    "        return train_loss, log_acc_train, log_acc_test\n",
    "\n",
    "\n",
    "    train_loss, train_log, test_log = run_mnist_training_loop(num_epochs,\n",
    "                                                            opt_state,\n",
    "                                                            net_type=\"MLP\")\n",
    "    \n",
    "\n",
    "# from helpers import plot_mnist_performance\n",
    "# plot_mnist_performance(train_loss, train_log, test_log,\n",
    "#                        \"MNIST MLP Performance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | T: 4.59 | Train A: 0.967 | Test A: 0.969\n",
      "Epoch 2 | T: 4.32 | Train A: 0.982 | Test A: 0.979\n",
      "Epoch 3 | T: 4.26 | Train A: 0.983 | Test A: 0.978\n",
      "Epoch 4 | T: 4.30 | Train A: 0.989 | Test A: 0.980\n",
      "Epoch 5 | T: 4.31 | Train A: 0.992 | Test A: 0.978\n",
      "Epoch 6 | T: 4.33 | Train A: 0.990 | Test A: 0.974\n",
      "Epoch 7 | T: 4.30 | Train A: 0.996 | Test A: 0.983\n",
      "Epoch 8 | T: 4.30 | Train A: 0.993 | Test A: 0.981\n",
      "Epoch 9 | T: 4.24 | Train A: 0.996 | Test A: 0.983\n",
      "Epoch 10 | T: 4.26 | Train A: 0.996 | Test A: 0.981\n"
     ]
    }
   ],
   "source": [
    "# Test our function locally\n",
    "func_mlp(train_images=train_images_mock, test_images=test_images_mock,\n",
    "         train_labels=train_labels_mock, test_labels=test_labels_mock)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5\n",
    "### Using the stax API to build Sequential Models - Case Study: A CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function(input_policy=sy.ExactMatch(\n",
    "                    train_images=train_images_asset.pointer, test_images=test_images_asset.pointer,\n",
    "                    train_labels=train_labels_asset.pointer, test_labels=test_labels_asset.pointer),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def func_cnn(train_images, test_images, train_labels, test_labels):\n",
    "    import time\n",
    "    import numpy as np\n",
    "    from jax.scipy.special import logsumexp\n",
    "    from jax.experimental import optimizers, stax\n",
    "    from jax import random, jit, grad, vmap, value_and_grad\n",
    "    from jax.experimental.stax import (BatchNorm, Conv, Dense, Flatten,\n",
    "                                   Relu, LogSoftmax)\n",
    "    import ipdb\n",
    "\n",
    "    key = random.PRNGKey(1)\n",
    "\n",
    "    # Set the batch size\n",
    "    batch_size = 100\n",
    "\n",
    "    # Shuffle the training data\n",
    "    shuffle_indices = np.random.permutation(len(train_images))\n",
    "    train_data = train_images[shuffle_indices]\n",
    "    train_labels = train_labels[shuffle_indices]\n",
    "\n",
    "    # reshape the data into the 4-dimensional format expected by the convolutional layer\n",
    "    train_data = jnp.expand_dims(train_data.astype('float64'), axis=1)\n",
    "    test_data = jnp.expand_dims(test_images.astype('float64'), axis=1)\n",
    "\n",
    "    num_epochs = 10\n",
    "    num_classes = 10\n",
    "    \n",
    "    def one_hot(x, k, dtype=jnp.float32):\n",
    "            \"\"\"Create a one-hot encoding of x of size k \"\"\"\n",
    "            return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "    \n",
    "    @jit\n",
    "    def update(params, x, y, opt_state):\n",
    "            \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "            value, grads = value_and_grad(loss)(params, x, y)\n",
    "            opt_state = opt_update(0, grads, opt_state)\n",
    "            return get_params(opt_state), opt_state, value\n",
    "    \n",
    "    def accuracy(params, data, labels):\n",
    "        \"\"\" Compute the accuracy for the CNN case (no flattening of input)\"\"\"\n",
    "        acc_total = 0\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            data_batch = data[i:i+batch_size]\n",
    "            labels_batch = labels[i:i+batch_size]\n",
    "            images = jnp.array(data_batch)\n",
    "            targets = one_hot(jnp.array(labels_batch), num_classes)\n",
    "\n",
    "            target_class = jnp.argmax(targets, axis=1)\n",
    "            predicted_class = jnp.argmax(conv_net(params, images), axis=1)\n",
    "            acc_total += jnp.sum(predicted_class == target_class)\n",
    "        return acc_total/len(data)\n",
    "\n",
    "    def loss(params, images, targets):\n",
    "        preds = conv_net(params, images)\n",
    "        return -jnp.sum(preds * targets)\n",
    "    \n",
    "    def run_mnist_training_loop(num_epochs, opt_state, net_type=\"MLP\"):\n",
    "            \"\"\" Implements a learning loop over epochs. \"\"\"\n",
    "            # Initialize placeholder for loggin\n",
    "            log_acc_train, log_acc_test, train_loss = [], [], []\n",
    "\n",
    "            # Get the initial set of parameters\n",
    "            params = get_params(opt_state)\n",
    "\n",
    "            # Get initial accuracy after random init\n",
    "            train_acc = accuracy(params, train_data, train_labels)\n",
    "            test_acc = accuracy(params, test_data, test_labels)\n",
    "            log_acc_train.append(train_acc)\n",
    "            log_acc_test.append(test_acc)\n",
    "\n",
    "            # Loop over the training epochs\n",
    "            for epoch in range(num_epochs):\n",
    "                start_time = time.time()\n",
    "                \n",
    "                for i in range(0, len(train_data), batch_size):\n",
    "                    data = train_data[i:i+batch_size]\n",
    "                    target = train_labels[i:i+batch_size]\n",
    "                    if net_type == \"MLP\":\n",
    "                        # Flatten the image into 784 vectors for the MLP\n",
    "                        x = jnp.array(data).reshape(data.shape[0], 28*28)\n",
    "                    elif net_type == \"CNN\":\n",
    "                        # No flattening of the input required for the CNN\n",
    "                        x = jnp.array(data)\n",
    "                    y = one_hot(jnp.array(target), num_classes)\n",
    "                    params, opt_state, loss = update(params, x, y, opt_state)\n",
    "                    train_loss.append(loss)\n",
    "\n",
    "                epoch_time = time.time() - start_time\n",
    "                train_acc = accuracy(params, train_data, train_labels)\n",
    "                test_acc = accuracy(params, test_data, test_labels)\n",
    "                log_acc_train.append(train_acc)\n",
    "                log_acc_test.append(test_acc)\n",
    "                print(\"Epoch {} | T: {:0.2f} | Train A: {:0.3f} | Test A: {:0.3f}\".format(epoch+1, epoch_time,\n",
    "                                                                            train_acc, test_acc))\n",
    "\n",
    "            return train_loss, log_acc_train, log_acc_test\n",
    "  \n",
    "    init_fun, conv_net = stax.serial(Conv(32, (5, 5), (2, 2), padding=\"SAME\"),\n",
    "                                    BatchNorm(), Relu,\n",
    "                                    Conv(32, (5, 5), (2, 2), padding=\"SAME\"),\n",
    "                                    BatchNorm(), Relu,\n",
    "                                    Conv(10, (3, 3), (2, 2), padding=\"SAME\"),\n",
    "                                    BatchNorm(), Relu,\n",
    "                                    Conv(10, (3, 3), (2, 2), padding=\"SAME\"), Relu,\n",
    "                                    Flatten,\n",
    "                                    Dense(num_classes),\n",
    "                                    LogSoftmax)\n",
    "\n",
    "    _, params = init_fun(key, (batch_size, 1, 28, 28))\n",
    "\n",
    "    \n",
    "    step_size = 1e-3\n",
    "    opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "    opt_state = opt_init(params)\n",
    "    num_epochs = 10\n",
    "\n",
    "    train_loss, train_log, test_log = run_mnist_training_loop(num_epochs,\n",
    "                                                                opt_state,\n",
    "                                                                net_type=\"CNN\")\n",
    "\n",
    "\n",
    "# Plot the loss curve over time\n",
    "# from helpers import plot_mnist_performance\n",
    "# plot_mnist_performance(train_loss, train_log, test_log,\n",
    "#                        \"MNIST MLP Performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** (30000, 1, 28, 28)\n",
      "Epoch 1 | T: 7.29 | Train A: 0.956 | Test A: 0.968\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Test our function locally\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m func_cnn(train_images\u001b[39m=\u001b[39;49mtrain_images_mock, test_images\u001b[39m=\u001b[39;49mtest_images_mock,\n\u001b[1;32m      3\u001b[0m          train_labels\u001b[39m=\u001b[39;49mtrain_labels_mock, test_labels\u001b[39m=\u001b[39;49mtest_labels_mock)\n",
      "File \u001b[0;32m~/dev/openmined/PySyft/packages/syft/src/syft/service/code/user_code.py:396\u001b[0m, in \u001b[0;36mSubmitUserCode.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    394\u001b[0m         filtered_kwargs[k] \u001b[39m=\u001b[39m debox_asset(v)\n\u001b[0;32m--> 396\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocal_function(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfiltered_kwargs)\n\u001b[1;32m    397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[85], line 123\u001b[0m, in \u001b[0;36mfunc_cnn\u001b[0;34m(train_images, test_images, train_labels, test_labels)\u001b[0m\n\u001b[1;32m    120\u001b[0m opt_state \u001b[39m=\u001b[39m opt_init(params)\n\u001b[1;32m    121\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m--> 123\u001b[0m train_loss, train_log, test_log \u001b[39m=\u001b[39m run_mnist_training_loop(num_epochs,\n\u001b[1;32m    124\u001b[0m                                                             opt_state,\n\u001b[1;32m    125\u001b[0m                                                             net_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCNN\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[85], line 91\u001b[0m, in \u001b[0;36mfunc_cnn.<locals>.run_mnist_training_loop\u001b[0;34m(num_epochs, opt_state, net_type)\u001b[0m\n\u001b[1;32m     89\u001b[0m         x \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray(data)\n\u001b[1;32m     90\u001b[0m     y \u001b[39m=\u001b[39m one_hot(jnp\u001b[39m.\u001b[39marray(target), num_classes)\n\u001b[0;32m---> 91\u001b[0m     params, opt_state, loss \u001b[39m=\u001b[39m update(params, x, y, opt_state)\n\u001b[1;32m     92\u001b[0m     train_loss\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     94\u001b[0m epoch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/jax_1/lib/python3.10/site-packages/jax/example_libraries/optimizers.py:119\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(data, xs)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39m# The implementation here basically works by flattening pytrees. There are two\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m# levels of pytrees to think about: the pytree of params, which we can think of\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m# as defining an \"outer pytree\", and a pytree produced by applying init_fun to\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[39m# each leaf of the params pytree, which we can think of as the \"inner pytrees\".\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39m# Since pytrees can be flattened, that structure is isomorphic to a list of\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39m# lists (with no further nesting).\u001b[39;00m\n\u001b[1;32m    114\u001b[0m OptimizerState \u001b[39m=\u001b[39m namedtuple(\u001b[39m\"\u001b[39m\u001b[39mOptimizerState\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m                             [\u001b[39m\"\u001b[39m\u001b[39mpacked_state\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtree_def\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msubtree_defs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    116\u001b[0m register_pytree_node(\n\u001b[1;32m    117\u001b[0m     OptimizerState,\n\u001b[1;32m    118\u001b[0m     \u001b[39mlambda\u001b[39;00m xs: ((xs\u001b[39m.\u001b[39mpacked_state,), (xs\u001b[39m.\u001b[39mtree_def, xs\u001b[39m.\u001b[39msubtree_defs)),\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mlambda\u001b[39;00m data, xs: OptimizerState(xs[\u001b[39m0\u001b[39m], data[\u001b[39m0\u001b[39m], data[\u001b[39m1\u001b[39m]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m    122\u001b[0m Array \u001b[39m=\u001b[39m Any\n\u001b[1;32m    123\u001b[0m Params \u001b[39m=\u001b[39m Any  \u001b[39m# Parameters are arbitrary nests of `jnp.ndarrays`.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test our function locally\n",
    "func_cnn(train_images=train_images_mock, test_images=test_images_mock,\n",
    "         train_labels=train_labels_mock, test_labels=test_labels_mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
