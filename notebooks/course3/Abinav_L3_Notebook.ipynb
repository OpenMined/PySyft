{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "completed-wagner",
   "metadata": {},
   "source": [
    "## Data acquisition and preparation (Lesson 7)\n",
    "\n",
    "\n",
    "Welcome to Lesson 3 of Course 3. In this module, we will be looking at various steps in the data acquisition and data preparation stage.\n",
    "\n",
    "We will focus on a single source of dataset containing medical images (NIH Chest XRay Dataset) for a binary classification task, which is identifying whether there are abnormal findings in the x-rays.\n",
    "\n",
    "#### Data is the responsability of the data owner\n",
    "\n",
    "In a remote data science setup, the data scientist does not have access to the data to explore and prepare it and it becomes the responsability of the data owner to ensure a good quality of the dataset. This includes high quality annotations, using the right preprocessing steps in handling the dataset and adding differential privacy to preserve the data privacy.\n",
    "\n",
    "#### What you'll learn\n",
    "\n",
    "By the end of this notebook, the student will learn how to handle preprocessing on a medical dataset example, adding differential privacy to the data and loading the model in PyGrid to be used further. \n",
    "\n",
    "#### Instructors\n",
    "The instructors of this course are ------\n",
    "\n",
    "#### Milestones\n",
    "The main milestones of this lesson are:\n",
    "- Acquisition\n",
    "<!-- <p> Please see these external resources/examples ---Abinav's resources--- </p> -->\n",
    "- Quality check on dataset\n",
    "<!-- <p> Please see these external resources/examples ---Abinav's resources--- </p> -->\n",
    "- Annotation\n",
    "<!-- <p> Please see these external resources/examples ---Abinav's resources--- </p> -->\n",
    "- Converting data to PyGrid format\n",
    "\n",
    "- Linking data from multiple sources\n",
    "<!-- <p> Please see these external resources/examples ---Abinav's resources--- </p> -->\n",
    "\n",
    "- Adding Differential Privacy to the metadata of the dataset\n",
    "\n",
    "- Loading data into the node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09632b5e-6a1a-4d9f-811c-3cc9bc0a22e2",
   "metadata": {},
   "source": [
    "## Pre-requisites for the notebook\n",
    "\n",
    "To install the prerequisites for running the current notebook, it could be easier to setup a virtual environment using conda, by running the following: \n",
    "\n",
    "```\n",
    "conda create -n lab python=3.9\n",
    "\n",
    "conda activate lab\n",
    "```\n",
    "In your environment, syft can be installed by the following commands sequentially:\n",
    "```\n",
    "git clone https://github.com/OpenMined/PySyft && cd PySyft\n",
    "\n",
    "git fetch origin dev\n",
    "\n",
    "git checkout dev\n",
    "\n",
    "cd packages/syft && pip install -e .\n",
    "```\n",
    "For the other requirements, please run the following command:\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "A specific requirement for us today is a library called PyDicom, which helps up to read the medical images from the dataset and it is not part of PySyft. Therefore, it needs to be installed before proceeding as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287cb0f-6e42-429c-a09e-1773fbc968cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pydicom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d02bb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lesbian-bikini",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydicom import dcmread\n",
    "import numpy as np\n",
    "import torch\n",
    "from syft.core.adp.entity import DataSubject\n",
    "import os\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023fccd-bb03-4083-aa20-f9fef1b06798",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "We will make use of a data loading utility from PyTorch called DataLoader, which allows us to easily iterate over the dataset. More about it can be read <a href=\"https://pytorch.org/docs/stable/data.html\">here</a>.\n",
    "\n",
    "To achieve this, we define our own class that inherits the Dataset utility and requires us to define the following methods:\n",
    "- <i> \\_\\_init\\_\\_ </i> method which:\n",
    "    - loads into the memory the images, done by passing the path of the directory containg the images (<i> root_path </i>)\n",
    "    - parses any metadata about the images that can be found in <i> label_file </i>\n",
    "\n",
    "- <i> \\_\\_getitem_\\_ </i> method that offers support for fetching a data sample for a given key\n",
    "\n",
    "- <i> \\_\\_len_\\_</i> method (optionally) that returns the outer dimension of our dataset\n",
    "\n",
    "- <i> to_tensor</i> method that converts the data given as param (images) to a numeric format, preferably as a NumPy array, or as a Tensor.\n",
    "\n",
    "<b>Note: Need to add some images inline for easier user visualization/conception</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "nominated-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXRayDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Data loading utility for the NIH Chest X-Ray Dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_path, label_file):\n",
    "        # Reads the dataset from csv and sets the root path\n",
    "        self.dataset = pd.read_csv(label_file)\n",
    "        self.root = root_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the outer-most dimension of the dataset.\"\"\"\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def to_tensor(self, image):\n",
    "        \"\"\"Transform the image from a HWC-layout (height, width, channels) to the CHW-layout for PyTorch.\"\"\"\n",
    "        image_tensor = torch.from_numpy(np.ascontiguousarray(image.transpose((2, 0, 1))))\n",
    "        return image_tensor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Identify the file path of the current sample.\n",
    "        filename = self.dataset.loc[idx].at[\"patientId\"]\n",
    "        file_path = os.path.join(root_path, filename)\n",
    "        \n",
    "        label = self.dataset.loc[idx].at[\"Target\"]\n",
    "\n",
    "        # Load the image into memory.\n",
    "        dicom = dcmread(file_path)\n",
    "        image = dicom.pixel_array\n",
    "        \n",
    "        # Cast the tensor to integer to can be used by our ADP\n",
    "        image = self.to_tensor(image).int16() \n",
    "        # Convert tensor into a syft private tensor.\n",
    "        tensor_image = sy.Tensor(image).private(0, 255, entities=DataSubject(dicom.PatientID))\n",
    "        \n",
    "        return tensor_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fbdc12",
   "metadata": {},
   "source": [
    "Now we can instantiate the class above to initialize our dataset and inspect how a random sample looks like:\n",
    "\n",
    "<b> Note: Missing a root_path and a label file. Add here a sample picture after paths are added: maybe we download it from someplace?. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a45df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = ...\n",
    "LABEL_FILE = ...\n",
    "\n",
    "remote_dataset = ChestXRayDataset(ROOT_PATH, LABEL_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb79b73-5268-47ec-a90f-6f81f8c23d19",
   "metadata": {},
   "source": [
    "## Connect to PyGrid Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527f25b-25a9-4936-8c0e-698cbae95266",
   "metadata": {},
   "source": [
    "It is time to connect to a PyGrid Domain node and upload our dataset. \n",
    "\n",
    "In the following lesson, you will learn how to setup a Domain Node yourself and how to manage it once it is there as a data owner, so hold tight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a355bd3e-f2b0-4411-ab0b-4c7ca1727dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login into the already deployed Domain node\n",
    "\n",
    "# Please note that the credentials might change if you choose to use a different user or if the node is\n",
    "# deployed at a different port.\n",
    "remote = sy.login(\n",
    "    email=\"info@openmined.org\", \n",
    "    password=\"changethis\", \n",
    "    port=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9348138-29f5-4142-99d1-291bf37fcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the dataset we have created above and specify required fields. \n",
    "# Note the dataset is specifed via a dict in the 'assets' argument.\n",
    "\n",
    "remote.load_dataset(\n",
    "    assets = {\"chest_xray\" : remote_dataset},\n",
    "    name = \"chest xray from NIH\",\n",
    "    description = \"Chest xray dataset of NIH for pneumonia classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fdacfd",
   "metadata": {},
   "source": [
    "### Congrats, your dataset should be now deployed in a node!\n",
    "\n",
    "Let's go to the next lesson to learn about deploying and different ways to query the dataset once it is deployed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
