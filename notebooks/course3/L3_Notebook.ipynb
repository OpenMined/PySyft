{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "completed-wagner",
   "metadata": {},
   "source": [
    "## Lesson 7\n",
    "\n",
    "### Concept 1\n",
    "Welcome to Lesson 3 of course 3. In this module we will be looking at various steps in the data acquisition and data preparation stage, The aim of this course is to make the students familiar with various steps in the data processing and acquisition stage. By the end of the course (this notebook) the student will learn how to apply preprocessing on a medical dataset example, adding differential privacy to the data and loading the model in pygrid. \n",
    "\n",
    "### Concept 2\n",
    "The instructors of this course are ------\n",
    "\n",
    "### Concept 3\n",
    "In Remote data science the data scientist doesn't have access to the data and hence is unable to understand the data in complete detail and it becomes the responsibility of the data owner to ensure good quality of the dataset. By Quality of the dataset we are referring to quality of annotations, the preprocessing steps in handling the dataset adding differential privacy to make the data more private. \n",
    "\n",
    "### Concept 4\n",
    "The big milestones of this lesson are\n",
    "- Acquisition\n",
    "<!-- <p> Please see these external resources/examples ---Abinav's resources--- </p> -->\n",
    "- Quality check on dataset\n",
    "<!-- <p> Please see these external resources/examples ---Abinav's resources--- </p> -->\n",
    "\n",
    "- Annotation\n",
    "<!-- <p> Please see these external resources/examples ---Abinav's resources--- </p> -->\n",
    "\n",
    "- Converting data to pygrid format\n",
    "\n",
    "- Linking data from multiple sources\n",
    "<!-- <p> Please see these external resources/examples ---Abinav's resources--- </p> -->\n",
    "\n",
    "- Adding Differential Privacy to the metadata of the dataset\n",
    "\n",
    "- Loading data into the node\n",
    "\n",
    "This notebook will focus on these milestones for a single source of dataset for medical images that has binary classification labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09632b5e-6a1a-4d9f-811c-3cc9bc0a22e2",
   "metadata": {},
   "source": [
    "## Pre-Requisites for the notebook\n",
    "\n",
    "It would be easier to setup a virtual environment which can be used to install \n",
    "\n",
    "```\n",
    "conda create -n lab python=3.9\n",
    "```\n",
    "After creating the environment syft can be installed by the following commands sequentially\n",
    "```\n",
    "git clone https://github.com/OpenMined/PySyft && cd PySyft\n",
    "\n",
    "git fetch origin dev\n",
    "\n",
    "git checkout dev\n",
    "\n",
    "cd packages/syft && pip install -e .\n",
    "```\n",
    "Then install other requirements by running this command\n",
    "\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4282b02f-d4aa-4364-a7e4-8603bb80e695",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "For this notebook, we will be using a library called PyDicom, which will help us read the medical images from our dataset. It isn't included in PySyft by default, so let's install it before proceeding! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d287cb0f-6e42-429c-a09e-1773fbc968cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydicom in /home/e/anaconda3/envs/Hagrid/lib/python3.9/site-packages (2.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lesbian-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pydicom import dcmread\n",
    "import numpy as np\n",
    "import torch\n",
    "from syft.core.adp.entity import Entity\n",
    "import os\n",
    "import syft as sy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a023fccd-bb03-4083-aa20-f9fef1b06798",
   "metadata": {},
   "source": [
    "## Dataset class\n",
    "\n",
    "We need a dataset class that needs to be sent and can be used as a dataloader.\n",
    "\n",
    "This dataset class needs to contain the following:\n",
    "- an <i> __init__ </i> method that collects all the images (here this is done by passing the directory the images are in to <i> root_path </i>), as well as parses any metadata about the images (done here with <i> label_file </i>\n",
    "- a method that converts the data to an integer format, preferably as a NumPy array, or as a Tensor.\n",
    "- a <i> __getitem__ </i> method\n",
    "\n",
    "Note: Need to add some images inline for easier user visualization/conception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nominated-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,root_path,label_file):\n",
    "        self.dataset = pd.read_csv(label_file)\n",
    "        self.root = root_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "    \n",
    "    def to_tensor(self,image):\n",
    "        img = torch.from_numpy(np.ascontiguousarray(image.transpose((2, 0, 1))))\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        filename = self.dataset.loc[idx].at[\"patientId\"]\n",
    "        file_path = os.path.join(root_path, filename)\n",
    "        label = self.dataset.loc[idx].at[\"Target\"]\n",
    "        dicom = dcmread(file_path)\n",
    "        image = dicom.pixel_array\n",
    "        ## Our adp can handle only integer tensors, so cast the tensor to be int\n",
    "        image = self.to_tensor(image).int16() \n",
    "        tensor_image = sy.Tensor(image).private(0,255,entities=Entity(dicom.PatientID))\n",
    "        return tensor_image,label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb79b73-5268-47ec-a90f-6f81f8c23d19",
   "metadata": {},
   "source": [
    "## Connect to Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527f25b-25a9-4936-8c0e-698cbae95266",
   "metadata": {},
   "source": [
    "Now that we've created our dataloader class, we're going to connect to the domain node and upload our dataset. We'll be showing you how to setup a Domain Node in the very next lesson, so hold tight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a355bd3e-f2b0-4411-ab0b-4c7ca1727dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to http://localhost:8081... done! \t Logging into adp... done!\n"
     ]
    }
   ],
   "source": [
    "# Let's login into the domain\n",
    "remote = sy.login(email=\"info@openmined.org\", password=\"changethis\", port=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "551655e6-2850-4c61-9bf4-42e00af2ece4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'root_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_167502/1941161767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mremote_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'root_path' is not defined"
     ]
    }
   ],
   "source": [
    "remote_dataset = dataset(root_path,label_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9348138-29f5-4142-99d1-291bf37fcabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote.load_dataset(\n",
    "    assets = {\"chest_xray\":remote_dataset},\n",
    "    name = \"chest xray from NIH\",\n",
    "    description = \"Chest xray dataset of NIH for pneumonia classification\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
