{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# HOW TO AUDIT AN AI MODEL OWNED BY SOMEONE ELSE (PART 1 - USER LOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this tutorial, we show how external parties can audit internal AI systems without accessing them â€” mitigating privacy, security, and IP costs and risks. **This tutorial uses syft 0.8.2.b0, with a datasite setup that does not use networking, to run the tutorial with networking read more in section 1.1.1**\n",
    "\n",
    "You can read more about this tutorial and the follow up tutorials here on the [blog post](https://blog.openmined.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Model Owner Launches Stage 1 Audit Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "**Note** : Kindly use light theme when running the demo for better visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# install syft\n",
    "SYFT_VERSION = \">=0.9,<1.0.0\"\n",
    "package_string = f'\"syft{SYFT_VERSION}\"'\n",
    "%pip install {package_string} -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party\n",
    "import pandas as pd\n",
    "\n",
    "# syft absolute\n",
    "import syft as sy\n",
    "\n",
    "sy.requires(SYFT_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Launch PySyft datasite server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "To start we launch a `PySyft` datasite server. This is the backend that stores the private data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "server = sy.orchestra.launch(name=\"syft-datasite\", reset=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "There are 3 ways to launch a `PySyft` datasite\n",
    "\n",
    "**A) From a notebook, with simulated networking \\*\\*THIS NOTEBOOK\\*\\***\n",
    "  - Apart from the network calls, this uses exactly the same code as other setups\n",
    "  - run orchestra **without a port**: `sy.orchestra.launch(name=\"syft-datasite\")`\n",
    "  \n",
    "**B) From a notebook with networking (also supports docker)**\n",
    "  - This spawns a separate process that starts a uvicorn webserver\n",
    "  - run orchestra **with a port**:`sy.orchestra.launch(name=\"syft-datasite\", port=8080)`\n",
    "  \n",
    "**C) From the command line (supports docker/kubernetes)**\n",
    "  - setup for production\n",
    "  - run `syft launch`  from the terminal\n",
    "  \n",
    "  \n",
    "We are using the **A)** here, as it is the only option available using google colab, switching to a real webserver is as easy as running this notebook in jupyter locally and adding a port. Read more about deployment on our [README.md](https://github.com/OpenMined/PySyft) and other setups for syft [here](https://github.com/OpenMined/PySyft/tree/dev/notebooks/tutorials/data-engineer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Login\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "We can now login to our datasite using the default admin credentials. In production we would change these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mo_client = server.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Configure server to allow user registration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "For this tutorial we allow other users to create their own account. New accounts will get limited permissions and will only be able to see the mock version of any datasets we upload to the datasite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_client.settings.allow_guest_signup(enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Model Owner Uploads What will be Audited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We are ready to create a dataset. Our dataset consists of prompts that were used as input for our language model, and their corresponding continuations. For example, in the first row we see that the `prompt` for the model was *\"Jacob Zachar is an American actor whose\"*, and the `result` was \"*erythemal body image makes him look like an infant in the bedroom.*\"  We also have a mock version of the same dataset. The mock dataframe contains no meaningful data, but it has the same columns, size and datatypes as the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = \"https://github.com/OpenMined/datasets/raw/main/AuditingBlogpost\"\n",
    "model_log = pd.read_csv(f\"{dataset_url}/gpt2_100row.csv\")\n",
    "mock_model_log = pd.read_csv(f\"{dataset_url}/gpt2_100row_mock.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_model_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "To upload our dataset to the datasite we need to wrap it in a `Syft Dataset` object. We can add some metadata to the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_contributor = sy.Contributor(\n",
    "    name=\"Jeffrey Salazar\", role=\"Dataset Creator\", email=\"jsala@ailab.com\"\n",
    ")\n",
    "\n",
    "gpt2_user_log = sy.Dataset(\n",
    "    name=\"GPT-2 Activity Log\",\n",
    "    description=\"User interactions from GPT-2 usage in text completion.\",\n",
    "    contributors=[main_contributor],\n",
    "    asset_list=[\n",
    "        sy.Asset(\n",
    "            name=\"gpt2-mar23-prompts-responses\",\n",
    "            description=\"Text prompts and corresponding model predictions from GPT-2 (March 2023)\",\n",
    "            contributors=[main_contributor],\n",
    "            data=model_log,\n",
    "            mock=mock_model_log,\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_client.upload_dataset(gpt2_user_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "This was the bulk of the work for the Model owner, its the auditors turn now to propose a project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Auditor Creates Account and Proposes Project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "We first create an account and login."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auditor_client = server.register(\n",
    "    name=\"Peter Jones\",\n",
    "    email=\"pjones@aisb.org\",\n",
    "    password=\"password1234\",\n",
    "    password_verify=\"password1234\",\n",
    ")\n",
    "auditor_client = server.login(email=\"pjones@aisb.org\", password=\"password1234\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Our account has limited permissions, but we are able to access the mock part of the dataset to code against. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = auditor_client.datasets[0]\n",
    "asset = dataset.assets[0]\n",
    "asset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock = asset.mock\n",
    "mock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "We can now create a `Syft Project` which will act as a wrapper for all the requests on this `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_project = sy.Project(\n",
    "    name=\"Model Output Audit\",\n",
    "    description=\"Auditing GPT2 model outputs for toxicity, bias, etc.\",\n",
    "    members=[auditor_client],\n",
    ")\n",
    "audit_project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Before we submit our actual audit code, we need to write the code. Writing code without input is often quite challenging and therefore we use the mock to write our code. Once we verified that everything works and we have no errors, we can submit the code for approval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party\n",
    "import evaluate\n",
    "\n",
    "toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
    "indices, inputs = mock.id.tolist(), mock[\"result\"].tolist()\n",
    "toxicity_results = toxicity.compute(predictions=inputs)\n",
    "mock_result = pd.DataFrame(\n",
    "    toxicity_results[\"toxicity\"], index=indices, columns=[\"toxicity\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "With that set up, we are ready to write the code that we want to execute on the dataset. We do this by writing a function and wrapping that function with a `@sy.syft_function` decorator, this particular decorator requests that we can run this function exactly once on the dataset that was just uploaded. Within the function we compute and return the toxicity scores for the results of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "@sy.syft_function_single_use(data=dataset.assets[0])\n",
    "def model_output_analysis(data):\n",
    "    \"\"\"\n",
    "    Evaluate the model's quantify the toxicity of the input texts using the R4 Target Model,\n",
    "    a pretrained hate speech classification model\n",
    "    Evaluate the model's estimated language polarity towards and social perceptions of a demographic\n",
    "    (e.g. gender, race, sexual orientation).\n",
    "    \"\"\"\n",
    "    # third party\n",
    "    import evaluate\n",
    "    import pandas as pd\n",
    "\n",
    "    toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
    "    indices, inputs = data.id.tolist(), data[\"result\"].tolist()\n",
    "    toxicity_results = toxicity.compute(predictions=inputs)\n",
    "    return pd.DataFrame(\n",
    "        toxicity_results[\"toxicity\"], index=indices, columns=[\"toxicity\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "We can now request code execution of our function by calling the `.create_code_request` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_project.create_code_request(model_output_analysis, auditor_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "We can inspect our code submission, which means we now have to wait for approval from the model owner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "auditor_client.code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "As a last step we start out project, and we switch back to the perspective of the model owner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = audit_project.send()\n",
    "project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Model Owner Reviews Proposed Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Now that the model owner has a new incoming request, the goal is to approve or deny the request based on the code. This may include running the code on mock data first or asking questions to the auditor. In our case we will simply review the code and approve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_client.projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Lets view the newly created project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = mo_client.projects[0]\n",
    "project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "And now view the corresponding request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = mo_client.requests[0]\n",
    "request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "We can view the code to review it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "request.code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Once the model owner feels confident that this code is not malicious, we can run the function on the real data to inspect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = mo_client.datasets[0].assets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_result = request.code.run(data=asset.data)\n",
    "real_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "If everything looks good, we can approve the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "request.approve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## Auditor computes Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "asset = auditor_client.datasets[0].assets[0]\n",
    "result = auditor_client.code.model_output_analysis(data=asset).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "ðŸ‘ Tutorial Complete, you can read more about PySyft on the accompanying [blog post](https://blog.openmined.org/) or on our GitHub [README.md](https://github.com/OpenMined/pysyft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "Share this Colab Notebook:<br />\n",
    "<a href=\"http://colab.research.google.com/github/OpenMined/PySyft/blob/dev/notebooks/tutorials/model-auditing/colab/01-user-log.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" />\n",
    "</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
