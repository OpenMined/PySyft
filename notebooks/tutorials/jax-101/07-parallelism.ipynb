{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JAX 101 - 07 Parallel Evaluation\n",
    "Link to the original JAX tutorial: https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0 - Data Owner Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import syft as sy\n",
    "sy.requires(\">=0.8,<0.9\")\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the domain\n",
    "node = sy.orchestra.launch(name=\"test-domain-1\", reset=True, dev_mode=True)\n",
    "data_owner_client = node.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Data Scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register a client to the domain\n",
    "node = sy.orchestra.launch(name=\"test-domain-1\")\n",
    "data_scientist_client = node.client\n",
    "data_scientist_client.register(name=\"Jane Doe\", email=\"jane@caltech.edu\", password=\"abc123\", institution=\"Caltech\", website=\"https://www.caltech.edu/\")\n",
    "data_scientist_client.login(email=\"jane@caltech.edu\", password=\"abc123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for code execution\n",
    "# ATTENTION: ALL LIBRARIES USED SHOULD BE DEFINED INSIDE THE FUNCTION CONTEXT!!!\n",
    "\n",
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def basics():\n",
    "    import jax\n",
    "    import numpy as np\n",
    "    import jax.numpy as jnp\n",
    "\n",
    "    x = np.arange(5)\n",
    "    w = np.array([2., 3., 4.])\n",
    "\n",
    "    def convolve(x, w):\n",
    "        output = []\n",
    "        for i in range(1, len(x)-1):\n",
    "            output.append(jnp.dot(x[i-1:i+2], w))\n",
    "        return jnp.array(output)\n",
    "\n",
    "    convolve(x, w)\n",
    "    \n",
    "    n_devices = jax.local_device_count() \n",
    "    xs = np.arange(5 * n_devices).reshape(-1, 5)\n",
    "    ws = np.stack([w] * n_devices)\n",
    "\n",
    "    xs\n",
    "    ws\n",
    "    jax.vmap(convolve)(xs, ws)\n",
    "    jax.pmap(convolve)(xs, ws)\n",
    "    jax.pmap(convolve)(xs, jax.pmap(convolve)(xs, ws))\n",
    "    jax.pmap(convolve, in_axes=(0, None))(xs, w)\n",
    "    \n",
    "    def normalized_convolution(x, w):\n",
    "        output = []\n",
    "        for i in range(1, len(x)-1):\n",
    "            output.append(jnp.dot(x[i-1:i+2], w))\n",
    "        output = jnp.array(output)\n",
    "        return output / jax.lax.psum(output, axis_name='p')\n",
    "\n",
    "    jax.pmap(normalized_convolution, axis_name='p')(xs, ws)\n",
    "    jax.vmap(normalized_convolution, axis_name='p')(xs, ws)\n",
    "\n",
    "@sy.syft_function(input_policy=sy.ExactMatch(),\n",
    "                  output_policy=sy.SingleExecutionExactOutput())\n",
    "def complex_example():\n",
    "    from typing import NamedTuple, Tuple\n",
    "    import functools\n",
    "    import jax\n",
    "    import jax.numpy as jnp\n",
    "    import numpy as np\n",
    "\n",
    "    class Params(NamedTuple):\n",
    "        weight: jnp.ndarray\n",
    "        bias: jnp.ndarray\n",
    "\n",
    "\n",
    "    def init(rng) -> Params:\n",
    "        \"\"\"Returns the initial model params.\"\"\"\n",
    "        weights_key, bias_key = jax.random.split(rng)\n",
    "        weight = jax.random.normal(weights_key, ())\n",
    "        bias = jax.random.normal(bias_key, ())\n",
    "        return Params(weight, bias)\n",
    "\n",
    "\n",
    "    def loss_fn(params: Params, xs: jnp.ndarray, ys: jnp.ndarray) -> jnp.ndarray:\n",
    "        \"\"\"Computes the least squares error of the model's predictions on x against y.\"\"\"\n",
    "        pred = params.weight * xs + params.bias\n",
    "        return jnp.mean((pred - ys) ** 2)\n",
    "\n",
    "    LEARNING_RATE = 0.005\n",
    "\n",
    "    # So far, the code is identical to the single-device case. Here's what's new:\n",
    "\n",
    "\n",
    "    # Remember that the `axis_name` is just an arbitrary string label used\n",
    "    # to later tell `jax.lax.pmean` which axis to reduce over. Here, we call it\n",
    "    # 'num_devices', but could have used anything, so long as `pmean` used the same.\n",
    "    @functools.partial(jax.pmap, axis_name='num_devices')\n",
    "    def update(params: Params, xs: jnp.ndarray, ys: jnp.ndarray) -> Tuple[Params, jnp.ndarray]:\n",
    "        \"\"\"Performs one SGD update step on params using the given data.\"\"\"\n",
    "\n",
    "        # Compute the gradients on the given minibatch (individually on each device).\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, xs, ys)\n",
    "\n",
    "        # Combine the gradient across all devices (by taking their mean).\n",
    "        grads = jax.lax.pmean(grads, axis_name='num_devices')\n",
    "\n",
    "        # Also combine the loss. Unnecessary for the update, but useful for logging.\n",
    "        loss = jax.lax.pmean(loss, axis_name='num_devices')\n",
    "\n",
    "        # Each device performs its own update, but since we start with the same params\n",
    "        # and synchronise gradients, the params stay in sync.\n",
    "        new_params = jax.tree_map(\n",
    "            lambda param, g: param - g * LEARNING_RATE, params, grads)\n",
    "\n",
    "        return new_params, loss\n",
    "    \n",
    "    # Generate true data from y = w*x + b + noise\n",
    "    true_w, true_b = 2, -1\n",
    "    xs = np.random.normal(size=(128, 1))\n",
    "    noise = 0.5 * np.random.normal(size=(128, 1))\n",
    "    ys = xs * true_w + true_b + noise\n",
    "\n",
    "    # Initialise parameters and replicate across devices.\n",
    "    params = init(jax.random.PRNGKey(123))\n",
    "    n_devices = jax.local_device_count()\n",
    "    replicated_params = jax.tree_map(lambda x: jnp.array([x] * n_devices), params)\n",
    "    \n",
    "    type(replicated_params.weight)\n",
    "    \n",
    "    def split(arr):\n",
    "        \"\"\"Splits the first axis of `arr` evenly across the number of devices.\"\"\"\n",
    "        return arr.reshape(n_devices, arr.shape[0] // n_devices, *arr.shape[1:])\n",
    "\n",
    "    # Reshape xs and ys for the pmapped `update()`.\n",
    "    x_split = split(xs)\n",
    "    y_split = split(ys)\n",
    "\n",
    "    type(x_split)\n",
    "        \n",
    "    def type_after_update(name, obj):\n",
    "        print(f\"after first `update()`, `{name}` is a\", type(obj))\n",
    "\n",
    "    # Actual training loop.\n",
    "    for i in range(1000):\n",
    "\n",
    "        # This is where the params and data gets communicated to devices:\n",
    "        replicated_params, loss = update(replicated_params, x_split, y_split)\n",
    "\n",
    "        # The returned `replicated_params` and `loss` are now both ShardedDeviceArrays,\n",
    "        # indicating that they're on the devices.\n",
    "        # `x_split`, of course, remains a NumPy array on the host.\n",
    "        if i == 0:\n",
    "            type_after_update('replicated_params.weight', replicated_params.weight)\n",
    "            type_after_update('loss', loss)\n",
    "            type_after_update('x_split', x_split)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            # Note that loss is actually an array of shape [num_devices], with identical\n",
    "            # entries, because each device returns its copy of the loss.\n",
    "            # So, we take the first element to print it.\n",
    "            print(f\"Step {i:3d}, loss: {loss[0]:.3f}\")\n",
    "\n",
    "\n",
    "    # Plot results.\n",
    "\n",
    "    # Like the loss, the leaves of params have an extra leading dimension,\n",
    "    # so we take the params from the first device.\n",
    "    params = jax.device_get(jax.tree_map(lambda x: x[0], replicated_params))\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.scatter(xs, ys)\n",
    "    plt.plot(xs, params.weight * xs + params.bias, c='red', label='Model Prediction')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our function locally \n",
    "basics()\n",
    "complex_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the function for code execution\n",
    "data_scientist_client.api.services.code.request_code_execution(basics)\n",
    "data_scientist_client.api.services.code.request_code_execution(complex_example)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Data Owner Reviewing and Approving Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_owner_client = node.login(email=\"info@openmined.org\", password=\"changethis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get messages from domain\n",
    "messages = data_owner_client.api.services.messages.get_all()\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import review_request, run_submitted_function, accept_request\n",
    "\n",
    "for message in messages:\n",
    "    review_request(message)\n",
    "    real_result = run_submitted_function(message)\n",
    "    accept_request(message, real_result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Downloading the Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tutorial complete üëè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = data_scientist_client.api.services.code.basics()\n",
    "assert not isinstance(result, sy.SyftError)\n",
    "\n",
    "result = data_scientist_client.api.services.code.complex_example()\n",
    "assert not isinstance(result, sy.SyftError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if node.node_type.value == \"python\":\n",
    "    node.land()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
